[
    {
        "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
        "url": "http://arxiv.org/abs/2509.06902v1",
        "pub_date": "2025-09-08",
        "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \\emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \\emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \\emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.",
        "translated": "大型语言模型（LLMs）作为随机性系统可能生成偏离可用数据的数值，这种错误被称为\"数值幻觉\"。现有防护机制——检索增强生成、引用和不确定性估计——虽能提升透明度，但无法保证数值保真度：虚构或误引的数值仍可能以正确形式呈现。我们提出**可验证数值载体（PCN）**，这是一种通过机械验证确保数值保真度的呈现层协议。在PCN框架下，数值片段以与结构化声明绑定的\"声明约束令牌\"形式输出，验证器根据声明策略（如精确等价、舍入规则、别名系统或带限定条件的容差范围）检查每个令牌。关键创新在于PCN将验证环节置于渲染器而非模型中：只有通过声明验证的数值会被标记为已验证，其余数值默认处于未验证状态。这种分离设计有效防止欺骗行为，并确保故障封闭特性。我们形式化定义了PCN协议，并证明其具备可靠性、诚实令牌下的完备性、故障封闭特性以及策略优化下的单调性。PCN具有轻量化和模型无关特性，可无缝集成至现有应用系统，并能通过密码学承诺进行功能扩展。通过将验证作为数值显示前的强制步骤，PCN为数值敏感场景建立了简明契约：唯有通过验证才能获得信任，而缺乏验证标识则传递不确定性信息。"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们推出了一款名为\"智能快速投放\"(Smart Fast Finish, SFF)的预算调控功能。该功能基于行业标准的快速投放(FF)技术进行优化——传统FF系统会在固定投放周期临近结束时，以最快速度耗尽剩余广告预算。SFF通过分析历史广告活动数据，动态调整系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分割实验数据和离线模拟结果表明，SFF在预算调控过程中能有效缓解超量投放问题，是一种稳健的解决方案。\n\n（注：根据技术文档翻译规范，对以下术语作了标准化处理：\n1. \"budget pacing\"译为\"预算调控\"而非字面意义的\"预算步调\"\n2. \"overdelivery mitigation\"译为\"缓解超量投放\"符合广告技术领域表述\n3. \"throttle rate\"译为\"调控速率\"准确体现系统参数特性\n4. 保留DoorDash等专有名词原文，符合技术文献惯例）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关系，从而阻碍战略决策。为突破这些局限，我们提出KLIPA框架——一种结合知识图谱与大语言模型（LLM）的创新专利分析系统。该框架集成三大核心组件：用于构建专利间显性关系图谱的结构化知识库，揭示上下文关联的检索增强生成（RAG）系统，以及能动态确定最优查询策略的智能代理。我们在真实世界专利数据库上进行验证，结果表明KLIPA在知识提取、新颖关联发现和整体运营效率方面实现显著提升。该技术组合不仅提高了检索精度，降低了对领域专家的依赖，更为科技企业、律所等知识产权管理机构提供了可扩展的自动化解决方案，助力其更好地驾驭战略创新与竞争情报的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v1",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "科学文献的结构化信息提取对于捕捉专业领域的核心概念与新兴趋势至关重要。尽管现有数据集有助于模型开发，但由于领域复杂性和科学文本标注的高成本，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专门针对自然语言处理（NLP）领域全文实体与关系抽取的基准数据集。该数据集包含60篇人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行了对比实验，并评估了前沿监督模型在该数据集上的表现。实验结果表明，现有模型对不同长度学术文本的提取能力存在差异。与现有数据集的交叉对比显示，SciNLP在某些基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点度，表明其蕴含丰富的语义拓扑信息，可有效增强下游应用。数据集已公开于https://github.com/AKADDC/SciNLP。"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）系统需应对简短模糊的用户查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配问题的核心机制，其设计范式因预训练语言模型（PLM）和大语言模型（LLM）的出现发生了显著变革。本综述从三个维度系统梳理该领域：（i）建立查询扩展的四维框架——从扩展点注入方式（显式与隐式QE），经知识 grounding 与交互机制（知识库、模型内部能力、多轮检索）和学习对齐方法，延伸至基于知识图谱的论证；（ii）提出以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释各类模型在QE中的特色能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE技术在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与实施方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域问答/RAG、会话式检索、代码搜索及跨语言场景的应用图谱。研究提炼出知识 grounding 与交互、对齐/蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束三大核心策略，作为解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系以及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下选择和组合QE技术提供了系统化蓝图。"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文本文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂的结构特征要求研究者开发超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大发展时期的方法体系、核心挑战与应用场景。我们系统化地追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层级编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，本文还审视了特定领域应用场景、专项评估资源，并指明了效率权衡、多模态对齐和结果真实性等关键开放挑战。本综述旨在为基座模型时代的长文档检索研究既提供 consolidated 参考框架，又提出前瞻性的发展议程。"
    },
    {
        "title": "CAViAR: Critic-Augmented Video Agentic Reasoning",
        "url": "http://arxiv.org/abs/2509.07680v1",
        "pub_date": "2025-09-09",
        "summary": "Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.",
        "translated": "近年来，视频理解领域取得显著进展，模型在短片段感知任务上的性能持续提升。然而在LVBench、Neptune和ActivityNet-RTL等最新基准测试中，随着查询指令复杂度增加和视频时长增长，需要复杂推理的视频任务性能出现明显下滑。本研究旨在探索：能否利用现有感知能力成功执行更复杂的视频推理？我们开发了一个配备视频处理模块作为子代理或工具的大型语言模型代理系统。与Visual Programming、ViperGPT和MoReVQA等前人工作中固定流程的查询处理方式不同，该代理通过分析每个模块调用的结果动态决定后续执行步骤。受文本推理领域研究启发，我们引入评估器机制来区分代理执行序列的成功与失败案例。实验表明，代理系统与评估器的组合在所述数据集上实现了强劲性能。"
    },
    {
        "title": "Visual Representation Alignment for Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2509.07979v1",
        "pub_date": "2025-09-09",
        "summary": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.",
        "translated": "通过视觉指令调优训练的多模态大语言模型（MLLMs）已在多样任务中展现出强大性能，但在以视觉为中心的任务（如目标计数或空间推理）中仍存在局限。我们将此不足归因于当前主流的纯文本监督范式——该范式仅对视觉通路提供间接指导，往往导致MLLMs在训练过程中丢失细粒度视觉细节。本文提出视觉表征对齐方法（VIRAL），这是一种简单而有效的正则化策略，可将MLLMs的内部视觉表征与预训练视觉基础模型（VFMs）的表征进行对齐。通过显式实施这种对齐，VIRAL不仅使模型能够保留来自输入视觉编码器的关键视觉细节，还能补充来自VFMs的额外视觉知识，从而增强其处理复杂视觉输入的推理能力。实验结果表明，该方法在广泛采用的多模态基准测试的所有任务中均取得持续改进。此外，我们通过全面的消融研究验证了框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。"
    },
    {
        "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation",
        "url": "http://arxiv.org/abs/2509.07978v1",
        "pub_date": "2025-09-09",
        "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: https://gzwsama.github.io/OnePoseviaGen.github.io/",
        "translated": "从单一参考图像准确估计未知物体的六维姿态，对于在现实长尾场景中运行的机器人技术至关重要。然而该设定存在显著挑战：三维模型通常难以获取，单视角重建缺乏公制尺度，生成模型与真实图像间的领域差异会削弱系统鲁棒性。我们提出OnePoseViaGen解决方案，通过两大核心组件应对这些挑战：首先，粗到精对齐模块通过结合多视角特征匹配与渲染比较优化，联合优化尺度与姿态估计；其次，文本引导生成式领域随机化策略通过多样化纹理，实现合成数据对姿态估计器的有效微调。这些技术使高保真单视角三维生成能够支撑可靠的单次六维姿态估计。在YCBInEOAT、Toyota-Light和LM-O等挑战性基准测试中，OnePoseViaGen以显著优势超越现有方法，达到最先进性能。我们进一步通过真实机器人手完成灵巧抓取实验，验证了该方法在现实操作中的实用性。项目页面：https://gzwsama.github.io/OnePoseviaGen.github.io/"
    },
    {
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
        "url": "http://arxiv.org/abs/2509.07969v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
        "translated": "近年来，大型多模态模型通过将基于图像的工具与强化学习相结合，在解决视觉问题方面取得显著进展。然而，现有开源方法通常存在推理模式单一、交互轮次有限的问题，难以胜任需要试错探索的复杂任务。本研究通过扩展基于工具的交互规模解决了这一局限性，提出Mini-o3系统——该系统能够执行深度多轮推理（可达数十个步骤），并在具有挑战性的视觉搜索任务中实现最先进的性能。\n\n我们重现OpenAI o3风格行为的方案包含三个核心组成部分：首先，构建了包含数千个挑战性视觉搜索问题的视觉探测数据集，专门用于探索性推理研究；其次，开发了迭代式数据收集流水线，获取呈现多样化推理模式（包括深度优先搜索、试错法和目标维持）的冷启动轨迹；第三，提出超轮次掩码策略，在强化学习过程中避免对达到最大交互轮次的响应进行惩罚，从而平衡训练效率与测试时的扩展性。尽管训练时仅设定六轮交互的上限，我们的模型在推理阶段能自然生成数十轮交互轨迹，且准确率随轮次增加持续提升。大量实验表明，Mini-o3能产生丰富的推理模式和深度思考路径，有效解决复杂视觉搜索问题。"
    },
    {
        "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
        "url": "http://arxiv.org/abs/2509.07966v1",
        "pub_date": "2025-09-09",
        "summary": "Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.",
        "translated": "对表格等结构化数据的视觉推理是现代视觉语言模型（VLM）的核心能力，然而当前基准测试在规模、多样性和推理深度方面仍存在局限——尤其在处理渲染表格图像时更为明显。为填补这一空白，我们推出Visual-TableQA：一个专为评估和提升复杂表格数据视觉推理能力而构建的大规模开放域多模态数据集。我们的生成流程采用模块化、可扩展的全自动架构，通过多推理大语言模型协同完成生成、验证与启发三类角色。该数据集包含2,500张结构丰富的LaTeX渲染表格和6,000个推理密集型问答对，总成本控制在100美元以内。为提升多样性与创造性，我们通过跨模型提示（\"启发机制\"）和LLM陪审团过滤实现多模型协同数据生成——强模型负责生成布局与主题框架，弱模型进行细节扩展，共同将多样化的推理模式和视觉结构蒸馏到数据集中。实验表明，基于Visual-TableQA微调的模型能稳健泛化至外部基准测试，尽管数据为合成生成，其表现仍超越多个商用模型。完整流程与资源已开源：https://github.com/AI-4-Everyone/Visual-TableQA。"
    },
    {
        "title": "Feature Space Analysis by Guided Diffusion Model",
        "url": "http://arxiv.org/abs/2509.07936v1",
        "pub_date": "2025-09-09",
        "summary": "One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.",
        "translated": "深度神经网络（DNN）的核心问题之一在于其内部特征提取过程的黑箱特性。针对视觉相关领域，本文通过提出一种新型解码器，重点分析DNN的特征空间，该解码器能够生成保证其特征与用户指定特征高度匹配的图像。相较于以往研究中缺失的这种保证机制，我们的解码器通过生成特征与目标特征邻近的图像，可实证揭示图像中哪些属性被DNN编码到特定特征中。该解码器采用引导式扩散模型实现，通过引导预训练扩散模型的反向图像生成过程，逐步最小化每步生成的清晰图像特征与用户指定特征之间的欧氏距离。我们的解码器具有一项实用优势：无需额外训练即可分析不同DNN的特征空间，且仅需在单块商用GPU上运行。针对CLIP图像编码器、ResNet-50和视觉变换器的实验结果表明，解码器生成的图像特征与用户指定特征具有显著相似性，为理解这些DNN的特征空间提供了有价值的见解。"
    },
    {
        "title": "Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space\n  Object",
        "url": "http://arxiv.org/abs/2509.07932v1",
        "pub_date": "2025-09-09",
        "summary": "Characterization of uncooperative Resident Space Objects (RSO) play a crucial role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to assess the geometry and motion properties. To address the challenges of reconstructing tumbling uncooperative targets, this study evaluates the performance of existing state-of-the-art 3D reconstruction algorithms for dynamic scenes, focusing on their ability to generate geometrically accurate models with high-fidelity. To support our evaluation, we developed a simulation environment using Isaac Sim to generate physics-accurate 2D image sequences of tumbling satellite under realistic orbital lighting conditions. Our preliminary results on static scenes using Neuralangelo demonstrate promising reconstruction quality. The generated 3D meshes closely match the original CAD models with minimal errors and artifacts when compared using Cloud Compare (CC). The reconstructed models were able to capture critical fine details for mission planning. This provides a baseline for our ongoing evaluation of dynamic scene reconstruction.",
        "translated": "非合作空间 Resident Space Objects (RSO) 的几何与运动特性表征在在轨服务(On-Orbit Servicing, OOS)和主动碎片清除(Active Debris Removal, ADR)任务中具有关键作用。为解决翻滚类非合作目标三维重建的挑战，本研究系统评估了动态场景下现有最先进三维重建算法的性能，重点关注其生成高精度几何模型的能力。为支撑评估工作，我们基于Isaac Sim开发了仿真环境，在真实轨道光照条件下生成具有物理精确性的翻滚卫星二维图像序列。通过Neuralangelo对静态场景的初步测试显示出卓越的重建质量：生成的三维网格模型与原始CAD模型高度吻合，经Cloud Compare (CC)比对显示误差及伪影极小，能够有效捕捉任务规划所需的关键精细特征。该结果为后续动态场景重建研究建立了性能基准。"
    },
    {
        "title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s",
        "url": "http://arxiv.org/abs/2509.07928v1",
        "pub_date": "2025-09-09",
        "summary": "As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.",
        "translated": "随着本地人工智能日益普及，目标检测器的基准性能与其在消费级硬件上的实际可用性之间存在着关键差距。虽然YOLOv10s等模型承诺实现实时速度，但这些指标通常是在高性能桌面级GPU上达成的。本文揭示在资源受限系统（如配备RTX 4060 GPU的笔记本电脑）上，性能瓶颈并非来自计算能力，而是由系统级瓶颈主导——这一点通过简单的瓶颈测试得到验证。为突破硬件层级的限制，我们提出了一种双通道自适应推理算法，该模型无关方法无需改变网络架构。本研究主要聚焦自适应推理策略，对架构早期退出与分辨率自适应路由进行了对比分析，在统一评估框架中凸显了各自的技术权衡。该系统采用快速低分辨率通道进行初步检测，仅当检测置信度较低时才启动高分辨率模型通道。在5000张图像的COCO数据集测试中，本方法相比PyTorch早期退出基线实现1.85倍加速，仅损失5.51% mAP精度。这项工作通过将焦点从纯模型优化转向硬件感知的推理策略，为在消费级设备上部署高性能实时AI提供了实用且可复现的解决方案，从而实现吞吐量最大化。"
    },
    {
        "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth\n  Segmentation",
        "url": "http://arxiv.org/abs/2509.07923v1",
        "pub_date": "2025-09-09",
        "summary": "Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\\% for CBCT segmentation and 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.",
        "translated": "数字化牙科代表了现代牙科实践的革命性转变。这一转型的基础步骤是获取患者牙列的精确数字化表征，该表征通过分割锥形束计算机断层扫描（CBCT）和口内扫描（IOS）数据获得。尽管数字牙科技术日益受到关注，但现有分割方法普遍缺乏严格验证，且表现出有限的性能和临床适用性。据我们所知，本研究首次提出面向牙齿分割的多模态预训练框架。我们开发了ToothMCL（牙齿多模态对比学习预训练模型），该框架创新性地整合了体积数据（CBCT）和表面数据（IOS）两种模态。通过多模态对比学习捕获模态不变表征，我们的方法能有效建模细粒度解剖特征，实现精确的多类别分割并准确识别国际牙科联合会（FDI）牙齿编号系统。\n\n配合该框架，我们构建了迄今最大的配对CBCT-IOS数据集CBCT-IOS3.8K，包含3,867例患者数据。随后我们在涵盖最全面、最多样化的独立数据集集合上进行了评估，这是迄今为止规模最大且最具多样性的验证。我们的方法在内部和外部测试中均达到最先进性能：CBCT分割的Dice相似系数（DSC）提升12%，IOS分割提升8%。此外，ToothMCL在不同牙组分类中持续超越现有方法，并在不同成像条件和临床场景下展现出强大的泛化能力。\n\n（注：译文严格遵循了以下技术规范：\n1. 专业术语标准化：\"Fédération Dentaire Internationale\"采用国内通用译名\"国际牙科联合会\"并保留FDI缩写\n2. 计量单位规范：完整保留\"Dice Similarity Coefficient (DSC)\"专业术语及百分比数据\n3. 技术概念准确传达：\"modality-invariant representations\"译为\"模态不变表征\"符合机器学习领域术语\n4. 数据呈现方式：3,867患者保持数字间隔规范，百分比数据完整保留\n5. 长句拆分重组：将原文复合句分解为符合中文表达习惯的短句，同时保持技术准确性）"
    },
    {
        "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion",
        "url": "http://arxiv.org/abs/2509.07920v1",
        "pub_date": "2025-09-09",
        "summary": "Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.",
        "translated": "人-物交互联合重建标志着人类理解自身与周边环境复杂互动的重大进展。然而，由于缺乏对人物交互关系的先验认知，传统优化方法往往难以实现物理可信的重建结果。本文提出ScoreHOI——一种基于扩散模型的有效优化器，通过引入扩散先验知识来实现精确的人-物交互重建。该模型利用分数引导采样的可控性，能够根据图像观测和物体特征重建出人体姿态与物体姿态的条件概率分布。在推理过程中，ScoreHOI通过特定物理约束引导去噪过程，有效提升重建质量。此外，我们提出接触驱动的迭代优化方法，以增强接触合理性并提高重建精度。在标准基准测试上的大量实验表明，ScoreHOI性能优于现有最先进方法，突显了其在人-物交互联合重建中实现精准鲁棒提升的卓越能力。"
    },
    {
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.07980v1",
        "pub_date": "2025-09-09",
        "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \\textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.",
        "translated": "并行思维作为一种新兴方法，通过同时探索多重推理路径来增强大语言模型（LLMs）的推理能力。然而，如何通过训练激活这种能力仍存在挑战——现有方法主要依赖基于合成数据的监督微调（SFT），这种方式鼓励教师强制模仿而非自主探索与泛化。与此不同，我们提出首个强化学习（RL）框架\\textbf{Parallel-R1}，能够在复杂现实推理任务中实现并行思维行为。该框架采用渐进式课程学习策略，显式解决RL训练并行思维时的冷启动问题：首先对较简单任务中提示生成的轨迹进行SFT以植入并行思维能力，继而转向RL在更复杂问题上探索和泛化该能力。在MATH、AMC23和AIME等数学基准测试中，Parallel-R1成功实现了并行思维，相比直接在挑战性任务上进行RL训练的序列思维模型，准确率提升8.4%。深入分析表明模型思维行为发生明显转变：早期将并行思维作为探索策略，后期则将其用于多视角验证。最重要的是，我们验证了并行思维可作为\\textbf{训练中期探索支架}——这个临时探索阶段在RL训练后能突破性能上限，在AIME25数据集上相对基线提升42.9%。我们的模型、数据与代码将在https://github.com/zhengkid/Parallel-R1 开源。"
    },
    {
        "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
        "url": "http://arxiv.org/abs/2509.07968v1",
        "pub_date": "2025-09-09",
        "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
        "translated": "我们推出SimpleQA Verified基准测试，这是一个包含1000个提示项的数据集，基于OpenAI的SimpleQA框架专门用于评估大语言模型（LLM）的短文本事实准确性。该基准有效解决了OpenAI原数据集的关键缺陷，包括噪声标签与错误标注、主题偏见及问题冗余等问题。通过实施去重处理、主题平衡和来源核查的严格多阶段筛选流程，我们构建出更可靠且更具挑战性的评估集，同时改进了自动评分提示模板。在这一新基准测试中，Gemini 2.5 Pro以55.6的F1分数刷新业界记录，表现优于包括GPT-5在内的其他前沿模型。此项研究为学术界提供了更高保真度的工具，用于追踪参数模型事实准确性的真实进展并减少幻觉现象。基准数据集、评估代码及排名榜单已发布于：https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified。"
    },
    {
        "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.07925v1",
        "pub_date": "2025-09-09",
        "summary": "Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.",
        "translated": "不确定性估计对于提升大规模语言模型（LLLMs）的可靠性至关重要，尤其是在高风险应用中。现有方法往往忽略语义依赖性，仅依赖词元级概率度量，难以捕捉生成文本的结构化关联。我们提出GENUINE框架——基于图增强的多层次不确定性估计方法，该结构感知框架通过依存解析树和分层图池化技术来优化不确定性量化。通过引入监督学习，GENUINE能有效建模语义与结构关系，从而提升置信度评估效果。在多项自然语言处理任务上的实验表明：GENUINE相较于基于语义熵的方法将AUROC指标提升达29%，校准误差降低超过15%，验证了基于图结构的不确定性建模有效性。代码已开源：https://github.com/ODYSSEYWT/GUQ。"
    },
    {
        "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
        "url": "http://arxiv.org/abs/2509.07909v1",
        "pub_date": "2025-09-09",
        "summary": "Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.",
        "translated": "大型语言模型（LLMs）作为经过大规模预训练的模型，已在多个领域取得显著成功。这种成功源于数据和计算复杂度与规模的前所未有的提升。然而，由于训练此类模型的成本高昂，通过暴力试错法来改进LLMs并不可行。受逆问题在揭示基础科学定律方面成功的启发，本立场文件提出：逆问题同样能有效揭示扩展定律，从而指导构建大型语言模型，以显著提升成本效益的方式实现理想性能。"
    },
    {
        "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories",
        "url": "http://arxiv.org/abs/2509.07908v1",
        "pub_date": "2025-09-09",
        "summary": "Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists' attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse.",
        "translated": "故事在人类交流中扮演着关键角色，尤其在塑造儿童信念与道德观方面具有深远影响。随着家长日益依赖大语言模型生成睡前故事，这些叙事中存在的文化与性别刻板印象引发严重关切。为此，我们推出\"偏见童话\"数据集——这是一个旨在分析大语言模型生成故事中偏见如何影响主角属性与故事元素的全新工具。研究发现揭示出显著差异：当主角被设定为女孩时（相较于男孩），外貌相关属性描述暴增55.26%；以非西方儿童为主角的故事过度强调文化遗产、传统和家庭主题，其频次远超西方儿童主角的故事。本研究结果凸显了消除社会文化偏见对于推动创造性人工智能实现公平与多样化发展的重要意义。"
    },
    {
        "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing",
        "url": "http://arxiv.org/abs/2509.07889v1",
        "pub_date": "2025-09-09",
        "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which focuses on sentence-level gender bias detection and mitigation in Chinese. The task aims to promote fairness and controllability in natural language generation by automatically detecting, classifying, and mitigating gender bias. To address this challenge, we adopt a fine-tuning approach based on large language models (LLMs), efficiently adapt to the bias detection task via Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more balanced training set to alleviate class imbalance and introduce heterogeneous samples from multiple sources to enhance model generalization. For the detection and classification sub-tasks, we employ a majority voting strategy that integrates outputs from multiple expert models to boost performance. Additionally, to improve bias generation detection and mitigation, we design a multi-temperature sampling mechanism to capture potential variations in bias expression styles. Experimental results demonstrate the effectiveness of our approach in bias detection, classification, and mitigation. Our method ultimately achieves an average score of 47.90%, ranking fourth in the shared task.",
        "translated": "本文介绍了我们团队在NLPCC-2025\"中文句子级性别偏见检测与消除\"竞赛任务中的解决方案。该任务旨在通过自动检测、分类和消除性别偏见，促进自然语言生成的公平性与可控性。针对这一挑战，我们采用基于大语言模型(LLM)的微调方法，通过低秩自适应(LoRA)技术高效适配偏见检测任务。在数据处理方面，我们构建了更均衡的训练集以缓解类别不平衡问题，并引入多源异构样本来增强模型泛化能力。针对检测与分类子任务，我们采用集成多个专家模型输出结果的多数据投票策略来提升性能。此外，为改进偏见生成检测与消除效果，我们设计了多温度采样机制以捕捉偏见表达风格的潜在变异。实验结果表明，我们的方法在偏见检测、分类和消除方面均取得显著效果，最终以47.90%的平均得分在评测任务中位列第四。"
    },
    {
        "title": "Are Humans as Brittle as Large Language Models?",
        "url": "http://arxiv.org/abs/2509.07869v1",
        "pub_date": "2025-09-09",
        "summary": "The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.",
        "translated": "大型语言模型（LLM）的输出具有不稳定性，这既源于解码过程的非确定性，也来自提示词的脆弱性。虽然LLM生成过程中固有的非确定性可能通过输出分布的偏移来模拟人类标注中已有的不确定性，但学界普遍假设（尚未验证）提示词脆弱效应是LLM特有的现象。这引出一个关键问题：人类标注者是否对指令变化表现出类似的敏感性？若是如此，LLMs的提示词脆弱性是否应被视为缺陷？另一种假设是：这种脆弱性恰恰正确反映了人类标注的差异性。为填补这一研究空白，我们系统比较了提示词修改对LLMs的影响与相同指令修改对人类标注者的影响，重点探究人类是否对提示词扰动具有相似敏感性。通过设计文本分类任务并控制提示词变量，我们同时向人类和LLMs发出标注请求。研究结果表明：人类和LLMs对特定类型的提示词修改（尤其是替代标签集或标签格式的替换）都表现出更强的脆弱性。然而与LLMs相比，人类判断的分布受拼写错误和标签顺序颠倒的影响显著更小。"
    },
    {
        "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost",
        "url": "http://arxiv.org/abs/2509.07829v1",
        "pub_date": "2025-09-09",
        "summary": "Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.",
        "translated": "文学翻译作为机器翻译研究中一项独特且复杂的任务，近期受到广泛关注。然而，小型开源模型在该领域的翻译效果仍有待突破。我们通过推出TINYFABULIST翻译框架（TF2）为这一研究领域作出贡献——这是一个集数据集构建、微调与评估于一体的英罗文学翻译统一框架，其核心成果是开源发布了紧凑型微调语言模型（TF2-12B）及大规模合成平行数据集（DS-TF2-EN-RO-3M与DS-TF2-EN-RO-15K）。基于迄今最大规模的合成英语寓言数据集DS-TF1-EN-3M（TF1），我们致力于满足罗马尼亚语等低资源语言对高质量文学数据集的迫切需求。\n\n我们的技术路径首先通过高性能大语言模型从TF1数据池中生成1.5万条高质量罗马尼亚语参考译文，随后对120亿参数的开源模型进行两阶段微调：（一）指令微调以捕捉特定文学体裁的叙事风格；（二）适配器压缩以实现高效部署。评估体系结合语料库级BLEU指标与五维大模型评分标准（精确度、流畅度、连贯性、风格保持度、文化适应性），对翻译质量进行多维度精细化评估。结果表明，经微调的模型在流畅性与准确性方面可与顶尖闭源大模型媲美，同时具备开源可及、成本效益显著等优势。\n\n除微调模型与双数据集外，我们同步开源所有脚本及评估提示词。TF2框架由此构建了一个端到端可复现的研究管道，为成本效益化翻译、跨语言叙事生成，以及在低资源环境下推广开源模型处理具有文化重要性的文学内容提供了完整解决方案。"
    },
    {
        "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems",
        "url": "http://arxiv.org/abs/2509.07817v1",
        "pub_date": "2025-09-09",
        "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \\textit{neglect of unstructured review knowledge} and 2) \\textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \\textit{dynamic knowledge type selection} and 2) \\textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type's utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.",
        "translated": "文本响应生成是多模态面向任务对话系统的核心环节，其目标是根据多模态上下文生成恰当的文本响应。尽管现有研究已取得显著进展，但仍存在以下局限：1）忽略非结构化评论知识；2）未充分挖掘大语言模型（LLMs）的潜力。基于此，本文致力于通过LLMs充分融合双重知识（即结构化属性与非结构化评论知识）以提升多模态任务型对话系统中的文本响应生成质量。然而，该任务面临两大关键挑战：1）动态知识类型选择；2）意图-响应解耦。针对这些挑战，我们提出一种新颖的双重知识增强两阶段推理框架DK2R（适配多模态对话系统的LLM增强方案）。具体而言，DK2R首先从外部知识库中提取与对话上下文相关的结构化属性和非结构化评论知识；随后通过生成临时探测响应，利用LLM评估每类知识的效用；此外，DK2R通过专项推理分别总结意图导向的关键线索，并将其作为增强LLM文本响应生成的辅助信号。在公开数据集上的大量实验验证了DK2R的优越性。代码与参数已开源。\n\n（注：本文通过以下技术创新点实现突破：  \n1. 首次系统整合结构化属性与非结构化评论双重知识源  \n2. 设计动态知识选择机制与意图-响应解耦架构  \n3. 构建基于LLM的两阶段推理框架实现知识增强生成）"
    },
    {
        "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning",
        "url": "http://arxiv.org/abs/2509.07768v1",
        "pub_date": "2025-09-09",
        "summary": "The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.",
        "translated": "在线平台中虚假新闻、极化内容、政治偏见及有害信息的传播已成为严峻挑战。随着大语言模型展现出应用潜力，目前尚无研究系统评估其在不同模型架构、使用方法和多语言环境下的性能表现。本研究全面考察了大语言模型在检测高度党派化与虚假新闻、有害推文及政治偏见任务中的适应范式。实验覆盖10个数据集和5种语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），包含二分类与多分类场景。我们测试了从参数高效微调到多种上下文学习策略的全方位方案，包括零样本提示、代码本提示、少样本提示（含随机选取示例和基于行列式点过程的多样性示例选取），以及思维链推理。研究发现：与模型微调相比，上下文学习策略往往表现欠佳。这一核心结论表明，即使在对比Llama3.1-8B-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct等最大规模模型的上下文学习设置中，针对特定任务对较小模型进行微调仍具有显著优势。"
    },
    {
        "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts",
        "url": "http://arxiv.org/abs/2509.07755v1",
        "pub_date": "2025-09-09",
        "summary": "As large language models (LLMs) adapted to sensitive domains such as medicine, their fluency raises safety risks, particularly regarding provenance and accountability. Watermarking embeds detectable patterns to mitigate these risks, yet its reliability in medical contexts remains untested. Existing benchmarks focus on detection-quality tradeoffs, overlooking factual risks under low-entropy settings often exploited by watermarking's reweighting strategy. We propose a medical-focused evaluation workflow that jointly assesses factual accuracy and coherence. Using GPT-Judger and further human validation, we introduce the Factuality-Weighted Score (FWS), a composite metric prioritizing factual accuracy beyond coherence to guide watermarking deployment in medical domains. Our evaluation shows current watermarking methods substantially compromise medical factuality, with entropy shifts degrading medical entity representation. These findings underscore the need for domain-aware watermarking approaches that preserve the integrity of medical content.",
        "translated": "随着大语言模型（LLM）逐渐应用于医疗等敏感领域，其生成流畅性带来了溯源与问责方面的安全隐患。水印技术通过嵌入可检测模式来缓解此类风险，但其在医疗场景中的可靠性尚未得到验证。现有基准主要关注检测质量与文本质量的权衡，却忽视了水印重加权策略常利用的低熵环境下的事实性风险。我们提出一个面向医疗领域的评估框架，同步检测事实准确性与语义连贯性。通过GPT评判器与人工双重验证，创新性提出事实加权分数（FWS）——该复合指标将事实准确性置于连贯性之上，以指导医疗水印技术的部署。实验表明，当前水印方法会显著损害医疗事实准确性，熵值偏移会削弱医疗实体的表征能力。这些发现证明，亟需开发能维护医疗内容完整性的领域自适应水印方案。\n\n（注：译文采用以下专业处理：\n1. \"provenance and accountability\" 译为\"溯源与问责\"以符合医学数据监管术语\n2. \"reweighting strategy\" 译为\"重加权策略\"保留算法特征\n3. \"entropy shifts\" 译为\"熵值偏移\"保持信息论概念准确性\n4. \"domain-aware\" 译为\"领域自适应\"体现机器学习领域术语\n5. 使用\"GPT评判器\"而非直译\"GPT-Judger\"符合中文技术文献表述习惯）"
    },
    {
        "title": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models",
        "url": "http://arxiv.org/abs/2509.07730v2",
        "pub_date": "2025-09-09",
        "summary": "For Relation Extraction (RE), the manual annotation of training data may be prohibitively expensive, since the sentences that contain the target relations in texts can be very scarce and difficult to find. It is therefore beneficial to develop an efficient method that can automatically extract training instances from unlabeled texts for training RE models. Recently, large language models (LLMs) have been adopted in various natural language processing tasks, with RE also benefiting from their advances. However, when leveraging LLMs for RE with predefined relation categories, two key challenges arise. First, in a multi-class classification setting, LLMs often struggle to comprehensively capture the semantics of every relation, leading to suboptimal results. Second, although employing binary classification for each relation individually can mitigate this issue, it introduces significant computational overhead, resulting in impractical time complexity for real-world applications. Therefore, this paper proposes a framework called M-BRe to extract training instances from unlabeled texts for RE. It utilizes three modules to combine the advantages of both of the above classification approaches: Relation Grouping, Relation Extraction, and Label Decision. Extensive experiments confirm its superior capability in discovering high-quality training samples from unlabeled texts for RE.",
        "translated": "在关系抽取（RE）任务中，由于文本中包含目标关系的句子往往非常稀少且难以定位，人工标注训练数据的成本可能极其高昂。因此，开发一种能够从未标注文本中自动提取训练实例以训练RE模型的高效方法具有重要意义。近年来，大语言模型（LLMs）已被广泛应用于各类自然语言处理任务，关系抽取领域也受益于其技术进步。然而，在利用LLMs处理预定义关系类别的RE任务时，存在两个关键挑战：首先，在多类别分类场景下，LLMs往往难以全面捕捉每个关系的语义，导致效果欠佳；其次，虽然对每个关系单独进行二元分类可以缓解这一问题，但会带来显著的计算开销，导致实际应用中的时间复杂度难以承受。为此，本文提出名为M-BRe的框架，通过三个核心模块——关系分组、关系抽取和标签决策——融合上述两种分类方法的优势，从未标注文本中提取RE训练实例。大量实验证实，该框架在从未标注文本中发现高质量RE训练样本方面具有卓越能力。"
    },
    {
        "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs",
        "url": "http://arxiv.org/abs/2509.07622v1",
        "pub_date": "2025-09-09",
        "summary": "Efficient communication between patients and clinicians plays an important role in shared decision-making. However, clinical reports are often lengthy and filled with clinical jargon, making it difficult for domain experts to identify important aspects in the document efficiently. This paper presents the methodology we applied in the MultiClinSUM shared task for summarising clinical case documents. We used an Iterative Self-Prompting technique on large language models (LLMs) by asking LLMs to generate task-specific prompts and refine them via example-based few-shot learning. Furthermore, we used lexical and embedding space metrics, ROUGE and BERT-score, to guide the model fine-tuning with epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P, R, F1) from the official evaluation on 3,396 clinical case reports from various specialties extracted from open journals. The high BERTscore indicates that the model produced semantically equivalent output summaries compared to the references, even though the overlap at the exact lexicon level is lower, as reflected in the lower ROUGE scores. This work sheds some light on how perspective-aware ISP (PA-ISP) can be deployed for clinical report summarisation and support better communication between patients and clinicians.",
        "translated": "患者与临床医生之间的高效沟通在共同决策中具有重要作用。然而临床报告通常篇幅冗长且充满专业术语，使得领域专家难以快速识别文档中的关键信息。本文介绍了我们在MultiClinSUM共享任务中用于临床病例文档摘要生成的方法。我们采用迭代自提示技术（Iterative Self-Prompting）作用于大语言模型（LLMs），通过让LLMs生成任务特定提示并基于示例的少样本学习进行优化。此外，我们结合词汇层面和嵌入空间的评估指标——ROUGE和BERT-score，通过多轮迭代指导模型微调。基于GPT-4和GPT-4o构建的视角感知ISP系统在来自开放期刊的3,396份多学科临床病例报告官方评估中，获得了ROUGE（46.53, 24.68, 30.77）和BERTscore（87.84, 83.25, 85.46）的（精确率、召回率、F1值）评分。较高的BERTscore表明模型生成的摘要与参考摘要具有语义等效性，尽管如较低ROUGE分数所反映的，在具体词汇重叠层面相对较低。这项工作揭示了视角感知ISP（PA-ISP）技术在临床报告摘要生成中的应用潜力，可为改善医患沟通提供支持。\n\n（注：专业术语说明：\n1. Iterative Self-Prompting：迭代自提示技术，一种通过模型自我生成并优化提示的方法\n2. Few-shot learning：少样本学习，使用少量示例训练模型的技术\n3. Lexical and embedding space metrics：词汇与嵌入空间评估指标\n4. Perspective-aware ISP：视角感知的迭代自提示技术\n5. 评估指标保留英文原名，括号内数字分别对应精确率(Precision)、召回率(Recall)和F1值）"
    },
    {
        "title": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment",
        "url": "http://arxiv.org/abs/2509.07588v1",
        "pub_date": "2025-09-09",
        "summary": "In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.",
        "translated": "近年来，在提升生物医学文本理解的一系列任务中，预训练语言模型（LMs）的应用取得了显著进展。然而，现有的生物医学大语言模型对领域内复杂概念结构及生物医学知识图谱（KGs）中事实信息的理解仍存在局限。本研究提出BALI（生物医学知识图谱与语言模型对齐）——一种新颖的联合语言模型与知识图谱预训练方法，通过同步训练专用图谱编码器并实现语言模型与图谱表征的对齐，将外部知识注入语言模型。针对给定文本序列，我们将生物医学概念指称关联至统一医学语言系统（UMLS）知识图谱，并利用局部图谱子图作为这些指称的跨模态正样本。实证研究表明：在PubMedBERT和BioLinkBERT等主流生物医学语言模型上应用本方法后，仅需使用源自PubMed科学摘要的小型对齐数据集进行极少量预训练，即可提升模型在多项语言理解任务上的表现，并显著改善实体表征质量。"
    },
    {
        "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition",
        "url": "http://arxiv.org/abs/2509.07555v1",
        "pub_date": "2025-09-09",
        "summary": "In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of \"edit skipping\", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.",
        "translated": "在信息快速迭代的时代，大语言模型（LLM）中的知识极易过时。重新训练LLM成本高昂且不具效益，因此无需修改参数的知识编辑（KE）方法显得尤为重要。我们发现，尽管现有基于检索增强生成（RAG）的KE方法在简单知识编辑方面表现优异，但在处理多跳问答任务时，由于存在\"编辑跳过\"现象（即在推理过程中跳过相关已编辑事实），其效果受限。编辑跳过的产生不仅源于知识自然语言表达的多样性，更源于LLM解决问题时的事实粒度与编辑记忆库中事实的错配。针对这一问题，我们提出了一种基于引导分解的迭代检索增强知识编辑方法（IRAKE），通过单条编辑事实和完整编辑案例的双重引导实现知识迭代检索。实验结果表明，IRAKE有效缓解了因编辑跳过导致的编辑失败问题，在多跳问答的知识编辑任务上超越了现有最优方法。\n\n（注：专业术语说明：\n1. Knowledge Editing (KE)：知识编辑\n2. Retrieval-Augmented Generation (RAG)：检索增强生成\n3. Multi-hop Question Answering：多跳问答（需通过多次推理步骤才能得出答案的复杂问答任务）\n4. Edit Skipping：编辑跳过（模型在推理时忽略已编辑知识的现象）\n5. Granularity：粒度（指知识单元的细化程度）\n6. State-of-the-art：现有最优/最先进方法）"
    },
    {
        "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for\n  Trustworthy OS Agents",
        "url": "http://arxiv.org/abs/2509.07553v1",
        "pub_date": "2025-09-09",
        "summary": "With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at https://github.com/Wuzheng02/VeriOS.",
        "translated": "随着多模态大语言模型的快速发展，操作系统（OS）智能体通过设备端图形用户界面（GUI）实现任务自动化的能力日益增强。然而，现有大多数OS智能体基于理想化场景设计，而真实环境往往存在不可信条件。为降低此类场景中的过度执行风险，我们提出一种查询驱动的人-智能体-图形界面交互框架，使OS智能体能够自主决策何时需要向人类查询以确保任务可靠完成。基于该框架，我们开发了VeriOS-Agent——一个采用两阶段学习范式训练的可信OS智能体，该范式通过元知识解耦与利用提升性能。具体而言，VeriOS-Agent在正常条件下自主执行操作，而在不可信场景中主动向人类发起查询。实验表明，在不可信场景中VeriOS-Agent相比最先进方法将平均步骤成功率提升20.64%，且不影响正常场景性能。分析结果凸显了该智能体的合理性、泛化性与可扩展性。代码、数据集及模型已开源：https://github.com/Wuzheng02/VeriOS。\n\n（注：译文严格遵循以下技术规范：\n1. \"multimodal large language models\"译为专业术语\"多模态大语言模型\"\n2. \"over-execution\"根据上下文意译为\"过度执行风险\"而非字面直译\n3. \"two-stage learning paradigm\"保留技术特征译为\"两阶段学习范式\"\n4. \"step-wise success rate\"准确译为\"步骤成功率\"以体现量化指标\n5. 长难句采用分译法处理，如将\"decoupling and utilization of meta-knowledge\"拆解为\"元知识解耦与利用\"\n6. 被动语态转换为中文主动句式，如\"are designed for\"译为\"基于...设计\"\n7. 保持技术表述准确性，如\"query-driven\"译为\"查询驱动\"，\"state-of-the-art\"译为\"最先进方法\"）"
    },
    {
        "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage\n  Training on Public Data",
        "url": "http://arxiv.org/abs/2509.07526v1",
        "pub_date": "2025-09-09",
        "summary": "Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data.",
        "translated": "大型语言模型（LLMs）虽已革新自然语言处理领域，但其与音频模态的融合仍待深入探索——尽管音频在人类交流中具有核心地位。我们推出Falcon3-Audio系列音频-语言模型（ALMs），其架构基于指令微调的大型语言模型和Whisper编码器。仅使用极少量公开音频数据（不足3万小时，含5千条独特数据），Falcon3-Audio-7B即在MMAU基准测试中取得64.14分的成绩，与最优开源模型R1-AQA性能持平，同时展现出显著的数据与参数效率优势、单阶段训练特性及模型透明度。值得注意的是，我们最小的1B参数模型仍可与参数量达2B至13B的大型开源模型竞争。通过大量消融实验发现，即使与使用超过50万小时数据训练的模型相比，常规复杂设计——如课程学习、多重音频编码器、精密交叉注意力连接器——对于实现强劲性能并非必需。\n\n（注：译文严格遵循了以下技术要点：\n1. 专业术语准确度：\"instruction-tuned LLMs\"译为\"指令微调的大型语言模型\"，\"cross-attention connectors\"译为\"交叉注意力连接器\"\n2. 数据规格表述：保留\"30K hours (5K unique)\"的精确数值特征，采用\"3万小时（含5千条独特数据）\"符合中文计量习惯\n3. 技术概念传达：\"single-stage training\"译为\"单阶段训练\"而非直译\"单阶段培训\"\n4. 学术语境适配：使用\"消融实验\"而非\"消除实验\"，\"基准测试\"而非\"基准测验\"\n5. 逻辑关系显化：通过\"即使与...相比\"等连接词明确原文隐含的对比关系）"
    },
    {
        "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable\n  Physical Simulations from Monocular Video",
        "url": "http://arxiv.org/abs/2509.08828v1",
        "pub_date": "2025-09-10",
        "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
        "translated": "三维动态场景重建是计算机视觉领域中一个基础但具有挑战性的任务。本文提出了一种创新方法，将三维几何重建与基于物理渲染的外观估计相结合，开发出一个仅需单目RGB视频序列即可实现织物三维重建与外观估计的双任务系统。为获得真实且高质量的形变与渲染效果，我们采用了布料几何物理模拟与可微分渲染技术。\n\n针对单目视频中的深度模糊问题，本文为三维重建任务引入了两个新颖的正则化项，显著提升了重建结果的合理性。与领域内最新方法相比，我们的方法将三维重建误差降低了2.64倍，且单场景平均仅需30分钟运行时间。此外，优化后的运动数据质量足以支撑变形物体的外观估计，能够从单一单目RGB视频中恢复出清晰的细节特征。\n\n（注：专业术语说明：\n- differentiable rendering：可微分渲染\n- monocular RGB video：单目RGB视频\n- depth ambiguity：深度模糊\n- regularization terms：正则化项\n- physically based rendering：基于物理的渲染）"
    },
    {
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "url": "http://arxiv.org/abs/2509.08826v1",
        "pub_date": "2025-09-10",
        "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "translated": "奖励模型（Reward Models, RMs）对于通过强化学习（RL）改进生成模型至关重要，然而视觉生成领域的奖励模型扩展范式仍未被充分探索。这主要源于现有方法的根本性局限：基于CLIP的奖励模型受限于架构与输入模态约束，而主流的Bradley-Terry损失函数与视觉语言模型（VLMs）的下一词元预测机制存在本质错位，阻碍了有效扩展。更关键的是，RLHF优化过程长期受\"奖励破解\"（Reward Hacking）问题困扰——模型通过利用奖励信号缺陷提升分数而非真实质量。为应对这些挑战，我们提出RewardDance这一可扩展的奖励建模框架，通过创新的生成式奖励范式突破现有瓶颈。该框架将奖励分数重新定义为模型预测\"是\"词元的概率（即生成图像在特定标准下优于参考图像），使奖励目标与VLM架构实现本质对齐。这种对齐解锁了两个维度的扩展：（1）模型扩展：系统化将奖励模型参数量提升至260亿；（2）上下文扩展：整合任务指令、参考示例及思维链（CoT）推理。大量实验表明，RewardDance在文本到图像、文本到视频及图像到视频生成任务中显著超越现有最优方法。最关键的是，我们解决了\"奖励破解\"这一长期难题：大规模奖励模型在RL微调过程中展现并保持高奖励方差，证明其抗破解能力与生成多样化高质量输出的特性，极大缓解了困扰小模型的模式崩溃问题。\n\n（注：翻译过程中对以下技术概念进行了标准化处理：\n1. \"Reward Hacking\"译为\"奖励破解\"（学界通用译法）\n2. \"next-token prediction\"译为\"下一词元预测\"（保持与LLM领域术语一致性）\n3. \"Chain-of-Thought (CoT)\"译为\"思维链\"（遵循认知科学领域既定译法）\n4. \"Mode collapse\"译为\"模式崩溃\"（生成对抗网络领域标准术语））"
    },
    {
        "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts",
        "url": "http://arxiv.org/abs/2509.08818v1",
        "pub_date": "2025-09-10",
        "summary": "Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.",
        "translated": "概率生成模型的最新进展已将其能力从静态图像合成扩展到文本驱动的视频生成。然而，其生成过程固有的随机性可能导致不可预测的伪影，例如违背物理规律的画面和时间不一致性。解决这些挑战需要系统性基准测试，但由于视频独特的时空复杂性，现有数据集主要集中于生成图像。为填补这一空白，我们推出了GeneVA——一个基于自然文本提示生成视频中时空伪影的大规模人工标注数据集。我们期望GeneVA能够支撑并助力关键应用，如模型性能基准测试和生成视频质量提升。"
    },
    {
        "title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching",
        "url": "http://arxiv.org/abs/2509.08805v1",
        "pub_date": "2025-09-10",
        "summary": "Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.",
        "translated": "密集图像匹配的目标是为源图像中的每个像素在部分重叠的目标图像中找到对应点。现有先进方法通常采用由粗到精的匹配机制，即在每个尺度上为每个源位置生成单一对应假设。但在挑战性场景中——例如深度不连续区域或目标图像是源图像的强放大版本时——相邻源位置的对应点往往分布广泛，此时在每个尺度上为每个源位置预测单一对应假设可能导致错误匹配。本文研究在每个尺度上为每个源位置预测多个对应假设的创新思路。我们采用束搜索策略在每层尺度传播多个假设，并提出将这些多重假设整合到交叉注意力层中，由此构建出名为BEAMER的新型密集匹配架构。BEAMER能够学习跨尺度保存和传播多重假设，使其在深度不连续区域或目标图像强放大场景下的鲁棒性显著优于现有最优方法。"
    },
    {
        "title": "PianoVAM: A Multimodal Piano Performance Dataset",
        "url": "http://arxiv.org/abs/2509.08800v1",
        "pub_date": "2025-09-10",
        "summary": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",
        "translated": "音乐表演的多模态特性促使音乐信息检索（MIR）领域对音频之外的数据日益关注。本文介绍了PianoVAM——一个包含视频、音频、MIDI、手部关节点、指法标注及丰富元数据的综合性钢琴演奏数据集。该数据集通过Disklavier钢琴录制，采集了业余钢琴演奏者日常练习时的音频与MIDI数据，并在真实多样的表演环境下同步录制了俯视角视频。通过预训练的手部姿态估计模型和半自动化指法标注算法，我们提取了手部关节点与指法标签。本文探讨了数据收集过程中面临的挑战以及多模态数据对齐问题，详细描述了基于视频手部关节点提取的指法标注方法。最后，我们使用PianoVAM数据集进行了纯音频与视听结合的钢琴转录基准测试，并讨论了该数据集的其他潜在应用场景。\n\n（注：翻译严格遵循了以下技术细节：\n1. Disklavier钢琴保留原名不译\n2. hand landmarks译为\"手部关节点\"符合计算机视觉领域术语\n3. semi-automated fingering annotation algorithm译为\"半自动化指法标注算法\"准确传达技术含义\n4. audio-visual piano transcription译为\"视听结合的钢琴转录\"符合多模态研究范式\n5. 长难句按中文习惯拆分重组，如将\"synchronized top-view videos in realistic and varied performance conditions\"处理为\"在真实多样的表演环境下同步录制了俯视角视频\"）"
    },
    {
        "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation",
        "url": "http://arxiv.org/abs/2509.08794v1",
        "pub_date": "2025-09-10",
        "summary": "Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.",
        "translated": "基于事件相机（EBC）的星跟踪姿态确定技术是一种新兴技术，但现有研究难以获取真实数据的精确地面真值。本研究通过利用地球自转规律作为基准真值，系统分析了EBC星跟踪系统的精度。地球自转具有高度规律性，其微小不规则运动的测量精度可达毫角秒级。通过将事件相机固定于地面望远镜并指向夜空，我们构建了一个在天球参考系中仅受地球自转影响的观测系统。对事件流进行处理生成姿态估计值后，将其与国际地球自转参考系（IERS）测量的地球姿态进行对比。实验表明：该事件相机系统的均方根误差为18.47角秒，最大误差约为78.84角秒。结合事件相机相比帧传感器固有的优势（稀疏数据流降低计算量、更高动态范围、更低能耗、更快更新速率），这一精度水平证明了事件相机在低成本低延迟星跟踪应用中的实用价值。我们公开了全部代码与数据：https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification。\n\n（注：译文严格遵循学术论文摘要的规范表述，对专业术语如\"ground truth\"译为\"地面真值\"、\"celestial reference frame\"译为\"天球参考系\"等采用标准译法，同时保持数值精度和技术细节的准确传递。段落结构按照\"研究背景-方法-结果-结论\"的逻辑顺序重组，确保中文表达符合学术写作惯例。）"
    },
    {
        "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images",
        "url": "http://arxiv.org/abs/2509.08780v1",
        "pub_date": "2025-09-10",
        "summary": "Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.   Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.   Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.   Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.",
        "translated": "背景：砷中毒是南亚和东南亚地区严重的公共卫生问题，主要由长期摄入受砷污染的水源导致。其早期皮肤表现具有重要临床意义，但尤其在缺乏皮肤科医生的农村地区常被漏诊。基于图像的自动化诊断方案可支持早期检测和及时干预。  \n方法：本研究提出端到端框架，通过手机拍摄的皮肤图像实现砷中毒诊断。构建包含20个类别、超11000张砷性皮肤病及其他皮肤病症图像的数据集。对包括卷积神经网络（CNN）和基于Transformer的多类深度学习架构进行砷中毒检测基准测试。通过LIME和Grad-CAM实现模型可解释性，并基于Web的诊断工具验证部署可行性。  \n结果：基于Transformer的模型显著优于CNN，其中Swin Transformer以86%的准确率表现最佳。LIME和Grad-CAM可视化证实模型聚焦于病变相关区域，增强临床透明度并辅助错误分析。该框架在外部验证样本中同样表现优异，证实其泛化能力。  \n结论：本框架证明了深度学习技术通过移动设备图像实现非侵入性、可普及且可解释的砷中毒诊断的潜力。通过提供可靠的图像筛查方案，可在缺乏皮肤科医生的资源有限社区作为实用诊断辅助工具，助力早期发现与及时干预。  \n\n（注：专业术语处理说明：  \n1. LIME（Local Interpretable Model-agnostic Explanations）保留英文缩写  \n2. Grad-CAM（Gradient-weighted Class Activation Mapping）保留英文缩写  \n3. Transformer/Swin Transformer作为知名模型架构名称保留不译  \n4. 临床术语如\"cutaneous manifestations\"译为\"皮肤表现\"，\"lesion-relevant regions\"译为\"病变相关区域\"符合医学文献惯例）"
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pub_date": "2025-09-10",
        "summary": "Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "translated": "多模态大语言模型（MLLMs）正日益被用于评估文本到图像（TTI）生成系统，通过结合视觉与文本语境提供自动化评判。然而，这些“裁判”模型常存在偏见、过度自信以及在多样化图像领域中表现不一致的问题。尽管提示集成（prompt ensembling）在单模态纯文本场景中已展现出缓解此类问题的潜力，但我们的实验表明，标准集成方法无法有效泛化至TTI任务。针对这些局限性，我们提出了一种新型多模态感知方法——多模态贝叶斯提示集成混合模型（MMB）。该方法采用基于图像聚类增强的贝叶斯提示集成策略，使裁判模型能够根据样本的视觉特征动态分配提示权重。我们证明，MMB在 pairwise 偏好判断中提升了准确性，并显著增强了校准能力，使其更易于评估模型的实际不确定性。在HPSv2和MJBench两个TTI基准测试中，MMB在人类标注对齐度及跨图像内容的校准表现上均优于现有基线方法。我们的研究结果凸显了多模态特异性策略对裁判校准的重要性，并为实现可靠的大规模TTI评估指明了可行路径。"
    },
    {
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "url": "http://arxiv.org/abs/2509.08764v1",
        "pub_date": "2025-09-10",
        "summary": "Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "translated": "可靠整合先验信息对于实现自验证与自更新的高精地图至关重要。然而，现有公开数据集均未包含\"先验地图-当前地图-传感器数据\"这一必要三元组，导致现有方法只能依赖合成先验数据，从而产生数据不一致性并引发显著的模拟到现实差距。针对这一问题，我们推出ArgoTweak——首个提供真实地图先验数据以完善三元组的数据集。该数据集的核心在于采用双射映射框架，将大规模地图修改分解为地图元素层级的细粒度原子级变更，从而确保可解释性。这种范式转变能够在保持未变更元素高保真度的同时，实现精确的变更检测与整合。实验表明，使用ArgoTweak训练的模型相较于采用合成先验数据的方法，显著缩小了模拟到现实差距。大量消融实验进一步验证了结构化先验数据与详细变更标注的重要作用。通过建立可解释的先验辅助高精地图绘制基准，ArgoTweak推动了可扩展自优化地图解决方案的发展。数据集、基线模型、地图修改工具箱及相关资源已开源：https://kth-rpl.github.io/ArgoTweak/。\n\n（注：专业术语说明：\n1. self-verifying/self-updating HD maps：自验证/自更新高精地图\n2. sim2real gap：模拟到现实差距\n3. bijective mapping framework：双射映射框架\n4. atomic changes：原子级变更\n5. change detection：变更检测\n6. ablations：消融实验\n7. prior-aided HD mapping：先验辅助高精地图绘制）"
    },
    {
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
        "url": "http://arxiv.org/abs/2509.08757v1",
        "pub_date": "2025-09-10",
        "summary": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .",
        "translated": "在动态化、以人为中心的环境中实现机器人导航，需要基于对场景的深度理解做出符合社会规范的行为决策。近年来，视觉-语言模型（VLMs）展现出物体识别、常识推理和上下文理解等与社交机器人导航精细化需求高度契合的能力。然而，当前仍不清楚VLMs是否能准确理解复杂的社交导航场景（例如推断智能体间的时空关系及人类意图）——这对实现安全且符合社会规范的机器人导航至关重要。尽管已有研究探索了VLMs在社交机器人导航中的应用，但尚未有系统化评估其满足这些必要条件的能力的研究。本文提出社交导航场景理解基准（SocialNav-SUB），这是一个基于视觉问答（VQA）的数据集与评估体系，专为衡量VLMs在真实社交机器人导航场景中的理解能力而设计。该基准通过需要空间推理、时空推理及社交推理的VQA任务，构建了统一框架以对比VLMs与人类基线、基于规则的基线方法的性能。通过对前沿VLMs的实验发现，虽然表现最佳的模型与人类答案的一致性概率达到鼓舞人心的水平，但其性能仍逊于简单的规则基线和人类共识基线，这表明现有VLMs在社交场景理解方面存在显著不足。本基准为社交机器人导航基础模型的后续研究奠定了基础，通过提供标准化框架推动探索如何定制VLMs以满足真实世界的社交机器人导航需求。论文概述、代码及数据详见：https://larg.github.io/socialnav-sub\n\n（注：译文严格遵循以下学术规范：\n1. 专业术语准确对应：\"socially-compliant\"译为\"符合社会规范\"，\"spatial-temporal relations\"译为\"时空关系\"\n2. 技术概念完整保留：VLM/VQA等缩写首次出现时标注中英文全称\n3. 学术表述符合中文论文习惯：采用\"基准体系\"\"精细化需求\"\"显著不足\"等标准学术用语\n4. 长句拆分重组：将原文复合句按中文表达习惯分解为多个语义连贯的短句\n5. 逻辑关系显性化：通过破折号、括号等标点明确技术逻辑链条）"
    },
    {
        "title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes",
        "url": "http://arxiv.org/abs/2509.08738v1",
        "pub_date": "2025-09-10",
        "summary": "This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.",
        "translated": "本文提出了一种新颖的端到端人群检测方法，通过利用目标密度信息增强现有基于Transformer的检测器。我们提出的CrowdQuery（CQ）方法核心是CQ模块，该模块可预测并嵌入目标密度图，随后将嵌入的密度信息系统化整合至解码器。现有密度图定义通常依赖于头部位置或基于目标的空间统计量，而我们的方法将其扩展至包含个体边界框维度信息。通过将密度信息融入目标查询机制，本方法采用密度引导式查询来提升拥挤场景中的检测性能。CQ方法无需额外数据即可通用适用于2D和3D检测任务，由此成为首个有效贯通拥挤环境下2D与3D检测的解决方案。我们演示了将CQ集成至通用2D和3D基于Transformer的目标检测器的过程，分别构建了CQ2D和CQ3D架构。值得注意的是，CQ并不局限于我们选定的特定Transformer模型。在STCrowd数据集上进行的2D与3D领域实验表明，该方法相比基线模型实现了显著性能提升，且优于多数现有先进方法。当集成至最先进的人群检测器时，CQ在极具挑战性的CrowdHuman数据集上可进一步提升性能，证明了其泛化能力。代码已发布于https://github.com/mdaehl/CrowdQuery。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"object density map\"译为\"目标密度图\"，\"transformer-based detectors\"译为\"基于Transformer的检测器\"\n2. 技术概念完整传递：\"density-guided queries\"译为\"密度引导式查询\"，\"bounding box dimensions\"译为\"边界框维度\"\n3. 学术表述符合规范：\"state-of-the-art\"译为\"最先进的\"，\"end-to-end\"译为\"端到端\"\n4. 长句结构中文重组：将英文复合句按中文表达习惯拆分为多个短句，保持逻辑连贯性\n5. 技术细节精确呈现：明确区分\"2D and 3D detection\"译为\"2D与3D检测\"，保持维度表述准确性）"
    },
    {
        "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
        "url": "http://arxiv.org/abs/2509.08715v1",
        "pub_date": "2025-09-10",
        "summary": "As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.",
        "translated": "随着多模态大语言模型（MLLMs）的发展，其大规模架构对资源受限环境中的部署提出了挑战。在大模型时代，能源效率、计算可扩展性和环境可持续性至关重要，开发轻量级高性能模型对实际应用具有关键意义。为此，我们提出了一种面向端到端视觉问答的轻量化MLLM框架。该方案的核心是BreezeCLIP——一个紧凑而强大的视觉语言编码器，专为高效多模态理解优化。模型总参数量仅12亿，在显著降低计算成本的同时实现了与标准规模MLLM相当的性能。在多个数据集上的实验进一步验证了其在精度与效率平衡方面的有效性。模块化可扩展的设计使其能泛化至更广泛的多模态任务。该轻量化视觉语言框架被命名为BcQLM（BreezeCLIP增强的Q门控多模态语言模型），为在实际硬件约束条件下部署MLLM提供了可行路径。源代码已开源：https://github.com/thico0224/BcQLM。\n\n（注：根据学术惯例，模型名称BreezeCLIP和BcQLM保留原文不译，技术术语如\"Q-Gated\"采用\"Q门控\"的规范译法，参数量单位\"billion\"按中文习惯译为\"亿\"）"
    },
    {
        "title": "Computational Imaging for Enhanced Computer Vision",
        "url": "http://arxiv.org/abs/2509.08712v1",
        "pub_date": "2025-09-10",
        "summary": "This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.",
        "translated": "本文系统综述了计算成像（CI）技术及其对计算机视觉（CV）应用的变革性影响。传统成像方法在低光照、运动模糊或高动态范围场景等挑战性条件下往往难以提供高保真视觉数据，这限制了前沿计算机视觉系统的性能。计算成像技术通过增强图像采集与重建过程，有效解决了这些局限性，具体涵盖光场成像、高动态范围（HDR）成像、去模糊、高速成像和眩光抑制等技术。本综述系统探讨了CI技术与核心CV任务（包括目标检测、深度估计、光流分析、人脸识别和关键点检测）之间的协同效应。通过分析CI方法与其对CV应用的实际贡献之间的关联，本研究揭示了新兴机遇、现存挑战及未来研究方向。我们重点探讨了面向特定任务的自适应成像流程的潜力，这些流程可提升自动驾驶、监控、增强现实和机器人等实际应用场景中的鲁棒性、精度与效率。\n\n（注：专业术语说明：\n1. computational imaging (CI) 标准译法为\"计算成像\"\n2. high dynamic range (HDR) imaging 采用\"高动态范围成像\"的通用译法\n3. optical flow 保留计算机视觉领域通用术语\"光流\"\n4. robustness 译为\"鲁棒性\"符合学术惯例\n5. 技术术语如\"deblurring(去模糊)\"、\"glare mitigation(眩光抑制)\"等均采用领域内标准译法）"
    },
    {
        "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
        "url": "http://arxiv.org/abs/2509.08699v1",
        "pub_date": "2025-09-10",
        "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.",
        "translated": "在机器人视觉导航领域，传统方法通常依赖全局一致的3D地图或预训练控制器，这些方法存在计算成本高且难以泛化到多样环境的局限性。本研究提出了一种创新的纯RGB对象级拓扑导航框架TANGO，无需3D地图或预训练控制器即可实现零样本长距离导航。该方案通过融合全局拓扑路径规划与局部轨迹控制，使机器人能够在规避障碍物的同时导航至对象级子目标。我们通过单目深度估计与可通行性预测实现持续局部轨迹规划，并引入自动切换机制在必要时回退至基线控制器，从而解决了现有方法的关键缺陷。该系统基于基础模型构建，无需领域特异性微调即可实现开放场景的适用性。通过仿真环境与真实场景测试，我们验证了该方法在鲁棒性和部署便利性方面的优势。实验表明本方法优于现有最优方案，为开放场景视觉导航提供了更具适应性的解决方案。源代码已公开：https://github.com/podgorki/TANGO。\n\n（注：根据学术规范要求，对技术术语进行了标准化处理：\n1. \"topometric navigation\"译为\"拓扑导航\"以符合机器人学规范\n2. \"zero-shot\"保留零样本特性但采用更符合中文表达的\"零样本\"\n3. \"foundational models\"译为\"基础模型\"以与AI领域术语保持一致\n4. \"open-set applicability\"译为\"开放场景适用性\"以准确传达其技术内涵\n5. 补充了框架名称TANGO以确保技术完整性）"
    },
    {
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework",
        "url": "http://arxiv.org/abs/2509.08694v1",
        "pub_date": "2025-09-10",
        "summary": "Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "translated": "基于卫星影像的沿海水域分割任务面临光谱特征复杂与边界形态不规则的双重挑战。传统RGB方法在多样化海洋环境中常出现训练不稳定和泛化能力不足的问题。本文提出一种系统性鲁棒增强框架——Robust U-Net，通过引入HSV色彩空间监督与多模态约束机制提升沿海水域分割精度。该框架集成五大协同组件：HSV色彩引导监督、梯度驱动的海岸线优化、形态学后处理、海域净化及连通性控制。综合消融实验表明，HSV监督模块贡献度最高（影响系数0.85），完整框架实现显著训练稳定性提升（方差降低84%）并增强分割质量。本方法在多项评估指标上均取得稳定改进，同时保持计算高效性。为促进可重复性研究，训练配置与代码已开源：https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet。\n\n（注：根据学术规范，对原文中的未来会议年份\"ICASSP-2026\"保持原样呈现，实际应用中需根据论文正式发表年份调整）"
    },
    {
        "title": "FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical\n  Flow Estimation with Total Variation Regularization",
        "url": "http://arxiv.org/abs/2509.08670v1",
        "pub_date": "2025-09-10",
        "summary": "We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.",
        "translated": "我们提出了FractalPINN-Flow——一种无监督深度学习框架，用于直接从连续灰度帧中学习稠密光流估计而无需真实标注数据。该架构的核心是分形变形网络（FDN），这是一个受分形几何和自相似性启发的递归编码器-解码器结构。与传统采用顺序下采样的CNN不同，FDN通过带有跳跃连接的重复编解码器嵌套来同时捕获细粒度细节和长程运动模式。训练目标基于采用全变分（TV）正则化的经典变分公式，具体通过最小化结合$L^1$和$L^2$数据保真项的能量泛函来保证亮度恒定性，并利用TV项增强空间平滑性及流场一致性。在合成数据和基准数据集上的实验表明，FractalPINN-Flow能生成精确、平滑且保持边缘细节的光流场。该模型尤其适用于高分辨率数据和标注有限的场景。\n\n（注：专业术语说明：\n1. Fractal Deformation Network (FDN) 保留英文缩写并译为\"分形变形网络\"\n2. total variation (TV) 采用学界通用译法\"全变分\"并保留英文缩写\n3. skip connections 译为\"跳跃连接\"（CNN中的标准术语）\n4. optical flow 译为\"光流\"（计算机视觉领域标准译法）\n5. encoder-decoder 译为\"编码器-解码器\"（深度学习通用译法））"
    },
    {
        "title": "Skeleton-based sign language recognition using a dual-stream\n  spatio-temporal dynamic graph convolutional network",
        "url": "http://arxiv.org/abs/2509.08661v1",
        "pub_date": "2025-09-10",
        "summary": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "translated": "孤立手语识别（ISLR）面临的核心挑战在于形态相似但语义迥异的手势辨识，这一难题源于手部形状与运动轨迹之间复杂的相互作用。现有方法通常依赖单一参考系，难以解决此类几何模糊性问题。本文提出双参考系双流架构Dual-SignLanguageNet（DSLNet），通过在两个互补坐标系中解耦并分别建模手势形态与运动轨迹。该方法采用腕部中心坐标系实现视角无关的手形分析，结合面部中心坐标系进行上下文感知的轨迹建模。两个分支分别由专用网络处理——采用拓扑感知图卷积网络处理形状特征，基于芬斯勒几何的编码器处理轨迹特征，最终通过几何驱动的最优传输融合机制进行集成。DSLNet在参数量显著少于竞争模型的前提下，在WLASL-100、WLASL-300和LSA64三个高难度数据集上分别达到93.70%、89.97%和99.79%的准确率，创造了新的性能标杆。\n\n（注：译文严格遵循学术论文摘要的规范，对关键术语如\"Finsler geometry\"译为\"芬斯勒几何\"、\"optimal transport\"译为\"最优传输\"等保持专业一致性，同时通过\"解耦\"\"互补坐标系\"\"视角无关\"等措辞准确传达技术细节，最后以数据对比突出创新性。）"
    },
    {
        "title": "X-Part: high fidelity and structure coherent shape decomposition",
        "url": "http://arxiv.org/abs/2509.08643v1",
        "pub_date": "2025-09-10",
        "summary": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.",
        "translated": "在部件级别生成三维形状对于网格重拓扑、UV映射和3D打印等下游应用至关重要。然而，现有的基于部件的生成方法往往缺乏足够的可控性，且存在语义分解质量不佳的问题。为此，我们提出了X-Part——一种可控生成模型，能够将整体三维对象分解为具有高几何保真度的语义明确且结构连贯的部件。该模型采用边界框作为部件生成的提示条件，并通过注入逐点语义特征实现有意义的分解。此外，我们设计了支持交互式部件生成的可编辑流程。大量实验结果表明，X-Part在部件级形状生成任务上达到了最先进的性能。这项工作为创建生产就绪、可编辑且结构合理的三维资产建立了新范式。代码将公开发布以促进学术研究。\n\n（注：根据学术规范，对\"production-ready\"采用\"生产就绪\"的标准技术译法，\"structurally sound\"译为\"结构合理\"以符合工程语境；\"point-wise semantic features\"保留\"逐点语义特征\"的专业表述；\"paradigm\"译为\"范式\"符合计算机领域术语习惯。）"
    },
    {
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and\n  Correct Image Interpretation Model Shortcuts",
        "url": "http://arxiv.org/abs/2509.08640v1",
        "pub_date": "2025-09-10",
        "summary": "Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\\% of cases, correctly incorporated the specified finding in 89-99\\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\\% AUC in internal validation and by 1-11\\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.",
        "translated": "胸部X光片（CXR）是临床最常见的检查项目之一。自动化影像解读有望减轻放射科医生的工作负担并扩大诊断专业知识的可及性。深度学习多任务与基础模型在CXR解读中展现出强大性能，但存在捷径学习缺陷——模型依赖虚假或偏离目标的关联而非临床相关特征进行决策。我们提出RoentMod反事实图像编辑框架，该框架能生成保留原始扫描无关解剖特征、同时包含用户指定合成病变的解剖学真实CXR图像。RoentMod将开源医学图像生成器（RoentGen）与图像到图像修改模型相结合，无需重新训练。经执业放射医师和放射科住院医师参与的阅片研究证实：RoentMod生成图像的真实性获得93%的认可率，89-99%的案例正确融合指定病灶特征，其原生解剖结构保存能力与真实随访CXR相当。通过RoentMod实验，我们发现当前先进的多任务与基础模型频繁利用偏离目标的病变特征作为捷径，限制了模型特异性。在训练过程中加入Roent生成的反事实图像后，模型抗干扰能力显著提升：内部验证中多项病变的判别AUC提升3-19%，外部测试中6类病变有5类提升1-11%。这些发现确立了RoentMod作为探测和修正医疗AI捷径学习的通用工具。通过实现可控的反事实干预，RoentMod增强了CXR解读模型的鲁棒性与可解释性，为改进医学影像基础模型提供了可推广的策略。"
    },
    {
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain\n  Translation",
        "url": "http://arxiv.org/abs/2509.08628v1",
        "pub_date": "2025-09-10",
        "summary": "Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "translated": "扩散模型在生成高质量输出方面表现出色，但在数据稀缺领域面临挑战，通常需要完全重新训练或成本高昂的配对数据。为突破这些限制，我们提出潜在对齐扩散桥（LADB）——一种基于部分配对数据的半监督样本转换框架，能有效弥合领域间差距。该方法通过在共享潜在空间中对齐源域与目标域分布，将预训练的源域扩散模型与目标域潜在对齐扩散模型（LADM）无缝集成，后者通过部分配对的潜在表征进行训练。这种设计实现了无需完全监督的确定性领域映射。与缺乏可控性的非配对方法以及需要大规模领域特定数据集的完全配对方法相比，LADB通过融合配对与非配对潜在目标耦合机制，在保真度与多样性之间取得了平衡。实验结果表明，该方法在部分监督下的深度图到图像转换任务中表现优异。此外，我们将LADB扩展到多源转换（从深度图和分割掩码）以及类别条件风格迁移中的多目标转换任务，展现了其处理多样化和异构用例的灵活性。最终，LADB被证明是一种可扩展且通用的现实领域转换解决方案，尤其适用于数据标注成本高昂或不完整的场景。"
    }
]