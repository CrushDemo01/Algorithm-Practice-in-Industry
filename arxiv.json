[
    {
        "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
        "url": "http://arxiv.org/abs/2509.06902v1",
        "pub_date": "2025-09-08",
        "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \\emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \\emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \\emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.",
        "translated": "大型语言模型（LLMs）作为随机性系统可能生成偏离可用数据的数值，这种错误被称为\"数值幻觉\"。现有防护机制——检索增强生成、引用和不确定性估计——虽能提升透明度，但无法保证数值保真度：虚构或误引的数值仍可能以正确形式呈现。我们提出**可验证数值载体（PCN）**，这是一种通过机械验证确保数值保真度的呈现层协议。在PCN框架下，数值片段以与结构化声明绑定的\"声明约束令牌\"形式输出，验证器根据声明策略（如精确等价、舍入规则、别名系统或带限定条件的容差范围）检查每个令牌。关键创新在于PCN将验证环节置于渲染器而非模型中：只有通过声明验证的数值会被标记为已验证，其余数值默认处于未验证状态。这种分离设计有效防止欺骗行为，并确保故障封闭特性。我们形式化定义了PCN协议，并证明其具备可靠性、诚实令牌下的完备性、故障封闭特性以及策略优化下的单调性。PCN具有轻量化和模型无关特性，可无缝集成至现有应用系统，并能通过密码学承诺进行功能扩展。通过将验证作为数值显示前的强制步骤，PCN为数值敏感场景建立了简明契约：唯有通过验证才能获得信任，而缺乏验证标识则传递不确定性信息。"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们推出了一款名为\"智能快速投放\"(Smart Fast Finish, SFF)的预算调控功能。该功能基于行业标准的快速投放(FF)技术进行优化——传统FF系统会在固定投放周期临近结束时，以最快速度耗尽剩余广告预算。SFF通过分析历史广告活动数据，动态调整系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分割实验数据和离线模拟结果表明，SFF在预算调控过程中能有效缓解超量投放问题，是一种稳健的解决方案。\n\n（注：根据技术文档翻译规范，对以下术语作了标准化处理：\n1. \"budget pacing\"译为\"预算调控\"而非字面意义的\"预算步调\"\n2. \"overdelivery mitigation\"译为\"缓解超量投放\"符合广告技术领域表述\n3. \"throttle rate\"译为\"调控速率\"准确体现系统参数特性\n4. 保留DoorDash等专有名词原文，符合技术文献惯例）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关系，从而阻碍战略决策。为突破这些局限，我们提出KLIPA框架——一种结合知识图谱与大语言模型（LLM）的创新专利分析系统。该框架集成三大核心组件：用于构建专利间显性关系图谱的结构化知识库，揭示上下文关联的检索增强生成（RAG）系统，以及能动态确定最优查询策略的智能代理。我们在真实世界专利数据库上进行验证，结果表明KLIPA在知识提取、新颖关联发现和整体运营效率方面实现显著提升。该技术组合不仅提高了检索精度，降低了对领域专家的依赖，更为科技企业、律所等知识产权管理机构提供了可扩展的自动化解决方案，助力其更好地驾驭战略创新与竞争情报的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v1",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "科学文献的结构化信息提取对于捕捉专业领域的核心概念与新兴趋势至关重要。尽管现有数据集有助于模型开发，但由于领域复杂性和科学文本标注的高成本，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专门针对自然语言处理（NLP）领域全文实体与关系抽取的基准数据集。该数据集包含60篇人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行了对比实验，并评估了前沿监督模型在该数据集上的表现。实验结果表明，现有模型对不同长度学术文本的提取能力存在差异。与现有数据集的交叉对比显示，SciNLP在某些基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点度，表明其蕴含丰富的语义拓扑信息，可有效增强下游应用。数据集已公开于https://github.com/AKADDC/SciNLP。"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）系统需应对简短模糊的用户查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配问题的核心机制，其设计范式因预训练语言模型（PLM）和大语言模型（LLM）的出现发生了显著变革。本综述从三个维度系统梳理该领域：（i）建立查询扩展的四维框架——从扩展点注入方式（显式与隐式QE），经知识 grounding 与交互机制（知识库、模型内部能力、多轮检索）和学习对齐方法，延伸至基于知识图谱的论证；（ii）提出以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释各类模型在QE中的特色能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE技术在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与实施方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域问答/RAG、会话式检索、代码搜索及跨语言场景的应用图谱。研究提炼出知识 grounding 与交互、对齐/蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束三大核心策略，作为解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系以及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下选择和组合QE技术提供了系统化蓝图。"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文本文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂的结构特征要求研究者开发超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大发展时期的方法体系、核心挑战与应用场景。我们系统化地追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层级编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，本文还审视了特定领域应用场景、专项评估资源，并指明了效率权衡、多模态对齐和结果真实性等关键开放挑战。本综述旨在为基座模型时代的长文档检索研究既提供 consolidated 参考框架，又提出前瞻性的发展议程。"
    },
    {
        "title": "CAViAR: Critic-Augmented Video Agentic Reasoning",
        "url": "http://arxiv.org/abs/2509.07680v1",
        "pub_date": "2025-09-09",
        "summary": "Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.",
        "translated": "近年来，视频理解领域取得显著进展，模型在短片段感知任务上的性能持续提升。然而在LVBench、Neptune和ActivityNet-RTL等最新基准测试中，随着查询指令复杂度增加和视频时长增长，需要复杂推理的视频任务性能出现明显下滑。本研究旨在探索：能否利用现有感知能力成功执行更复杂的视频推理？我们开发了一个配备视频处理模块作为子代理或工具的大型语言模型代理系统。与Visual Programming、ViperGPT和MoReVQA等前人工作中固定流程的查询处理方式不同，该代理通过分析每个模块调用的结果动态决定后续执行步骤。受文本推理领域研究启发，我们引入评估器机制来区分代理执行序列的成功与失败案例。实验表明，代理系统与评估器的组合在所述数据集上实现了强劲性能。"
    },
    {
        "title": "Visual Representation Alignment for Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2509.07979v1",
        "pub_date": "2025-09-09",
        "summary": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.",
        "translated": "通过视觉指令调优训练的多模态大语言模型（MLLMs）已在多样任务中展现出强大性能，但在以视觉为中心的任务（如目标计数或空间推理）中仍存在局限。我们将此不足归因于当前主流的纯文本监督范式——该范式仅对视觉通路提供间接指导，往往导致MLLMs在训练过程中丢失细粒度视觉细节。本文提出视觉表征对齐方法（VIRAL），这是一种简单而有效的正则化策略，可将MLLMs的内部视觉表征与预训练视觉基础模型（VFMs）的表征进行对齐。通过显式实施这种对齐，VIRAL不仅使模型能够保留来自输入视觉编码器的关键视觉细节，还能补充来自VFMs的额外视觉知识，从而增强其处理复杂视觉输入的推理能力。实验结果表明，该方法在广泛采用的多模态基准测试的所有任务中均取得持续改进。此外，我们通过全面的消融研究验证了框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。"
    },
    {
        "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation",
        "url": "http://arxiv.org/abs/2509.07978v1",
        "pub_date": "2025-09-09",
        "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: https://gzwsama.github.io/OnePoseviaGen.github.io/",
        "translated": "从单一参考图像准确估计未知物体的六维姿态，对于在现实长尾场景中运行的机器人技术至关重要。然而该设定存在显著挑战：三维模型通常难以获取，单视角重建缺乏公制尺度，生成模型与真实图像间的领域差异会削弱系统鲁棒性。我们提出OnePoseViaGen解决方案，通过两大核心组件应对这些挑战：首先，粗到精对齐模块通过结合多视角特征匹配与渲染比较优化，联合优化尺度与姿态估计；其次，文本引导生成式领域随机化策略通过多样化纹理，实现合成数据对姿态估计器的有效微调。这些技术使高保真单视角三维生成能够支撑可靠的单次六维姿态估计。在YCBInEOAT、Toyota-Light和LM-O等挑战性基准测试中，OnePoseViaGen以显著优势超越现有方法，达到最先进性能。我们进一步通过真实机器人手完成灵巧抓取实验，验证了该方法在现实操作中的实用性。项目页面：https://gzwsama.github.io/OnePoseviaGen.github.io/"
    },
    {
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
        "url": "http://arxiv.org/abs/2509.07969v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
        "translated": "近年来，大型多模态模型通过将基于图像的工具与强化学习相结合，在解决视觉问题方面取得显著进展。然而，现有开源方法通常存在推理模式单一、交互轮次有限的问题，难以胜任需要试错探索的复杂任务。本研究通过扩展基于工具的交互规模解决了这一局限性，提出Mini-o3系统——该系统能够执行深度多轮推理（可达数十个步骤），并在具有挑战性的视觉搜索任务中实现最先进的性能。\n\n我们重现OpenAI o3风格行为的方案包含三个核心组成部分：首先，构建了包含数千个挑战性视觉搜索问题的视觉探测数据集，专门用于探索性推理研究；其次，开发了迭代式数据收集流水线，获取呈现多样化推理模式（包括深度优先搜索、试错法和目标维持）的冷启动轨迹；第三，提出超轮次掩码策略，在强化学习过程中避免对达到最大交互轮次的响应进行惩罚，从而平衡训练效率与测试时的扩展性。尽管训练时仅设定六轮交互的上限，我们的模型在推理阶段能自然生成数十轮交互轨迹，且准确率随轮次增加持续提升。大量实验表明，Mini-o3能产生丰富的推理模式和深度思考路径，有效解决复杂视觉搜索问题。"
    },
    {
        "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
        "url": "http://arxiv.org/abs/2509.07966v1",
        "pub_date": "2025-09-09",
        "summary": "Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.",
        "translated": "对表格等结构化数据的视觉推理是现代视觉语言模型（VLM）的核心能力，然而当前基准测试在规模、多样性和推理深度方面仍存在局限——尤其在处理渲染表格图像时更为明显。为填补这一空白，我们推出Visual-TableQA：一个专为评估和提升复杂表格数据视觉推理能力而构建的大规模开放域多模态数据集。我们的生成流程采用模块化、可扩展的全自动架构，通过多推理大语言模型协同完成生成、验证与启发三类角色。该数据集包含2,500张结构丰富的LaTeX渲染表格和6,000个推理密集型问答对，总成本控制在100美元以内。为提升多样性与创造性，我们通过跨模型提示（\"启发机制\"）和LLM陪审团过滤实现多模型协同数据生成——强模型负责生成布局与主题框架，弱模型进行细节扩展，共同将多样化的推理模式和视觉结构蒸馏到数据集中。实验表明，基于Visual-TableQA微调的模型能稳健泛化至外部基准测试，尽管数据为合成生成，其表现仍超越多个商用模型。完整流程与资源已开源：https://github.com/AI-4-Everyone/Visual-TableQA。"
    },
    {
        "title": "Feature Space Analysis by Guided Diffusion Model",
        "url": "http://arxiv.org/abs/2509.07936v1",
        "pub_date": "2025-09-09",
        "summary": "One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.",
        "translated": "深度神经网络（DNN）的核心问题之一在于其内部特征提取过程的黑箱特性。针对视觉相关领域，本文通过提出一种新型解码器，重点分析DNN的特征空间，该解码器能够生成保证其特征与用户指定特征高度匹配的图像。相较于以往研究中缺失的这种保证机制，我们的解码器通过生成特征与目标特征邻近的图像，可实证揭示图像中哪些属性被DNN编码到特定特征中。该解码器采用引导式扩散模型实现，通过引导预训练扩散模型的反向图像生成过程，逐步最小化每步生成的清晰图像特征与用户指定特征之间的欧氏距离。我们的解码器具有一项实用优势：无需额外训练即可分析不同DNN的特征空间，且仅需在单块商用GPU上运行。针对CLIP图像编码器、ResNet-50和视觉变换器的实验结果表明，解码器生成的图像特征与用户指定特征具有显著相似性，为理解这些DNN的特征空间提供了有价值的见解。"
    },
    {
        "title": "Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space\n  Object",
        "url": "http://arxiv.org/abs/2509.07932v1",
        "pub_date": "2025-09-09",
        "summary": "Characterization of uncooperative Resident Space Objects (RSO) play a crucial role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to assess the geometry and motion properties. To address the challenges of reconstructing tumbling uncooperative targets, this study evaluates the performance of existing state-of-the-art 3D reconstruction algorithms for dynamic scenes, focusing on their ability to generate geometrically accurate models with high-fidelity. To support our evaluation, we developed a simulation environment using Isaac Sim to generate physics-accurate 2D image sequences of tumbling satellite under realistic orbital lighting conditions. Our preliminary results on static scenes using Neuralangelo demonstrate promising reconstruction quality. The generated 3D meshes closely match the original CAD models with minimal errors and artifacts when compared using Cloud Compare (CC). The reconstructed models were able to capture critical fine details for mission planning. This provides a baseline for our ongoing evaluation of dynamic scene reconstruction.",
        "translated": "非合作空间 Resident Space Objects (RSO) 的几何与运动特性表征在在轨服务(On-Orbit Servicing, OOS)和主动碎片清除(Active Debris Removal, ADR)任务中具有关键作用。为解决翻滚类非合作目标三维重建的挑战，本研究系统评估了动态场景下现有最先进三维重建算法的性能，重点关注其生成高精度几何模型的能力。为支撑评估工作，我们基于Isaac Sim开发了仿真环境，在真实轨道光照条件下生成具有物理精确性的翻滚卫星二维图像序列。通过Neuralangelo对静态场景的初步测试显示出卓越的重建质量：生成的三维网格模型与原始CAD模型高度吻合，经Cloud Compare (CC)比对显示误差及伪影极小，能够有效捕捉任务规划所需的关键精细特征。该结果为后续动态场景重建研究建立了性能基准。"
    },
    {
        "title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s",
        "url": "http://arxiv.org/abs/2509.07928v1",
        "pub_date": "2025-09-09",
        "summary": "As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.",
        "translated": "随着本地人工智能日益普及，目标检测器的基准性能与其在消费级硬件上的实际可用性之间存在着关键差距。虽然YOLOv10s等模型承诺实现实时速度，但这些指标通常是在高性能桌面级GPU上达成的。本文揭示在资源受限系统（如配备RTX 4060 GPU的笔记本电脑）上，性能瓶颈并非来自计算能力，而是由系统级瓶颈主导——这一点通过简单的瓶颈测试得到验证。为突破硬件层级的限制，我们提出了一种双通道自适应推理算法，该模型无关方法无需改变网络架构。本研究主要聚焦自适应推理策略，对架构早期退出与分辨率自适应路由进行了对比分析，在统一评估框架中凸显了各自的技术权衡。该系统采用快速低分辨率通道进行初步检测，仅当检测置信度较低时才启动高分辨率模型通道。在5000张图像的COCO数据集测试中，本方法相比PyTorch早期退出基线实现1.85倍加速，仅损失5.51% mAP精度。这项工作通过将焦点从纯模型优化转向硬件感知的推理策略，为在消费级设备上部署高性能实时AI提供了实用且可复现的解决方案，从而实现吞吐量最大化。"
    },
    {
        "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth\n  Segmentation",
        "url": "http://arxiv.org/abs/2509.07923v1",
        "pub_date": "2025-09-09",
        "summary": "Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\\% for CBCT segmentation and 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.",
        "translated": "数字化牙科代表了现代牙科实践的革命性转变。这一转型的基础步骤是获取患者牙列的精确数字化表征，该表征通过分割锥形束计算机断层扫描（CBCT）和口内扫描（IOS）数据获得。尽管数字牙科技术日益受到关注，但现有分割方法普遍缺乏严格验证，且表现出有限的性能和临床适用性。据我们所知，本研究首次提出面向牙齿分割的多模态预训练框架。我们开发了ToothMCL（牙齿多模态对比学习预训练模型），该框架创新性地整合了体积数据（CBCT）和表面数据（IOS）两种模态。通过多模态对比学习捕获模态不变表征，我们的方法能有效建模细粒度解剖特征，实现精确的多类别分割并准确识别国际牙科联合会（FDI）牙齿编号系统。\n\n配合该框架，我们构建了迄今最大的配对CBCT-IOS数据集CBCT-IOS3.8K，包含3,867例患者数据。随后我们在涵盖最全面、最多样化的独立数据集集合上进行了评估，这是迄今为止规模最大且最具多样性的验证。我们的方法在内部和外部测试中均达到最先进性能：CBCT分割的Dice相似系数（DSC）提升12%，IOS分割提升8%。此外，ToothMCL在不同牙组分类中持续超越现有方法，并在不同成像条件和临床场景下展现出强大的泛化能力。\n\n（注：译文严格遵循了以下技术规范：\n1. 专业术语标准化：\"Fédération Dentaire Internationale\"采用国内通用译名\"国际牙科联合会\"并保留FDI缩写\n2. 计量单位规范：完整保留\"Dice Similarity Coefficient (DSC)\"专业术语及百分比数据\n3. 技术概念准确传达：\"modality-invariant representations\"译为\"模态不变表征\"符合机器学习领域术语\n4. 数据呈现方式：3,867患者保持数字间隔规范，百分比数据完整保留\n5. 长句拆分重组：将原文复合句分解为符合中文表达习惯的短句，同时保持技术准确性）"
    },
    {
        "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion",
        "url": "http://arxiv.org/abs/2509.07920v1",
        "pub_date": "2025-09-09",
        "summary": "Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.",
        "translated": "人-物交互联合重建标志着人类理解自身与周边环境复杂互动的重大进展。然而，由于缺乏对人物交互关系的先验认知，传统优化方法往往难以实现物理可信的重建结果。本文提出ScoreHOI——一种基于扩散模型的有效优化器，通过引入扩散先验知识来实现精确的人-物交互重建。该模型利用分数引导采样的可控性，能够根据图像观测和物体特征重建出人体姿态与物体姿态的条件概率分布。在推理过程中，ScoreHOI通过特定物理约束引导去噪过程，有效提升重建质量。此外，我们提出接触驱动的迭代优化方法，以增强接触合理性并提高重建精度。在标准基准测试上的大量实验表明，ScoreHOI性能优于现有最先进方法，突显了其在人-物交互联合重建中实现精准鲁棒提升的卓越能力。"
    },
    {
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.07980v1",
        "pub_date": "2025-09-09",
        "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \\textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.",
        "translated": "并行思维作为一种新兴方法，通过同时探索多重推理路径来增强大语言模型（LLMs）的推理能力。然而，如何通过训练激活这种能力仍存在挑战——现有方法主要依赖基于合成数据的监督微调（SFT），这种方式鼓励教师强制模仿而非自主探索与泛化。与此不同，我们提出首个强化学习（RL）框架\\textbf{Parallel-R1}，能够在复杂现实推理任务中实现并行思维行为。该框架采用渐进式课程学习策略，显式解决RL训练并行思维时的冷启动问题：首先对较简单任务中提示生成的轨迹进行SFT以植入并行思维能力，继而转向RL在更复杂问题上探索和泛化该能力。在MATH、AMC23和AIME等数学基准测试中，Parallel-R1成功实现了并行思维，相比直接在挑战性任务上进行RL训练的序列思维模型，准确率提升8.4%。深入分析表明模型思维行为发生明显转变：早期将并行思维作为探索策略，后期则将其用于多视角验证。最重要的是，我们验证了并行思维可作为\\textbf{训练中期探索支架}——这个临时探索阶段在RL训练后能突破性能上限，在AIME25数据集上相对基线提升42.9%。我们的模型、数据与代码将在https://github.com/zhengkid/Parallel-R1 开源。"
    },
    {
        "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
        "url": "http://arxiv.org/abs/2509.07968v1",
        "pub_date": "2025-09-09",
        "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
        "translated": "我们推出SimpleQA Verified基准测试，这是一个包含1000个提示项的数据集，基于OpenAI的SimpleQA框架专门用于评估大语言模型（LLM）的短文本事实准确性。该基准有效解决了OpenAI原数据集的关键缺陷，包括噪声标签与错误标注、主题偏见及问题冗余等问题。通过实施去重处理、主题平衡和来源核查的严格多阶段筛选流程，我们构建出更可靠且更具挑战性的评估集，同时改进了自动评分提示模板。在这一新基准测试中，Gemini 2.5 Pro以55.6的F1分数刷新业界记录，表现优于包括GPT-5在内的其他前沿模型。此项研究为学术界提供了更高保真度的工具，用于追踪参数模型事实准确性的真实进展并减少幻觉现象。基准数据集、评估代码及排名榜单已发布于：https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified。"
    },
    {
        "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.07925v1",
        "pub_date": "2025-09-09",
        "summary": "Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.",
        "translated": "不确定性估计对于提升大规模语言模型（LLLMs）的可靠性至关重要，尤其是在高风险应用中。现有方法往往忽略语义依赖性，仅依赖词元级概率度量，难以捕捉生成文本的结构化关联。我们提出GENUINE框架——基于图增强的多层次不确定性估计方法，该结构感知框架通过依存解析树和分层图池化技术来优化不确定性量化。通过引入监督学习，GENUINE能有效建模语义与结构关系，从而提升置信度评估效果。在多项自然语言处理任务上的实验表明：GENUINE相较于基于语义熵的方法将AUROC指标提升达29%，校准误差降低超过15%，验证了基于图结构的不确定性建模有效性。代码已开源：https://github.com/ODYSSEYWT/GUQ。"
    },
    {
        "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems",
        "url": "http://arxiv.org/abs/2509.07909v1",
        "pub_date": "2025-09-09",
        "summary": "Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.",
        "translated": "大型语言模型（LLMs）作为经过大规模预训练的模型，已在多个领域取得显著成功。这种成功源于数据和计算复杂度与规模的前所未有的提升。然而，由于训练此类模型的成本高昂，通过暴力试错法来改进LLMs并不可行。受逆问题在揭示基础科学定律方面成功的启发，本立场文件提出：逆问题同样能有效揭示扩展定律，从而指导构建大型语言模型，以显著提升成本效益的方式实现理想性能。"
    },
    {
        "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories",
        "url": "http://arxiv.org/abs/2509.07908v1",
        "pub_date": "2025-09-09",
        "summary": "Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists' attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse.",
        "translated": "故事在人类交流中扮演着关键角色，尤其在塑造儿童信念与道德观方面具有深远影响。随着家长日益依赖大语言模型生成睡前故事，这些叙事中存在的文化与性别刻板印象引发严重关切。为此，我们推出\"偏见童话\"数据集——这是一个旨在分析大语言模型生成故事中偏见如何影响主角属性与故事元素的全新工具。研究发现揭示出显著差异：当主角被设定为女孩时（相较于男孩），外貌相关属性描述暴增55.26%；以非西方儿童为主角的故事过度强调文化遗产、传统和家庭主题，其频次远超西方儿童主角的故事。本研究结果凸显了消除社会文化偏见对于推动创造性人工智能实现公平与多样化发展的重要意义。"
    },
    {
        "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts\n  via Efficient Tuning and Voting-Based Rebalancing",
        "url": "http://arxiv.org/abs/2509.07889v1",
        "pub_date": "2025-09-09",
        "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which focuses on sentence-level gender bias detection and mitigation in Chinese. The task aims to promote fairness and controllability in natural language generation by automatically detecting, classifying, and mitigating gender bias. To address this challenge, we adopt a fine-tuning approach based on large language models (LLMs), efficiently adapt to the bias detection task via Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more balanced training set to alleviate class imbalance and introduce heterogeneous samples from multiple sources to enhance model generalization. For the detection and classification sub-tasks, we employ a majority voting strategy that integrates outputs from multiple expert models to boost performance. Additionally, to improve bias generation detection and mitigation, we design a multi-temperature sampling mechanism to capture potential variations in bias expression styles. Experimental results demonstrate the effectiveness of our approach in bias detection, classification, and mitigation. Our method ultimately achieves an average score of 47.90%, ranking fourth in the shared task.",
        "translated": "本文介绍了我们团队在NLPCC-2025\"中文句子级性别偏见检测与消除\"竞赛任务中的解决方案。该任务旨在通过自动检测、分类和消除性别偏见，促进自然语言生成的公平性与可控性。针对这一挑战，我们采用基于大语言模型(LLM)的微调方法，通过低秩自适应(LoRA)技术高效适配偏见检测任务。在数据处理方面，我们构建了更均衡的训练集以缓解类别不平衡问题，并引入多源异构样本来增强模型泛化能力。针对检测与分类子任务，我们采用集成多个专家模型输出结果的多数据投票策略来提升性能。此外，为改进偏见生成检测与消除效果，我们设计了多温度采样机制以捕捉偏见表达风格的潜在变异。实验结果表明，我们的方法在偏见检测、分类和消除方面均取得显著效果，最终以47.90%的平均得分在评测任务中位列第四。"
    },
    {
        "title": "Are Humans as Brittle as Large Language Models?",
        "url": "http://arxiv.org/abs/2509.07869v1",
        "pub_date": "2025-09-09",
        "summary": "The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.",
        "translated": "大型语言模型（LLM）的输出具有不稳定性，这既源于解码过程的非确定性，也来自提示词的脆弱性。虽然LLM生成过程中固有的非确定性可能通过输出分布的偏移来模拟人类标注中已有的不确定性，但学界普遍假设（尚未验证）提示词脆弱效应是LLM特有的现象。这引出一个关键问题：人类标注者是否对指令变化表现出类似的敏感性？若是如此，LLMs的提示词脆弱性是否应被视为缺陷？另一种假设是：这种脆弱性恰恰正确反映了人类标注的差异性。为填补这一研究空白，我们系统比较了提示词修改对LLMs的影响与相同指令修改对人类标注者的影响，重点探究人类是否对提示词扰动具有相似敏感性。通过设计文本分类任务并控制提示词变量，我们同时向人类和LLMs发出标注请求。研究结果表明：人类和LLMs对特定类型的提示词修改（尤其是替代标签集或标签格式的替换）都表现出更强的脆弱性。然而与LLMs相比，人类判断的分布受拼写错误和标签顺序颠倒的影响显著更小。"
    },
    {
        "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost",
        "url": "http://arxiv.org/abs/2509.07829v1",
        "pub_date": "2025-09-09",
        "summary": "Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.",
        "translated": "文学翻译作为机器翻译研究中一项独特且复杂的任务，近期受到广泛关注。然而，小型开源模型在该领域的翻译效果仍有待突破。我们通过推出TINYFABULIST翻译框架（TF2）为这一研究领域作出贡献——这是一个集数据集构建、微调与评估于一体的英罗文学翻译统一框架，其核心成果是开源发布了紧凑型微调语言模型（TF2-12B）及大规模合成平行数据集（DS-TF2-EN-RO-3M与DS-TF2-EN-RO-15K）。基于迄今最大规模的合成英语寓言数据集DS-TF1-EN-3M（TF1），我们致力于满足罗马尼亚语等低资源语言对高质量文学数据集的迫切需求。\n\n我们的技术路径首先通过高性能大语言模型从TF1数据池中生成1.5万条高质量罗马尼亚语参考译文，随后对120亿参数的开源模型进行两阶段微调：（一）指令微调以捕捉特定文学体裁的叙事风格；（二）适配器压缩以实现高效部署。评估体系结合语料库级BLEU指标与五维大模型评分标准（精确度、流畅度、连贯性、风格保持度、文化适应性），对翻译质量进行多维度精细化评估。结果表明，经微调的模型在流畅性与准确性方面可与顶尖闭源大模型媲美，同时具备开源可及、成本效益显著等优势。\n\n除微调模型与双数据集外，我们同步开源所有脚本及评估提示词。TF2框架由此构建了一个端到端可复现的研究管道，为成本效益化翻译、跨语言叙事生成，以及在低资源环境下推广开源模型处理具有文化重要性的文学内容提供了完整解决方案。"
    },
    {
        "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems",
        "url": "http://arxiv.org/abs/2509.07817v1",
        "pub_date": "2025-09-09",
        "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \\textit{neglect of unstructured review knowledge} and 2) \\textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \\textit{dynamic knowledge type selection} and 2) \\textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type's utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.",
        "translated": "文本响应生成是多模态面向任务对话系统的核心环节，其目标是根据多模态上下文生成恰当的文本响应。尽管现有研究已取得显著进展，但仍存在以下局限：1）忽略非结构化评论知识；2）未充分挖掘大语言模型（LLMs）的潜力。基于此，本文致力于通过LLMs充分融合双重知识（即结构化属性与非结构化评论知识）以提升多模态任务型对话系统中的文本响应生成质量。然而，该任务面临两大关键挑战：1）动态知识类型选择；2）意图-响应解耦。针对这些挑战，我们提出一种新颖的双重知识增强两阶段推理框架DK2R（适配多模态对话系统的LLM增强方案）。具体而言，DK2R首先从外部知识库中提取与对话上下文相关的结构化属性和非结构化评论知识；随后通过生成临时探测响应，利用LLM评估每类知识的效用；此外，DK2R通过专项推理分别总结意图导向的关键线索，并将其作为增强LLM文本响应生成的辅助信号。在公开数据集上的大量实验验证了DK2R的优越性。代码与参数已开源。\n\n（注：本文通过以下技术创新点实现突破：  \n1. 首次系统整合结构化属性与非结构化评论双重知识源  \n2. 设计动态知识选择机制与意图-响应解耦架构  \n3. 构建基于LLM的两阶段推理框架实现知识增强生成）"
    },
    {
        "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning",
        "url": "http://arxiv.org/abs/2509.07768v1",
        "pub_date": "2025-09-09",
        "summary": "The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.",
        "translated": "在线平台中虚假新闻、极化内容、政治偏见及有害信息的传播已成为严峻挑战。随着大语言模型展现出应用潜力，目前尚无研究系统评估其在不同模型架构、使用方法和多语言环境下的性能表现。本研究全面考察了大语言模型在检测高度党派化与虚假新闻、有害推文及政治偏见任务中的适应范式。实验覆盖10个数据集和5种语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），包含二分类与多分类场景。我们测试了从参数高效微调到多种上下文学习策略的全方位方案，包括零样本提示、代码本提示、少样本提示（含随机选取示例和基于行列式点过程的多样性示例选取），以及思维链推理。研究发现：与模型微调相比，上下文学习策略往往表现欠佳。这一核心结论表明，即使在对比Llama3.1-8B-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct等最大规模模型的上下文学习设置中，针对特定任务对较小模型进行微调仍具有显著优势。"
    },
    {
        "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for\n  Medical Texts",
        "url": "http://arxiv.org/abs/2509.07755v1",
        "pub_date": "2025-09-09",
        "summary": "As large language models (LLMs) adapted to sensitive domains such as medicine, their fluency raises safety risks, particularly regarding provenance and accountability. Watermarking embeds detectable patterns to mitigate these risks, yet its reliability in medical contexts remains untested. Existing benchmarks focus on detection-quality tradeoffs, overlooking factual risks under low-entropy settings often exploited by watermarking's reweighting strategy. We propose a medical-focused evaluation workflow that jointly assesses factual accuracy and coherence. Using GPT-Judger and further human validation, we introduce the Factuality-Weighted Score (FWS), a composite metric prioritizing factual accuracy beyond coherence to guide watermarking deployment in medical domains. Our evaluation shows current watermarking methods substantially compromise medical factuality, with entropy shifts degrading medical entity representation. These findings underscore the need for domain-aware watermarking approaches that preserve the integrity of medical content.",
        "translated": "随着大语言模型（LLM）逐渐应用于医疗等敏感领域，其生成流畅性带来了溯源与问责方面的安全隐患。水印技术通过嵌入可检测模式来缓解此类风险，但其在医疗场景中的可靠性尚未得到验证。现有基准主要关注检测质量与文本质量的权衡，却忽视了水印重加权策略常利用的低熵环境下的事实性风险。我们提出一个面向医疗领域的评估框架，同步检测事实准确性与语义连贯性。通过GPT评判器与人工双重验证，创新性提出事实加权分数（FWS）——该复合指标将事实准确性置于连贯性之上，以指导医疗水印技术的部署。实验表明，当前水印方法会显著损害医疗事实准确性，熵值偏移会削弱医疗实体的表征能力。这些发现证明，亟需开发能维护医疗内容完整性的领域自适应水印方案。\n\n（注：译文采用以下专业处理：\n1. \"provenance and accountability\" 译为\"溯源与问责\"以符合医学数据监管术语\n2. \"reweighting strategy\" 译为\"重加权策略\"保留算法特征\n3. \"entropy shifts\" 译为\"熵值偏移\"保持信息论概念准确性\n4. \"domain-aware\" 译为\"领域自适应\"体现机器学习领域术语\n5. 使用\"GPT评判器\"而非直译\"GPT-Judger\"符合中文技术文献表述习惯）"
    },
    {
        "title": "M-BRe: Discovering Training Samples for Relation Extraction from\n  Unlabeled Texts with Large Language Models",
        "url": "http://arxiv.org/abs/2509.07730v2",
        "pub_date": "2025-09-09",
        "summary": "For Relation Extraction (RE), the manual annotation of training data may be prohibitively expensive, since the sentences that contain the target relations in texts can be very scarce and difficult to find. It is therefore beneficial to develop an efficient method that can automatically extract training instances from unlabeled texts for training RE models. Recently, large language models (LLMs) have been adopted in various natural language processing tasks, with RE also benefiting from their advances. However, when leveraging LLMs for RE with predefined relation categories, two key challenges arise. First, in a multi-class classification setting, LLMs often struggle to comprehensively capture the semantics of every relation, leading to suboptimal results. Second, although employing binary classification for each relation individually can mitigate this issue, it introduces significant computational overhead, resulting in impractical time complexity for real-world applications. Therefore, this paper proposes a framework called M-BRe to extract training instances from unlabeled texts for RE. It utilizes three modules to combine the advantages of both of the above classification approaches: Relation Grouping, Relation Extraction, and Label Decision. Extensive experiments confirm its superior capability in discovering high-quality training samples from unlabeled texts for RE.",
        "translated": "在关系抽取（RE）任务中，由于文本中包含目标关系的句子往往非常稀少且难以定位，人工标注训练数据的成本可能极其高昂。因此，开发一种能够从未标注文本中自动提取训练实例以训练RE模型的高效方法具有重要意义。近年来，大语言模型（LLMs）已被广泛应用于各类自然语言处理任务，关系抽取领域也受益于其技术进步。然而，在利用LLMs处理预定义关系类别的RE任务时，存在两个关键挑战：首先，在多类别分类场景下，LLMs往往难以全面捕捉每个关系的语义，导致效果欠佳；其次，虽然对每个关系单独进行二元分类可以缓解这一问题，但会带来显著的计算开销，导致实际应用中的时间复杂度难以承受。为此，本文提出名为M-BRe的框架，通过三个核心模块——关系分组、关系抽取和标签决策——融合上述两种分类方法的优势，从未标注文本中提取RE训练实例。大量实验证实，该框架在从未标注文本中发现高质量RE训练样本方面具有卓越能力。"
    },
    {
        "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using\n  Perspective-Aware Iterative Self-Prompting with LLMs",
        "url": "http://arxiv.org/abs/2509.07622v1",
        "pub_date": "2025-09-09",
        "summary": "Efficient communication between patients and clinicians plays an important role in shared decision-making. However, clinical reports are often lengthy and filled with clinical jargon, making it difficult for domain experts to identify important aspects in the document efficiently. This paper presents the methodology we applied in the MultiClinSUM shared task for summarising clinical case documents. We used an Iterative Self-Prompting technique on large language models (LLMs) by asking LLMs to generate task-specific prompts and refine them via example-based few-shot learning. Furthermore, we used lexical and embedding space metrics, ROUGE and BERT-score, to guide the model fine-tuning with epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P, R, F1) from the official evaluation on 3,396 clinical case reports from various specialties extracted from open journals. The high BERTscore indicates that the model produced semantically equivalent output summaries compared to the references, even though the overlap at the exact lexicon level is lower, as reflected in the lower ROUGE scores. This work sheds some light on how perspective-aware ISP (PA-ISP) can be deployed for clinical report summarisation and support better communication between patients and clinicians.",
        "translated": "患者与临床医生之间的高效沟通在共同决策中具有重要作用。然而临床报告通常篇幅冗长且充满专业术语，使得领域专家难以快速识别文档中的关键信息。本文介绍了我们在MultiClinSUM共享任务中用于临床病例文档摘要生成的方法。我们采用迭代自提示技术（Iterative Self-Prompting）作用于大语言模型（LLMs），通过让LLMs生成任务特定提示并基于示例的少样本学习进行优化。此外，我们结合词汇层面和嵌入空间的评估指标——ROUGE和BERT-score，通过多轮迭代指导模型微调。基于GPT-4和GPT-4o构建的视角感知ISP系统在来自开放期刊的3,396份多学科临床病例报告官方评估中，获得了ROUGE（46.53, 24.68, 30.77）和BERTscore（87.84, 83.25, 85.46）的（精确率、召回率、F1值）评分。较高的BERTscore表明模型生成的摘要与参考摘要具有语义等效性，尽管如较低ROUGE分数所反映的，在具体词汇重叠层面相对较低。这项工作揭示了视角感知ISP（PA-ISP）技术在临床报告摘要生成中的应用潜力，可为改善医患沟通提供支持。\n\n（注：专业术语说明：\n1. Iterative Self-Prompting：迭代自提示技术，一种通过模型自我生成并优化提示的方法\n2. Few-shot learning：少样本学习，使用少量示例训练模型的技术\n3. Lexical and embedding space metrics：词汇与嵌入空间评估指标\n4. Perspective-aware ISP：视角感知的迭代自提示技术\n5. 评估指标保留英文原名，括号内数字分别对应精确率(Precision)、召回率(Recall)和F1值）"
    },
    {
        "title": "BALI: Enhancing Biomedical Language Representations through Knowledge\n  Graph and Language Model Alignment",
        "url": "http://arxiv.org/abs/2509.07588v1",
        "pub_date": "2025-09-09",
        "summary": "In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.",
        "translated": "近年来，在提升生物医学文本理解的一系列任务中，预训练语言模型（LMs）的应用取得了显著进展。然而，现有的生物医学大语言模型对领域内复杂概念结构及生物医学知识图谱（KGs）中事实信息的理解仍存在局限。本研究提出BALI（生物医学知识图谱与语言模型对齐）——一种新颖的联合语言模型与知识图谱预训练方法，通过同步训练专用图谱编码器并实现语言模型与图谱表征的对齐，将外部知识注入语言模型。针对给定文本序列，我们将生物医学概念指称关联至统一医学语言系统（UMLS）知识图谱，并利用局部图谱子图作为这些指称的跨模态正样本。实证研究表明：在PubMedBERT和BioLinkBERT等主流生物医学语言模型上应用本方法后，仅需使用源自PubMed科学摘要的小型对齐数据集进行极少量预训练，即可提升模型在多项语言理解任务上的表现，并显著改善实体表征质量。"
    },
    {
        "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with\n  Guided Decomposition",
        "url": "http://arxiv.org/abs/2509.07555v1",
        "pub_date": "2025-09-09",
        "summary": "In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of \"edit skipping\", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.",
        "translated": "在信息快速迭代的时代，大语言模型（LLM）中的知识极易过时。重新训练LLM成本高昂且不具效益，因此无需修改参数的知识编辑（KE）方法显得尤为重要。我们发现，尽管现有基于检索增强生成（RAG）的KE方法在简单知识编辑方面表现优异，但在处理多跳问答任务时，由于存在\"编辑跳过\"现象（即在推理过程中跳过相关已编辑事实），其效果受限。编辑跳过的产生不仅源于知识自然语言表达的多样性，更源于LLM解决问题时的事实粒度与编辑记忆库中事实的错配。针对这一问题，我们提出了一种基于引导分解的迭代检索增强知识编辑方法（IRAKE），通过单条编辑事实和完整编辑案例的双重引导实现知识迭代检索。实验结果表明，IRAKE有效缓解了因编辑跳过导致的编辑失败问题，在多跳问答的知识编辑任务上超越了现有最优方法。\n\n（注：专业术语说明：\n1. Knowledge Editing (KE)：知识编辑\n2. Retrieval-Augmented Generation (RAG)：检索增强生成\n3. Multi-hop Question Answering：多跳问答（需通过多次推理步骤才能得出答案的复杂问答任务）\n4. Edit Skipping：编辑跳过（模型在推理时忽略已编辑知识的现象）\n5. Granularity：粒度（指知识单元的细化程度）\n6. State-of-the-art：现有最优/最先进方法）"
    },
    {
        "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for\n  Trustworthy OS Agents",
        "url": "http://arxiv.org/abs/2509.07553v1",
        "pub_date": "2025-09-09",
        "summary": "With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at https://github.com/Wuzheng02/VeriOS.",
        "translated": "随着多模态大语言模型的快速发展，操作系统（OS）智能体通过设备端图形用户界面（GUI）实现任务自动化的能力日益增强。然而，现有大多数OS智能体基于理想化场景设计，而真实环境往往存在不可信条件。为降低此类场景中的过度执行风险，我们提出一种查询驱动的人-智能体-图形界面交互框架，使OS智能体能够自主决策何时需要向人类查询以确保任务可靠完成。基于该框架，我们开发了VeriOS-Agent——一个采用两阶段学习范式训练的可信OS智能体，该范式通过元知识解耦与利用提升性能。具体而言，VeriOS-Agent在正常条件下自主执行操作，而在不可信场景中主动向人类发起查询。实验表明，在不可信场景中VeriOS-Agent相比最先进方法将平均步骤成功率提升20.64%，且不影响正常场景性能。分析结果凸显了该智能体的合理性、泛化性与可扩展性。代码、数据集及模型已开源：https://github.com/Wuzheng02/VeriOS。\n\n（注：译文严格遵循以下技术规范：\n1. \"multimodal large language models\"译为专业术语\"多模态大语言模型\"\n2. \"over-execution\"根据上下文意译为\"过度执行风险\"而非字面直译\n3. \"two-stage learning paradigm\"保留技术特征译为\"两阶段学习范式\"\n4. \"step-wise success rate\"准确译为\"步骤成功率\"以体现量化指标\n5. 长难句采用分译法处理，如将\"decoupling and utilization of meta-knowledge\"拆解为\"元知识解耦与利用\"\n6. 被动语态转换为中文主动句式，如\"are designed for\"译为\"基于...设计\"\n7. 保持技术表述准确性，如\"query-driven\"译为\"查询驱动\"，\"state-of-the-art\"译为\"最先进方法\"）"
    },
    {
        "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage\n  Training on Public Data",
        "url": "http://arxiv.org/abs/2509.07526v1",
        "pub_date": "2025-09-09",
        "summary": "Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data.",
        "translated": "大型语言模型（LLMs）虽已革新自然语言处理领域，但其与音频模态的融合仍待深入探索——尽管音频在人类交流中具有核心地位。我们推出Falcon3-Audio系列音频-语言模型（ALMs），其架构基于指令微调的大型语言模型和Whisper编码器。仅使用极少量公开音频数据（不足3万小时，含5千条独特数据），Falcon3-Audio-7B即在MMAU基准测试中取得64.14分的成绩，与最优开源模型R1-AQA性能持平，同时展现出显著的数据与参数效率优势、单阶段训练特性及模型透明度。值得注意的是，我们最小的1B参数模型仍可与参数量达2B至13B的大型开源模型竞争。通过大量消融实验发现，即使与使用超过50万小时数据训练的模型相比，常规复杂设计——如课程学习、多重音频编码器、精密交叉注意力连接器——对于实现强劲性能并非必需。\n\n（注：译文严格遵循了以下技术要点：\n1. 专业术语准确度：\"instruction-tuned LLMs\"译为\"指令微调的大型语言模型\"，\"cross-attention connectors\"译为\"交叉注意力连接器\"\n2. 数据规格表述：保留\"30K hours (5K unique)\"的精确数值特征，采用\"3万小时（含5千条独特数据）\"符合中文计量习惯\n3. 技术概念传达：\"single-stage training\"译为\"单阶段训练\"而非直译\"单阶段培训\"\n4. 学术语境适配：使用\"消融实验\"而非\"消除实验\"，\"基准测试\"而非\"基准测验\"\n5. 逻辑关系显化：通过\"即使与...相比\"等连接词明确原文隐含的对比关系）"
    }
]