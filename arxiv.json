[
    {
        "title": "Soundtracks of Our Lives: How Age Influences Musical Preferences",
        "url": "http://arxiv.org/abs/2509.08337v1",
        "pub_date": "2025-09-10",
        "summary": "The majority of research in recommender systems, be it algorithmic improvements, context-awareness, explainability, or other areas, evaluates these systems on datasets that capture user interaction over a relatively limited time span. However, recommender systems can very well be used continuously for extended time. Similarly so, user behavior may evolve over that extended time. Although media studies and psychology offer a wealth of research on the evolution of user preferences and behavior as individuals age, there has been scant research in this regard within the realm of user modeling and recommender systems. In this study, we investigate the evolution of user preferences and behavior using the LFM-2b dataset, which, to our knowledge, is the only dataset that encompasses a sufficiently extensive time frame to permit real longitudinal studies and includes age information about its users. We identify specific usage and taste preferences directly related to the age of the user, i.e., while younger users tend to listen broadly to contemporary popular music, older users have more elaborate and personalized listening habits. The findings yield important insights that open new directions for research in recommender systems, providing guidance for future efforts.",
        "translated": "当前推荐系统领域的大多数研究——无论是算法改进、上下文感知、可解释性还是其他方向——均基于有限时间跨度的用户交互数据集进行评估。然而，推荐系统实际往往需要长期持续运行，用户行为也可能随时间推移产生演变。尽管媒体研究和心理学领域对用户偏好随年龄演变的规律已有丰富成果，但在用户建模与推荐系统领域中，针对这一问题的研究仍十分匮乏。本研究采用LFM-2b数据集（据我们所知，这是唯一兼具足够长时间跨度支持真实纵向研究、且包含用户年龄信息的数据集）深入探究用户偏好与行为的演变规律。我们发现：年轻用户倾向于广泛收听当代流行音乐，而年长用户则展现出更精细化、个性化的收听习惯——这种使用偏好与品味特征与用户年龄存在直接关联。这些发现为推荐系统研究开辟了新方向，为后续研究提供了重要指导。"
    },
    {
        "title": "Vector embedding of multi-modal texts: a tool for discovery?",
        "url": "http://arxiv.org/abs/2509.08216v1",
        "pub_date": "2025-09-10",
        "summary": "Computer science texts are particularly rich in both narrative content and illustrative charts, algorithms, images, annotated diagrams, etc. This study explores the extent to which vector-based multimodal retrieval, powered by vision-language models (VLMs), can improve discovery across multi-modal (text and images) content. Using over 3,600 digitized textbook pages largely from computer science textbooks and a Vision Language Model (VLM), we generate multi-vector representations capturing both textual and visual semantics. These embeddings are stored in a vector database. We issue a benchmark of 75 natural language queries and compare retrieval performance to ground truth and across four similarity (distance) measures. The study is intended to expose both the strengths and weakenesses of such an approach. We find that cosine similarity most effectively retrieves semantically and visually relevant pages. We further discuss the practicality of using a vector database and multi-modal embedding for operational information retrieval. Our paper is intended to offer design insights for discovery over digital libraries.   Keywords: Vector embedding, multi-modal document retrieval, vector database benchmark, digital library discovery",
        "translated": "计算机科学文献通常兼具丰富的叙述性内容与说明性图表、算法、图像及带标注的图示等。本研究探讨基于视觉语言模型（VLM）的向量多模态检索在提升跨文本与图像的多模态内容发现能力方面的效果。通过使用超过3,600页主要来自计算机科学教材的数字化页面及视觉语言模型，我们生成了能同时捕获文本与视觉语义的多向量表征。这些嵌入向量被存储于向量数据库中。我们发布了包含75个自然语言查询的基准测试集，并将检索效果与人工标注真值进行对比，同时评估了四种相似度（距离）度量方法的性能。本研究旨在揭示此类方法的优势与局限性。研究发现，余弦相似度能够最有效地检索出语义和视觉相关性较高的页面。我们进一步讨论了使用向量数据库与多模态嵌入在实际信息检索操作中的可行性。本文旨在为数字图书馆的文献发现机制提供设计参考。  \n关键词：向量嵌入，多模态文档检索，向量数据库基准测试，数字图书馆发现"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们提出了一种名为\"智能快速完成\"（Smart Fast Finish, SFF）的预算调控功能。该功能基于行业标准的快速完成（Fast Finish, FF）技术进行优化——在固定时间段末期将剩余广告预算以最快速度消耗完毕。SFF通过分析历史广告活动数据，动态更新系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分流实验数据和离线仿真结果表明，SFF是一种能有效缓解预算调控过程中超量投放问题的稳健解决方案。\n\n（注：根据学术论文摘要的规范要求，译文采用以下处理：\n1. 专业术语统一：\"budget pacing\"译为\"预算调控\"，\"overdelivery mitigation\"译为\"缓解超量投放\"\n2. 技术概念准确传达：\"dynamic updates\"译为\"动态更新\"，\"throttle rate\"译为\"调控速率\"\n3. 企业名称保留原文：\"DoorDash\"不译\n4. 研究论证表述规范：\"online/offline\"译为\"在线/离线\"，\"robust solution\"译为\"稳健解决方案\"\n5. 保持学术文本的客观性，避免口语化表达）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关联，从而阻碍战略决策。为突破这些局限，我们提出KLIPA这一创新框架，该框架通过结合知识图谱和大语言模型（LLM）显著推进专利分析能力。我们的方法整合了三个核心组件：用于构建专利间显性关联的结构化知识图谱、用于挖掘上下文联系的检索增强生成（RAG）系统，以及能动态确定最优解策策略的智能代理。我们在真实世界的全量专利数据库上验证了KLIPA，其在知识提取、新颖关联发现和整体操作效率方面均展现出显著提升。这种技术融合增强了检索精度，降低了对领域专家的依赖，为包括科技企业和律师事务所在内的知识产权管理机构提供了可扩展的自动化解决方案，使其能更好地应对战略创新与竞争情报领域的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v2",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "从科学文献中抽取结构化信息对于捕捉专业领域的核心概念与新兴趋势至关重要。虽然现有数据集有助于模型开发，但由于领域复杂性和科学文本标注成本高昂，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专为自然语言处理（NLP）领域设计的全文实体与关系抽取基准数据集。该数据集包含60篇经人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行对比实验，并评估了前沿监督模型在该数据集上的表现。结果显示现有模型对不同长度学术文本的抽取能力存在显著差异。与现有数据集的交叉对比表明，SciNLP在部分基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点连接度，展现出丰富的语义拓扑信息，可有效增强下游应用性能。数据集已公开于https://github.com/AKADDC/SciNLP。\n\n（注：译文严格遵循学术论文表述规范，对\"entity and relation extraction\"采用\"实体与关系抽取\"标准译法，\"knowledge graph\"译为\"知识图谱\"，\"node degree\"译为\"节点连接度\"等专业术语均符合计算机领域中文表达惯例。长难句如\"Results reveal varying...\"通过拆分重组转化为符合中文表达习惯的短句，同时保持逻辑严谨性。）"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）需要弥合简短模糊的查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配的关键机制，其设计范式已因预训练语言模型（PLM）和大语言模型（LLM）发生显著转变。本综述从三个维度系统梳理该领域：（i）提出查询扩展的四维框架——从注入方式（显式/隐式QE）出发，贯穿 grounding 与交互机制（知识库、模型内部能力、多轮检索）、学习对齐策略，直至基于知识图谱的论证；（ii）建立以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释其特有的QE能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域QA/RAG、会话式搜索、代码检索及跨语言场景的应用图谱。研究提炼出三大核心设计原则：基于 grounding 的交互机制、对齐与蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束——这些被证明是解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下QE技术的选择与组合提供了系统化蓝图。\n\n（注：术语处理说明：\n- grounding 保留英文，因中文尚无统一译法且该术语在AI领域常直接使用\n- SFT/PEFT/DPO 为技术缩写（全称：Supervised Fine-Tuning/Parameter-Efficient Fine-Tuning/Direct Preference Optimization）\n- QA/RAG 等缩写已在领域内广泛采用\n- 保持\"零样本/少样本\"等标准译法以符合技术文献惯例）"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂结构要求采用超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大技术演进阶段的方法体系、核心挑战与应用实践。我们系统化追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层次化编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，我们还审视了特定领域应用场景、专项评估资源，并指出效率权衡、多模态对齐和结果可信度等关键开放挑战。本综述旨在为基础模型时代的长文档检索研究提供系统化参考框架与前瞻性发展路线图。"
    },
    {
        "title": "Towards End-to-End Model-Agnostic Explanations for RAG Systems",
        "url": "http://arxiv.org/abs/2509.07620v1",
        "pub_date": "2025-09-09",
        "summary": "Retrieval Augmented Generation (RAG) systems, despite their growing popularity for enhancing model response reliability, often struggle with trustworthiness and explainability. In this work, we present a novel, holistic, model-agnostic, post-hoc explanation framework leveraging perturbation-based techniques to explain the retrieval and generation processes in a RAG system. We propose different strategies to evaluate these explanations and discuss the sufficiency of model-agnostic explanations in RAG systems. With this work, we further aim to catalyze a collaborative effort to build reliable and explainable RAG systems.",
        "translated": "尽管检索增强生成（RAG）系统在提升模型响应可靠性方面日益普及，但其可信度与可解释性仍面临挑战。本研究提出了一种新颖的、整体性的、模型无关的事后解释框架，该框架基于扰动技术来解释RAG系统中的检索与生成过程。我们提出了多种策略来评估这些解释的有效性，并探讨了模型无关解释在RAG系统中的充分性。通过此项研究，我们旨在推动学界与业界共同努力，构建更可靠、更可解释的RAG系统。\n\n（注：译文严格遵循了以下技术细节处理：\n1. \"post-hoc explanation\"译为\"事后解释\"（而非\"事后诸葛亮式解释\"），符合机器学习可解释性领域的术语规范\n2. \"perturbation-based techniques\"译为\"扰动技术\"，准确反映通过对输入进行微小扰动来评估模型敏感度的技术本质\n3. \"model-agnostic\"统一译为\"模型无关\"，保持与机器学习领域术语的一致性\n4. 使用\"可解释性\"而非\"解释性\"，符合人工智能透明度研究领域的标准译法\n5. 保留RAG、LLM等专业术语的英文缩写形式，确保学术严谨性）"
    },
    {
        "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2509.07594v1",
        "pub_date": "2025-09-09",
        "summary": "Click-through rate (CTR) prediction plays an important role in online advertising systems. On the one hand, traditional CTR prediction models capture the collaborative signals in tabular data via feature interaction modeling, but they lose semantics in text. On the other hand, Large Language Models (LLMs) excel in understanding the context and meaning behind text, but they face challenges in capturing collaborative signals and they have long inference latency. In this paper, we aim to leverage the benefits of both types of models and pursue collaboration, semantics and efficiency. We present ELEC, which is an Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for the CTR prediction task. In order to leverage the ability of the LLM but simultaneously keep efficiency, we utilize the pseudo-siamese network which contains a gain network and a vanilla network. We inject the high-level representation vector generated by the LLM into a collaborative CTR model to form the gain network such that it can take advantage of both tabular modeling and textual modeling. However, its reliance on the LLM limits its efficiency. We then distill the knowledge from the gain network to the vanilla network on both the score level and the representation level, such that the vanilla network takes only tabular data as input, but can still generate comparable performance as the gain network. Our approach is model-agnostic. It allows for the integration with various existing LLMs and collaborative CTR models. Experiments on real-world datasets demonstrate the effectiveness and efficiency of ELEC for CTR prediction.",
        "translated": "点击率（CTT）预测在在线广告系统中具有重要作用。一方面，传统CTR预测模型通过特征交互建模捕捉表格数据中的协同信号，但会丢失文本语义信息；另一方面，大语言模型（LLM）擅长理解文本背后的上下文和语义，但在捕捉协同信号方面存在局限且推理延迟较高。本文旨在融合两类模型的优势，实现协同性、语义理解与效率的平衡。我们提出ELEC框架——一种高效的大语言模型赋能CTR预测方案。首先针对CTR预测任务对大语言模型进行适配，为兼顾模型能力与效率，采用包含增益网络和基准网络的双伪孪生网络结构。通过将LLM生成的高层表征向量注入协同CTR模型形成增益网络，使其能同时利用表格建模和文本建模的优势。但该网络对LLM的依赖会影响效率，因此我们通过分数级和表征级蒸馏将增益网络的知识迁移至仅需输入表格数据的基准网络，使其在保持高效的同时达到与增益网络相当的性能。本方法具备模型无关性，可与多种现有LLM及协同CTR模型集成。真实场景数据集上的实验验证了ELEC在CTR预测中的有效性与高效性。"
    },
    {
        "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of\n  Scientific Documents",
        "url": "http://arxiv.org/abs/2509.07531v1",
        "pub_date": "2025-09-09",
        "summary": "Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.",
        "translated": "科学文献表示学习为各类任务提供了强大的嵌入向量，但现有方法面临三大挑战：1）基于引文结构信号的对比训练未能充分利用引文信息，且仍生成单一向量表示；2）细粒度表示学习虽能生成句子或方面级的多向量表示，但需要昂贵的人工整合且缺乏领域泛化能力；3）任务感知学习依赖人工预定义的任务分类，忽略了细微的任务差异，且需为特定任务模块提供额外训练数据。针对这些问题，我们提出统一三种范式的新方法FLeW以获取更优表示。具体而言，我们设计了一种新型三元组采样方法，通过引文意图（背景、方法、结果）和引用频次增强引文结构信号训练——引文意图与科学写作通用结构相契合，可为细粒度表示学习提供领域泛化的方面划分。随后采用简易权重搜索机制，无需任务感知微调即可自适应整合三个方面级嵌入形成任务适配的文档表示。实验表明，相较于现有模型，FLeW在多个科学任务和学科领域均展现出卓越的适用性与鲁棒性。"
    },
    {
        "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval",
        "url": "http://arxiv.org/abs/2509.07512v1",
        "pub_date": "2025-09-09",
        "summary": "Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\\%-10\\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.",
        "translated": "在化学与材料科学等自然科学的当代数据驱动研究中，大规模高性能的实体识别已成为科学数据集处理的关键需求。大型语言模型（LLMs）正被日益广泛地应用于实体识别任务，这一趋势也体现在全谱系自然语言处理任务中。当前主流的实体识别大模型依赖于微调技术，但微调过程往往伴随着高昂成本。为实现性能与成本的最优平衡，我们提出ALLabel——一个三阶段框架，通过三种主动学习策略的序列化应用，从标注数据中筛选信息量最大且最具代表性的样本，用于构建大模型上下文学习所需的真实标注检索库。在三个专业领域数据集上，ALLabel在相同标注预算下持续超越所有基线模型。实验结果表明：使用ALLabel仅需标注5%-10%的数据量即可达到全量标注方法的性能水平。进一步的解析与消融研究验证了该方案的有效性和泛化能力。\n\n（注：专业术语说明：\n1. entity recognition：实体识别\n2. large language models (LLMs)：大型语言模型\n3. fine-tuned technology：微调技术\n4. active learning strategies：主动学习策略\n5. in-context learning：上下文学习\n6. ablation studies：消融研究\n7. annotation budget：标注预算\n8. ground-truth retrieval corpus：真实标注检索库）"
    },
    {
        "title": "Multi-view-guided Passage Reranking with Large Language Models",
        "url": "http://arxiv.org/abs/2509.07485v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large language models (LLMs) have shown impressive performance in passage reranking tasks. Despite their success, LLM-based methods still face challenges in efficiency and sensitivity to external biases. (1) Existing models rely mostly on autoregressive generation and sliding window strategies to rank passages, which incur heavy computational overhead as the number of passages increases. (2) External biases, such as position or selection bias, hinder the model's ability to accurately represent passages and increase input-order sensitivity. To address these limitations, we introduce a novel passage reranking model, called Multi-View-guided Passage Reranking (MVP). MVP is a non-generative LLM-based reranking method that encodes query-passage information into diverse view embeddings without being influenced by external biases. For each view, it combines query-aware passage embeddings to produce a distinct anchor vector, which is then used to directly compute relevance scores in a single decoding step. In addition, it employs an orthogonal loss to make the views more distinctive. Extensive experiments demonstrate that MVP, with just 220M parameters, matches the performance of much larger 7B-scale fine-tuned models while achieving a 100x reduction in inference latency. Notably, the 3B-parameter variant of MVP achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks. The source code is available at: https://github.com/bulbna/MVP",
        "translated": "近年来，大语言模型（LLM）在段落重排序任务中展现出卓越性能。尽管成效显著，基于LLM的方法仍面临效率问题与外部偏差敏感性两大挑战：（1）现有模型主要依赖自回归生成和滑动窗口策略进行段落排序，随着段落数量增加会产生巨大计算开销；（2）位置偏差和选择偏差等外部因素会干扰模型对段落的准确表征，并增强对输入顺序的敏感性。为解决这些局限性，我们提出了一种新型段落重排序模型——多视角引导段落重排序（MVP）。该非生成式LLM重排序方法通过将查询-段落信息编码为多视角嵌入向量，有效规避外部偏差影响。针对每个视角，模型融合查询感知的段落嵌入生成独特锚点向量，进而通过单步解码直接计算相关性得分。此外，该方法采用正交损失函数以增强视角区分度。大量实验表明，仅需2.2亿参数的MVP模型在实现推理延迟降低100倍的同时，性能可媲美70亿参数规模的精调模型。特别值得注意的是，30亿参数版本的MVP在领域内和领域外基准测试中均达到了最先进的性能水平。源代码已开源：https://github.com/bulbna/MVP\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语统一处理：\"autoregressive generation\"译为\"自回归生成\"，\"orthogonal loss\"译为\"正交损失函数\"\n2. 数量单位规范：\"220M/7B\"转换为\"2.2亿/70亿\"符合中文计量习惯\n3. 技术概念准确传达：\"view embeddings\"意译为\"视角嵌入向量\"而非字面直译\n4. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n5. 被动语态转化：\"are used to\"转换为主动语态\"通过...实现\"\n6. 学术表达规范：\"state-of-the-art\"译为\"最先进的\"符合学术中文惯例）"
    },
    {
        "title": "MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for\n  Neural Recommendation Models",
        "url": "http://arxiv.org/abs/2509.07319v1",
        "pub_date": "2025-09-09",
        "summary": "Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance.",
        "translated": "神经协同过滤模型在推荐系统中应用广泛，但通常基于静态设定进行训练，假设数据分布固定不变。这限制了其在用户偏好动态演变环境中的适用性。增量学习虽提供了可行方案，但来自计算机视觉或自然语言处理领域的传统方法因数据稀疏性和任务范式差异，在推荐任务中面临挑战。现有神经推荐器的增量学习方法仍存在局限，且普遍缺乏泛化能力。为此，我们提出MEGG（基于极端GG分数的回放样本）——一种基于经验回放的增量学习框架。MEGG创新性地引入GGscore指标，通过量化样本影响力来实现高效选择性样本回放，从而有效缓解灾难性遗忘问题。该框架具备模型无关特性，可无缝集成至不同架构与框架。在三种神经模型和四个基准数据集上的实验表明，其性能显著优于现有最优基线方法，并展现出强大的可扩展性、高效性和鲁棒性。代码实现将在论文录用后开源发布。\n\n（注：GGscore保留英文大写形式，符合技术术语惯例；\"catastrophic forgetting\"译为专业术语\"灾难性遗忘\"；\"model-agnostic\"采用通用译法\"模型无关\"；\"state-of-the-art\"译为\"现有最优\"符合学术语境）"
    },
    {
        "title": "Datasets for Navigating Sensitive Topics in Recommendation Systems",
        "url": "http://arxiv.org/abs/2509.07269v1",
        "pub_date": "2025-09-08",
        "summary": "Personalized AI systems, from recommendation systems to chatbots, are a prevalent method for distributing content to users based on their learned preferences. However, there is growing concern about the adverse effects of these systems, including their potential tendency to expose users to sensitive or harmful material, negatively impacting overall well-being. To address this concern quantitatively, it is necessary to create datasets with relevant sensitivity labels for content, enabling researchers to evaluate personalized systems beyond mere engagement metrics. To this end, we introduce two novel datasets that include a taxonomy of sensitivity labels alongside user-content ratings: one that integrates MovieLens rating data with content warnings from the Does the Dog Die? community ratings website, and another that combines fan-fiction interaction data and user-generated warnings from Archive of Our Own.",
        "translated": "个性化人工智能系统（从推荐系统到聊天机器人）已成为根据用户学习偏好分发内容的普遍方式。然而，人们日益关注这些系统的负面影响，特别是其可能使用户接触敏感或有害内容，从而对整体福祉产生负面影响的倾向。为量化评估这一问题，需要构建带有内容敏感度标签的数据集，使研究者能够超越简单的参与度指标来评估个性化系统。为此，我们引入了两个新型数据集：一个将MovieLens评分数据与Does the Dog Die?社区评级网站的内容警示标签体系相结合，另一个整合了Archive of Our Own平台的同人小说互动数据与用户生成的警示标签。这些数据集不仅包含用户-内容评分，还提供了系统化的敏感度分类标注。"
    },
    {
        "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
        "url": "http://arxiv.org/abs/2509.07253v1",
        "pub_date": "2025-09-08",
        "summary": "Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.",
        "translated": "大型语言模型（LLM）是处理文本任务的卓越多功能工具，其催生了无数前所未有的应用场景。相比之下，检索模型领域尚未出现具备同等通用能力的模型。要实现这一目标，检索模型必须能够处理复杂检索任务——即查询语句包含多组成部分、约束条件或自然语言需求的场景。这类任务代表着对现有主流评估集中普遍采用的简单单维度查询的自然演进。随着用户期望搜索系统能处理更具体且更具挑战性的信息请求（正如基于LLM的信息系统的使用方式所展现的），复杂查询需求应运而生。\n\n尽管业界对检索模型拓展复杂检索能力的呼声日益高涨，但目前缺乏能够全面评估检索模型在多样化复杂任务上表现的标准资源。现有少数评估资源不仅覆盖范围有限，且往往缺乏真实场景设置，导致难以准确衡量检索模型在现实复杂检索任务中的真实能力。\n\n为弥补这一缺陷并推动下一代检索模型的发展，我们构建了具有多样性和真实性的复杂检索任务集，并对代表性前沿检索模型进行基准测试。此外，我们还探究了基于LLM的查询扩展与重写技术对检索质量的影响。实验结果表明：即使最优模型在复杂检索任务中也表现挣扎，所有任务的平均nDCG@10最高仅达0.346，R@100最高仅为0.587。虽然LLM增强技术能提升较弱模型的性能，但所有重写技术都会导致最强模型的全指标性能下降。"
    },
    {
        "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning\n  Intensive Retrieval",
        "url": "http://arxiv.org/abs/2509.07163v1",
        "pub_date": "2025-09-08",
        "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations: they are constrained by the initial retrieval quality of the top-k documents, and the growing computational demands of LLM-based rerankers restrict the number of documents that can be effectively processed. We introduce Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations by directly retrieving documents according to reranker preferences rather than following the traditional sequential reranking method. Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity. Experimental results demonstrate substantial performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9 on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100 documents. Our analysis suggests that, given a fixed pair of embedding and reranker models, strategically selecting documents to rerank can significantly improve retrieval accuracy under limited reranker budget.",
        "translated": "当前广泛使用的“检索-重排序”流程面临两个关键局限：其性能受限于前k篇文档的初始检索质量，且基于大语言模型的重排序器计算需求日益增长，限制了可有效处理的文档数量。我们提出了一种创新方法——重排序器引导搜索（Reranker-Guided-Search, RGS），通过直接根据重排序器的偏好检索文档（而非遵循传统的顺序重排序流程）来突破这些限制。该方法在近似最近邻算法生成的邻近图上进行贪婪搜索，基于文档相似性策略性地优先选择有潜力的文档进行重排序。实验结果表明，在多个基准测试中均取得显著性能提升：BRIGHT数据集提升3.5个点，FollowIR提升2.9个点，M-BEIR提升5.1个点——且所有这些改进均在100篇文档的重排序计算预算内实现。我们的分析表明，在固定嵌入模型和重排序器组合的前提下，通过策略性选择待重排序文档，能够在有限计算资源下显著提升检索精度。\n\n（注：专业术语说明：\n1. retrieve-and-rerank pipeline：译为\"检索-重排序流程\"\n2. LLM-based rerankers：译为\"基于大语言模型的重排序器\"\n3. proximity graphs：译为\"邻近图\"\n4. approximate nearest neighbor：译为\"近似最近邻\"\n5. constrained reranker budget：译为\"受限的重排序计算预算\"\n6. embedding model：译为\"嵌入模型\"\n所有技术概念均采用计算机信息检索领域标准译法，确保学术准确性。）"
    },
    {
        "title": "Avoiding Over-Personalization with Rule-Guided Knowledge Graph\n  Adaptation for LLM Recommendations",
        "url": "http://arxiv.org/abs/2509.07133v1",
        "pub_date": "2025-09-08",
        "summary": "We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods.",
        "translated": "我们提出了一种轻量级神经符号框架，通过推理时自适应调整用户侧知识图谱（KG）来缓解基于大语言模型的推荐系统中的过度个性化问题。与传统重训练模型或依赖不透明启发式方法不同，我们的方法通过重构用户个性化知识图谱（PKG），抑制那些强化个性化信息环境（PIE）的特征共现模式——即算法导致的限制内容多样性的信息茧房。调整后的PKG用于构建结构化提示，引导语言模型在保持主题相关性的同时生成更多样化的\"非PIE\"推荐。我们提出了一系列符号化适配策略，包括软重加权、硬反转和针对性移除偏见三元组，以及客户端学习算法以优化每用户的策略应用。在食谱推荐基准测试中，个性化PKG适配在保持推荐质量的同时显著提升内容新颖度，其效果优于全局适配和基于朴素提示的方法。\n\n（注：专业术语说明：\n1. Personalized Information Environments (PIEs) 译为\"个性化信息环境\"，特指算法导致的信息茧房效应\n2. Out-PIE recommendations 译为\"非PIE推荐\"，指突破信息茧房的推荐内容\n3. soft reweighting/hard inversion 分别译为\"软重加权/硬反转\"，保持机器学习领域的术语惯例\n4. client-side learning algorithm 译为\"客户端学习算法\"，强调分布式计算场景下的本地化特性）"
    },
    {
        "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
        "url": "http://arxiv.org/abs/2509.06888v1",
        "pub_date": "2025-09-08",
        "summary": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.",
        "translated": "编码器专用语言模型（Encoder-only language models）常被用于各类标准机器学习任务，包括分类与检索。然而当前针对编码器模型的研究，特别是多语言模型领域的研究仍显不足。我们提出了mmBERT——一种基于1800多种语言、3万亿多语种文本训练的纯编码器语言模型。在构建mmBERT过程中，我们引入了多项创新要素，包括逆向掩码比率调度机制和逆向温度采样策略。我们创新性地仅在训练衰减阶段加入1700余种低资源语言数据，实验表明这一方法显著提升模型性能，并最大限度利用了有限训练数据带来的增益。尽管这些低资源语言仅出现在短暂的衰减阶段，我们的模型在分类任务上达到了与OpenAI o3、Google Gemini 2.5 Pro等模型相当的性能。总体而言，mmBERT在高资源与低资源语言的分类和检索任务上均显著超越前代模型。"
    },
    {
        "title": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture",
        "url": "http://arxiv.org/abs/2509.06887v2",
        "pub_date": "2025-09-08",
        "summary": "Modern search systems play a crucial role in facilitating information acquisition. Traditional search engines typically rely on a cascaded architecture, where results are retrieved through recall, pre-ranking, and ranking stages. The complexity of designing and maintaining multiple modules makes it difficult to achieve holistic performance gains. Recent advances in generative recommendation have motivated the exploration of unified generative search as an alternative. However, existing approaches are not genuinely end-to-end: they typically train an item encoder to tokenize candidates first and then optimize a generator separately, leading to objective inconsistency and limited generalization. To address these limitations, we propose UniSearch, a unified generative search framework for Kuaishou Search. UniSearch replaces the cascaded pipeline with an end-to-end architecture that integrates a Search Generator and a Video Encoder. The Generator produces semantic identifiers of relevant items given a user query, while the Video Encoder learns latent item embeddings and provides their tokenized representations. A unified training framework jointly optimizes both components, enabling mutual enhancement and improving representation quality and generation accuracy. Furthermore, we introduce Search Preference Optimization (SPO), which leverages a reward model and real user feedback to better align generation with user preferences. Extensive experiments on industrial-scale datasets, together with online A/B testing in both short-video and live search scenarios, demonstrate the strong effectiveness and deployment potential of UniSearch. Notably, its deployment in live search yields the largest single-experiment improvement in recent years of our product's history, highlighting its practical value for real-world applications.",
        "translated": "现代搜索系统在促进信息获取方面发挥着关键作用。传统搜索引擎通常采用级联架构，通过召回、粗排和精排三个阶段获取结果。由于需要设计和维护多个模块，这种架构难以实现整体性能提升。生成式推荐的最新进展推动了统一生成式搜索的探索，但现有方法并非真正的端到端系统：它们通常先训练物品编码器对候选项目进行标记化，再单独优化生成器，导致目标不一致和泛化能力有限。\n\n为解决这些局限性，我们提出UniSearch——面向快手搜索的统一生成式搜索框架。该框架采用端到端架构替代级联流水线，集成搜索生成器与视频编码器。生成器根据用户查询生成相关项目的语义标识，视频编码器则学习潜在物品嵌入并提供其标记化表示。通过统一训练框架联合优化两个组件，实现相互增强并提升表示质量与生成准确性。此外，我们引入搜索偏好优化（SPO）技术，利用奖励模型和真实用户反馈使生成结果更贴合用户偏好。\n\n基于工业级数据集的大量实验，以及在短视频和直播搜索场景中的在线A/B测试，证明了UniSearch的强大有效性和部署潜力。值得注意的是，该框架在直播搜索场景的部署实现了我们产品近年来最大幅度的单次实验效果提升，凸显了其在实际应用中的重要价值。"
    },
    {
        "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
        "url": "http://arxiv.org/abs/2509.06883v1",
        "pub_date": "2025-09-08",
        "summary": "We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.",
        "translated": "我们参与了CheckThat!任务2的英文赛道，重点探索了多种提示方法与上下文学习策略，包括小样本提示（few-shot prompting）以及基于不同大语言模型（LLM）家族的微调方法，旨在从社交媒体文本中提取值得核查的声明。实验表明，通过对FLAN-T5模型进行微调，我们取得了最佳的METEOR评分。然而，我们注意到在某些情况下，即使其他方法的METEOR分数较低，仍能提取出质量更高的声明内容。"
    },
    {
        "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable\n  Physical Simulations from Monocular Video",
        "url": "http://arxiv.org/abs/2509.08828v1",
        "pub_date": "2025-09-10",
        "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
        "translated": "三维动态场景重建是计算机视觉领域中一个基础但具有挑战性的任务。本文提出了一种创新方法，将三维几何重建与基于物理渲染的外观估计相结合，开发出一个仅需单目RGB视频序列即可实现织物三维重建与外观估计的双任务系统。为获得逼真的高质量形变与渲染效果，我们采用布料物理仿真与可微分渲染技术。针对单目视频中的深度模糊问题，本文引入了两个新颖的正则化项以提升三维重建的合理性。与领域内最新方法相比，我们的方法将三维重建误差降低了2.64倍，且单场景平均仅需30分钟运行时间。此外，优化后的运动序列质量足以支持变形物体的外观估计，从单目RGB视频中成功恢复了清晰的细节特征。\n\n（注：专业术语说明：\n- differentiable rendering: 可微分渲染\n- monocular RGB video: 单目RGB视频\n- depth ambiguity: 深度模糊\n- regularization terms: 正则化项\n- physically based rendering: 基于物理的渲染）"
    },
    {
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "url": "http://arxiv.org/abs/2509.08826v1",
        "pub_date": "2025-09-10",
        "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "translated": "奖励模型（Reward Models, RMs）对于通过强化学习（RL）改进生成模型至关重要，但视觉生成领域的奖励模型规模化范式仍未被充分探索。这主要源于现有方法的根本性局限：基于CLIP的奖励模型受限于架构与输入模态约束，而主流的Bradley-Terry损失函数与视觉语言模型（VLMs）的下一词元预测机制存在本质错位，阻碍了有效扩展。更关键的是，RLHF优化过程长期受\"奖励破解\"（Reward Hacking）问题困扰——模型会利用奖励信号的缺陷而非真正提升生成质量。\n\n为解决这些挑战，我们提出RewardDance——一个可扩展的奖励建模框架。该框架通过创新的生成式奖励范式突破上述限制：将奖励分数重新定义为模型预测\"是\"词元的概率（即生成图像在特定标准下优于参考图像），使奖励目标与VLM架构本质对齐。这种对齐实现了两个维度的扩展：（1）模型规模：系统化将奖励模型参数量扩展至260亿；（2）上下文扩展：整合任务指令、参考示例和思维链（CoT）推理。大量实验表明，RewardDance在文本到图像、文本到视频及图像到视频生成任务上显著超越现有最优方法。\n\n尤为关键的是，我们解决了长期存在的\"奖励破解\"难题：大规模奖励模型在RL微调过程中始终展现并保持高奖励方差，证明其抗破解能力与生成多样化高质量输出的特性，极大缓解了困扰小模型的模式崩溃（mode collapse）问题。"
    },
    {
        "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts",
        "url": "http://arxiv.org/abs/2509.08818v1",
        "pub_date": "2025-09-10",
        "summary": "Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.",
        "translated": "近年来，概率生成模型的发展已从静态图像合成扩展到文本驱动视频生成。然而，其生成过程固有的随机性可能导致不可预测的伪影，例如违背物理规律的画面和时序不一致问题。解决这些挑战需要系统性基准测试，但由于视频独特的时空复杂性，现有数据集主要专注于生成式图像。为填补这一空白，我们推出了GeneVA——一个基于自然文本提示生成视频中时空伪影的大规模人工标注数据集。我们期待GeneVA能够支撑并助力关键应用，如模型性能基准测试与生成视频质量优化。"
    },
    {
        "title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching",
        "url": "http://arxiv.org/abs/2509.08805v1",
        "pub_date": "2025-09-10",
        "summary": "Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.",
        "translated": "密集图像匹配旨在为源图像中的每个像素在部分重叠的目标图像中找到对应点。现有先进方法通常采用由粗到精的匹配机制，即在每个尺度上为每个源位置生成单一对应假设。在具有挑战性的场景中——例如深度不连续区域或目标图像是源图像的强烈放大版本时——相邻源位置的对应点往往分布广泛，此时在每个尺度上为每个源位置预测单一对应假设可能导致错误匹配。本文研究了一种创新方案：在每个尺度上为每个源位置预测多个对应假设。我们采用波束搜索策略在每级尺度传播多重假设，并提出将这些多重假设集成到交叉注意力层中，由此构建出名为BEAMER的新型密集匹配架构。BEAMER能够学习跨尺度保存和传播多重假设，使其在深度不连续区域或目标图像强烈放大场景下的鲁棒性显著优于现有最优方法。"
    },
    {
        "title": "PianoVAM: A Multimodal Piano Performance Dataset",
        "url": "http://arxiv.org/abs/2509.08800v1",
        "pub_date": "2025-09-10",
        "summary": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",
        "translated": "音乐表演的多模态特性促使音乐信息检索（MIR）领域对音频之外的数据日益关注。本文推出PianoVAM——一个包含视频、音频、MIDI、手部关键点、指法标注及丰富元数据的综合性钢琴演奏数据集。该数据集通过Disklavier钢琴录制，采集了业余钢琴演奏者日常练习时的音频与MIDI数据，并在真实多样的表演环境下同步录制了俯视角视频。我们使用预训练的手部姿态估计模型和半自动化指法标注算法提取了手部关键点与指法标签。文中探讨了数据收集过程中面临的挑战以及多模态对齐的技术难点，详细介绍了基于视频手部关键点的指法标注方法。最后，我们使用PianoVAM数据集进行了纯音频与视听结合的钢琴转录基准测试，并讨论了该数据集的其他潜在应用场景。\n\n（注：根据学术规范，术语保持原文大写形式如PianoVAM/Disklavier，技术术语如MIDI/MIR等保留英文缩写，专业表述如\"手部关键点(hand landmarks)\"、\"半自动化指法标注(semi-automated fingering annotation)\"等符合计算机领域中文表达习惯。）"
    },
    {
        "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation",
        "url": "http://arxiv.org/abs/2509.08794v1",
        "pub_date": "2025-09-10",
        "summary": "Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.",
        "translated": "基于事件相机（EBC）的星体跟踪姿态测定技术是一种新兴技术，但以往研究难以获取真实数据的精确地面真值。本研究创新性地利用地球自转作为基准真值，对EBC星体跟踪系统的精度进行量化分析。地球以高度规律的周期自转，其微小扰动可达毫角秒级测量精度。通过将事件相机固定于地面望远镜并对准夜空，我们在天球参考系中构建了仅受地球自转影响的观测系统。处理生成的事件流数据后，我们将估算的姿态方向与国际地球自转参考系（IERS）测量的地球定向进行对比。实验表明：该事件相机系统的定向估计均方根误差为18.47角秒，最大误差约78.84角秒。结合事件相机相较于帧传感器固有的优势（稀疏数据流降低计算量、更高动态范围、更低能耗、更快更新速率），这一精度水平证实了事件相机在低成本低延迟星体跟踪中的应用价值。我们已公开全部代码与数据：https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification。\n\n（注：译文严格遵循学术规范，对专业术语如\"ground truth\"译为\"地面真值\"、\"celestial reference frame\"译为\"天球参考系\"等保持准确；通过拆分长难句、调整语序（如将英文被动语态转换为中文主动表述）确保技术细节的清晰传达；保留原始数值精度及专业机构缩写IERS；完整呈现原文的技术逻辑链条与创新点。）"
    },
    {
        "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images",
        "url": "http://arxiv.org/abs/2509.08780v1",
        "pub_date": "2025-09-10",
        "summary": "Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.   Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.   Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.   Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.",
        "translated": "背景：砷中毒是南亚和东南亚地区严重的公共卫生问题，主要由长期饮用受砷污染的水导致。其早期皮肤表现具有重要临床意义但常被漏诊，尤其在缺乏皮肤科医生的农村地区。基于图像的自动化诊断方案可支持早期发现和及时干预。  \n方法：本研究提出端到端框架，通过手机拍摄的皮肤图像实现砷中毒诊断。构建包含20个类别、11,000余张砷性皮肤病及其他皮肤病症图像的数据集。对包括卷积神经网络（CNN）和基于Transformer的模型在内的多种深度学习架构进行砷中毒检测性能评估。通过LIME和Grad-CAM实现模型可解释性，并基于Web的诊断工具验证部署可行性。  \n结果：基于Transformer的模型显著优于CNN，其中Swin Transformer以86%的准确率取得最佳性能。LIME和Grad-CAM可视化证实模型聚焦于病变相关区域，增强临床透明度并辅助错误分析。该框架在外部验证样本中同样表现优异，证实其泛化能力。  \n结论：本研究框架证明了深度学习技术通过移动设备图像实现非侵入性、可普及且可解释的砷中毒诊断的潜力。通过提供可靠的图像筛查方案，该技术可在缺乏皮肤科医生的资源有限地区作为实用诊断辅助工具，支持早期发现与及时干预。\n\n（注：专业术语处理说明：  \n1. LIME (Local Interpretable Model-agnostic Explanations) 保留英文缩写  \n2. Grad-CAM (Gradient-weighted Class Activation Mapping) 保留英文缩写  \n3. Transformer/Swin Transformer 作为特定模型名称保留英文  \n4. 临床术语如\"cutaneous manifestations\"译为\"皮肤表现\"，\"lesion-relevant regions\"译为\"病变相关区域\"符合医学文献表述规范）"
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pub_date": "2025-09-10",
        "summary": "Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "translated": "多模态大语言模型（MLLMs）正日益被用于评估文本到图像（TTI）生成系统，其能够基于视觉与文本上下文提供自动化评判。然而，这些“裁判”模型常存在偏见、过度自信以及在多样化图像领域中表现不一致的问题。尽管提示集成（prompt ensembling）在单模态纯文本场景中已展现出缓解这些问题的潜力，但我们的实验表明，标准集成方法无法有效推广至TTI任务。针对这些局限性，我们提出了一种新型多模态感知方法——多模态贝叶斯提示集成混合（MMB）。该方法通过贝叶斯提示集成框架结合图像聚类技术，使裁判模型能够根据样本的视觉特征动态分配提示权重。我们证明，MMB在 pairwise 偏好判断中提升了准确性，并显著增强了校准能力，使其更易于评估模型真实的不确定性。在HPSv2和MJBench两个TTI基准测试中，MMB在人类标注对齐度和跨图像内容的校准性能上均优于现有基线方法。我们的研究结果凸显了多模态特异性策略对裁判校准的重要性，并为实现可靠的大规模TTI评估指明了一条可行路径。"
    },
    {
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "url": "http://arxiv.org/abs/2509.08764v1",
        "pub_date": "2025-09-10",
        "summary": "Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "translated": "可靠整合先验信息对于实现自验证与自更新的高精地图至关重要。然而，现有公开数据集均未包含\"先验地图-当前地图-传感器数据\"的三元组。这导致现有方法只能依赖合成先验数据，从而产生数据不一致性并引发严重的模拟到现实差异。为此，我们推出ArgoTweak——首个提供真实地图先验数据的三元组数据集。该数据集核心采用双射映射框架，将大规模地图修改分解为地图元素层级的细粒度原子级变更，确保修改过程的可解释性。这种范式转变能够在保持未变化元素高保真度的同时，实现精确的变化检测与整合。实验表明，使用ArgoTweak训练的模型相较于采用合成先验数据的方法，显著缩小了模拟到现实的性能差距。大量消融实验进一步验证了结构化先验数据与精细化变更标注的重要性。通过建立可解释的先验辅助高精地图绘制基准，ArgoTweak推动了可扩展自优化地图解决方案的发展。数据集、基线模型、地图修改工具箱及相关资源已开源：https://kth-rpl.github.io/ArgoTweak/。\n\n（注：专业术语说明：\n1. self-verifying/self-updating：自验证/自更新\n2. HD maps：高精地图（High-Definition maps）\n3. sim2real gap：模拟到现实差异（simulation-to-reality gap）\n4. bijective mapping：双射映射（数学中的一一对应关系）\n5. atomic changes：原子级变更（不可再分的最小修改单元）\n6. change detection：变化检测\n7. ablations：消融实验（ablation studies））"
    },
    {
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
        "url": "http://arxiv.org/abs/2509.08757v1",
        "pub_date": "2025-09-10",
        "summary": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .",
        "translated": "在动态化、以人为中心的环境中，机器人导航需要基于对场景的深度理解做出符合社会规范的行为决策。当前，视觉-语言模型（VLMs）展现出与社交机器人导航精细化需求高度契合的多种能力，包括目标识别、常识推理和上下文理解等。然而，这类模型是否能准确理解复杂的社交导航场景（例如推断智能体间的时空关系及人类意图）——这一实现安全合规导航的关键前提——仍存在疑问。尽管已有研究尝试将VLMs应用于社交机器人导航，但尚未有系统性工作评估其满足这些必要条件的实际能力。本文提出社交导航场景理解基准（SocialNav-SUB），这是一个基于视觉问答（VQA）任务的数据集与评测体系，专为评估VLMs在真实社交机器人导航场景中的理解能力而设计。该基准通过需要空间推理、时空推理及社会推理的VQA任务，构建了统一框架以对比VLMs与人类基线、规则基线的表现。通过对前沿VLMs的实验发现：虽然性能最优的VLM模型与人类答案的一致性概率达到鼓舞人心的水平，但其表现仍逊于简单的规则基线和人类共识基线，这表明现有VLMs在社交场景理解方面存在显著不足。本基准为社交机器人导航基础模型的后续研究奠定了基础，通过提供标准化框架推动探索如何定制VLMs以满足真实世界的社交导航需求。论文概述、代码及数据详见：https://larg.github.io/socialnav-sub\n\n（注：译文严格遵循学术论文摘要的规范表述，关键技术术语如\"Vision-Language Models (VLMs)\"译为\"视觉-语言模型\"，\"socially-compliant\"译为\"符合社会规范的\"，\"spatial-temporal relations\"译为\"时空关系\"等均采用领域内标准译法。长难句按中文习惯拆分重组，如将原文复合从句\"capabilities that align with...\"处理为独立分句\"展现出与...高度契合的多种能力\"，确保专业性与可读性平衡。）"
    },
    {
        "title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes",
        "url": "http://arxiv.org/abs/2509.08738v1",
        "pub_date": "2025-09-10",
        "summary": "This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.",
        "translated": "本文提出了一种新颖的端到端人群检测方法，通过利用目标密度信息来增强现有基于Transformer的检测器。我们提出的CrowdQuery（CQ）方法核心是CQ模块，该模块可预测并嵌入目标密度图，随后将嵌入的密度信息系统化整合到解码器中。现有密度图定义通常依赖于头部位置或基于目标的空间统计量，而我们的方法扩展了这一定义，将个体边界框尺寸纳入考量。通过将密度信息融入目标查询机制，本方法采用密度引导查询来提升拥挤场景下的检测性能。CQ方法无需额外数据即可同时适用于2D和3D检测任务，由此成为首个有效贯通拥挤环境下2D与3D检测的解决方案。我们演示了将CQ集成至通用2D和3D基于Transformer的目标检测器中的架构CQ2D和CQ3D，且CQ的适用性不限于我们选择的特定Transformer模型。在STCrowd数据集上进行的2D与3D领域实验表明，相较于基线模型，该方法实现了显著性能提升，并优于多数现有先进方法。当集成至最先进的人群检测器时，CQ在具有挑战性的CrowdHuman数据集上可进一步提升性能，证明了其泛化能力。相关代码已发布于https://github.com/mdaehl/CrowdQuery。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"object density map\"译为\"目标密度图\"，\"transformer-based detectors\"译为\"基于Transformer的检测器\"\n2. 技术概念完整保留：\"density-guided queries\"译为\"密度引导查询\"，\"bounding box dimensions\"译为\"边界框尺寸\"\n3. 学术表述规范：\"end-to-end\"译为\"端到端\"，\"state-of-the-art\"译为\"最先进的\"\n4. 长句结构符合中文表达习惯，同时保持技术细节的精确性）"
    },
    {
        "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
        "url": "http://arxiv.org/abs/2509.08715v1",
        "pub_date": "2025-09-10",
        "summary": "As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.",
        "translated": "随着多模态大语言模型（MLLMs）的发展，其大规模架构在资源受限环境中的部署面临挑战。在大模型时代，能源效率、计算可扩展性和环境可持续性至关重要，开发轻量级高性能模型对实际应用具有关键意义。为此，我们提出了一种面向端到端视觉问答的轻量级MLLM框架。该方案的核心是BreezeCLIP——一个紧凑而强大的视觉语言编码器，专为高效多模态理解优化。模型总参数量仅12亿，在显著降低计算成本的同时实现了与标准规模MLLM相当的性能。在多数据集上的实验进一步验证了其在精度与效率平衡方面的有效性。模块化可扩展的设计使其能泛化至更广泛的多模态任务。该轻量级视觉语言框架被命名为BcQLM（BreezeCLIP增强型Q门控多模态语言模型），为在实际硬件限制下部署MLLM提供了可行路径。源代码已开源：https://github.com/thico0224/BcQLM。\n\n（注：译文严格遵循学术论文表述规范，关键技术术语如\"multimodal large language models\"译为\"多模态大语言模型\"，\"vision-language encoder\"译为\"视觉语言编码器\"，\"Q-Gated\"保留技术特征译为\"Q门控\"。数字单位遵循中文计量规范，\"1.2 billion\"转换为\"12亿\"。长难句按中文习惯拆分重组，如将英文复合从句\"where...\"处理为独立判断句\"至关重要...具有关键意义\"，同时保持逻辑严密性。开源链接等要素完整保留。）"
    },
    {
        "title": "Computational Imaging for Enhanced Computer Vision",
        "url": "http://arxiv.org/abs/2509.08712v1",
        "pub_date": "2025-09-10",
        "summary": "This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.",
        "translated": "本文系统综述了计算成像（CI）技术及其对计算机视觉（CV）应用的变革性影响。传统成像方法在低光照、运动模糊或高动态范围场景等挑战性条件下往往难以提供高保真视觉数据，这限制了前沿计算机视觉系统的性能。计算成像技术通过增强图像采集与重建过程，有效解决了这些局限性，具体包括光场成像、高动态范围（HDR）成像、去模糊、高速成像和眩光抑制等技术。本综述系统探讨了CI技术与核心CV任务（含目标检测、深度估计、光流分析、人脸识别和关键点检测）之间的协同效应。通过分析CI方法与其对CV应用的实际贡献之间的关联，本研究揭示了新兴机遇、现存挑战及未来研究方向。我们重点探讨了面向特定任务的自适应成像流程的潜力，这些流程能在自动驾驶、监控、增强现实和机器人等现实场景中提升系统的鲁棒性、精度与效率。"
    },
    {
        "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
        "url": "http://arxiv.org/abs/2509.08699v1",
        "pub_date": "2025-09-10",
        "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.",
        "translated": "在机器人视觉导航领域，传统方法通常依赖全局一致的3D地图或学习型控制器，这些方法存在计算成本高且难以跨环境泛化的局限性。本研究提出了一种创新的纯RGB对象级拓扑导航框架，无需3D地图或预训练控制器即可实现零样本的长程机器人导航。该方法通过融合全局拓扑路径规划与局部度量轨迹控制，使机器人能够在避开障碍物的同时导航至对象级子目标。\n\n我们通过以下核心创新解决了现有方法的缺陷：利用单目深度估计和可通行性预测实现连续局部轨迹规划，并引入自动切换机制在必要时回退至基线控制器。该系统基于基础模型构建，无需领域特异性微调即可实现开放场景的适用性。通过仿真环境与真实场景测试，我们验证了该方法在鲁棒性和部署便利性方面的优势。实验表明，本方法在开放环境视觉导航任务中优于现有最优方案，提供了更具适应性的解决方案。相关源代码已开源：https://github.com/podgorki/TANGO。\n\n（注：根据学术规范，对技术术语进行了标准化处理：\n1. \"topometric navigation\"译为\"拓扑导航\"以符合机器人学规范\n2. \"zero-shot\"保留零样本特性但采用\"零样本\"标准译法\n3. \"foundational models\"译为\"基础模型\"符合AI领域共识\n4. \"open-set applicability\"译为\"开放场景适用性\"以准确传达原文语义\n5. 保持中英文术语对应关系，如\"traversability estimation\"统一译为\"可通行性预测\"）"
    },
    {
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework",
        "url": "http://arxiv.org/abs/2509.08694v1",
        "pub_date": "2025-09-10",
        "summary": "Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "translated": "基于卫星影像的海岸水域分割任务面临光谱特征复杂与边界形态不规则等独特挑战。传统RGB方法在多样化海洋环境中常出现训练不稳定和泛化能力不足的问题。本文提出一种系统性鲁棒增强框架——Robust U-Net，通过引入HSV色彩空间监督与多模态约束机制提升海岸水域分割性能。该框架集成五大协同组件：HSV色彩引导监督、梯度式海岸线优化、形态学后处理、海域净化和连通性控制。综合消融实验表明，HSV监督模块贡献度最高（影响系数0.85），完整框架可实现显著训练稳定性提升（方差降低84%）并增强分割质量。本方法在多项评估指标中均保持稳定改进，同时维持计算效率。为促进可复现性，训练配置与代码已开源：https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet。"
    },
    {
        "title": "FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical\n  Flow Estimation with Total Variation Regularization",
        "url": "http://arxiv.org/abs/2509.08670v1",
        "pub_date": "2025-09-10",
        "summary": "We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.",
        "translated": "我们提出了FractalPINN-Flow——一种无监督深度学习框架，用于直接从连续灰度帧中学习稠密光流估计，无需真实标注数据。该架构的核心是分形形变网络（FDN），这是一个受分形几何和自相似性启发的递归编码器-解码器结构。与传统采用顺序下采样的CNN不同，FDN通过重复的编码器-解码器嵌套结构与跳跃连接，同时捕获细粒度细节和长程运动模式。训练目标基于经典变分公式，采用全变分（TV）正则化：具体通过最小化结合$L^1$和$L^2$数据保真项（用于增强亮度恒定性）与TV项（促进空间平滑性和流场一致性）的能量泛函实现。在合成数据和基准数据集上的实验表明，FractalPINN-Flow能生成精确、平滑且保持边缘细节的光流场。该模型尤其适用于高分辨率数据及标注有限的场景。\n\n（注：专业术语说明：\n- FractalPINN-Flow: 保留英文形式，体现模型命名\n- optical flow: 光流\n- encoder-decoder: 编码器-解码器\n- total variation: 全变分\n- brightness constancy: 亮度恒定性\n- edge-preserving: 边缘保持）"
    },
    {
        "title": "Skeleton-based sign language recognition using a dual-stream\n  spatio-temporal dynamic graph convolutional network",
        "url": "http://arxiv.org/abs/2509.08661v1",
        "pub_date": "2025-09-10",
        "summary": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "translated": "孤立手语识别（ISLR）面临形态相似但语义迥异的手势识别难题，这一挑战源于手部形状与运动轨迹之间复杂的相互作用。现有方法通常依赖单一参考帧，难以解决这种几何模糊性问题。本文提出双参考帧双流架构Dual-SignLanguageNet（DSLNet），通过在两个互补坐标系中解耦并分别建模手势形态和运动轨迹。我们的方法采用腕部中心坐标系实现视角无关的形状分析，以及面部中心坐标系实现上下文感知的轨迹建模——两个分支分别由专用网络处理：采用拓扑感知图卷积网络处理形状信息，基于芬斯勒几何的编码器处理轨迹信息，最终通过几何驱动的最优传输融合机制进行整合。DSLNet以显著更少的参数量，在具有挑战性的WLASL-100、WLASL-300和LSA64数据集上分别达到93.70%、89.97%和99.79%的准确率，创造了新的性能标杆。\n\n（注：专业术语说明：\n1. Finsler geometry保留为\"芬斯勒几何\"，这是微分几何中的标准术语\n2. optimal transport采用学界通用译法\"最优传输\"\n3. graph convolution遵循计算机视觉领域规范译为\"图卷积\"\n4. WLASL/LSA64作为标准数据集名称保留不译\n5. parameters根据上下文译为\"参数量\"而非\"参数\"，更符合深度学习领域的表述习惯）"
    },
    {
        "title": "X-Part: high fidelity and structure coherent shape decomposition",
        "url": "http://arxiv.org/abs/2509.08643v1",
        "pub_date": "2025-09-10",
        "summary": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.",
        "translated": "在部件级别生成三维形状对于网格重拓扑、UV映射和三维打印等下游应用至关重要。然而，现有的基于部件的生成方法往往缺乏足够的可控性，且语义化分解效果不佳。为此，我们提出了X-Part——一种可控生成模型，能够将整体三维对象分解为具有高几何保真度的语义化部件，并保持结构一致性。该模型以边界框作为部件生成提示，并通过注入点级语义特征实现有意义的结构分解。此外，我们设计了支持交互式部件生成的可编辑流程。大量实验结果表明，X-Part在部件级形状生成任务上达到了最先进的性能。这项工作为创建生产就绪、可编辑且结构合理的三维资产建立了新范式。代码将开源以供学术研究使用。\n\n（注：根据学术规范，对\"Codes will be released for public research\"采用国内计算机领域常用表述\"代码将开源\"进行意译，既符合中文表达习惯，也准确传递了原意）"
    },
    {
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and\n  Correct Image Interpretation Model Shortcuts",
        "url": "http://arxiv.org/abs/2509.08640v1",
        "pub_date": "2025-09-10",
        "summary": "Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\\% of cases, correctly incorporated the specified finding in 89-99\\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\\% AUC in internal validation and by 1-11\\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.",
        "translated": "胸部X光片（CXR）是临床最常用的检查手段之一。自动化影像解读可减轻放射科医生的工作负担并扩大诊断专业知识的可及性。深度学习多任务与基础模型虽在CXR解读中表现出色，但存在捷径学习缺陷——模型依赖虚假或偏离目标的关联而非临床相关特征进行决策。我们提出RoentMod反事实图像编辑框架，该框架能生成具有用户指定合成病变、且保留原始扫描中无关解剖特征的解剖学真实CXR图像。RoentMod将开源医学图像生成器（RoentGen）与图像修改模型相结合，无需重新训练。经执业放射医师和放射科住院医师参与的阅片研究显示：RoentMod生成的图像真实性获93%认可，89-99%的案例正确融合指定病灶特征，其原生解剖结构保存度与真实随访CXR相当。通过RoentMod实验，我们发现当前最优的多任务与基础模型频繁利用偏离目标的病变特征作为捷径，限制了模型特异性。在训练中加入Roent生成的反事实图像后，模型抗干扰能力显著提升：内部验证中多项病变的判别AUC提升3-19%，外部测试中6类病变有5类提升1-11%。这些发现表明RoentMod可作为普适性工具用于探测和修正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了CXR解读模型的鲁棒性与可解释性，为改进医学影像基础模型提供了可推广的策略。"
    },
    {
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain\n  Translation",
        "url": "http://arxiv.org/abs/2509.08628v1",
        "pub_date": "2025-09-10",
        "summary": "Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "translated": "扩散模型在生成高质量输出方面表现出色，但在数据稀缺领域面临挑战，这些领域通常需要 exhaustive 的重新训练或成本高昂的配对数据。为应对这些局限性，我们提出潜在对齐扩散桥（LADB），这是一种用于样本到样本转换的半监督框架，能够利用部分配对数据有效弥合领域差距。通过在共享潜在空间中对齐源域和目标域分布，LADB 将预训练的源域扩散模型与目标域潜在对齐扩散模型（LADM）无缝集成，后者基于部分配对的潜在表示进行训练。这一方法实现了确定性的领域映射，无需完全监督。与通常缺乏可控性的非配对方法以及需要大型领域特定数据集的完全配对方法相比，LADB 通过结合配对和非配对的潜在-目标耦合，在保真度和多样性之间取得了平衡。我们的实验结果表明，该方法在部分监督下的深度图到图像转换任务中表现优异。此外，我们将 LADB 扩展到多源转换（从深度图和分割掩码）以及类别条件风格迁移任务中的多目标转换，展示了其处理多样化和异构用例的灵活性。最终，我们提出 LADB 作为一种可扩展且通用的解决方案，适用于现实世界的领域转换任务，尤其是在数据标注成本高昂或不完整的场景中。"
    }
]