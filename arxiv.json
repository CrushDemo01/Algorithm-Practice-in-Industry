[
    {
        "title": "Soundtracks of Our Lives: How Age Influences Musical Preferences",
        "url": "http://arxiv.org/abs/2509.08337v1",
        "pub_date": "2025-09-10",
        "summary": "The majority of research in recommender systems, be it algorithmic improvements, context-awareness, explainability, or other areas, evaluates these systems on datasets that capture user interaction over a relatively limited time span. However, recommender systems can very well be used continuously for extended time. Similarly so, user behavior may evolve over that extended time. Although media studies and psychology offer a wealth of research on the evolution of user preferences and behavior as individuals age, there has been scant research in this regard within the realm of user modeling and recommender systems. In this study, we investigate the evolution of user preferences and behavior using the LFM-2b dataset, which, to our knowledge, is the only dataset that encompasses a sufficiently extensive time frame to permit real longitudinal studies and includes age information about its users. We identify specific usage and taste preferences directly related to the age of the user, i.e., while younger users tend to listen broadly to contemporary popular music, older users have more elaborate and personalized listening habits. The findings yield important insights that open new directions for research in recommender systems, providing guidance for future efforts.",
        "translated": "当前推荐系统领域的大多数研究——无论是算法改进、上下文感知、可解释性还是其他方向——均基于有限时间跨度的用户交互数据集进行评估。然而，推荐系统实际往往需要长期持续运行，用户行为也可能随时间推移产生演变。尽管媒体研究和心理学领域对用户偏好随年龄演变的规律已有丰富成果，但在用户建模与推荐系统领域中，针对这一问题的研究仍十分匮乏。本研究采用LFM-2b数据集（据我们所知，这是唯一兼具足够长时间跨度支持真实纵向研究、且包含用户年龄信息的数据集）深入探究用户偏好与行为的演变规律。我们发现：年轻用户倾向于广泛收听当代流行音乐，而年长用户则展现出更精细化、个性化的收听习惯——这种使用偏好与品味特征与用户年龄存在直接关联。这些发现为推荐系统研究开辟了新方向，为后续研究提供了重要指导。"
    },
    {
        "title": "Vector embedding of multi-modal texts: a tool for discovery?",
        "url": "http://arxiv.org/abs/2509.08216v1",
        "pub_date": "2025-09-10",
        "summary": "Computer science texts are particularly rich in both narrative content and illustrative charts, algorithms, images, annotated diagrams, etc. This study explores the extent to which vector-based multimodal retrieval, powered by vision-language models (VLMs), can improve discovery across multi-modal (text and images) content. Using over 3,600 digitized textbook pages largely from computer science textbooks and a Vision Language Model (VLM), we generate multi-vector representations capturing both textual and visual semantics. These embeddings are stored in a vector database. We issue a benchmark of 75 natural language queries and compare retrieval performance to ground truth and across four similarity (distance) measures. The study is intended to expose both the strengths and weakenesses of such an approach. We find that cosine similarity most effectively retrieves semantically and visually relevant pages. We further discuss the practicality of using a vector database and multi-modal embedding for operational information retrieval. Our paper is intended to offer design insights for discovery over digital libraries.   Keywords: Vector embedding, multi-modal document retrieval, vector database benchmark, digital library discovery",
        "translated": "计算机科学文献通常兼具丰富的叙述性内容与说明性图表、算法、图像及带标注的图示等。本研究探讨基于视觉语言模型（VLM）的向量多模态检索在提升跨文本与图像的多模态内容发现能力方面的效果。通过使用超过3,600页主要来自计算机科学教材的数字化页面及视觉语言模型，我们生成了能同时捕获文本与视觉语义的多向量表征。这些嵌入向量被存储于向量数据库中。我们发布了包含75个自然语言查询的基准测试集，并将检索效果与人工标注真值进行对比，同时评估了四种相似度（距离）度量方法的性能。本研究旨在揭示此类方法的优势与局限性。研究发现，余弦相似度能够最有效地检索出语义和视觉相关性较高的页面。我们进一步讨论了使用向量数据库与多模态嵌入在实际信息检索操作中的可行性。本文旨在为数字图书馆的文献发现机制提供设计参考。  \n关键词：向量嵌入，多模态文档检索，向量数据库基准测试，数字图书馆发现"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们提出了一种名为\"智能快速完成\"（Smart Fast Finish, SFF）的预算调控功能。该功能基于行业标准的快速完成（Fast Finish, FF）技术进行优化——在固定时间段末期将剩余广告预算以最快速度消耗完毕。SFF通过分析历史广告活动数据，动态更新系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分流实验数据和离线仿真结果表明，SFF是一种能有效缓解预算调控过程中超量投放问题的稳健解决方案。\n\n（注：根据学术论文摘要的规范要求，译文采用以下处理：\n1. 专业术语统一：\"budget pacing\"译为\"预算调控\"，\"overdelivery mitigation\"译为\"缓解超量投放\"\n2. 技术概念准确传达：\"dynamic updates\"译为\"动态更新\"，\"throttle rate\"译为\"调控速率\"\n3. 企业名称保留原文：\"DoorDash\"不译\n4. 研究论证表述规范：\"online/offline\"译为\"在线/离线\"，\"robust solution\"译为\"稳健解决方案\"\n5. 保持学术文本的客观性，避免口语化表达）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关联，从而阻碍战略决策。为突破这些局限，我们提出KLIPA这一创新框架，该框架通过结合知识图谱和大语言模型（LLM）显著推进专利分析能力。我们的方法整合了三个核心组件：用于构建专利间显性关联的结构化知识图谱、用于挖掘上下文联系的检索增强生成（RAG）系统，以及能动态确定最优解策策略的智能代理。我们在真实世界的全量专利数据库上验证了KLIPA，其在知识提取、新颖关联发现和整体操作效率方面均展现出显著提升。这种技术融合增强了检索精度，降低了对领域专家的依赖，为包括科技企业和律师事务所在内的知识产权管理机构提供了可扩展的自动化解决方案，使其能更好地应对战略创新与竞争情报领域的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v2",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "从科学文献中抽取结构化信息对于捕捉专业领域的核心概念与新兴趋势至关重要。虽然现有数据集有助于模型开发，但由于领域复杂性和科学文本标注成本高昂，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专为自然语言处理（NLP）领域设计的全文实体与关系抽取基准数据集。该数据集包含60篇经人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行对比实验，并评估了前沿监督模型在该数据集上的表现。结果显示现有模型对不同长度学术文本的抽取能力存在显著差异。与现有数据集的交叉对比表明，SciNLP在部分基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点连接度，展现出丰富的语义拓扑信息，可有效增强下游应用性能。数据集已公开于https://github.com/AKADDC/SciNLP。\n\n（注：译文严格遵循学术论文表述规范，对\"entity and relation extraction\"采用\"实体与关系抽取\"标准译法，\"knowledge graph\"译为\"知识图谱\"，\"node degree\"译为\"节点连接度\"等专业术语均符合计算机领域中文表达惯例。长难句如\"Results reveal varying...\"通过拆分重组转化为符合中文表达习惯的短句，同时保持逻辑严谨性。）"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）需要弥合简短模糊的查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配的关键机制，其设计范式已因预训练语言模型（PLM）和大语言模型（LLM）发生显著转变。本综述从三个维度系统梳理该领域：（i）提出查询扩展的四维框架——从注入方式（显式/隐式QE）出发，贯穿 grounding 与交互机制（知识库、模型内部能力、多轮检索）、学习对齐策略，直至基于知识图谱的论证；（ii）建立以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释其特有的QE能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域QA/RAG、会话式搜索、代码检索及跨语言场景的应用图谱。研究提炼出三大核心设计原则：基于 grounding 的交互机制、对齐与蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束——这些被证明是解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下QE技术的选择与组合提供了系统化蓝图。\n\n（注：术语处理说明：\n- grounding 保留英文，因中文尚无统一译法且该术语在AI领域常直接使用\n- SFT/PEFT/DPO 为技术缩写（全称：Supervised Fine-Tuning/Parameter-Efficient Fine-Tuning/Direct Preference Optimization）\n- QA/RAG 等缩写已在领域内广泛采用\n- 保持\"零样本/少样本\"等标准译法以符合技术文献惯例）"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂结构要求采用超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大技术演进阶段的方法体系、核心挑战与应用实践。我们系统化追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层次化编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，我们还审视了特定领域应用场景、专项评估资源，并指出效率权衡、多模态对齐和结果可信度等关键开放挑战。本综述旨在为基础模型时代的长文档检索研究提供系统化参考框架与前瞻性发展路线图。"
    },
    {
        "title": "Towards End-to-End Model-Agnostic Explanations for RAG Systems",
        "url": "http://arxiv.org/abs/2509.07620v1",
        "pub_date": "2025-09-09",
        "summary": "Retrieval Augmented Generation (RAG) systems, despite their growing popularity for enhancing model response reliability, often struggle with trustworthiness and explainability. In this work, we present a novel, holistic, model-agnostic, post-hoc explanation framework leveraging perturbation-based techniques to explain the retrieval and generation processes in a RAG system. We propose different strategies to evaluate these explanations and discuss the sufficiency of model-agnostic explanations in RAG systems. With this work, we further aim to catalyze a collaborative effort to build reliable and explainable RAG systems.",
        "translated": "尽管检索增强生成（RAG）系统在提升模型响应可靠性方面日益普及，但其可信度与可解释性仍面临挑战。本研究提出了一种新颖的、整体性的、模型无关的事后解释框架，该框架基于扰动技术来解释RAG系统中的检索与生成过程。我们提出了多种策略来评估这些解释的有效性，并探讨了模型无关解释在RAG系统中的充分性。通过此项研究，我们旨在推动学界与业界共同努力，构建更可靠、更可解释的RAG系统。\n\n（注：译文严格遵循了以下技术细节处理：\n1. \"post-hoc explanation\"译为\"事后解释\"（而非\"事后诸葛亮式解释\"），符合机器学习可解释性领域的术语规范\n2. \"perturbation-based techniques\"译为\"扰动技术\"，准确反映通过对输入进行微小扰动来评估模型敏感度的技术本质\n3. \"model-agnostic\"统一译为\"模型无关\"，保持与机器学习领域术语的一致性\n4. 使用\"可解释性\"而非\"解释性\"，符合人工智能透明度研究领域的标准译法\n5. 保留RAG、LLM等专业术语的英文缩写形式，确保学术严谨性）"
    },
    {
        "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2509.07594v1",
        "pub_date": "2025-09-09",
        "summary": "Click-through rate (CTR) prediction plays an important role in online advertising systems. On the one hand, traditional CTR prediction models capture the collaborative signals in tabular data via feature interaction modeling, but they lose semantics in text. On the other hand, Large Language Models (LLMs) excel in understanding the context and meaning behind text, but they face challenges in capturing collaborative signals and they have long inference latency. In this paper, we aim to leverage the benefits of both types of models and pursue collaboration, semantics and efficiency. We present ELEC, which is an Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for the CTR prediction task. In order to leverage the ability of the LLM but simultaneously keep efficiency, we utilize the pseudo-siamese network which contains a gain network and a vanilla network. We inject the high-level representation vector generated by the LLM into a collaborative CTR model to form the gain network such that it can take advantage of both tabular modeling and textual modeling. However, its reliance on the LLM limits its efficiency. We then distill the knowledge from the gain network to the vanilla network on both the score level and the representation level, such that the vanilla network takes only tabular data as input, but can still generate comparable performance as the gain network. Our approach is model-agnostic. It allows for the integration with various existing LLMs and collaborative CTR models. Experiments on real-world datasets demonstrate the effectiveness and efficiency of ELEC for CTR prediction.",
        "translated": "点击率（CTT）预测在在线广告系统中具有重要作用。一方面，传统CTR预测模型通过特征交互建模捕捉表格数据中的协同信号，但会丢失文本语义信息；另一方面，大语言模型（LLM）擅长理解文本背后的上下文和语义，但在捕捉协同信号方面存在局限且推理延迟较高。本文旨在融合两类模型的优势，实现协同性、语义理解与效率的平衡。我们提出ELEC框架——一种高效的大语言模型赋能CTR预测方案。首先针对CTR预测任务对大语言模型进行适配，为兼顾模型能力与效率，采用包含增益网络和基准网络的双伪孪生网络结构。通过将LLM生成的高层表征向量注入协同CTR模型形成增益网络，使其能同时利用表格建模和文本建模的优势。但该网络对LLM的依赖会影响效率，因此我们通过分数级和表征级蒸馏将增益网络的知识迁移至仅需输入表格数据的基准网络，使其在保持高效的同时达到与增益网络相当的性能。本方法具备模型无关性，可与多种现有LLM及协同CTR模型集成。真实场景数据集上的实验验证了ELEC在CTR预测中的有效性与高效性。"
    },
    {
        "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of\n  Scientific Documents",
        "url": "http://arxiv.org/abs/2509.07531v1",
        "pub_date": "2025-09-09",
        "summary": "Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.",
        "translated": "科学文献表示学习为各类任务提供了强大的嵌入向量，但现有方法面临三大挑战：1）基于引文结构信号的对比训练未能充分利用引文信息，且仍生成单一向量表示；2）细粒度表示学习虽能生成句子或方面级的多向量表示，但需要昂贵的人工整合且缺乏领域泛化能力；3）任务感知学习依赖人工预定义的任务分类，忽略了细微的任务差异，且需为特定任务模块提供额外训练数据。针对这些问题，我们提出统一三种范式的新方法FLeW以获取更优表示。具体而言，我们设计了一种新型三元组采样方法，通过引文意图（背景、方法、结果）和引用频次增强引文结构信号训练——引文意图与科学写作通用结构相契合，可为细粒度表示学习提供领域泛化的方面划分。随后采用简易权重搜索机制，无需任务感知微调即可自适应整合三个方面级嵌入形成任务适配的文档表示。实验表明，相较于现有模型，FLeW在多个科学任务和学科领域均展现出卓越的适用性与鲁棒性。"
    },
    {
        "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval",
        "url": "http://arxiv.org/abs/2509.07512v1",
        "pub_date": "2025-09-09",
        "summary": "Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\\%-10\\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.",
        "translated": "在化学与材料科学等自然科学的当代数据驱动研究中，大规模高性能的实体识别已成为科学数据集处理的关键需求。大型语言模型（LLMs）正被日益广泛地应用于实体识别任务，这一趋势也体现在全谱系自然语言处理任务中。当前主流的实体识别大模型依赖于微调技术，但微调过程往往伴随着高昂成本。为实现性能与成本的最优平衡，我们提出ALLabel——一个三阶段框架，通过三种主动学习策略的序列化应用，从标注数据中筛选信息量最大且最具代表性的样本，用于构建大模型上下文学习所需的真实标注检索库。在三个专业领域数据集上，ALLabel在相同标注预算下持续超越所有基线模型。实验结果表明：使用ALLabel仅需标注5%-10%的数据量即可达到全量标注方法的性能水平。进一步的解析与消融研究验证了该方案的有效性和泛化能力。\n\n（注：专业术语说明：\n1. entity recognition：实体识别\n2. large language models (LLMs)：大型语言模型\n3. fine-tuned technology：微调技术\n4. active learning strategies：主动学习策略\n5. in-context learning：上下文学习\n6. ablation studies：消融研究\n7. annotation budget：标注预算\n8. ground-truth retrieval corpus：真实标注检索库）"
    },
    {
        "title": "Multi-view-guided Passage Reranking with Large Language Models",
        "url": "http://arxiv.org/abs/2509.07485v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large language models (LLMs) have shown impressive performance in passage reranking tasks. Despite their success, LLM-based methods still face challenges in efficiency and sensitivity to external biases. (1) Existing models rely mostly on autoregressive generation and sliding window strategies to rank passages, which incur heavy computational overhead as the number of passages increases. (2) External biases, such as position or selection bias, hinder the model's ability to accurately represent passages and increase input-order sensitivity. To address these limitations, we introduce a novel passage reranking model, called Multi-View-guided Passage Reranking (MVP). MVP is a non-generative LLM-based reranking method that encodes query-passage information into diverse view embeddings without being influenced by external biases. For each view, it combines query-aware passage embeddings to produce a distinct anchor vector, which is then used to directly compute relevance scores in a single decoding step. In addition, it employs an orthogonal loss to make the views more distinctive. Extensive experiments demonstrate that MVP, with just 220M parameters, matches the performance of much larger 7B-scale fine-tuned models while achieving a 100x reduction in inference latency. Notably, the 3B-parameter variant of MVP achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks. The source code is available at: https://github.com/bulbna/MVP",
        "translated": "近年来，大语言模型（LLM）在段落重排序任务中展现出卓越性能。尽管成效显著，基于LLM的方法仍面临效率问题与外部偏差敏感性两大挑战：（1）现有模型主要依赖自回归生成和滑动窗口策略进行段落排序，随着段落数量增加会产生巨大计算开销；（2）位置偏差和选择偏差等外部因素会干扰模型对段落的准确表征，并增强对输入顺序的敏感性。为解决这些局限性，我们提出了一种新型段落重排序模型——多视角引导段落重排序（MVP）。该非生成式LLM重排序方法通过将查询-段落信息编码为多视角嵌入向量，有效规避外部偏差影响。针对每个视角，模型融合查询感知的段落嵌入生成独特锚点向量，进而通过单步解码直接计算相关性得分。此外，该方法采用正交损失函数以增强视角区分度。大量实验表明，仅需2.2亿参数的MVP模型在实现推理延迟降低100倍的同时，性能可媲美70亿参数规模的精调模型。特别值得注意的是，30亿参数版本的MVP在领域内和领域外基准测试中均达到了最先进的性能水平。源代码已开源：https://github.com/bulbna/MVP\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语统一处理：\"autoregressive generation\"译为\"自回归生成\"，\"orthogonal loss\"译为\"正交损失函数\"\n2. 数量单位规范：\"220M/7B\"转换为\"2.2亿/70亿\"符合中文计量习惯\n3. 技术概念准确传达：\"view embeddings\"意译为\"视角嵌入向量\"而非字面直译\n4. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n5. 被动语态转化：\"are used to\"转换为主动语态\"通过...实现\"\n6. 学术表达规范：\"state-of-the-art\"译为\"最先进的\"符合学术中文惯例）"
    },
    {
        "title": "MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for\n  Neural Recommendation Models",
        "url": "http://arxiv.org/abs/2509.07319v1",
        "pub_date": "2025-09-09",
        "summary": "Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance.",
        "translated": "神经协同过滤模型在推荐系统中应用广泛，但通常基于静态设定进行训练，假设数据分布固定不变。这限制了其在用户偏好动态演变环境中的适用性。增量学习虽提供了可行方案，但来自计算机视觉或自然语言处理领域的传统方法因数据稀疏性和任务范式差异，在推荐任务中面临挑战。现有神经推荐器的增量学习方法仍存在局限，且普遍缺乏泛化能力。为此，我们提出MEGG（基于极端GG分数的回放样本）——一种基于经验回放的增量学习框架。MEGG创新性地引入GGscore指标，通过量化样本影响力来实现高效选择性样本回放，从而有效缓解灾难性遗忘问题。该框架具备模型无关特性，可无缝集成至不同架构与框架。在三种神经模型和四个基准数据集上的实验表明，其性能显著优于现有最优基线方法，并展现出强大的可扩展性、高效性和鲁棒性。代码实现将在论文录用后开源发布。\n\n（注：GGscore保留英文大写形式，符合技术术语惯例；\"catastrophic forgetting\"译为专业术语\"灾难性遗忘\"；\"model-agnostic\"采用通用译法\"模型无关\"；\"state-of-the-art\"译为\"现有最优\"符合学术语境）"
    },
    {
        "title": "Datasets for Navigating Sensitive Topics in Recommendation Systems",
        "url": "http://arxiv.org/abs/2509.07269v1",
        "pub_date": "2025-09-08",
        "summary": "Personalized AI systems, from recommendation systems to chatbots, are a prevalent method for distributing content to users based on their learned preferences. However, there is growing concern about the adverse effects of these systems, including their potential tendency to expose users to sensitive or harmful material, negatively impacting overall well-being. To address this concern quantitatively, it is necessary to create datasets with relevant sensitivity labels for content, enabling researchers to evaluate personalized systems beyond mere engagement metrics. To this end, we introduce two novel datasets that include a taxonomy of sensitivity labels alongside user-content ratings: one that integrates MovieLens rating data with content warnings from the Does the Dog Die? community ratings website, and another that combines fan-fiction interaction data and user-generated warnings from Archive of Our Own.",
        "translated": "个性化人工智能系统（从推荐系统到聊天机器人）已成为根据用户学习偏好分发内容的普遍方式。然而，人们日益关注这些系统的负面影响，特别是其可能使用户接触敏感或有害内容，从而对整体福祉产生负面影响的倾向。为量化评估这一问题，需要构建带有内容敏感度标签的数据集，使研究者能够超越简单的参与度指标来评估个性化系统。为此，我们引入了两个新型数据集：一个将MovieLens评分数据与Does the Dog Die?社区评级网站的内容警示标签体系相结合，另一个整合了Archive of Our Own平台的同人小说互动数据与用户生成的警示标签。这些数据集不仅包含用户-内容评分，还提供了系统化的敏感度分类标注。"
    },
    {
        "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
        "url": "http://arxiv.org/abs/2509.07253v1",
        "pub_date": "2025-09-08",
        "summary": "Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.",
        "translated": "大型语言模型（LLM）是处理文本任务的卓越多功能工具，其催生了无数前所未有的应用场景。相比之下，检索模型领域尚未出现具备同等通用能力的模型。要实现这一目标，检索模型必须能够处理复杂检索任务——即查询语句包含多组成部分、约束条件或自然语言需求的场景。这类任务代表着对现有主流评估集中普遍采用的简单单维度查询的自然演进。随着用户期望搜索系统能处理更具体且更具挑战性的信息请求（正如基于LLM的信息系统的使用方式所展现的），复杂查询需求应运而生。\n\n尽管业界对检索模型拓展复杂检索能力的呼声日益高涨，但目前缺乏能够全面评估检索模型在多样化复杂任务上表现的标准资源。现有少数评估资源不仅覆盖范围有限，且往往缺乏真实场景设置，导致难以准确衡量检索模型在现实复杂检索任务中的真实能力。\n\n为弥补这一缺陷并推动下一代检索模型的发展，我们构建了具有多样性和真实性的复杂检索任务集，并对代表性前沿检索模型进行基准测试。此外，我们还探究了基于LLM的查询扩展与重写技术对检索质量的影响。实验结果表明：即使最优模型在复杂检索任务中也表现挣扎，所有任务的平均nDCG@10最高仅达0.346，R@100最高仅为0.587。虽然LLM增强技术能提升较弱模型的性能，但所有重写技术都会导致最强模型的全指标性能下降。"
    },
    {
        "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning\n  Intensive Retrieval",
        "url": "http://arxiv.org/abs/2509.07163v1",
        "pub_date": "2025-09-08",
        "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations: they are constrained by the initial retrieval quality of the top-k documents, and the growing computational demands of LLM-based rerankers restrict the number of documents that can be effectively processed. We introduce Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations by directly retrieving documents according to reranker preferences rather than following the traditional sequential reranking method. Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity. Experimental results demonstrate substantial performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9 on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100 documents. Our analysis suggests that, given a fixed pair of embedding and reranker models, strategically selecting documents to rerank can significantly improve retrieval accuracy under limited reranker budget.",
        "translated": "当前广泛使用的“检索-重排序”流程面临两个关键局限：其性能受限于前k篇文档的初始检索质量，且基于大语言模型的重排序器计算需求日益增长，限制了可有效处理的文档数量。我们提出了一种创新方法——重排序器引导搜索（Reranker-Guided-Search, RGS），通过直接根据重排序器的偏好检索文档（而非遵循传统的顺序重排序流程）来突破这些限制。该方法在近似最近邻算法生成的邻近图上进行贪婪搜索，基于文档相似性策略性地优先选择有潜力的文档进行重排序。实验结果表明，在多个基准测试中均取得显著性能提升：BRIGHT数据集提升3.5个点，FollowIR提升2.9个点，M-BEIR提升5.1个点——且所有这些改进均在100篇文档的重排序计算预算内实现。我们的分析表明，在固定嵌入模型和重排序器组合的前提下，通过策略性选择待重排序文档，能够在有限计算资源下显著提升检索精度。\n\n（注：专业术语说明：\n1. retrieve-and-rerank pipeline：译为\"检索-重排序流程\"\n2. LLM-based rerankers：译为\"基于大语言模型的重排序器\"\n3. proximity graphs：译为\"邻近图\"\n4. approximate nearest neighbor：译为\"近似最近邻\"\n5. constrained reranker budget：译为\"受限的重排序计算预算\"\n6. embedding model：译为\"嵌入模型\"\n所有技术概念均采用计算机信息检索领域标准译法，确保学术准确性。）"
    },
    {
        "title": "Avoiding Over-Personalization with Rule-Guided Knowledge Graph\n  Adaptation for LLM Recommendations",
        "url": "http://arxiv.org/abs/2509.07133v1",
        "pub_date": "2025-09-08",
        "summary": "We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods.",
        "translated": "我们提出了一种轻量级神经符号框架，通过推理时自适应调整用户侧知识图谱（KG）来缓解基于大语言模型的推荐系统中的过度个性化问题。与传统重训练模型或依赖不透明启发式方法不同，我们的方法通过重构用户个性化知识图谱（PKG），抑制那些强化个性化信息环境（PIE）的特征共现模式——即算法导致的限制内容多样性的信息茧房。调整后的PKG用于构建结构化提示，引导语言模型在保持主题相关性的同时生成更多样化的\"非PIE\"推荐。我们提出了一系列符号化适配策略，包括软重加权、硬反转和针对性移除偏见三元组，以及客户端学习算法以优化每用户的策略应用。在食谱推荐基准测试中，个性化PKG适配在保持推荐质量的同时显著提升内容新颖度，其效果优于全局适配和基于朴素提示的方法。\n\n（注：专业术语说明：\n1. Personalized Information Environments (PIEs) 译为\"个性化信息环境\"，特指算法导致的信息茧房效应\n2. Out-PIE recommendations 译为\"非PIE推荐\"，指突破信息茧房的推荐内容\n3. soft reweighting/hard inversion 分别译为\"软重加权/硬反转\"，保持机器学习领域的术语惯例\n4. client-side learning algorithm 译为\"客户端学习算法\"，强调分布式计算场景下的本地化特性）"
    },
    {
        "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
        "url": "http://arxiv.org/abs/2509.06888v1",
        "pub_date": "2025-09-08",
        "summary": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.",
        "translated": "编码器专用语言模型（Encoder-only language models）常被用于各类标准机器学习任务，包括分类与检索。然而当前针对编码器模型的研究，特别是多语言模型领域的研究仍显不足。我们提出了mmBERT——一种基于1800多种语言、3万亿多语种文本训练的纯编码器语言模型。在构建mmBERT过程中，我们引入了多项创新要素，包括逆向掩码比率调度机制和逆向温度采样策略。我们创新性地仅在训练衰减阶段加入1700余种低资源语言数据，实验表明这一方法显著提升模型性能，并最大限度利用了有限训练数据带来的增益。尽管这些低资源语言仅出现在短暂的衰减阶段，我们的模型在分类任务上达到了与OpenAI o3、Google Gemini 2.5 Pro等模型相当的性能。总体而言，mmBERT在高资源与低资源语言的分类和检索任务上均显著超越前代模型。"
    },
    {
        "title": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture",
        "url": "http://arxiv.org/abs/2509.06887v2",
        "pub_date": "2025-09-08",
        "summary": "Modern search systems play a crucial role in facilitating information acquisition. Traditional search engines typically rely on a cascaded architecture, where results are retrieved through recall, pre-ranking, and ranking stages. The complexity of designing and maintaining multiple modules makes it difficult to achieve holistic performance gains. Recent advances in generative recommendation have motivated the exploration of unified generative search as an alternative. However, existing approaches are not genuinely end-to-end: they typically train an item encoder to tokenize candidates first and then optimize a generator separately, leading to objective inconsistency and limited generalization. To address these limitations, we propose UniSearch, a unified generative search framework for Kuaishou Search. UniSearch replaces the cascaded pipeline with an end-to-end architecture that integrates a Search Generator and a Video Encoder. The Generator produces semantic identifiers of relevant items given a user query, while the Video Encoder learns latent item embeddings and provides their tokenized representations. A unified training framework jointly optimizes both components, enabling mutual enhancement and improving representation quality and generation accuracy. Furthermore, we introduce Search Preference Optimization (SPO), which leverages a reward model and real user feedback to better align generation with user preferences. Extensive experiments on industrial-scale datasets, together with online A/B testing in both short-video and live search scenarios, demonstrate the strong effectiveness and deployment potential of UniSearch. Notably, its deployment in live search yields the largest single-experiment improvement in recent years of our product's history, highlighting its practical value for real-world applications.",
        "translated": "现代搜索系统在促进信息获取方面发挥着关键作用。传统搜索引擎通常采用级联架构，通过召回、粗排和精排三个阶段获取结果。由于需要设计和维护多个模块，这种架构难以实现整体性能提升。生成式推荐的最新进展推动了统一生成式搜索的探索，但现有方法并非真正的端到端系统：它们通常先训练物品编码器对候选项目进行标记化，再单独优化生成器，导致目标不一致和泛化能力有限。\n\n为解决这些局限性，我们提出UniSearch——面向快手搜索的统一生成式搜索框架。该框架采用端到端架构替代级联流水线，集成搜索生成器与视频编码器。生成器根据用户查询生成相关项目的语义标识，视频编码器则学习潜在物品嵌入并提供其标记化表示。通过统一训练框架联合优化两个组件，实现相互增强并提升表示质量与生成准确性。此外，我们引入搜索偏好优化（SPO）技术，利用奖励模型和真实用户反馈使生成结果更贴合用户偏好。\n\n基于工业级数据集的大量实验，以及在短视频和直播搜索场景中的在线A/B测试，证明了UniSearch的强大有效性和部署潜力。值得注意的是，该框架在直播搜索场景的部署实现了我们产品近年来最大幅度的单次实验效果提升，凸显了其在实际应用中的重要价值。"
    },
    {
        "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
        "url": "http://arxiv.org/abs/2509.06883v1",
        "pub_date": "2025-09-08",
        "summary": "We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.",
        "translated": "我们参与了CheckThat!任务2的英文赛道，重点探索了多种提示方法与上下文学习策略，包括小样本提示（few-shot prompting）以及基于不同大语言模型（LLM）家族的微调方法，旨在从社交媒体文本中提取值得核查的声明。实验表明，通过对FLAN-T5模型进行微调，我们取得了最佳的METEOR评分。然而，我们注意到在某些情况下，即使其他方法的METEOR分数较低，仍能提取出质量更高的声明内容。"
    },
    {
        "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable\n  Physical Simulations from Monocular Video",
        "url": "http://arxiv.org/abs/2509.08828v1",
        "pub_date": "2025-09-10",
        "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
        "translated": "三维动态场景重建是计算机视觉领域中一个基础但具有挑战性的任务。本文提出了一种创新方法，将三维几何重建与基于物理渲染的外观估计相结合，开发出一个仅需单目RGB视频序列即可实现织物三维重建与外观估计的双任务系统。为获得逼真的高质量形变与渲染效果，我们采用布料物理仿真与可微分渲染技术。针对单目视频中的深度模糊问题，本文引入了两个新颖的正则化项以提升三维重建的合理性。与领域内最新方法相比，我们的方法将三维重建误差降低了2.64倍，且单场景平均仅需30分钟运行时间。此外，优化后的运动序列质量足以支持变形物体的外观估计，从单目RGB视频中成功恢复了清晰的细节特征。\n\n（注：专业术语说明：\n- differentiable rendering: 可微分渲染\n- monocular RGB video: 单目RGB视频\n- depth ambiguity: 深度模糊\n- regularization terms: 正则化项\n- physically based rendering: 基于物理的渲染）"
    },
    {
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "url": "http://arxiv.org/abs/2509.08826v1",
        "pub_date": "2025-09-10",
        "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "translated": "奖励模型（Reward Models, RMs）对于通过强化学习（RL）改进生成模型至关重要，但视觉生成领域的奖励模型规模化范式仍未被充分探索。这主要源于现有方法的根本性局限：基于CLIP的奖励模型受限于架构与输入模态约束，而主流的Bradley-Terry损失函数与视觉语言模型（VLMs）的下一词元预测机制存在本质错位，阻碍了有效扩展。更关键的是，RLHF优化过程长期受\"奖励破解\"（Reward Hacking）问题困扰——模型会利用奖励信号的缺陷而非真正提升生成质量。\n\n为解决这些挑战，我们提出RewardDance——一个可扩展的奖励建模框架。该框架通过创新的生成式奖励范式突破上述限制：将奖励分数重新定义为模型预测\"是\"词元的概率（即生成图像在特定标准下优于参考图像），使奖励目标与VLM架构本质对齐。这种对齐实现了两个维度的扩展：（1）模型规模：系统化将奖励模型参数量扩展至260亿；（2）上下文扩展：整合任务指令、参考示例和思维链（CoT）推理。大量实验表明，RewardDance在文本到图像、文本到视频及图像到视频生成任务上显著超越现有最优方法。\n\n尤为关键的是，我们解决了长期存在的\"奖励破解\"难题：大规模奖励模型在RL微调过程中始终展现并保持高奖励方差，证明其抗破解能力与生成多样化高质量输出的特性，极大缓解了困扰小模型的模式崩溃（mode collapse）问题。"
    },
    {
        "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts",
        "url": "http://arxiv.org/abs/2509.08818v1",
        "pub_date": "2025-09-10",
        "summary": "Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.",
        "translated": "近年来，概率生成模型的发展已从静态图像合成扩展到文本驱动视频生成。然而，其生成过程固有的随机性可能导致不可预测的伪影，例如违背物理规律的画面和时序不一致问题。解决这些挑战需要系统性基准测试，但由于视频独特的时空复杂性，现有数据集主要专注于生成式图像。为填补这一空白，我们推出了GeneVA——一个基于自然文本提示生成视频中时空伪影的大规模人工标注数据集。我们期待GeneVA能够支撑并助力关键应用，如模型性能基准测试与生成视频质量优化。"
    },
    {
        "title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching",
        "url": "http://arxiv.org/abs/2509.08805v1",
        "pub_date": "2025-09-10",
        "summary": "Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.",
        "translated": "密集图像匹配旨在为源图像中的每个像素在部分重叠的目标图像中找到对应点。现有先进方法通常采用由粗到精的匹配机制，即在每个尺度上为每个源位置生成单一对应假设。在具有挑战性的场景中——例如深度不连续区域或目标图像是源图像的强烈放大版本时——相邻源位置的对应点往往分布广泛，此时在每个尺度上为每个源位置预测单一对应假设可能导致错误匹配。本文研究了一种创新方案：在每个尺度上为每个源位置预测多个对应假设。我们采用波束搜索策略在每级尺度传播多重假设，并提出将这些多重假设集成到交叉注意力层中，由此构建出名为BEAMER的新型密集匹配架构。BEAMER能够学习跨尺度保存和传播多重假设，使其在深度不连续区域或目标图像强烈放大场景下的鲁棒性显著优于现有最优方法。"
    },
    {
        "title": "PianoVAM: A Multimodal Piano Performance Dataset",
        "url": "http://arxiv.org/abs/2509.08800v1",
        "pub_date": "2025-09-10",
        "summary": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",
        "translated": "音乐表演的多模态特性促使音乐信息检索（MIR）领域对音频之外的数据日益关注。本文推出PianoVAM——一个包含视频、音频、MIDI、手部关键点、指法标注及丰富元数据的综合性钢琴演奏数据集。该数据集通过Disklavier钢琴录制，采集了业余钢琴演奏者日常练习时的音频与MIDI数据，并在真实多样的表演环境下同步录制了俯视角视频。我们使用预训练的手部姿态估计模型和半自动化指法标注算法提取了手部关键点与指法标签。文中探讨了数据收集过程中面临的挑战以及多模态对齐的技术难点，详细介绍了基于视频手部关键点的指法标注方法。最后，我们使用PianoVAM数据集进行了纯音频与视听结合的钢琴转录基准测试，并讨论了该数据集的其他潜在应用场景。\n\n（注：根据学术规范，术语保持原文大写形式如PianoVAM/Disklavier，技术术语如MIDI/MIR等保留英文缩写，专业表述如\"手部关键点(hand landmarks)\"、\"半自动化指法标注(semi-automated fingering annotation)\"等符合计算机领域中文表达习惯。）"
    },
    {
        "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation",
        "url": "http://arxiv.org/abs/2509.08794v1",
        "pub_date": "2025-09-10",
        "summary": "Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.",
        "translated": "基于事件相机（EBC）的星体跟踪姿态测定技术是一种新兴技术，但以往研究难以获取真实数据的精确地面真值。本研究创新性地利用地球自转作为基准真值，对EBC星体跟踪系统的精度进行量化分析。地球以高度规律的周期自转，其微小扰动可达毫角秒级测量精度。通过将事件相机固定于地面望远镜并对准夜空，我们在天球参考系中构建了仅受地球自转影响的观测系统。处理生成的事件流数据后，我们将估算的姿态方向与国际地球自转参考系（IERS）测量的地球定向进行对比。实验表明：该事件相机系统的定向估计均方根误差为18.47角秒，最大误差约78.84角秒。结合事件相机相较于帧传感器固有的优势（稀疏数据流降低计算量、更高动态范围、更低能耗、更快更新速率），这一精度水平证实了事件相机在低成本低延迟星体跟踪中的应用价值。我们已公开全部代码与数据：https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification。\n\n（注：译文严格遵循学术规范，对专业术语如\"ground truth\"译为\"地面真值\"、\"celestial reference frame\"译为\"天球参考系\"等保持准确；通过拆分长难句、调整语序（如将英文被动语态转换为中文主动表述）确保技术细节的清晰传达；保留原始数值精度及专业机构缩写IERS；完整呈现原文的技术逻辑链条与创新点。）"
    },
    {
        "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images",
        "url": "http://arxiv.org/abs/2509.08780v1",
        "pub_date": "2025-09-10",
        "summary": "Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.   Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.   Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.   Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.",
        "translated": "背景：砷中毒是南亚和东南亚地区严重的公共卫生问题，主要由长期饮用受砷污染的水导致。其早期皮肤表现具有重要临床意义但常被漏诊，尤其在缺乏皮肤科医生的农村地区。基于图像的自动化诊断方案可支持早期发现和及时干预。  \n方法：本研究提出端到端框架，通过手机拍摄的皮肤图像实现砷中毒诊断。构建包含20个类别、11,000余张砷性皮肤病及其他皮肤病症图像的数据集。对包括卷积神经网络（CNN）和基于Transformer的模型在内的多种深度学习架构进行砷中毒检测性能评估。通过LIME和Grad-CAM实现模型可解释性，并基于Web的诊断工具验证部署可行性。  \n结果：基于Transformer的模型显著优于CNN，其中Swin Transformer以86%的准确率取得最佳性能。LIME和Grad-CAM可视化证实模型聚焦于病变相关区域，增强临床透明度并辅助错误分析。该框架在外部验证样本中同样表现优异，证实其泛化能力。  \n结论：本研究框架证明了深度学习技术通过移动设备图像实现非侵入性、可普及且可解释的砷中毒诊断的潜力。通过提供可靠的图像筛查方案，该技术可在缺乏皮肤科医生的资源有限地区作为实用诊断辅助工具，支持早期发现与及时干预。\n\n（注：专业术语处理说明：  \n1. LIME (Local Interpretable Model-agnostic Explanations) 保留英文缩写  \n2. Grad-CAM (Gradient-weighted Class Activation Mapping) 保留英文缩写  \n3. Transformer/Swin Transformer 作为特定模型名称保留英文  \n4. 临床术语如\"cutaneous manifestations\"译为\"皮肤表现\"，\"lesion-relevant regions\"译为\"病变相关区域\"符合医学文献表述规范）"
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pub_date": "2025-09-10",
        "summary": "Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "translated": "多模态大语言模型（MLLMs）正日益被用于评估文本到图像（TTI）生成系统，其能够基于视觉与文本上下文提供自动化评判。然而，这些“裁判”模型常存在偏见、过度自信以及在多样化图像领域中表现不一致的问题。尽管提示集成（prompt ensembling）在单模态纯文本场景中已展现出缓解这些问题的潜力，但我们的实验表明，标准集成方法无法有效推广至TTI任务。针对这些局限性，我们提出了一种新型多模态感知方法——多模态贝叶斯提示集成混合（MMB）。该方法通过贝叶斯提示集成框架结合图像聚类技术，使裁判模型能够根据样本的视觉特征动态分配提示权重。我们证明，MMB在 pairwise 偏好判断中提升了准确性，并显著增强了校准能力，使其更易于评估模型真实的不确定性。在HPSv2和MJBench两个TTI基准测试中，MMB在人类标注对齐度和跨图像内容的校准性能上均优于现有基线方法。我们的研究结果凸显了多模态特异性策略对裁判校准的重要性，并为实现可靠的大规模TTI评估指明了一条可行路径。"
    },
    {
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "url": "http://arxiv.org/abs/2509.08764v1",
        "pub_date": "2025-09-10",
        "summary": "Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "translated": "可靠整合先验信息对于实现自验证与自更新的高精地图至关重要。然而，现有公开数据集均未包含\"先验地图-当前地图-传感器数据\"的三元组。这导致现有方法只能依赖合成先验数据，从而产生数据不一致性并引发严重的模拟到现实差异。为此，我们推出ArgoTweak——首个提供真实地图先验数据的三元组数据集。该数据集核心采用双射映射框架，将大规模地图修改分解为地图元素层级的细粒度原子级变更，确保修改过程的可解释性。这种范式转变能够在保持未变化元素高保真度的同时，实现精确的变化检测与整合。实验表明，使用ArgoTweak训练的模型相较于采用合成先验数据的方法，显著缩小了模拟到现实的性能差距。大量消融实验进一步验证了结构化先验数据与精细化变更标注的重要性。通过建立可解释的先验辅助高精地图绘制基准，ArgoTweak推动了可扩展自优化地图解决方案的发展。数据集、基线模型、地图修改工具箱及相关资源已开源：https://kth-rpl.github.io/ArgoTweak/。\n\n（注：专业术语说明：\n1. self-verifying/self-updating：自验证/自更新\n2. HD maps：高精地图（High-Definition maps）\n3. sim2real gap：模拟到现实差异（simulation-to-reality gap）\n4. bijective mapping：双射映射（数学中的一一对应关系）\n5. atomic changes：原子级变更（不可再分的最小修改单元）\n6. change detection：变化检测\n7. ablations：消融实验（ablation studies））"
    },
    {
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
        "url": "http://arxiv.org/abs/2509.08757v1",
        "pub_date": "2025-09-10",
        "summary": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .",
        "translated": "在动态化、以人为中心的环境中，机器人导航需要基于对场景的深度理解做出符合社会规范的行为决策。当前，视觉-语言模型（VLMs）展现出与社交机器人导航精细化需求高度契合的多种能力，包括目标识别、常识推理和上下文理解等。然而，这类模型是否能准确理解复杂的社交导航场景（例如推断智能体间的时空关系及人类意图）——这一实现安全合规导航的关键前提——仍存在疑问。尽管已有研究尝试将VLMs应用于社交机器人导航，但尚未有系统性工作评估其满足这些必要条件的实际能力。本文提出社交导航场景理解基准（SocialNav-SUB），这是一个基于视觉问答（VQA）任务的数据集与评测体系，专为评估VLMs在真实社交机器人导航场景中的理解能力而设计。该基准通过需要空间推理、时空推理及社会推理的VQA任务，构建了统一框架以对比VLMs与人类基线、规则基线的表现。通过对前沿VLMs的实验发现：虽然性能最优的VLM模型与人类答案的一致性概率达到鼓舞人心的水平，但其表现仍逊于简单的规则基线和人类共识基线，这表明现有VLMs在社交场景理解方面存在显著不足。本基准为社交机器人导航基础模型的后续研究奠定了基础，通过提供标准化框架推动探索如何定制VLMs以满足真实世界的社交导航需求。论文概述、代码及数据详见：https://larg.github.io/socialnav-sub\n\n（注：译文严格遵循学术论文摘要的规范表述，关键技术术语如\"Vision-Language Models (VLMs)\"译为\"视觉-语言模型\"，\"socially-compliant\"译为\"符合社会规范的\"，\"spatial-temporal relations\"译为\"时空关系\"等均采用领域内标准译法。长难句按中文习惯拆分重组，如将原文复合从句\"capabilities that align with...\"处理为独立分句\"展现出与...高度契合的多种能力\"，确保专业性与可读性平衡。）"
    },
    {
        "title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes",
        "url": "http://arxiv.org/abs/2509.08738v1",
        "pub_date": "2025-09-10",
        "summary": "This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.",
        "translated": "本文提出了一种新颖的端到端人群检测方法，通过利用目标密度信息来增强现有基于Transformer的检测器。我们提出的CrowdQuery（CQ）方法核心是CQ模块，该模块可预测并嵌入目标密度图，随后将嵌入的密度信息系统化整合到解码器中。现有密度图定义通常依赖于头部位置或基于目标的空间统计量，而我们的方法扩展了这一定义，将个体边界框尺寸纳入考量。通过将密度信息融入目标查询机制，本方法采用密度引导查询来提升拥挤场景下的检测性能。CQ方法无需额外数据即可同时适用于2D和3D检测任务，由此成为首个有效贯通拥挤环境下2D与3D检测的解决方案。我们演示了将CQ集成至通用2D和3D基于Transformer的目标检测器中的架构CQ2D和CQ3D，且CQ的适用性不限于我们选择的特定Transformer模型。在STCrowd数据集上进行的2D与3D领域实验表明，相较于基线模型，该方法实现了显著性能提升，并优于多数现有先进方法。当集成至最先进的人群检测器时，CQ在具有挑战性的CrowdHuman数据集上可进一步提升性能，证明了其泛化能力。相关代码已发布于https://github.com/mdaehl/CrowdQuery。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"object density map\"译为\"目标密度图\"，\"transformer-based detectors\"译为\"基于Transformer的检测器\"\n2. 技术概念完整保留：\"density-guided queries\"译为\"密度引导查询\"，\"bounding box dimensions\"译为\"边界框尺寸\"\n3. 学术表述规范：\"end-to-end\"译为\"端到端\"，\"state-of-the-art\"译为\"最先进的\"\n4. 长句结构符合中文表达习惯，同时保持技术细节的精确性）"
    },
    {
        "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
        "url": "http://arxiv.org/abs/2509.08715v1",
        "pub_date": "2025-09-10",
        "summary": "As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.",
        "translated": "随着多模态大语言模型（MLLMs）的发展，其大规模架构在资源受限环境中的部署面临挑战。在大模型时代，能源效率、计算可扩展性和环境可持续性至关重要，开发轻量级高性能模型对实际应用具有关键意义。为此，我们提出了一种面向端到端视觉问答的轻量级MLLM框架。该方案的核心是BreezeCLIP——一个紧凑而强大的视觉语言编码器，专为高效多模态理解优化。模型总参数量仅12亿，在显著降低计算成本的同时实现了与标准规模MLLM相当的性能。在多数据集上的实验进一步验证了其在精度与效率平衡方面的有效性。模块化可扩展的设计使其能泛化至更广泛的多模态任务。该轻量级视觉语言框架被命名为BcQLM（BreezeCLIP增强型Q门控多模态语言模型），为在实际硬件限制下部署MLLM提供了可行路径。源代码已开源：https://github.com/thico0224/BcQLM。\n\n（注：译文严格遵循学术论文表述规范，关键技术术语如\"multimodal large language models\"译为\"多模态大语言模型\"，\"vision-language encoder\"译为\"视觉语言编码器\"，\"Q-Gated\"保留技术特征译为\"Q门控\"。数字单位遵循中文计量规范，\"1.2 billion\"转换为\"12亿\"。长难句按中文习惯拆分重组，如将英文复合从句\"where...\"处理为独立判断句\"至关重要...具有关键意义\"，同时保持逻辑严密性。开源链接等要素完整保留。）"
    },
    {
        "title": "Computational Imaging for Enhanced Computer Vision",
        "url": "http://arxiv.org/abs/2509.08712v1",
        "pub_date": "2025-09-10",
        "summary": "This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.",
        "translated": "本文系统综述了计算成像（CI）技术及其对计算机视觉（CV）应用的变革性影响。传统成像方法在低光照、运动模糊或高动态范围场景等挑战性条件下往往难以提供高保真视觉数据，这限制了前沿计算机视觉系统的性能。计算成像技术通过增强图像采集与重建过程，有效解决了这些局限性，具体包括光场成像、高动态范围（HDR）成像、去模糊、高速成像和眩光抑制等技术。本综述系统探讨了CI技术与核心CV任务（含目标检测、深度估计、光流分析、人脸识别和关键点检测）之间的协同效应。通过分析CI方法与其对CV应用的实际贡献之间的关联，本研究揭示了新兴机遇、现存挑战及未来研究方向。我们重点探讨了面向特定任务的自适应成像流程的潜力，这些流程能在自动驾驶、监控、增强现实和机器人等现实场景中提升系统的鲁棒性、精度与效率。"
    },
    {
        "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
        "url": "http://arxiv.org/abs/2509.08699v1",
        "pub_date": "2025-09-10",
        "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.",
        "translated": "在机器人视觉导航领域，传统方法通常依赖全局一致的3D地图或学习型控制器，这些方法存在计算成本高且难以跨环境泛化的局限性。本研究提出了一种创新的纯RGB对象级拓扑导航框架，无需3D地图或预训练控制器即可实现零样本的长程机器人导航。该方法通过融合全局拓扑路径规划与局部度量轨迹控制，使机器人能够在避开障碍物的同时导航至对象级子目标。\n\n我们通过以下核心创新解决了现有方法的缺陷：利用单目深度估计和可通行性预测实现连续局部轨迹规划，并引入自动切换机制在必要时回退至基线控制器。该系统基于基础模型构建，无需领域特异性微调即可实现开放场景的适用性。通过仿真环境与真实场景测试，我们验证了该方法在鲁棒性和部署便利性方面的优势。实验表明，本方法在开放环境视觉导航任务中优于现有最优方案，提供了更具适应性的解决方案。相关源代码已开源：https://github.com/podgorki/TANGO。\n\n（注：根据学术规范，对技术术语进行了标准化处理：\n1. \"topometric navigation\"译为\"拓扑导航\"以符合机器人学规范\n2. \"zero-shot\"保留零样本特性但采用\"零样本\"标准译法\n3. \"foundational models\"译为\"基础模型\"符合AI领域共识\n4. \"open-set applicability\"译为\"开放场景适用性\"以准确传达原文语义\n5. 保持中英文术语对应关系，如\"traversability estimation\"统一译为\"可通行性预测\"）"
    },
    {
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework",
        "url": "http://arxiv.org/abs/2509.08694v1",
        "pub_date": "2025-09-10",
        "summary": "Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "translated": "基于卫星影像的海岸水域分割任务面临光谱特征复杂与边界形态不规则等独特挑战。传统RGB方法在多样化海洋环境中常出现训练不稳定和泛化能力不足的问题。本文提出一种系统性鲁棒增强框架——Robust U-Net，通过引入HSV色彩空间监督与多模态约束机制提升海岸水域分割性能。该框架集成五大协同组件：HSV色彩引导监督、梯度式海岸线优化、形态学后处理、海域净化和连通性控制。综合消融实验表明，HSV监督模块贡献度最高（影响系数0.85），完整框架可实现显著训练稳定性提升（方差降低84%）并增强分割质量。本方法在多项评估指标中均保持稳定改进，同时维持计算效率。为促进可复现性，训练配置与代码已开源：https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet。"
    },
    {
        "title": "FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical\n  Flow Estimation with Total Variation Regularization",
        "url": "http://arxiv.org/abs/2509.08670v1",
        "pub_date": "2025-09-10",
        "summary": "We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.",
        "translated": "我们提出了FractalPINN-Flow——一种无监督深度学习框架，用于直接从连续灰度帧中学习稠密光流估计，无需真实标注数据。该架构的核心是分形形变网络（FDN），这是一个受分形几何和自相似性启发的递归编码器-解码器结构。与传统采用顺序下采样的CNN不同，FDN通过重复的编码器-解码器嵌套结构与跳跃连接，同时捕获细粒度细节和长程运动模式。训练目标基于经典变分公式，采用全变分（TV）正则化：具体通过最小化结合$L^1$和$L^2$数据保真项（用于增强亮度恒定性）与TV项（促进空间平滑性和流场一致性）的能量泛函实现。在合成数据和基准数据集上的实验表明，FractalPINN-Flow能生成精确、平滑且保持边缘细节的光流场。该模型尤其适用于高分辨率数据及标注有限的场景。\n\n（注：专业术语说明：\n- FractalPINN-Flow: 保留英文形式，体现模型命名\n- optical flow: 光流\n- encoder-decoder: 编码器-解码器\n- total variation: 全变分\n- brightness constancy: 亮度恒定性\n- edge-preserving: 边缘保持）"
    },
    {
        "title": "Skeleton-based sign language recognition using a dual-stream\n  spatio-temporal dynamic graph convolutional network",
        "url": "http://arxiv.org/abs/2509.08661v1",
        "pub_date": "2025-09-10",
        "summary": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "translated": "孤立手语识别（ISLR）面临形态相似但语义迥异的手势识别难题，这一挑战源于手部形状与运动轨迹之间复杂的相互作用。现有方法通常依赖单一参考帧，难以解决这种几何模糊性问题。本文提出双参考帧双流架构Dual-SignLanguageNet（DSLNet），通过在两个互补坐标系中解耦并分别建模手势形态和运动轨迹。我们的方法采用腕部中心坐标系实现视角无关的形状分析，以及面部中心坐标系实现上下文感知的轨迹建模——两个分支分别由专用网络处理：采用拓扑感知图卷积网络处理形状信息，基于芬斯勒几何的编码器处理轨迹信息，最终通过几何驱动的最优传输融合机制进行整合。DSLNet以显著更少的参数量，在具有挑战性的WLASL-100、WLASL-300和LSA64数据集上分别达到93.70%、89.97%和99.79%的准确率，创造了新的性能标杆。\n\n（注：专业术语说明：\n1. Finsler geometry保留为\"芬斯勒几何\"，这是微分几何中的标准术语\n2. optimal transport采用学界通用译法\"最优传输\"\n3. graph convolution遵循计算机视觉领域规范译为\"图卷积\"\n4. WLASL/LSA64作为标准数据集名称保留不译\n5. parameters根据上下文译为\"参数量\"而非\"参数\"，更符合深度学习领域的表述习惯）"
    },
    {
        "title": "X-Part: high fidelity and structure coherent shape decomposition",
        "url": "http://arxiv.org/abs/2509.08643v1",
        "pub_date": "2025-09-10",
        "summary": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.",
        "translated": "在部件级别生成三维形状对于网格重拓扑、UV映射和三维打印等下游应用至关重要。然而，现有的基于部件的生成方法往往缺乏足够的可控性，且语义化分解效果不佳。为此，我们提出了X-Part——一种可控生成模型，能够将整体三维对象分解为具有高几何保真度的语义化部件，并保持结构一致性。该模型以边界框作为部件生成提示，并通过注入点级语义特征实现有意义的结构分解。此外，我们设计了支持交互式部件生成的可编辑流程。大量实验结果表明，X-Part在部件级形状生成任务上达到了最先进的性能。这项工作为创建生产就绪、可编辑且结构合理的三维资产建立了新范式。代码将开源以供学术研究使用。\n\n（注：根据学术规范，对\"Codes will be released for public research\"采用国内计算机领域常用表述\"代码将开源\"进行意译，既符合中文表达习惯，也准确传递了原意）"
    },
    {
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and\n  Correct Image Interpretation Model Shortcuts",
        "url": "http://arxiv.org/abs/2509.08640v1",
        "pub_date": "2025-09-10",
        "summary": "Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\\% of cases, correctly incorporated the specified finding in 89-99\\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\\% AUC in internal validation and by 1-11\\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.",
        "translated": "胸部X光片（CXR）是临床最常用的检查手段之一。自动化影像解读可减轻放射科医生的工作负担并扩大诊断专业知识的可及性。深度学习多任务与基础模型虽在CXR解读中表现出色，但存在捷径学习缺陷——模型依赖虚假或偏离目标的关联而非临床相关特征进行决策。我们提出RoentMod反事实图像编辑框架，该框架能生成具有用户指定合成病变、且保留原始扫描中无关解剖特征的解剖学真实CXR图像。RoentMod将开源医学图像生成器（RoentGen）与图像修改模型相结合，无需重新训练。经执业放射医师和放射科住院医师参与的阅片研究显示：RoentMod生成的图像真实性获93%认可，89-99%的案例正确融合指定病灶特征，其原生解剖结构保存度与真实随访CXR相当。通过RoentMod实验，我们发现当前最优的多任务与基础模型频繁利用偏离目标的病变特征作为捷径，限制了模型特异性。在训练中加入Roent生成的反事实图像后，模型抗干扰能力显著提升：内部验证中多项病变的判别AUC提升3-19%，外部测试中6类病变有5类提升1-11%。这些发现表明RoentMod可作为普适性工具用于探测和修正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了CXR解读模型的鲁棒性与可解释性，为改进医学影像基础模型提供了可推广的策略。"
    },
    {
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain\n  Translation",
        "url": "http://arxiv.org/abs/2509.08628v1",
        "pub_date": "2025-09-10",
        "summary": "Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "translated": "扩散模型在生成高质量输出方面表现出色，但在数据稀缺领域面临挑战，这些领域通常需要 exhaustive 的重新训练或成本高昂的配对数据。为应对这些局限性，我们提出潜在对齐扩散桥（LADB），这是一种用于样本到样本转换的半监督框架，能够利用部分配对数据有效弥合领域差距。通过在共享潜在空间中对齐源域和目标域分布，LADB 将预训练的源域扩散模型与目标域潜在对齐扩散模型（LADM）无缝集成，后者基于部分配对的潜在表示进行训练。这一方法实现了确定性的领域映射，无需完全监督。与通常缺乏可控性的非配对方法以及需要大型领域特定数据集的完全配对方法相比，LADB 通过结合配对和非配对的潜在-目标耦合，在保真度和多样性之间取得了平衡。我们的实验结果表明，该方法在部分监督下的深度图到图像转换任务中表现优异。此外，我们将 LADB 扩展到多源转换（从深度图和分割掩码）以及类别条件风格迁移任务中的多目标转换，展示了其处理多样化和异构用例的灵活性。最终，我们提出 LADB 作为一种可扩展且通用的解决方案，适用于现实世界的领域转换任务，尤其是在数据标注成本高昂或不完整的场景中。"
    },
    {
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "url": "http://arxiv.org/abs/2509.08827v1",
        "pub_date": "2025-09-10",
        "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "translated": "本文综述了强化学习（RL）在大语言模型（LLM）推理任务中的最新进展。强化学习在拓展大语言模型能力边界方面取得了显著成就，尤其在数学与编程等复杂逻辑任务上表现突出。因此，强化学习已成为将大语言模型升级为逻辑推理模型（LRM）的基础方法论。随着该领域的快速发展，逻辑推理模型的强化学习规模化应用正面临基础性挑战，这些挑战不仅存在于计算资源层面，更涉及算法设计、训练数据与基础设施等多个维度。为此，有必要重新审视该领域的发展路径，评估其演进轨迹，并探索增强强化学习可扩展性以实现人工超智能（ASI）的策略。本文重点分析了自DeepSeek-R1发布以来，强化学习在提升大语言模型与逻辑推理模型推理能力方面的研究进展，涵盖基础组件、核心问题、训练资源及下游应用等维度，以揭示这一快速发展领域的未来机遇与方向。我们希望本综述能推动强化学习在更广泛推理模型中的深入研究。GitHub项目地址：https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs\n\n（注：译文严格遵循学术规范，对RL（强化学习）、LLM（大语言模型）、LRM（逻辑推理模型）、ASI（人工超智能）等专业术语采用括号标注全称的规范译法，保持技术表述的准确性。长难句按中文习惯拆分重组，确保逻辑清晰性。GitHub链接等数字资源信息完整保留。）"
    },
    {
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
        "url": "http://arxiv.org/abs/2509.08825v1",
        "pub_date": "2025-09-10",
        "summary": "Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.   We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.   Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.",
        "translated": "大型语言模型（LLMs）正通过自动化数据标注和文本分析等劳动密集型任务，迅速改变社会科学研究范式。然而，LLM的输出结果会因研究者的实施选择（如模型选择、提示策略或温度参数设置）产生显著差异。这种差异可能引入系统性偏差和随机误差，进而蔓延至下游分析，导致第一类错误、第二类错误、符号错误或量级错误。我们将这种现象称为\"LLM黑客行为\"。  \n\n我们通过复现21项已发表社会科学研究中的37个数据标注任务，使用18种不同模型进行量化分析。基于对1300万个LLM生成标签的检验，我们针对2,361个现实假设测试了研究者不同选择对统计结论的影响。研究发现：使用最先进模型标注数据时，约三分之一的假设会得出错误结论；而使用小型语言模型时，错误结论比例高达二分之一。虽然任务表现提升和模型通用能力增强可降低LLM黑客风险，但即使高精度模型也无法完全消除该风险。随着效应量增大，LLM黑客风险会相应降低，这表明对接近显著性阈值的研究发现需要更严格的验证。  \n\n我们对LLM黑客风险缓解技术的深入分析表明：人工标注对于降低假阳性发现和改进模型选择至关重要。令人惊讶的是，常见的回归估计量校正技术对降低LLM黑客风险基本无效，因为它们需要大幅权衡第一类与第二类错误。  \n\n除意外误差外，我们发现故意实施LLM黑客行为异常简单：仅需使用少量LLM模型和提示词变体，任何结论都可以被呈现为具有统计显著性。"
    },
    {
        "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora",
        "url": "http://arxiv.org/abs/2509.08824v1",
        "pub_date": "2025-09-10",
        "summary": "The performance of large language models (LLMs) is deeply influenced by the quality and composition of their training data. While much of the existing work has centered on English, there remains a gap in understanding how to construct effective training corpora for other languages. We explore scalable methods for building web-based corpora for LLMs. We apply them to build a new 120B token corpus in Portuguese that achieves competitive results to an industrial-grade corpus. Using a continual pretraining setup, we study how different data selection and preprocessing strategies affect LLM performance when transitioning a model originally trained in English to another language. Our findings demonstrate the value of language-specific filtering pipelines, including classifiers for education, science, technology, engineering, and mathematics (STEM), as well as toxic content. We show that adapting a model to the target language leads to performance improvements, reinforcing the importance of high-quality, language-specific data. While our case study focuses on Portuguese, our methods are applicable to other languages, offering insights for multilingual LLM development.",
        "translated": "大型语言模型（LLM）的性能深受其训练数据质量与构成的影响。尽管现有研究多集中于英语语料，但针对其他语言如何构建有效训练语料仍存在认知空白。本研究探索了基于网络数据构建多语言LLM语料库的可扩展方法，并成功构建了包含1200亿词元的葡萄牙语语料库，其性能可与工业级语料库相媲美。通过持续预训练实验框架，我们系统分析了当将原基于英语训练的模型迁移至其他语言时，不同数据筛选与预处理策略对模型性能的影响。研究结果表明：采用针对目标语言的专项过滤流程（包括教育类、STEM领域及有害内容分类器）具有显著价值；模型适应目标语言后性能持续提升，印证了高质量语言专属数据的重要性。虽然本案例研究聚焦葡萄牙语，但所提出的方法适用于其他语言，为多语言LLM开发提供了重要实践洞见。"
    },
    {
        "title": "Merge-of-Thought Distillation",
        "url": "http://arxiv.org/abs/2509.08814v1",
        "pub_date": "2025-09-10",
        "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different \"best teachers,\" and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into student with overcoming conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students.",
        "translated": "尽管实际中存在多个候选教师模型且思维链语料库日益丰富，高效的长链思维推理蒸馏仍长期受限于单一最优教师的假设约束。我们重新审视教师选择机制，发现不同学生存在差异化的\"最佳教师\"，甚至同一学生在不同数据集上的最优教师也会变化。为此，我们提出融合思维蒸馏框架（MoT），通过交替执行教师专属的监督微调分支与权重空间融合，将多位教师的推理能力统一注入学生模型，有效解决多教师监督间的冲突。在数学竞赛基准测试中，仅使用约200个高质量思维链样本，对Qwen3-14B学生模型应用MoT后，其表现超越了DEEPSEEK-R1、QWEN3-30B-A3B、QWEN3-32B及OPENAI-O1等强劲模型，实现显著性能提升。该框架不仅持续优于最佳单教师蒸馏和朴素多教师联合方法，在提升性能上限的同时缓解过拟合现象，还对分布偏移和同级别教师表现出鲁棒性。此外，MoT能减少灾难性遗忘，提升数学领域外的泛化推理能力，甚至培育出更优质的教师模型，表明经过共识过滤的推理特征具有广泛迁移性。这些成果使MoT成为从多元教师向紧凑学生模型高效蒸馏长链思维能力的简洁可扩展方案。"
    },
    {
        "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
        "url": "http://arxiv.org/abs/2509.08812v1",
        "pub_date": "2025-09-10",
        "summary": "Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC",
        "translated": "基于子词的标记化方法往往难以有效保留形态学边界，这一局限在资源匮乏且形态复杂的语言（如使用吉兹字母的文字体系）中尤为明显。为此，我们提出MoVoC（形态素感知的子词词汇构建方法），并训练出集成监督式形态分析的标记器MoVoC-Tok。该混合分词方法结合了基于形态素的分词与字节对编码（BPE）标记，在保持词汇语义的同时维护形态完整性。针对资源稀缺问题，我们整理并发布了四种吉兹字母语言的人工标注形态素数据集，以及其中两种语言的形态素感知词汇表。虽然所提出的标记化方法未显著提升自动翻译质量，但我们观察到内在评估指标MorphoScore和边界精确率（Boundary Precision）的持续改善，这凸显了形态感知分词在提升语言保真度和标记效率方面的价值。我们公开提供形态素标注数据集与标记器，以支持对资源匮乏型形态丰富语言的进一步研究。代码与数据详见GitHub：https://github.com/hailaykidu/MoVoC\n\n（注：Geez script作为专有名词保留\"吉兹字母\"译法，MorphoScore作为专业指标名称保留英文形式，技术术语如\"Byte Pair Encoding (BPE)\"采用学界通用译法\"字节对编码\"）"
    },
    {
        "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals",
        "url": "http://arxiv.org/abs/2509.08809v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have significantly reduced data annotation costs and reliance on human annotators. However, evaluating the quality of their annotations remains challenging in dynamic, unsupervised environments where oracle feedback is scarce and conventional methods fail. To address this challenge, we propose a novel agentic annotation paradigm, where a student model collaborates with a noisy teacher (the LLM) to assess and refine annotation quality without relying on oracle feedback. The student model, acting as an unsupervised feedback mechanism, employs a user preference-based majority voting strategy to evaluate the consistency of the LLM outputs. To systematically measure the reliability of LLM-generated annotations, we introduce the Consistent and Inconsistent (CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only quantifies the annotation quality of the noisy teacher under limited user preferences but also plays a critical role in model selection, enabling the identification of robust LLMs in dynamic, unsupervised environments. Applied to ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a strong positive correlation with LLM accuracy, establishing it as an essential tool for unsupervised evaluation and model selection in real-world settings.",
        "translated": "在基于提示的任务中，大语言模型（LLMs）显著降低了数据标注成本并减少了对人工标注者的依赖。然而，在动态无监督环境中，由于缺乏真实反馈且传统方法失效，评估其标注质量仍具挑战性。针对这一问题，我们提出了一种新型智能标注范式：通过学生模型与噪声教师（即大语言模型）协作，在不依赖真实反馈的情况下评估并优化标注质量。该学生模型作为无监督反馈机制，采用基于用户偏好的多数投票策略来评估大语言模型输出的一致性。为系统衡量大语言模型生成标注的可靠性，我们提出了\"一致性与不一致性比率\"（CAI Ratio）这一新型无监督评估指标。CAI比率不仅能量化噪声教师在有限用户偏好下的标注质量，还在模型选择中发挥关键作用，帮助在动态无监督环境中识别稳健的大语言模型。通过在四个大语言模型和十个开放域NLP数据集上的实验验证，CAI比率与大语言模型准确率呈现强正相关，证明了其作为现实场景中无监督评估与模型选择核心工具的有效性。"
    },
    {
        "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
        "url": "http://arxiv.org/abs/2509.08803v1",
        "pub_date": "2025-09-10",
        "summary": "The rise of misinformation underscores the need for scalable and reliable fact-checking solutions. Large language models (LLMs) hold promise in automating fact verification, yet their effectiveness across global contexts remains uncertain. We systematically evaluate nine established LLMs across multiple categories (open/closed-source, multiple sizes, diverse architectures, reasoning-based) using 5,000 claims previously assessed by 174 professional fact-checking organizations across 47 languages. Our methodology tests model generalizability on claims postdating training cutoffs and four prompting strategies mirroring both citizen and professional fact-checker interactions, with over 240,000 human annotations as ground truth. Findings reveal a concerning pattern resembling the Dunning-Kruger effect: smaller, accessible models show high confidence despite lower accuracy, while larger models demonstrate higher accuracy but lower confidence. This risks systemic bias in information verification, as resource-constrained organizations typically use smaller models. Performance gaps are most pronounced for non-English languages and claims originating from the Global South, threatening to widen existing information inequalities. These results establish a multilingual benchmark for future research and provide an evidence base for policy aimed at ensuring equitable access to trustworthy, AI-assisted fact-checking.",
        "translated": "错误信息的泛滥凸显了对可扩展且可靠的事实核查解决方案的迫切需求。大型语言模型（LLMs）在自动化事实核查领域展现出潜力，但其在全球语境下的有效性仍存疑问。本研究系统评估了九种主流LLMs（涵盖开源/闭源、多种参数量级、不同架构及基于推理的模型），使用由174家专业事实核查机构以47种语言验证过的5,000条声明作为测试集。我们的方法通过训练截止日期后产生的声明测试模型泛化能力，并采用四种提示策略模拟普通用户和专业核查人员的交互场景，以超过24万条人工标注作为基准真值。\n\n研究发现存在类似邓宁-克鲁格效应的风险模式：轻量级开源模型虽准确率较低却呈现高置信度，而大型模型准确率更高却表现更谨慎。这种偏差可能导致系统性风险——资源有限的机构通常使用小型模型进行信息验证。性能差距在非英语语境及源自全球南方的声明中尤为显著，这种技术鸿沟可能加剧现有的信息不平等。本研究为后续研究建立了多语言基准测试框架，并为制定保障公平获取可信AI辅助事实核查政策的制定提供了实证依据。"
    },
    {
        "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A\n  Cross-Architecture Analysis of Recall Mechanisms",
        "url": "http://arxiv.org/abs/2509.08778v1",
        "pub_date": "2025-09-10",
        "summary": "Understanding how Transformer-based language models store and retrieve factual associations is critical for improving interpretability and enabling targeted model editing. Prior work, primarily on GPT-style models, has identified MLP modules in early layers as key contributors to factual recall. However, it remains unclear whether these findings generalize across different autoregressive architectures. To address this, we conduct a comprehensive evaluation of factual recall across several models -- including GPT, LLaMA, Qwen, and DeepSeek -- analyzing where and how factual information is encoded and accessed. Consequently, we find that Qwen-based models behave differently from previous patterns: attention modules in the earliest layers contribute more to factual recall than MLP modules. Our findings suggest that even within the autoregressive Transformer family, architectural variations can lead to fundamentally different mechanisms of factual recall.",
        "translated": "理解基于Transformer的语言模型如何存储和检索事实关联，对于提升模型可解释性和实现定向模型编辑至关重要。先前针对GPT类模型的研究发现，早期层的MLP模块是事实召回的关键贡献者。然而，这些发现是否适用于不同自回归架构仍不明确。为此，我们对包括GPT、LLaMA、Qwen和DeepSeek在内的多个模型进行了事实召回能力的综合评估，分析事实信息编码与访问的位置及机制。研究发现，基于Qwen的模型表现出与既往模式不同的特性：其最早层的注意力模块对事实召回的贡献度超过MLP模块。这一结果表明，即使在自回归Transformer架构家族内部，结构差异也可能导致事实召回机制的根本性不同。"
    },
    {
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.08755v1",
        "pub_date": "2025-09-10",
        "summary": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.",
        "translated": "开发能够通过一系列智能决策解决复杂现实任务的自主大语言模型（LLM）智能体，是当前快速演进的前沿领域。与人类认知发展类似，智能体需要通过与环境探索和交互来获取知识与技能。尽管已有诸多进展，学界仍缺乏一个统一的交互式强化学习（RL）框架，能够在多样化的现实环境中完全从零开始（无需监督微调SFT）有效训练此类智能体。为填补这一空白，我们提出了AgentGym-RL——一个通过强化学习训练多轮交互决策LLM智能体的新型框架。该框架采用模块化解耦架构，确保高度灵活性与可扩展性，涵盖多样化现实场景，并支持主流RL算法。\n\n此外，我们提出ScalingInter-RL训练方法，旨在实现探索-利用的平衡与稳定的RL优化。该方法在早期阶段通过限制交互次数强调利用策略，随后逐步扩大探索范围以鼓励多样化问题解决策略。这种设计使智能体能够发展出更丰富的行为模式，并降低长周期任务中的策略崩溃风险。我们通过大量实验验证了AgentGym-RL框架与ScalingInter-RL方法的稳定性与有效性：训练的智能体在27个跨领域任务中达到或超越了商业模型性能。我们将提供关键洞见并开源完整的AgentGym-RL框架（含代码与数据集），以推动新一代智能体的研发。\n\n（注：专业术语说明：\n- LLM：大语言模型（Large Language Model）\n- RL：强化学习（Reinforcement Learning）\n- SFT：监督微调（Supervised Fine-Tuning）\n- 探索-利用平衡（exploration-exploitation tradeoff）：强化学习中智能体在尝试新策略（探索）与执行已知有效策略（利用）之间的平衡机制）"
    },
    {
        "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
        "url": "http://arxiv.org/abs/2509.08753v1",
        "pub_date": "2025-09-10",
        "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at https://github.com/kyutai-labs/delayed-streams-modeling",
        "translated": "我们提出了延迟流建模（Delayed Streams Modeling, DSM），这是一种面向流式多模态序列到序列学习的灵活框架。传统的序列到序列生成通常以离线方式进行，即模型需完整读取输入序列后才开始生成第一个输出时间步。而流式序列到序列方法则依赖于学习一种策略来决定何时推进输入流或写入输出流。与之不同，DSM采用仅含解码器的语言模型来处理已经时间对齐的流数据。通过将对齐过程移至预处理阶段，并在不同流之间引入适当延迟，DSM能够实现从任意输入组合到任意输出序列的流式推理，使其可广泛应用于多种序列到序列问题。特别地，当给定文本和音频流时，自动语音识别（ASR）对应于文本流延迟的情况，而相反配置则构成文本到语音（TTS）模型。我们针对这两大核心序列到序列任务进行了大量实验，结果表明DSM在支持任意长序列的同时，实现了最先进的性能与延迟表现，甚至可与离线基线模型相媲美。代码、样本及演示详见https://github.com/kyutai-labs/delayed-streams-modeling。\n\n（注：根据学术规范，术语处理说明：\n1. \"streaming\"在计算领域统一译为\"流式\"\n2. \"decoder-only language model\"译为\"仅含解码器的语言模型\"\n3. \"state-of-the-art\"遵循国内学术惯例译为\"最先进的\"\n4. 专业缩写ASR/TTS首次出现时标注英文全称及中文译名）"
    },
    {
        "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
        "url": "http://arxiv.org/abs/2509.08729v1",
        "pub_date": "2025-09-10",
        "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.   Maintaining selection pressure by setting the success threshold to $\\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.   Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.",
        "translated": "多轮转单轮（M2S）方法将迭代式红队测试压缩至单个结构化提示中，但以往研究依赖少量人工编写的模板。我们提出X-Teaming Evolutionary M2S框架，通过语言模型引导的进化自动发现并优化M2S模板。该系统结合了从12个数据源的智能采样策略，采用受StrongREJECT启发的LLM-as-judge评估机制，并记录完全可审计的日志。  \n通过将成功阈值设定为θ=0.70以维持选择压力，我们获得了五代进化结果、两个新模板家族，并在GPT-4.1上实现44.8%的整体成功率（103/230）。针对2,500次实验的平衡跨模型评估（固定评判标准）表明，结构增益具有可迁移性但随目标模型变化；有两个模型在相同阈值下成功率为零。我们还发现提示长度与得分呈正相关，这启发了长度感知的评判机制。  \n本研究证明：结构级搜索是增强单轮探测的有效可复现路径，同时强调了阈值校准和跨模型评估的重要性。代码、配置与实验成果详见https://github.com/hyunjun1121/M2S-x-teaming。\n\n（注：根据学术规范，关键术语处理如下：  \n- \"red-teaming\" 译为\"红队测试\"（安全评估方法）  \n- \"LLM-as-judge\" 译为\"LLM-as-judge评估机制\"（保留英文缩写并添加说明）  \n- \"selection pressure\" 译为\"选择压力\"（进化算法术语）  \n- \"cross-model evaluation\" 译为\"跨模型评估\"）"
    },
    {
        "title": "Generative Data Refinement: Just Ask for Better Data",
        "url": "http://arxiv.org/abs/2509.08653v1",
        "pub_date": "2025-09-10",
        "summary": "For a fixed parameter size, the capabilities of large models are primarily determined by the quality and quantity of its training data. Consequently, training datasets now grow faster than the rate at which new data is indexed on the web, leading to projected data exhaustion over the next decade. Much more data exists as user-generated content that is not publicly indexed, but incorporating such data comes with considerable risks, such as leaking private information and other undesirable content. We introduce a framework, Generative Data Refinement (GDR), for using pretrained generative models to transform a dataset with undesirable content into a refined dataset that is more suitable for training. Our experiments show that GDR can outperform industry-grade solutions for dataset anonymization, as well as enable direct detoxification of highly unsafe datasets. Moreover, we show that by generating synthetic data that is conditioned on each example in the real dataset, GDR's refined outputs naturally match the diversity of web scale datasets, and thereby avoid the often challenging task of generating diverse synthetic data via model prompting. The simplicity and effectiveness of GDR make it a powerful tool for scaling up the total stock of training data for frontier models.",
        "translated": "对于固定参数规模的大模型而言，其能力主要取决于训练数据的质量与数量。当前训练数据集的增长速度已超过网络公开索引数据的增长率，预计未来十年将面临数据枯竭问题。大量用户生成内容虽未公开索引，但直接使用此类数据存在显著风险，包括隐私泄露和不良内容等问题。我们提出生成式数据精炼框架（GDR），通过预训练生成模型将含不良内容的数据集转化为更适合训练的精炼数据集。实验表明，GDR在数据集匿名化处理方面优于工业级解决方案，并能直接对高度不安全数据集进行脱毒处理。通过基于真实数据集样本生成条件化合成数据，GDR的精炼输出自然保持网络规模数据集的多样性，避免了通过模型提示生成多样化合成数据的技术挑战。GDR的简洁性与有效性使其成为扩展前沿模型训练数据总量的有力工具。"
    },
    {
        "title": "OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for\n  Aspect-Based Sentiment Analysis",
        "url": "http://arxiv.org/abs/2509.08612v1",
        "pub_date": "2025-09-10",
        "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and determine their sentiment polarity. While dependency trees combined with contextual semantics effectively identify aspect sentiment, existing methods relying on syntax trees and aspect-aware attention struggle to model complex semantic relationships. Their dependence on linear dot-product features fails to capture nonlinear associations, allowing noisy similarity from irrelevant words to obscure key opinion terms. Motivated by Differentiable Optimal Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN), which introduces a Syntactic-Semantic Collaborative Attention. It comprises a Syntactic Graph-Aware Attention for mining latent syntactic dependencies and modeling global syntactic topology, as well as a Semantic Optimal Transport Attention designed to uncover fine-grained semantic alignments amidst textual noise, thereby accurately capturing sentiment signals obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates these heterogeneous features, and contrastive regularization further improves robustness. Experiments demonstrate that OTESGN achieves state-of-the-art results, outperforming previous best models by +1.01% F1 on Twitter and +1.30% F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its efficacy in precise localization of opinion words and noise resistance.",
        "translated": "基于方面的情感分析（ABSA）旨在识别文本中的方面术语并判定其情感极性。尽管依赖树与上下文语义结合能有效识别方面情感，但现有基于语法树和方面感知注意力机制的方法难以建模复杂语义关系。这些方法对线性点积特征的依赖无法捕捉非线性关联，导致无关词汇产生的噪声相似度干扰关键观点词的识别。受可微分最优匹配理论启发，我们提出基于最优传输增强的语法-语义图网络（OTESGN），创新性地引入语法-语义协同注意力机制：该机制包含语法图感知注意力（挖掘潜在语法依赖并建模全局语法拓扑结构）和语义最优传输注意力（在文本噪声中发现细粒度语义对齐，从而精准捕获被无关词元遮蔽的情感信号）。自适应注意力融合模块整合这些异构特征，对比正则化进一步提升了模型鲁棒性。实验表明OTESNN取得最先进性能，在Twitter数据集上F1值较之前最佳模型提升1.01%，在Laptop14基准上提升1.30%。消融研究与可视化分析验证了该方法在观点词精确定位和噪声抵抗方面的有效性。\n\n（注：根据学术规范，对关键术语进行标准化处理：\n- Differentiable Optimal Matching 译为\"可微分最优匹配\"\n- Optimal Transport Enhanced Syntactic-Semantic Graph Network 保留首字母缩写OTESGN并给出全称\"最优传输增强的语法-语义图网络\"\n- Syntactic Graph-Aware Attention/Semantic Optimal Transport Attention 采用\"语法图感知注意力/语义最优传输注意力\"的译法\n- 技术指标F1值保留原始计量单位%）"
    },
    {
        "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
        "url": "http://arxiv.org/abs/2509.08604v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.",
        "translated": "大型语言模型（LLMs）在医学领域展现出巨大潜力。迄今，LLMs已被广泛应用于辅助诊断、医学问答和临床信息整合等任务。然而，一个关键问题尚未解决：LLMs对医学训练数据的记忆程度究竟如何？本研究首次对医学领域LLMs的记忆现象开展系统性评估，从普遍性（发生频率）、特征（记忆内容类型）、体量（记忆信息量）及潜在下游影响（记忆如何影响医学应用）四个维度进行探究。我们系统分析了三种常见适应场景：（1）基于医学语料的持续预训练；（2）在标准医学基准上的微调；（3）基于真实世界临床数据的微调，包括来自耶鲁纽黑文医疗系统的超13,000条独特住院记录。结果表明：记忆现象在所有适应场景中普遍存在，其程度显著高于通用领域报道值。记忆效应影响医学LLMs的开发与应用，可归为三类：有益记忆（如准确回忆临床指南和生物医学参考文献）、无意义记忆（如重复的免责声明或模板化医疗文书语言）以及有害记忆（如再生数据集特异性内容或敏感临床信息）。基于这些发现，我们提出实践建议：促进有益记忆以增强领域特异性推理和事实准确性，减少无意义记忆以推动超越表面模式的深度学习，并遏制有害记忆以防止敏感或可识别患者信息的泄露。"
    },
    {
        "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge",
        "url": "http://arxiv.org/abs/2509.08596v1",
        "pub_date": "2025-09-10",
        "summary": "Biomedical question answering (QA) poses significant challenges due to the need for precise interpretation of specialized knowledge drawn from a vast, complex, and rapidly evolving corpus. In this work, we explore how large language models (LLMs) can be used for information retrieval (IR), and an ensemble of zero-shot models can accomplish state-of-the-art performance on a domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge tasks, we show that ensembles can outperform individual LLMs and in some cases rival or surpass domain-tuned systems - all while preserving generalizability and avoiding the need for costly fine-tuning or labeled data. Our method aggregates outputs from multiple LLM variants, including models from Anthropic and Google, to synthesize more accurate and robust answers. Moreover, our investigation highlights a relationship between context length and performance: while expanded contexts are meant to provide valuable evidence, they simultaneously risk information dilution and model disorientation. These findings emphasize IR as a critical foundation in Retrieval-Augmented Generation (RAG) approaches for biomedical QA systems. Precise, focused retrieval remains essential for ensuring LLMs operate within relevant information boundaries when generating answers from retrieved documents. Our results establish that ensemble-based zero-shot approaches, when paired with effective RAG pipelines, constitute a practical and scalable alternative to domain-tuned systems for biomedical question answering.",
        "translated": "生物医学问答（QA）面临重大挑战，因为需要从庞大、复杂且快速更新的专业文献中精确解读专业知识。本研究探索了如何利用大语言模型（LLM）进行信息检索（IR），并通过零样本模型的集成方法在特定领域的二元问答任务中实现最先进性能。通过在BioASQ挑战任务上的评估，我们证明集成模型不仅能超越单个LLM的性能，在某些情况下甚至可与经过领域调优的系统相媲美或更优——同时保持泛化能力，无需昂贵的微调或标注数据。我们的方法聚合了包括Anthropic和谷歌多个LLM变体的输出，以生成更准确、更稳健的答案。\n\n研究还揭示了上下文长度与性能之间的关系：扩展的上下文本意是提供更有价值的证据，但同时也可能导致信息稀释和模型方向迷失。这些发现强调了信息检索在生物医学QA系统的检索增强生成（RAG）方法中的关键基础作用。当LLM基于检索到的文档生成答案时，精确且聚焦的检索对于确保模型在相关信息边界内运作至关重要。我们的结果表明：基于集成的零样本方法与有效的RAG流程结合时，可为生物医学问答提供一种兼具实用性、可扩展性的领域调优系统替代方案。\n\n（注：专业术语说明：\n- LLM：大语言模型（Large Language Model）\n- IR：信息检索（Information Retrieval）\n- RAG：检索增强生成（Retrieval-Augmented Generation）\n- 零样本（Zero-shot）：指模型未经特定任务训练即可执行该任务\n- 领域调优（Domain-tuned）：针对特定领域进行参数优化的系统）"
    },
    {
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models",
        "url": "http://arxiv.org/abs/2509.08541v1",
        "pub_date": "2025-09-10",
        "summary": "Current large language models (LLMs) generally show a significant performance gap in alignment between English and other languages. To bridge this gap, existing research typically leverages the model's responses in English as a reference to select the best/worst responses in other languages, which are then used for Direct Preference Optimization (DPO) training. However, we argue that there are two limitations in the current methods that result in noisy multilingual preference data and further limited alignment performance: 1) Not all English responses are of high quality, and using a response with low quality may mislead the alignment for other languages. 2) Current methods usually use biased or heuristic approaches to construct multilingual preference pairs. To address these limitations, we design a consistency-based data selection method to construct high-quality multilingual preference data for improving multilingual alignment (CM-Align). Specifically, our method includes two parts: consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction. Experimental results on three LLMs and three common tasks demonstrate the effectiveness and superiority of our method, which further indicates the necessity of constructing high-quality preference data.",
        "translated": "当前的大型语言模型（LLM）在英语与其他语言的对齐性能上普遍存在显著差距。为弥补这一差距，现有研究通常以模型的英文回复作为参考，筛选其他语言中的最佳/最差回复，进而用于直接偏好优化（DPO）训练。然而，我们认为当前方法存在两个局限性，导致生成的多语言偏好数据存在噪声，进而限制对齐效果：1）并非所有英文回复都具有高质量，使用低质量回复可能误导其他语言的对齐过程；2）现有方法通常依赖有偏或启发式策略构建多语言偏好对。针对这些问题，我们设计了一种基于一致性的数据选择方法，用于构建高质量多语言偏好数据以提升多语言对齐性能（CM-Align）。具体而言，该方法包含两个核心模块：一致性引导的英文参考选择机制和基于跨语言一致性的多语言偏好数据构建机制。在三个大型语言模型和三项常见任务上的实验结果表明，本方法具有显著的有效性和优越性，进一步印证了构建高质量偏好数据的必要性。"
    },
    {
        "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
        "url": "http://arxiv.org/abs/2509.08494v1",
        "pub_date": "2025-09-10",
        "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.",
        "translated": "随着人类将更多任务和决策权委托给人工智能（AI），我们正逐渐丧失对个人及集体未来的掌控。相对简单的算法系统已在引导人类决策，例如社交媒体信息流算法导致用户无意识、机械地浏览 engagement 优化内容。本文通过整合哲学与科学领域的能动性理论，结合AI辅助评估方法——使用大语言模型（LLMs）模拟验证用户查询并评估AI响应，系统阐述了人类能动性理念。我们开发了HumanAgencyBench（HAB），这是一个基于典型AI使用场景、包含六维人类能动性指标的可扩展自适应基准测试框架。HAB通过六大维度衡量AI助手或智能体的行为倾向：提出澄清性问题、避免价值操纵、纠正错误信息、推迟重大决策、鼓励学习以及维护社交边界。研究发现，当前基于LLM的助手对人类能动性的支持程度为低至中等水平，且不同系统开发者和维度间存在显著差异。例如，虽然Anthropic的LLM整体对人类能动性支持度最高，但在\"避免价值操纵\"维度却是支持度最低的模型。研究还表明，能动性支持度的提升并非源于LLM能力增强或指令遵循行为（如RLHF）的线性发展，我们呼吁转向更鲁棒的安全性与对齐目标建设。\n\n（注：专业术语处理说明：\n1. \"human agency\"译为\"人类能动性\"（哲学与社会科学领域标准译法）\n2. \"engagement-optimized content\"译为\"engagement优化内容\"（保留专业表述）\n3. \"LLMs\"统一译为\"大语言模型\"并标注英文缩写\n4. \"RLHF\"译为\"人类反馈强化学习\"并保留英文缩写\n5. \"alignment\"译为\"对齐\"（AI安全领域专业术语）\n6. \"Anthropic\"等企业名按行业惯例保留英文）"
    },
    {
        "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
        "url": "http://arxiv.org/abs/2509.08486v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range of NLP tasks, yet aligning their outputs with the principles of Helpfulness, Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing methods often optimize for individual alignment dimensions in isolation, leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE) architectures offer modularity, they suffer from poorly calibrated routing, limiting their effectiveness in alignment tasks. We propose TrinityX, a modular alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE) within the Transformer architecture. TrinityX leverages separately trained experts for each HHH dimension, integrating their outputs through a calibrated, task-adaptive routing mechanism that combines expert signals into a unified, alignment-aware representation. Extensive experiments on three standard alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines, achieving relative improvements of 32.5% in win rate, 33.9% in safety score, and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and inference latency by over 40% compared to prior MoE-based approaches. Ablation studies highlight the importance of calibrated routing, and cross-model evaluations confirm TrinityX's generalization across diverse LLM backbones.",
        "translated": "大型语言模型（LLMs）在众多自然语言处理任务中展现出强大性能，但其输出与有用性（Helpfulness）、无害性（Harmlessness）和真实性（Honesty）的\"HHH\"原则对齐仍存在持续挑战。现有方法往往孤立地优化单一对齐维度，导致性能权衡与行为不一致。虽然混合专家（MoE）架构提供模块化能力，但其路由机制校准不足，限制了对齐任务的有效性。我们提出TrinityX框架，通过在Transformer架构中引入校准专家混合模块（MoCaE），为每个HHH维度独立训练专家模型，并通过任务自适应的校准路由机制整合专家输出，形成统一且具有对齐意识的表征。在三大标准对齐基准测试——Alpaca（有用性）、BeaverTails（无害性）和TruthfulQA（真实性）上的实验表明，TrinityX显著优于基线模型，在胜率、安全评分和真实性指标上分别实现32.5%、33.9%和28.4%的相对提升。此外，相比现有基于MoE的方法，该框架降低40%以上的内存占用与推理延迟。消融实验验证了校准路由机制的关键作用，跨模型评估则证实了TrinityX在不同LLM骨干网络上的泛化能力。\n\n（注：专业术语说明：\n1. Mixture-of-Experts (MoE)：混合专家模型\n2. Transformer architecture：Transformer架构\n3. Win rate：胜率（模型输出优于基线的比例）\n4. Ablation studies：消融实验（通过移除组件验证其重要性））"
    },
    {
        "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text",
        "url": "http://arxiv.org/abs/2509.08484v1",
        "pub_date": "2025-09-10",
        "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating particular perspectives or linguistic styles through the lens of a specified identity. While this method is often used to personalize outputs, its impact on how LLMs represent social groups remains underexplored. In this paper, we investigate whether persona-prompting leads to different levels of linguistic abstraction - an established marker of stereotyping - when generating short texts linking socio-demographic categories with stereotypical or non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias framework, we analyze outputs from six open-weight LLMs under three prompting conditions, comparing 11 persona-driven responses to those of a generic AI assistant. To support this analysis, we introduce Self-Stereo, a new dataset of self-reported stereotypes from Reddit. We measure abstraction through three metrics: concreteness, specificity, and negation. Our results highlight the limits of persona-prompting in modulating abstraction in language, confirming criticisms about the ecology of personas as representative of socio-demographic groups and raising concerns about the risk of propagating stereotypes even when seemingly evoking the voice of a marginalized group.",
        "translated": "角色提示是一种新兴策略，通过指定身份视角引导大语言模型模拟特定观点或语言风格。虽然该方法常用于个性化输出，但其对大语言模型表征社会群体的影响仍待深入探究。本文研究在生成关联社会人口类别与刻板/非刻板属性的短文本时，角色提示是否会引发不同层级的语言抽象化——这是衡量刻板印象的既定指标。基于语言预期偏差理论框架，我们分析六种开源权重LLM在三种提示条件下的输出，将11种角色驱动响应与通用AI助手响应进行对比。为此我们引入Self-Stereo新数据集，该数据集收集来自Reddit平台的自我报告型刻板印象。我们通过具体性、特定性和否定性三项指标测量抽象化程度。研究结果揭示了角色提示在调节语言抽象化方面的局限性，既印证了关于\"角色作为社会人口群体代表性\"的生态效度批评，也警示了即使看似调用边缘群体声音仍可能传播刻板印象的风险。"
    },
    {
        "title": "FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
        "url": "http://arxiv.org/abs/2509.09680v1",
        "pub_date": "2025-09-11",
        "summary": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
        "translated": "开源文本生成图像（T2I）模型的发展长期受制于缺乏大规模推理导向数据集和综合评估基准，导致其性能与领先闭源系统存在显著差距。为应对这一挑战，我们推出FLUX-Reason-6M数据集与PRISM-Bench（精准鲁棒图像合成测量基准）。FLUX-Reason-6M包含600万张高质量FLUX生成图像及2000万条中英双语描述，专门针对复杂推理能力训练设计。该数据集依据六大核心特性进行组织：想象力（Imagination）、实体（Entity）、文本渲染（Text rendering）、风格（Style）、情感（Affection）和构图（Composition），并通过显式生成思维链（GCoT）技术对图像生成步骤进行细粒度解析。整个数据构建过程耗费15,000个A100 GPU日，为学术界提供了以往仅大型工业实验室才能获得的资源。PRISM-Bench提出包含七大评估维度的新型评测标准，其中采用GCoT的长文本挑战任务尤为突出。通过精心设计的提示词，该基准利用先进视觉语言模型对文本-图像对齐度和图像美学进行类人化精细评估。我们对19个主流模型开展的全面测试揭示了关键性能差距，并明确了需重点改进的领域。现已公开数据集、基准测试框架及评估代码，以推动下一代推理导向T2I生成技术的发展。项目页面：https://flux-reason-6m.github.io/\n\n（注：翻译过程中对以下术语采用专业译法：\n- Reasoning-focused：推理导向\n- Generation Chain-of-Thought (GCoT)：生成思维链\n- Vision-language models：视觉语言模型\n- Prompt-image alignment：文本-图像对齐度\n- Human-aligned assessment：类人化评估\n严格保持技术术语准确性，同时确保中文表达符合学术论文摘要的正式文体特征。）"
    },
    {
        "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
        "url": "http://arxiv.org/abs/2509.09679v1",
        "pub_date": "2025-09-11",
        "summary": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$ entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \\log n)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
        "translated": "大型语言模型需要巨大的内存占用，这严重限制了其在消费级硬件上的部署。量化技术通过降低数值精度来减少内存需求，但极端2位量化会因激活值中的异常值而遭受灾难性的性能损失。基于旋转的方法（如QuIP和QuaRot）利用计算不变性原理：对于正交矩阵$\\mathbf{Q}$满足$\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$，在量化前通过正交变换消除异常值。然而这些方法采用固定变换——使用达到最优最差情况相干性$\\mu = 1/\\sqrt{n}$的哈达玛矩阵——无法适配特定权重分布。我们发现不同Transformer层展现出截然不同的异常值模式，这启发了我们采用层自适应旋转而非通用方案。我们提出ButterflyQuant方法，用可学习的蝴蝶变换替代哈达玛旋转，该变换通过连续Givens旋转角进行参数化。与哈达玛矩阵不可微的离散$\\{+1, -1\\}$元素（阻碍基于梯度的学习）不同，蝴蝶变换的连续参数化在保证构造正交性的同时支持平滑优化。这种正交约束既确保了异常值抑制的理论保证，又以$O(n \\log n)$计算复杂度和仅$\\frac{n \\log n}{2}$可学习参数实现。我们进一步对变换后的激活值引入均匀性正则化，以促进更平滑的量化友好分布。该方法仅需128个校准样本即可完成学习，并在单GPU上数分钟内收敛——这种一次性成本可忽略不计。在LLaMA-2-7B模型的2位量化测试中，ButterflyQuant实现了15.4的困惑度，显著优于QuaRot的22.1。"
    },
    {
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.09674v1",
        "pub_date": "2025-09-11",
        "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$ on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
        "translated": "视觉-语言-动作（VLA）模型近年来已成为机器人操控领域的重要范式。尽管通过大规模预训练和监督微调（SFT）取得了显著进展，这类模型仍面临两个核心挑战：（1）SFT扩展所需的大规模人工操作机器人轨迹数据稀缺且成本高昂；（2）对存在分布偏移任务的泛化能力有限。大型推理模型（LRMs）的最新突破表明，强化学习（RL）能显著增强逐步推理能力，这引出一个关键问题：RL是否同样能提升VLA模型的长期分层动作规划能力？本研究提出SimpleVLA-RL——一个专为VLA模型设计的高效RL框架。基于veRL架构，我们引入了VLA特化的轨迹采样、可扩展并行化、多环境渲染及优化损失计算。当应用于OpenVLA-OFT模型时，SimpleVLA-RL在LIBERO基准上达到最先进性能，并通过我们提出的探索增强策略，在RoboTwin 1.0和2.0环境中甚至超越了$\\pi_0$基线。该框架不仅降低了对大规模数据的依赖并实现了强泛化能力，更在真实任务中显著优于SFT方法。此外，我们在RL训练过程中发现了一种名为\"pushcut\"的新现象——智能体能发现训练历史中未曾出现的新行为模式。项目地址：https://github.com/PRIME-RL/SimpleVLA-RL\n\n（注：专业术语说明：\n1. VLA：视觉-语言-动作多模态模型\n2. SFT：监督微调（Supervised Fine-Tuning）\n3. RL：强化学习（Reinforcement Learning）\n4. LRM：大型推理模型（Large Reasoning Models）\n5. SoTA：最先进水平（State-of-the-Art）\n6. $\\pi_0$：强化学习中的初始策略基线\n7. pushcut：本文发现的新型训练现象，暂译为\"推切现象\"）"
    },
    {
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
        "url": "http://arxiv.org/abs/2509.09675v1",
        "pub_date": "2025-09-11",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
        "translated": "【论文核心思想】  \n针对强化学习可验证奖励框架（RLVR）中存在的探索不足、早熟收敛和熵崩溃问题，本研究提出好奇心驱动探索（CDE）框架，通过智能体自身的内在好奇心信号引导探索过程。该方法从行动者和评论者双视角量化好奇心：行动者侧采用生成响应的困惑度，评论者侧利用多头架构的价值估计方差，两者共同构成探索奖励项。理论分析表明，行动者奖励项能有效抑制过度自信错误并提升正确答案的多样性，而评论者奖励项与RL中经典的基于计数的探索奖励具有理论等价性。在AIME基准测试中，该方法较GRPO/PPO标准RLVR实现约3个百分点的性能提升，同时揭示了RLVR中存在的校准崩溃机制。\n\n【关键技术贡献】  \n1. 提出双路径好奇心量化机制：  \n   - 行动者路径：基于生成响应的困惑度（perplexity）衡量不确定性  \n   - 评论者路径：通过多头价值网络输出的方差表征认知分歧  \n2. 建立理论连接：  \n   - 证明评论者方差奖励与基于计数的探索方法数学等价  \n   - 揭示行动者困惑度奖励对过度自信的惩罚机制和答案多样性的促进效应  \n3. 实证效果：  \n   - 在AIME基准上显著提升RLVR性能  \n   - 首次发现并解析RLVR中的校准崩溃现象  \n\n【学术价值】  \n本研究通过引入受人类认知启发的内在好奇心机制，有效解决了大语言模型在强化学习中的探索-利用权衡难题，为理解LLM失败模式提供了新的理论视角，同时为多模态大模型的探索策略设计提供了可迁移的框架。"
    },
    {
        "title": "Steering MoE LLMs via Expert (De)Activation",
        "url": "http://arxiv.org/abs/2509.09660v1",
        "pub_date": "2025-09-11",
        "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.",
        "translated": "专家混合（MoE）架构通过将每个令牌路由至特定的前馈网络（FFN）专家子集来实现大语言模型的高效计算。本文提出SteerMoE框架，通过检测和控制行为关联专家来实现对MoE模型的定向调控。我们的检测方法能够识别在具有对立行为特征的配对输入中呈现显著激活差异的专家。通过在推理过程中选择性（解）激活此类专家，我们无需重新训练或修改权重即可控制模型的忠实性和安全性等行为特征。在11个基准测试和6个大语言模型上的实验表明，我们的调控方法最高可提升20%的安全性指标和27%的忠实性指标。在对抗攻击模式下，该方法单独使用可降低41%的安全性指标，与现有越狱方法结合时甚至能完全突破安全防护（-100%），暴露出专家网络中隐藏的\"对齐伪装\"新维度。\n\n（注：译文采用以下专业术语处理：\n- Mixture-of-Experts: 专家混合架构\n- routing: 路由\n- Feed-Forward Networks: 前馈网络\n- activation patterns: 激活模式\n- faithfulness: 忠实性\n- jailbreak methods: 越狱方法\n- alignment faking: 对齐伪装\n严格保持技术术语准确性，同时通过\"对立行为特征\"\"显著激活差异\"等表述确保学术文本的精确性，最后使用破折号补充说明-100%的具体含义，符合中文学术表达规范。）"
    },
    {
        "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations",
        "url": "http://arxiv.org/abs/2509.09651v1",
        "pub_date": "2025-09-11",
        "summary": "We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.",
        "translated": "我们针对无线电法规这一法律敏感且高风险的领域展开问答系统研究。提出了一种电信领域专用的检索增强生成（RAG）流程，并基于权威资料通过自动化筛选和人工验证构建了该领域首个多选评估数据集。为评估检索质量，我们定义了领域特异性检索指标，在该指标下我们的检索器达到约97%的准确率。除检索性能外，该方法在所有测试模型上均持续提升生成准确率。特别值得注意的是，当未经结构化检索直接插入文档时，GPT-4o仅获得边际增益（低于1%），而应用我们的流程可实现近12%的相对提升。这些发现表明，经过精准定位的基准确立既提供了简单而强大的基线，也为法规问答提供了有效的领域特异性解决方案。所有代码、评估脚本及衍生的问答数据集已开源：https://github.com/Zakaria010/Radio-RAG。\n\n（注：根据学术规范，对原文中\"grounding\"的翻译采用\"基准确立\"这一符合计算机领域术语的译法，其在此语境中指通过检索增强为模型提供准确的事实依据）"
    },
    {
        "title": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens",
        "url": "http://arxiv.org/abs/2509.09650v1",
        "pub_date": "2025-09-11",
        "summary": "Large language models (LLMs) demonstrate proficiency across numerous computational tasks, yet their inner workings remain unclear. In theory, the combination of causal self-attention and multilayer perceptron layers allows every token to access and compute information based on all preceding tokens. In practice, to what extent are such operations present? In this paper, on mental math tasks (i.e., direct math calculation via next-token prediction without explicit reasoning), we investigate this question in three steps: inhibiting input-specific token computations in the initial layers, restricting the routes of information transfer across token positions in the next few layers, and forcing all computation to happen at the last token in the remaining layers. With two proposed techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with high accuracy on a wide variety of mental math tasks, where meaningful computation occurs very late (in terms of layer depth) and only at the last token, which receives information of other tokens in few specific middle layers. Experiments on a variety of models and arithmetic expressions show that this subgraph is sufficient and necessary for high model performance, transfers across different models, and works on a variety of input styles. Ablations on different CAMA and ABP alternatives reveal their unique advantages over other methods, which may be of independent interest.",
        "translated": "大型语言模型（LLMs）在众多计算任务中展现出卓越能力，但其内部工作机制仍不明确。理论上，因果自注意力机制与多层感知器的结合使得每个词元都能访问并基于所有前序词元进行信息计算。但在实际运行中，此类操作究竟以何种程度存在？本文通过心算任务（即通过下一词元预测直接进行数学计算，无需显式推理）分三步探究该问题：抑制初始层中针对特定输入的词元计算，限制后续若干层中跨词元位置的信息传递路径，以及在剩余层中强制所有计算集中于末位词元。通过提出的两种技术——上下文感知均值消融（CAMA）和基于注意力的窥探（ABP）——我们识别出一个\"万为一\"子图（AF1），该结构在多种心算任务中均呈现高精度，其显著特征在于：有意义计算发生时间极晚（就网络深度而言），且仅发生于末位词元；该词元通过少数特定中间层接收其他词元的信息。在不同模型和算术表达式上的实验表明，该子图是实现高模型性能的充分必要条件，具有跨模型迁移能力，且适用于多种输入格式。对CAMA与ABP替代方案的消融实验揭示了二者相较于其他方法的独特优势，这一发现可能具有独立研究价值。\n\n（注：译文严格遵循以下技术要点：\n1. \"mental math\"译为专业术语\"心算\"\n2. \"next-token prediction\"保留技术概念译为\"下一词元预测\"\n3. \"All-for-One subgraph\"采用意译\"万为一子图\"并保留英文缩写AF1\n4. \"Context-Aware Mean Ablation\"等专业术语保持英文缩写并给出完整中文译名\n5. 复杂句式按中文习惯拆分重组，如将原文三个步骤的英文分词结构转换为中文分号并列结构\n6. 保持学术论文的客观表述风格，避免口语化表达）"
    },
    {
        "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for\n  Low-Latency Zero-Shot Text-To-Speech",
        "url": "http://arxiv.org/abs/2509.09631v1",
        "pub_date": "2025-09-11",
        "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
        "translated": "零样本文本到语音合成（Zero-shot TTS）旨在仅通过短时参考样本合成模仿未见说话人声音的高质量语音，这不仅需要说话人自适应，还需精确建模韵律属性。基于语言模型、扩散模型和流匹配的最新方法在零样本TTS中展现出潜力，但仍存在推理速度慢和重复伪影的问题。离散编解码表示已被广泛用于语音合成，近期研究开始探索纯离散场景下的扩散模型，这揭示了离散生成模型在语音合成中的潜力。然而现有流匹配方法通常将离散标记嵌入连续空间并应用连续流匹配，可能未能充分发挥离散表示的优势。\n\n为解决这些挑战，我们提出了DiFlow-TTS——据我们所知，这是首个探索纯离散流匹配（Discrete Flow Matching）的语音合成模型。该模型通过紧凑统一的架构显式建模分解的语音属性，利用上下文学习机制，以文本内容及从参考语音中提取的韵律和声学属性为条件，实现零样本场景下的有效属性克隆。此外，模型采用具有独立韵律头和声学头的分解流预测机制，能够学习特定方面的分布。实验结果表明，DiFlow-TTS在自然度、韵律表现、说话人风格保持和能量控制等关键指标上均取得优异表现，同时保持紧凑的模型规模，推理延迟显著降低，生成速度较现有最新基线提升达25.8倍。\n\n（注：专业术语说明：\n1. Zero-shot Text-to-Speech：零样本文本转语音\n2. prosodic attributes：韵律属性\n3. diffusion models：扩散模型\n4. flow matching：流匹配\n5. discrete codec representations：离散编解码表示\n6. in-context learning：上下文学习\n7. acoustic attributes：声学属性\n8. factorized flow prediction：分解流预测）"
    },
    {
        "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems",
        "url": "http://arxiv.org/abs/2509.09629v1",
        "pub_date": "2025-09-11",
        "summary": "The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.",
        "translated": "大型语言模型（LLMs）的发展推动了多智能体系统的构建，通过将职责分配给专业化智能体（如负责生成子目标的规划智能体和执行工具使用操作的落地智能体）来解决复杂任务。现有方法通常独立微调各智能体，导致其能力存在差距且协作效率低下。为此，我们提出MOAT——一种多智能体联合对齐调优框架，通过迭代对齐机制提升智能体协作能力。MOAT交替执行两个关键阶段：（1）规划智能体对齐：优化规划智能体以生成能更好指导落地智能体的子目标序列；（2）落地智能体改进：利用智能体自身生成的多样化子目标-动作对微调落地智能体，增强其泛化能力。理论分析证明MOAT能确保训练过程具有非递减且逐步收敛的特性。在六个基准测试上的实验表明，MOAT优于现有最先进基线方法，在已知任务和未知任务上分别实现了3.1%和4.4%的平均性能提升。\n\n（注：专业术语说明：\n1. \"grounding agent\"译为\"落地智能体\"，强调其将抽象规划转化为具体执行动作的特性；\n2. \"held-in/held-out tasks\"采用\"已知任务/未知任务\"的译法，体现模型对训练见过/未见任务的泛化能力；\n3. \"non-decreasing and progressively convergent\"译为\"非递减且逐步收敛\"，保持数学描述的精确性；\n4. 保留英文缩写MOAT，符合学术惯例）"
    },
    {
        "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination",
        "url": "http://arxiv.org/abs/2509.09602v1",
        "pub_date": "2025-09-11",
        "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings.",
        "translated": "言语尸检（VA）是在缺乏医疗死亡证明的资源有限地区进行死因推断的重要工具。本研究提出LA-VA概念验证流程，通过结合大语言模型（LLM）与传统算法方法及基于嵌入的分类技术，显著提升死因预测准确率。基于人口健康指标研究联盟（PHMRC）数据集的三类年龄组数据（成人7,580例；儿童1,960例；新生儿2,438例），我们评估了多种方法：GPT-5预测、LCVA基线模型、文本嵌入分类以及元学习集成模型。实验结果表明，GPT-5在测试集上取得最优单体性能，平均准确率分别达到成人48.6%、儿童50.5%、新生儿53.5%，较传统统计机器学习基线方法提升5-10%。研究发现表明，现成的轻量级LLM辅助方法可显著提升言语尸检准确率，这对资源匮乏地区的全球健康监测具有重要实践价值。\n\n（注：根据学术规范，对GPT-5的表述保留模型原名，因原文未明确说明是否为假设模型或笔误，若实际应为GPT-3.5/4需根据上下文调整）"
    },
    {
        "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
        "url": "http://arxiv.org/abs/2509.09593v1",
        "pub_date": "2025-09-11",
        "summary": "The versatility of Large Language Models (LLMs) in natural language understanding has made them increasingly popular in mental health research. While many studies explore LLMs' capabilities in emotion recognition, a critical gap remains in evaluating whether LLMs align with human emotions at a fine-grained level. Existing research typically focuses on classifying emotions into predefined, limited categories, overlooking more nuanced expressions. To address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit communities featuring 251 fine-grained, self-disclosed emotion labels. Our comprehensive evaluation framework examines predicted emotion terms and decomposes them into eight basic emotions using established emotion theories, enabling a fine-grained comparison. Systematic testing of prevalent LLMs under various prompt settings reveals that accurately predicting emotions that align with human self-disclosed emotions remains challenging. Qualitative analysis further shows that while certain LLMs generate emotion terms consistent with established emotion theories and definitions, they sometimes fail to capture contextual cues as effectively as human self-disclosures. These findings highlight the limitations of LLMs in fine-grained emotion alignment and offer insights for future research aimed at enhancing their contextual understanding.",
        "translated": "大型语言模型（LLMs）在自然语言理解方面的多功能性使其在心理健康研究中日益受到关注。尽管许多研究探索了LLMs在情绪识别方面的能力，但在评估这些模型是否能在细粒度层面与人类情绪保持一致方面仍存在关键空白。现有研究通常将情绪分类为预定义的有限类别，忽略了更细微的情绪表达。为填补这一空白，我们推出了EXPRESS——一个从Reddit社区精选的基准数据集，包含251个细粒度的自我披露情绪标签。我们通过综合评估框架，不仅分析预测的情绪术语，还借助成熟的情绪理论将其分解为八种基本情绪，从而实现细粒度对比。在不同提示设置下对主流LLMs进行的系统测试表明，准确预测与人类自我披露情绪相符的情感仍然具有挑战性。定性分析进一步发现，虽然某些LLMs能生成符合既有情绪理论和定义的情绪术语，但它们有时难以像人类自我披露那样有效捕捉上下文线索。这些发现凸显了LLMs在细粒度情绪对齐方面的局限性，并为未来提升其上下文理解能力的研究提供了重要参考。"
    },
    {
        "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role\n  of Personality Detection in Matchmaking",
        "url": "http://arxiv.org/abs/2509.09583v1",
        "pub_date": "2025-09-11",
        "summary": "Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.",
        "translated": "社交互动是学习过程中的重要组成部分，然而在线课程环境为自然社交群体的形成设置了障碍。SAMI通过促进学生间的连接提供了一种解决方案，但其有效性受限于不完善的心理理论（Theory of Mind），导致难以构建有效的学生心智模型。其中一个关键局限是无法推断用户性格，而性格特征可能影响推荐结果的关联性。为此，我们提出一种人格检测模型，利用GPT的零样本能力从在线课程中常见的论坛自我介绍帖子中推断大五人格特质。通过与传统基准模型对比，验证了该方法在此任务中的有效性。进一步地，我们将该模型集成至SAMI基于实体的匹配系统中，实现融合人格特征的社交推荐。初步集成结果表明，人格特质能够对现有匹配因素形成有效补充，但仍需进一步评估以确定其对学习者参与度和匹配质量的完整影响。\n\n（注：专业术语说明：\n1. Theory of Mind：译为\"心理理论\"，指系统理解他人心理状态的能力\n2. Big-Five personality traits：采用心理学界通用译法\"大五人格特质\"，包含开放性、尽责性、外向性、宜人性和神经质性五个维度\n3. zero-shot capability：译为\"零样本能力\"，指模型无需特定训练即可处理新任务的能力\n4. entity-based matchmaking system：译为\"基于实体的匹配系统\"，强调以用户实体特征为核心的匹配机制）"
    },
    {
        "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)",
        "url": "http://arxiv.org/abs/2509.09544v1",
        "pub_date": "2025-09-11",
        "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling new tasks and driving a proliferation of datasets and diversification of data sources. Yet, this transformation has outpaced traditional surveys. In this paper, we present MetaGraph, a generalizable methodology for extracting knowledge graphs from scientific literature and analyzing them to obtain a structured, queryable view of research trends. We define an ontology for financial NLP research and apply an LLM-based extraction pipeline to 681 papers (2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals three key phases: early LLM adoption and task/dataset innovation; critical reflection on LLM limitations; and growing integration of peripheral techniques into modular systems. This structured view offers both practitioners and researchers a clear understanding of how financial NLP has evolved - highlighting emerging trends, shifting priorities, and methodological shifts-while also demonstrating a reusable approach for mapping scientific progress in other domains.",
        "translated": "大型语言模型（LLMs）正在快速重塑金融自然语言处理领域，不仅催生了新型任务，还推动了数据集的激增与数据来源的多元化。然而，这种变革速度已超越传统文献综述的追踪能力。本文提出MetaGraph——一种可泛化的方法论，能够从科学文献中提取知识图谱，并通过分析获得结构化、可查询的研究趋势视图。我们定义了金融自然语言处理研究的本体框架，并基于LLM构建提取管道，对681篇文献（2022-2025年）进行大规模数据驱动分析。MetaGraph揭示了三个关键发展阶段：早期LLM采用与任务/数据集创新阶段；对LLM局限性的批判性反思阶段；以及外围技术逐渐融入模块化系统的整合阶段。这种结构化视角既为从业者和研究者提供了对金融NLP演进路径的清晰认知——突出新兴趋势、优先级转变与方法论变迁——同时也展示了可用于其他学科领域科学进展图谱构建的可复用方法。\n\n（注：译文严格遵循以下技术处理原则：\n1. 专业术语准确对应：\"ontology\"译作\"本体框架\"，\"extraction pipeline\"译为\"提取管道\"\n2. 复杂句式重构：将英文长句拆解为符合中文表达习惯的短句结构\n3. 概念显化处理：\"peripheral techniques\"意译为\"外围技术\"而非字面直译\n4. 学术语境保持：使用\"方法论\"\"可泛化\"\"模块化系统\"等符合学术论文风格的表述\n5. 时间跨度处理：保留\"2022-2025\"原格式符合学术文献惯例）"
    },
    {
        "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning\n  and Label Distribution Learning",
        "url": "http://arxiv.org/abs/2509.09524v1",
        "pub_date": "2025-09-11",
        "summary": "This system paper presents the DeMeVa team's approaches to the third edition of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et al., 2025). We explore two directions: in-context learning (ICL) with large language models, where we compare example sampling strategies; and label distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we evaluate several fine-tuning methods. Our contributions are twofold: (1) we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance; and (2) we argue that LDL methods are promising for soft label predictions and merit further exploration by the perspectivist community.",
        "translated": "本系统论文介绍了DeMeVa团队针对第三届\"学习中的分歧\"共享任务（LeWiDi 2025；Leonardelli等人，2025）提出的解决方案。我们探索了两个研究方向：基于大语言模型的上下文学习（ICL）方法——重点比较了不同的示例采样策略；以及基于RoBERTa（Liu等人，2019b）的标签分布学习（LDL）方法——评估了多种微调技术。我们的主要贡献包括：（1）证明了ICL能够有效预测标注者特异性注释（视角主义标注），且将这些预测聚合为软标签后可获得具有竞争力的性能；（2）论证了LDL方法在软标签预测方面的潜力，值得视角主义研究社区进一步探索。"
    },
    {
        "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual\n  Relatedness and Knowledge Graphs",
        "url": "http://arxiv.org/abs/2509.09522v1",
        "pub_date": "2025-09-11",
        "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between texts that extend beyond superficial lexical similarity. In this study, we investigate STR in the context of job title matching - a key challenge in resume recommendation systems, where overlapping terms are often limited or misleading. We introduce a self-supervised hybrid architecture that combines dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to improve both semantic alignment and explainability. Unlike previous work that evaluated models on aggregate performance, our approach emphasizes data stratification by partitioning the STR score continuum into distinct regions: low, medium, and high semantic relatedness. This stratified evaluation enables a fine-grained analysis of model performance across semantically meaningful subspaces. We evaluate several embedding models, both with and without KG integration via graph neural networks. The results show that fine-tuned SBERT models augmented with KGs produce consistent improvements in the high-STR region, where the RMSE is reduced by 25% over strong baselines. Our findings highlight not only the benefits of combining KGs with text embeddings, but also the importance of regional performance analysis in understanding model behavior. This granular approach reveals strengths and weaknesses hidden by global metrics, and supports more targeted model selection for use in Human Resources (HR) systems and applications where fairness, explainability, and contextual matching are essential.",
        "translated": "语义文本关联性（Semantic Textual Relatedness, STR）能够捕捉文本间超越表层词汇相似度的深层语义关系。本研究针对简历推荐系统中的关键挑战——职位名称匹配问题展开STR分析，该场景中文本间的重合术语往往有限且具有误导性。我们提出一种自监督混合架构，通过将稠密句子嵌入与领域知识图谱（KG）相结合，同步提升语义对齐能力和可解释性。与既往仅关注整体性能评估的研究不同，本方法采用数据分层策略，将STR分数连续体划分为低、中、高三个语义关联区间。这种分层评估机制实现了对语义子空间模型性能的细粒度解析。\n\n我们评估了多种嵌入模型（包含结合图神经网络的KG增强版本与独立版本）。实验结果表明：经过微调的SBERT模型在集成KG后，在高STR区域持续表现出性能提升，其均方根误差（RMSE）较基线模型降低25%。研究发现不仅验证了知识图谱与文本嵌入结合的优势，更揭示了区域性能分析对理解模型行为的重要性。这种精细化分析方法能够发现被全局指标掩盖的模型优缺点，为人力资源系统等对公平性、可解释性和上下文匹配要求严格的应用场景提供更具针对性的模型选择依据。\n\n（注：专业术语说明：\n- STR: 语义文本关联性\n- KG: 知识图谱\n- SBERT:  Sentence-BERT句子嵌入模型\n- RMSE: 均方根误差\n- 图神经网络: Graph Neural Networks\n- 自监督: self-supervised\n- 语义对齐: semantic alignment）"
    },
    {
        "title": "Mitigating Language Barriers in Education: Developing Multilingual\n  Digital Learning Materials with Machine Translation",
        "url": "http://arxiv.org/abs/2509.09473v1",
        "pub_date": "2025-09-11",
        "summary": "The EdUKate project combines digital education, linguistics, translation studies, and machine translation to develop multilingual learning materials for Czech primary and secondary schools. Launched through collaboration between a major Czech academic institution and the country's largest educational publisher, the project is aimed at translating up to 9,000 multimodal interactive exercises from Czech into Ukrainian, English, and German for an educational web portal. It emphasizes the development and evaluation of a direct Czech-Ukrainian machine translation system tailored to the educational domain, with special attention to processing formatted content such as XML and PDF and handling technical and scientific terminology. We present findings from an initial survey of Czech teachers regarding the needs of non-Czech-speaking students and describe the system's evaluation and implementation on the web portal. All resulting applications are freely available to students, educators, and researchers.",
        "translated": "EdUKate项目融合数字教育、语言学、翻译研究与机器翻译技术，致力于为捷克中小学开发多语言学习材料。该项目由捷克顶尖学术机构与国内最大教育出版商联合启动，旨在将多达9000个多模态交互式习题从捷克语翻译成乌克兰语、英语和德语，并集成至教育网络门户。项目重点开发并评估针对教育领域定制的捷克语-乌克兰语直接机器翻译系统，特别关注XML和PDF等格式化内容的处理技术，以及科技专业术语的翻译方案。我们通过初步调研呈现了捷克教师对非母语学生需求的评估结果，并详细说明了该系统在网络门户中的实施与评估过程。所有最终应用均向学生、教育工作者及研究人员免费开放。\n\n（注：根据学术论文摘要的规范要求，译文采用客观严谨的学术表述风格，确保以下专业要素的准确传达：\n1. 跨学科特性：数字教育/语言学/翻译研究的学科交叉性\n2. 技术细节：明确机器翻译系统类型（直接翻译）、处理格式（XML/PDF）、术语处理等关键技术点\n3. 项目规模：量化数据（9000个练习）和语言对（4种语言）的精确表述\n4. 学术价值：强调系统评估、需求调研等研究环节\n5. 社会效益：突出免费开放的服务模式）"
    },
    {
        "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.09438v1",
        "pub_date": "2025-09-11",
        "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence elicitation is a prominent approach to AI safety in high-stakes applications, such as healthcare and finance. Existing methods either require expensive computational overhead or suffer from poor calibration, making them impractical and unreliable for real-world deployment. In this work, we propose GrACE, a Generative Approach to Confidence Elicitation that enables scalable and reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in which the model expresses confidence by the similarity between the last hidden state and the embedding of a special token appended to the vocabulary, in real-time. We fine-tune the model for calibrating the confidence with calibration targets associated with accuracy. Experiments with three LLMs and two benchmark datasets show that the confidence produced by GrACE achieves the best discriminative capacity and calibration on open-ended generation tasks, outperforming six competing methods without resorting to additional sampling or an auxiliary model. Moreover, we propose two strategies for improving test-time scaling based on confidence induced by GrACE. Experimental results show that using GrACE not only improves the accuracy of the final decision but also significantly reduces the number of required samples in the test-time scaling scheme, indicating the potential of GrACE as a practical solution for deploying LLMs with scalable, reliable, and real-time confidence estimation.",
        "translated": "在医疗健康与金融等高风险应用领域，通过置信度激发来评估大语言模型（LLUMs）的可靠性是人工智能安全领域的重要研究方向。现有方法要么需要高昂的计算开销，要么存在校准效果不佳的问题，导致其在实际部署中缺乏实用性和可靠性。本研究提出GrACE（生成式置信度激发方法），一种可实现规模化、高可靠性置信度激发的新方案。GrACE采用创新机制：模型通过实时计算最后一层隐藏状态与词表中特殊标记嵌入的相似度来表达置信度。我们通过关联准确率的校准目标对模型进行微调以实现置信度校准。在三个大语言模型和两个基准数据集上的实验表明：在开放式生成任务中，GrACE产生的置信度在判别能力和校准效果方面均优于六种对比方法，且无需额外采样或辅助模型。此外，我们提出两种基于GrACE置信度的测试时缩放优化策略。实验结果表明，使用GrACE不仅能提升最终决策的准确率，还能显著减少测试时缩放方案所需的样本量，这证明GrACE具备作为实用解决方案的潜力——能够以可扩展、高可靠性且实时的方式部署具备置信度评估能力的大语言模型。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语统一：\"confidence elicitation\"译为\"置信度激发\"，\"hidden state\"译为\"隐藏状态\"，\"fine-tune\"译为\"微调\"\n2. 技术概念准确处理：\"test-time scaling\"译为\"测试时缩放\"，\"discriminative capacity\"译为\"判别能力\"\n3. 长难句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n4. 被动语态转换：将英文被动结构转换为中文主动表述\n5. 逻辑连接显性化：通过\"不仅...还能...\"等连接词明确技术优势的递进关系）"
    },
    {
        "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations",
        "url": "http://arxiv.org/abs/2509.09396v1",
        "pub_date": "2025-09-11",
        "summary": "To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",
        "translated": "为实现与人类的有效协作，语言模型需具备用自然语言解释其决策的能力。本研究聚焦于一类特殊的自我解释形式：自生成反事实解释（SCEs），即模型通过修改输入内容来阐释为何原本会做出不同预测。我们评估了大语言模型能否生成既有效（达成预期结果）又最小化（仅进行必要修改）的SCEs。实验发现：当被要求生成反事实解释时，大语言模型通常能产生有效的SCEs，但远未达到最小化标准，这导致其难以揭示模型的决策机制。更令人担忧的是，当被要求生成最小化反事实时，模型往往进行过度细微的编辑，以致无法真正改变预测结果。这种有效性与最小化之间的权衡关系在多个大语言模型、数据集和评估设置中均保持一致。我们的研究结果表明，SCEs往好了说是一种低效的可解释性工具，往坏了说可能对模型行为产生误导性解读。在高风险场景中部署大语言模型时，必须考虑不可靠的自我解释对下游决策的影响。代码已开源：https://github.com/HarryMayne/SCEs。\n\n（注：翻译过程中对以下要点进行了专业处理：\n1. \"self-generated counterfactual explanations (SCEs)\" 译为专业术语\"自生成反事实解释（SCEs）\"\n2. 保持\"validity\"和\"minimality\"在机器学习可解释性领域的标准译法\"有效性\"和\"最小化\"\n3. \"high-stakes settings\"译为\"高风险场景\"以符合学术文献表述\n4. 采用中文长句拆分技巧处理英语复合句结构\n5. 保留技术术语一致性（如LLMs统一译为\"大语言模型\"））"
    },
    {
        "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs",
        "url": "http://arxiv.org/abs/2509.09388v1",
        "pub_date": "2025-09-11",
        "summary": "We revisit hierarchical bracketing encodings from a practical perspective in the context of dependency graph parsing. The approach encodes graphs as sequences, enabling linear-time parsing with $n$ tagging actions, and still representing reentrancies, cycles, and empty nodes. Compared to existing graph linearizations, this representation substantially reduces the label space while preserving structural information. We evaluate it on a multilingual and multi-formalism benchmark, showing competitive results and consistent improvements over other methods in exact match accuracy.",
        "translated": "我们以实用角度重新审视了依存图解析中的层次括号编码方法。该方法将图结构编码为序列形式，仅需n次标注操作即可实现线性时间解析，同时仍能表示重入边、循环结构和空节点。与现有图线性化方法相比，该表征在保持结构信息完整的前提下显著缩小了标签空间。我们在多语言多形式主义基准测试中进行评估，结果显示该方法在精确匹配准确率上取得了具有竞争力的结果，并持续优于其他方法。\n\n（注：专业术语说明：\n1. reentrancies：重入边，指图中多个边指向同一节点的现象\n2. cycles：循环结构，指图中形成环路的依赖关系\n3. linear-time parsing：线性时间解析，指时间复杂度与输入长度成线性关系\n4. multi-formalism：多形式主义，指支持不同语法理论体系\n5. exact match accuracy：精确匹配准确率，指整个解析结构完全正确的评估指标）"
    },
    {
        "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive\n  Science Theory and NLP Research",
        "url": "http://arxiv.org/abs/2509.09381v1",
        "pub_date": "2025-09-11",
        "summary": "Analogical reasoning is an essential aspect of human cognition. In this paper, we summarize key theory about the processes underlying analogical reasoning from the cognitive science literature and relate it to current research in natural language processing. While these processes can be easily linked to concepts in NLP, they are generally not viewed through a cognitive lens. Furthermore, we show how these notions are relevant for several major challenges in NLP research, not directly related to analogy solving. This may guide researchers to better optimize relational understanding in text, as opposed to relying heavily on entity-level similarity.",
        "translated": "类比推理是人类认知的重要组成部分。本文从认知科学文献中总结了关于类比推理过程的核心理论，并将其与自然语言处理领域的研究现状相联系。尽管这些认知过程能够与自然语言处理中的概念建立明确关联，但现有研究通常未从认知视角进行解读。进一步地，我们论证了这些理论认知如何适用于自然语言处理研究中若干与类比求解不直接相关的重要挑战。这将引导研究者优化文本关系理解机制，而非过度依赖实体层面的相似性匹配。"
    },
    {
        "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
        "url": "http://arxiv.org/abs/2509.09676v1",
        "pub_date": "2025-09-11",
        "summary": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
        "translated": "在空间智能领域，空间重建与世界探索均取得了显著进展。然而当前模型的可扩展性和现实世界保真度仍受制于大规模高质量训练数据的稀缺性。虽然现有若干数据集提供相机位姿信息，但其在规模、多样性和标注丰富度方面存在局限，尤其缺乏具有真实相机运动标注的现实动态场景数据。为此，我们构建了\\textbf{SpatialVID}数据集——一个包含海量真实场景视频的语料库，涵盖多样化场景、相机运动模式以及密集的3D标注（包括逐帧相机位姿、深度信息和运动指令）。具体而言，我们收集超过21,000小时的原始视频，通过分级过滤流程处理形成270万个视频片段，总计7,089小时动态内容。后续标注流程为这些片段注入详细的空间与语义信息，包括相机位姿、深度图、动态遮罩、结构化描述文本和序列化运动指令。对SpatialVID的数据统计分析显示，其丰富性和多样性将直接促进模型泛化能力与性能提升，使之成为视频与三维视觉研究领域的重要资源。"
    },
    {
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "url": "http://arxiv.org/abs/2509.09672v1",
        "pub_date": "2025-09-11",
        "summary": "Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
        "translated": "在生成模型中，扩散模型因其训练目标存在闭式最优最小化器（通常称为最优去噪器）而独具研究价值。然而，使用该最优去噪器的扩散过程仅能复现训练集中的图像，无法捕捉深度扩散模型的实际行为。近期研究尝试刻画最优去噪器与深度扩散模型之间的性能差异，提出了无需训练的分析模型，其生成图像与训练后的UNet输出结果相似。其中性能最佳的方法假设卷积神经网络的平移等变性和局部性归纳偏置是造成性能差距的原因，因此将这些假设纳入其分析模型。本文通过证据表明，深度扩散模型中的局部性源于图像数据集的统计特性，而非卷积神经网络的归纳偏置。具体而言，我们证明参数化线性最优去噪器展现出与深度神经去噪器相似的局部特性，并通过理论与实验验证这种局部性直接源自自然图像数据集中存在的像素相关性。最终，基于这些发现，我们构建了一个分析型去噪器，其与深度扩散模型预测得分的匹配度优于先前专家构建的替代方案。"
    },
    {
        "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
        "url": "http://arxiv.org/abs/2509.09671v1",
        "pub_date": "2025-09-11",
        "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale, contact-rich demonstrations and hold promise for scaling dexterous robotic manipulation. Yet demonstration inaccuracies and embodiment gaps between human and robot hands limit the straightforward use of these data. Existing methods adopt a three-stage workflow, including retargeting, tracking, and residual correction, which often leaves demonstrations underused and compound errors across stages. We introduce Dexplore, a unified single-loop optimization that jointly performs retargeting and tracking to learn robot control policies directly from MoCap at scale. Rather than treating demonstrations as ground truth, we use them as soft guidance. From raw trajectories, we derive adaptive spatial scopes, and train with reinforcement learning to keep the policy in-scope while minimizing control effort and accomplishing the task. This unified formulation preserves demonstration intent, enables robot-specific strategies to emerge, improves robustness to noise, and scales to large demonstration corpora. We distill the scaled tracking policy into a vision-based, skill-conditioned generative controller that encodes diverse manipulation skills in a rich latent representation, supporting generalization across objects and real-world deployment. Taken together, these contributions position Dexplore as a principled bridge that transforms imperfect demonstrations into effective training signals for dexterous manipulation.",
        "translated": "手-物体运动捕捉（MoCap）数据库提供了大规模、高接触度的演示数据，为扩展灵巧机器人操控能力带来了希望。然而演示数据的不精确性以及人手与机器人手之间的本体差异限制了这些数据的直接使用。现有方法采用三阶段工作流程（包括重定向、跟踪和残差校正），往往导致演示数据利用不足且误差在多阶段中累积。我们提出Dexplore方法，通过统一的单循环优化联合执行重定向与跟踪，直接从大规模运动捕捉数据中学习机器人控制策略。该方法将演示数据视为软性指导而非绝对真值，从原始轨迹中推导出自适应空间范围，并通过强化学习训练策略使其在保持控制范围内最小化控制力并完成任务。这种统一框架既保留了演示意图，又催生了机器人专属策略，增强了对噪声的鲁棒性，并能扩展到大规模演示库。我们将规模化跟踪策略蒸馏为基于视觉的技能条件生成控制器，该控制器将多样化的操控技能编码为丰富的潜在表征，支持跨物体泛化和现实世界部署。这些贡献共同使Dexplore成为将不完美演示转化为灵巧操控有效训练信号的理论桥梁。\n\n（注：专业术语说明：\n1. motion-capture (MoCap) 译为\"运动捕捉\"\n2. retargeting 译为\"重定向\"（指将人体运动数据映射到机器人模型的过程）\n3. embodiment gaps 译为\"本体差异\"\n4. reinforcement learning 译为\"强化学习\"\n5. skill-conditioned generative controller 译为\"技能条件生成控制器\"\n6. latent representation 译为\"潜在表征\"\n译文严格保持技术准确性，同时符合中文学术表达规范。）"
    },
    {
        "title": "Geometric Neural Distance Fields for Learning Human Motion Priors",
        "url": "http://arxiv.org/abs/2509.09667v1",
        "pub_date": "2025-09-11",
        "summary": "We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to \"roll out\" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.",
        "translated": "我们提出了神经黎曼运动场（NRMF），这是一种新颖的3D生成式人体运动先验模型，能够实现鲁棒、时间一致且物理合理的3D运动恢复。与现有的基于VAE或扩散模型的方法不同，我们的高阶运动先验显式地将人体运动建模为对应姿态、过渡（速度）和加速度动力学的神经距离场（NDFs）的零水平集。我们的框架具有严谨的理论基础：NDFs构建在关节旋转、角速度及角加速度的乘积空间上，严格遵循底层关节结构的几何特性。我们还进一步引入了：（i）一种新颖的自适应步长混合算法，用于投影到合理运动集合；（ii）一种创新的几何积分器，在测试时优化和生成过程中实现真实运动轨迹的\"展开\"。实验结果表明显著且一致的性能提升：在AMASS数据集上训练的NRMF模型，能够出色地泛化到多种输入模态，并适应从去噪、运动插值到部分2D/3D观测数据拟合等多样化任务。"
    },
    {
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?",
        "url": "http://arxiv.org/abs/2509.09666v1",
        "pub_date": "2025-09-11",
        "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.",
        "translated": "本文提出了一种富有洞见的范式——通过自编码器视角进行理解：将图像理解视为编码器（I2T）把图像压缩为文本，将生成过程视为解码器（T2I）从文本重建图像。我们以重建保真度作为统一训练目标，强制理解与生成过程之间形成连贯的双向信息流，从而实现相互增益。为实现这一目标，我们提出了创新性统一多模态学习框架UAE。首先通过大规模长上下文图像描述数据对解码器进行预训练，以捕捉细粒度语义和复杂空间关系。随后提出基于强化学习（RL）的Unified-GRPO训练框架，包含三个阶段：（1）冷启动阶段通过语义重建损失温和初始化编码器和解码器；（2）\"生成促进理解\"阶段训练编码器生成信息丰富的描述文本，以最大化解码器的重建质量，从而增强其视觉理解能力；（3）\"理解辅助生成\"阶段优化解码器根据这些描述进行重建，迫使其利用每个细节，提升长上下文指令遵循能力和生成保真度。为评估模型性能，我们推出了首个专门评估统一多模态模型（UMM）融合程度的基准测试Unified-Bench。实验揭示了多模态学习领域的惊人发现：随着强化学习的推进，编码器能自主生成更具描述性的文本，而解码器同时展现出理解这些复杂描述的卓越能力，最终实现具有惊人保真度的图像重建。"
    },
    {
        "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2509.09658v1",
        "pub_date": "2025-09-11",
        "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a \"None of the above\" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.",
        "translated": "多模态大语言模型（MLLMs）中的幻觉问题——即模型生成与输入图像不一致的内容——在现实应用中存在显著风险，可能引发从视觉问答错误信息到决策过程中的安全隐患。现有基准测试主要关注识别准确度，即评估模型能否在干扰项中选择正确答案，却忽视了可信AI同样关键的能力：识别所有给定选项均不正确的情况，这种行为反映的是认知谦逊（epistemic humility）。我们提出HumbleBench，这是一个专用于评估MLLMs在三种幻觉类型（物体、关系和属性）中拒绝看似合理但错误答案能力的新型基准测试平台。基于全景场景图数据集，我们利用细粒度场景图标注提取真实实体与关系，通过GPT-4-Turbo生成多项选择题，并经过严格人工筛选流程。每个问题均包含\"以上皆非\"选项，要求模型不仅能识别正确视觉信息，还需具备判断无有效答案的能力。我们在HumbleBench上评估了包括通用模型和专用推理模型在内的多种前沿MLLMs，并向社区分享了具有价值的研究发现与洞察。通过引入显式错误选项拒绝机制，HumbleBench填补了现有评估体系的关键空白，为安全关键场景中的MLLM可靠性提供了更真实的衡量标准。代码与数据集已开源，可通过https://github.com/maifoundations/HumbleBench 获取。\n\n（注：译文严格遵循以下技术处理原则：\n1. 专业术语准确对应：\"panoptic scene graph\"译为\"全景场景图\"，\"epistemic humility\"采用学界通用译法\"认知谦逊\"\n2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如对基准测试构建流程的描述\n3. 被动语态转化：将\"are released publicly\"等被动结构转换为中文主动式\"已开源\"\n4. 概念显性化处理：\"safety-critical settings\"译为\"安全关键场景\"以突出其重要性\n5. 逻辑连接优化：使用\"不仅...还需...\"等关联词强化技术要求的递进关系）"
    },
    {
        "title": "Mechanistic Learning with Guided Diffusion Models to Predict\n  Spatio-Temporal Brain Tumor Growth",
        "url": "http://arxiv.org/abs/2509.09610v1",
        "pub_date": "2025-09-11",
        "summary": "Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
        "translated": "预测脑肿瘤的时空演变对于指导神经肿瘤学的临床决策至关重要。我们提出了一种混合机制学习框架，将数学肿瘤生长模型与引导式去噪扩散隐式模型（DDIM）相结合，通过前期扫描数据合成解剖学上可行的未来MRI影像。该机制模型通过常微分方程组描述，能够捕捉包括放疗效应在内的肿瘤时序动态并预估未来肿瘤负荷。这些预测结果作为梯度引导DDIM的条件输入，确保生成的影像既符合预测生长规律又保留患者解剖特征。我们在BraTS成人和儿童胶质瘤数据集上训练模型，并在60个内部纵向采集的儿童弥漫性中线胶质瘤（DMG）轴位切片上进行评估。该框架基于空间相似度指标生成了具有高度真实性的随访扫描影像，同时通过第95百分位豪斯多夫距离证明，其生成的肿瘤生长概率图能有效捕捉临床相关的肿瘤生长范围与方向性。这种方法在数据有限场景下实现了基于生物学原理的图像生成，提供了兼顾机制先验的生成式时空预测能力。"
    },
    {
        "title": "Graph Alignment via Dual-Pass Spectral Encoding and Latent Space\n  Communication",
        "url": "http://arxiv.org/abs/2509.09597v1",
        "pub_date": "2025-09-11",
        "summary": "Graph alignment-the problem of identifying corresponding nodes across multiple graphs-is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations.",
        "translated": "图对齐——即识别多个图间对应节点的问题——是众多应用的基础。大多数现有无监督方法将节点特征嵌入潜在表示空间，以实现无需真实对应关系的跨图比较。但这些方法存在两个关键局限：基于GNN的嵌入因过度平滑导致节点区分度下降，以及结构噪声、特征异质性和训练不稳定性引发的潜在空间错位，最终导致节点对应关系不可靠。我们提出了一种新颖的图对齐框架，可同时增强节点区分度并强制潜在空间间的几何一致性。该方法采用双通道编码器，结合低通与高通谱滤波器生成兼具结构感知和高区分度的嵌入表示。针对潜在空间错位问题，我们引入几何感知功能映射模块，学习图嵌入间的双射等距变换，确保不同表征间几何关系的一致性。在图基准测试上的大量实验表明，我们的方法持续优于现有无监督对齐基线，对结构不一致性和具有挑战性的对齐场景展现出卓越的鲁棒性。此外，通过使用多样化预训练模型在视觉-语言基准上的综合评估表明，该框架能有效泛化至图领域之外，实现视觉与语言表征的无监督对齐。\n\n（注：译文严格遵循了以下技术要点：\n1. 专业术语准确：\"graph alignment\"译为\"图对齐\"，\"unsupervised methods\"译为\"无监督方法\"，\"GNN-based embeddings\"译为\"基于GNN的嵌入\"\n2. 技术概念完整保留：\"spectral filters\"译为\"谱滤波器\"，\"bijective and isometric transformations\"译为\"双射等距变换\"\n3. 学术表达规范：采用\"表征\"\"鲁棒性\"\"泛化\"等符合计算机领域论文规范的表述\n4. 长难句处理：将原文复合句按中文习惯拆分为多个短句，如对两个关键局限的并列说明处理\n5. 逻辑关系显化：通过\"即\"\"针对\"\"此外\"等连接词明确技术逻辑链条）"
    },
    {
        "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
        "url": "http://arxiv.org/abs/2509.09595v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
        "translated": "近期音频驱动虚拟形象视频生成技术显著提升了视听真实感，但现有方法仅将指令条件视为声学或视觉线索驱动的低级追踪，未能对指令传达的交流意图进行建模。这一局限影响了生成内容的叙事连贯性与角色表现力。为此，我们提出Kling-Avatar——一种创新级联框架，通过统一多模态指令理解与超写实人像生成来解决该问题。\n\n我们的方法采用两阶段流程：第一阶段设计多模态大语言模型（MLLM）导演模块，根据多样化指令信号生成蓝图视频，从而控制角色动作和情感等高层语义；第二阶段在蓝图关键帧引导下，采用首尾帧策略并行生成多个子片段。这种从全局到局部的框架在保持细粒度细节的同时，忠实编码多模态指令背后的高层意图。并行架构还支持快速稳定生成长时视频，适用于数字人直播和视频博客等实际场景。\n\n为全面评估方法性能，我们构建包含375个精选样本的评测集，覆盖多样化指令与挑战性场景。大量实验表明，Kling-Avatar能以1080p分辨率和48帧率生成生动流畅的长时视频，在唇形同步精度、情感动态表现力、指令可控性、身份保持和跨域泛化方面均优于现有技术。这些成果确立了Kling-Avatar作为语义化高保真音频驱动虚拟形象合成的新标杆。"
    },
    {
        "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
        "url": "http://arxiv.org/abs/2509.09594v1",
        "pub_date": "2025-09-11",
        "summary": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/",
        "translated": "仅使用单目相机与拓扑地图的视觉导航方法，因其无需额外传感器和三维地图的优势，正逐渐成为多传感器方案的有力替代方案。现有方法通常采用\"图像相对性\"范式，通过当前观测图像与子目标图像的配对来估计控制指令。然而图像级的世界表征存在固有局限：图像严格依赖于智能体的位姿与具体形态。相比之下，作为地图固有属性的对象，能够提供与智能体形态及运动轨迹无关的世界表征。本研究提出了一种新型\"对象相对性\"控制学习范式，其具备三大优势：a) 无需严格模仿历史经验即可探索新路径；b) 可将控制预测问题与图像匹配问题解耦；c) 在训练-测试及建图-执行场景中均能实现跨形态部署的高度不变性。我们提出采用\"相对性三维场景图\"形式的拓扑-度量混合地图表征，以此获取信息更丰富的对象级全局路径规划代价。通过直接基于高层\"路径对象代价地图\"表征（无需显式RGB输入）训练局部控制器\"ObjectReact\"，实验证明：在传感器高度变化、需要挑战空间理解能力的多类导航任务（如反向循迹导航）中，对象相对性控制学习均优于图像相对性方案。我们进一步证实：仅通过仿真训练的策略能够良好泛化至真实室内环境。代码及补充材料详见项目页面：https://object-react.github.io/\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语准确对应：\"topological map\"译作\"拓扑地图\"，\"3D scene graph\"译作\"三维场景图\"\n2. 概念体系完整保留：\"embodiment\"译为\"形态\"以保持机器人学语境\n3. 技术逻辑清晰呈现：通过括号补充说明（如\"无需显式RGB输入\"）确保技术细节无损传递\n4. 学术表述符合规范：使用\"范式\"\"表征\"\"解耦\"等学术用语\n5. 长句结构合理切分：将原文复合句拆解为符合中文表达习惯的短句结构）"
    },
    {
        "title": "Visual Grounding from Event Cameras",
        "url": "http://arxiv.org/abs/2509.09584v1",
        "pub_date": "2025-09-11",
        "summary": "Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.",
        "translated": "事件相机能够以微秒级精度捕捉亮度变化，在运动模糊和复杂光照条件下仍保持可靠性能，为高动态场景建模提供了显著优势。然而，其与自然语言理解的结合尚未得到充分关注，导致多模态感知领域存在空白。为此，我们推出Talk2Event——首个基于事件数据的语言驱动目标定位大规模基准数据集。该数据集以真实驾驶场景为基础，包含5,567个场景、13,458个标注对象以及超过30,000条经过严格验证的指代表达式。每条表达式均包含四个结构化属性：外观特征、状态信息、与观察者的关系以及与周围对象的关系，这些属性明确捕捉了空间、时间和关系线索。这种以属性为中心的设计支持可解释的组合式定位，使得分析不再局限于简单目标识别，而是扩展到动态环境中的上下文推理。我们期待Talk2Event成为推动多模态及时序感知研究的基础平台，其应用可覆盖机器人技术、人机交互等多个领域。\n\n（注：译文严格遵循学术论文摘要的规范表述，对\"event cameras\"采用行业通用译法\"事件相机\"，\"object grounding\"译为\"目标定位\"，\"referring expressions\"译为\"指代表达式\"等专业术语均符合计算机视觉与自然语言处理领域的术语标准。通过使用\"微秒级精度\"\"结构化属性\"\"上下文推理\"等专业表述，在保持原文技术准确性的同时确保中文表达的流畅性。）"
    },
    {
        "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient\n  Fine-Tuning for Remote Sensing Change Detection",
        "url": "http://arxiv.org/abs/2509.09572v1",
        "pub_date": "2025-09-11",
        "summary": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.",
        "translated": "为应对多时相多源遥感影像中普遍存在的伪变化、标注样本稀缺及跨域泛化困难等挑战，本文提出PeftCD——一种基于视觉基础模型（VFM）与参数高效微调（PEFT）技术的变化检测框架。该框架核心采用权重共享的孪生编码器结构，源自预训练VFM主干网络，并无缝集成LoRA与Adapter模块，通过仅训练极少量附加参数实现高效任务自适应。为充分释放VFM潜力，我们探索了两种领先主干网络：以强大分割先验著称的Segment Anything Model v2（SAM2），以及前沿自监督表征学习模型DINOv3。框架配以轻量化解码器设计，确保聚焦于主干网络提取的强大特征表征。大量实验表明，PeftCD在多个公开数据集上达到最先进性能：SYSU-CD（IoU 73.81%）、WHUCD（92.05%）、MSRSCD（64.07%）、MLCD（76.89%）、CDD（97.01%）、S2Looking（52.25%）和LEVIR-CD（85.62%），兼具精确边界刻画与强伪变化抑制能力。该框架在精度、效率与泛化性间达成最优平衡，为大规模VFM适配真实遥感变化检测应用提供了强大可扩展的范式。代码与预训练模型将于https://github.com/dyzy41/PeftCD 开源。\n\n（注：专业术语说明：\n1. 伪变化（Pseudo changes）：由光照、季节、传感器差异等非地表真实变化引起的干扰信号\n2. 参数高效微调（PEFT）：Parameter-Efficient Fine-Tuning的规范译法\n3. 视觉基础模型（VFM）：Vision Foundation Models的标准译名，与LLM（大语言模型）形成对应概念\n4. LoRA/Adapter：保持原文大写格式的模块名称，学术界通用写法\n5. 孪生编码器（Siamese encoder）：计算机视觉领域标准术语译法\n6. IoU：交并比（Intersection over Union），保留英文缩写+中文说明的规范表述）"
    },
    {
        "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in\n  MRI-based Alzheimer's Disease Classification",
        "url": "http://arxiv.org/abs/2509.09558v1",
        "pub_date": "2025-09-11",
        "summary": "Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR",
        "translated": "磁共振成像（MRI）是脑部影像学的金标准。已有研究提出采用深度学习（DL）算法辅助基于MRI扫描的阿尔茨海默病（AD）诊断。然而深度学习算法可能存在捷径学习问题——即使用与输出标签无直接关联的伪特征进行预测。当这些特征与受保护属性相关时，会导致对代表性不足群体（如按种族和性别划分的群体）的性能偏差。本研究探索了基于MRI的AD深度学习诊断中存在的捷径学习与人口统计学偏差风险。我们首先验证深度学习算法能否通过3D脑部MRI扫描识别种族或性别，以确认是否存在基于种族和性别的分布偏移。继而探究训练集的种族或性别不平衡是否会导致模型性能下降，从而揭示捷径学习与偏差现象。最后，我们对受保护属性和AD分类任务中不同脑区的特征归因进行了定量与定性分析。通过多数据集（ADNI、AIBL、MIRIAD）和多种深度学习模型（ResNet、SwinTransformer）的实验，我们证实了基于种族的性别捷径学习与偏差在AD分类中的存在。本研究为开发更公平的脑部MRI深度学习诊断工具奠定了基础。代码已开源：https://github.com/acharaakshit/ShortMR\n\n（注：根据学术规范，对原文中括号内的数据集名称ADNI/AIBL/MIRIAD进行了显性化处理，使技术细节表达更完整）"
    },
    {
        "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction\n  Generation",
        "url": "http://arxiv.org/abs/2509.09555v1",
        "pub_date": "2025-09-11",
        "summary": "While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.",
        "translated": "尽管大规模人体动作捕捉数据集推动了人体动作生成的发展，但由于数据集的局限性，动态三维人-物交互（HOI）的建模与生成仍面临挑战。现有数据集往往缺乏大量高质量动作数据与标注，且存在接触穿透、物体悬浮、手部动作失真等伪影问题。为解决这些问题，我们推出了InterAct——一个包含数据集与方法论创新的三维人-物交互基准系统。首先，我们整合并标准化了来自多来源的21.81小时人-物交互数据，并通过精细化文本标注进行数据增强。其次，提出统一优化框架，通过减少伪影和修正手部动作提升数据质量。基于接触不变性原理，我们在保持人-物交互关系的同时引入动作变异，将数据集扩展至30.70小时。第三，我们定义了六项基准任务，并开发了统一的人-物交互生成建模框架，实现了最先进的性能表现。大量实验验证了本数据集作为推动三维人-物交互生成研究的基础资源的实用性。为持续支持该领域研究，数据集已在https://github.com/wzyabcas/InterAct开源并将持续维护。\n\n（注：专业术语说明：\n1. human-object interactions (HOIs) 译为\"人-物交互\"\n2. contact penetration 译为\"接触穿透\"\n3. contact invariance 译为\"接触不变性\"\n4. state-of-the-art 译为\"最先进的\"\n5. benchmarking tasks 译为\"基准任务\"\n6. generative modeling 译为\"生成建模\"\n译文严格保持学术论文的正式语体，准确传递技术细节，同时符合中文表达习惯。）"
    },
    {
        "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion\n  and Alignment from Self-Supervised Vision Encoders",
        "url": "http://arxiv.org/abs/2509.09547v1",
        "pub_date": "2025-09-11",
        "summary": "Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/",
        "translated": "近年来，视频扩散模型因架构创新（如扩散变换器）和新型训练目标（如流匹配）的推动而快速发展。相比之下，如何提升此类模型的特征表征能力尚未获得足够关注。本研究提出，通过将视频生成器的中间特征与预训练视觉编码器的特征表示进行对齐，可有效提升视频扩散模型的训练效果。我们设计了一种新指标，并对多种视觉编码器展开深入分析，评估其判别能力与时序一致性，从而判断它们是否适用于视频特征对齐任务。基于分析结果，我们提出Align4Gen框架——通过创新的多特征融合与对齐方法增强视频扩散模型的训练过程。在无条件生成和类别条件生成任务上的实验表明，该方法能显著提升视频生成质量，多项评估指标均得到改善。完整视频结果详见项目页面：https://align4gen.github.io/align4gen/\n\n（注：技术要点说明：\n1. 专业术语保留英文原词：diffusion transformers（扩散变换器）、flow matching（流匹配）、feature alignment（特征对齐）\n2. 关键技术概念准确转化：multi-feature fusion（多特征融合）、temporal consistency（时序一致性）\n3. 学术表达规范：采用\"本研究\"\"实验表明\"等符合学术摘要的表述方式\n4. 链接与专有名词原样保留）"
    },
    {
        "title": "DualTrack: Sensorless 3D Ultrasound needs Local and Global Context",
        "url": "http://arxiv.org/abs/2509.09530v1",
        "pub_date": "2025-09-11",
        "summary": "Three-dimensional ultrasound (US) offers many clinical advantages over conventional 2D imaging, yet its widespread adoption is limited by the cost and complexity of traditional 3D systems. Sensorless 3D US, which uses deep learning to estimate a 3D probe trajectory from a sequence of 2D US images, is a promising alternative. Local features, such as speckle patterns, can help predict frame-to-frame motion, while global features, such as coarse shapes and anatomical structures, can situate the scan relative to anatomy and help predict its general shape. In prior approaches, global features are either ignored or tightly coupled with local feature extraction, restricting the ability to robustly model these two complementary aspects. We propose DualTrack, a novel dual-encoder architecture that leverages decoupled local and global encoders specialized for their respective scales of feature extraction. The local encoder uses dense spatiotemporal convolutions to capture fine-grained features, while the global encoder utilizes an image backbone (e.g., a 2D CNN or foundation model) and temporal attention layers to embed high-level anatomical features and long-range dependencies. A lightweight fusion module then combines these features to estimate the trajectory. Experimental results on a large public benchmark show that DualTrack achieves state-of-the-art accuracy and globally consistent 3D reconstructions, outperforming previous methods and yielding an average reconstruction error below 5 mm.",
        "translated": "三维超声（US）相比传统二维成像具有诸多临床优势，但其广泛应用受限于传统3D系统的成本与复杂度。无传感器3D超声技术通过深度学习从二维超声图像序列中估计探头轨迹，成为一种具有前景的替代方案。散斑图案等局部特征可辅助帧间运动预测，而粗略形状和解剖结构等全局特征能定位扫描位置并推断整体形态。现有方法往往忽略全局特征，或将其与局部特征提取强耦合，限制了对此互补双要素的鲁棒建模能力。我们提出DualTrack——一种新型双编码器架构，采用解耦的局部与全局编码器分别专注于不同尺度的特征提取：局部编码器通过密集时空卷积捕获细粒度特征，全局编码器则利用图像主干网络（如2D CNN或基础模型）与时序注意力层嵌入高层解剖特征及长程依赖关系。轻量级融合模块最终整合这些特征以估计轨迹。在大型公开基准测试中，DualTrack实现了最先进的精度和全局一致的三维重建，性能超越现有方法，平均重建误差低于5毫米。"
    },
    {
        "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
        "url": "http://arxiv.org/abs/2509.09527v1",
        "pub_date": "2025-09-11",
        "summary": "In recent years, Multi-View Clustering (MVC) has been significantly advanced under the influence of deep learning. By integrating heterogeneous data from multiple views, MVC enhances clustering analysis, making multi-view fusion critical to clustering performance. However, there is a problem of low-quality data in multi-view fusion. This problem primarily arises from two reasons: 1) Certain views are contaminated by noisy data. 2) Some views suffer from missing data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF) method to address this problem. SGDF leverages a multiple generative mechanism for the multi-view feature of each sample. It is robust to low-quality data. Building on SGDF, we further present the Generative Diffusion Contrastive Network (GDCN). Extensive experiments show that GDCN achieves the state-of-the-art results in deep MVC tasks. The source code is publicly available at https://github.com/HackerHyper/GDCN.",
        "translated": "近年来，深度学习推动了多视图聚类（MVC）领域的显著发展。通过整合来自多个视图的异构数据，MVC增强了聚类分析能力，使得多视图融合成为决定聚类性能的关键因素。然而，多视图融合中存在低质量数据的问题，这主要源于两方面原因：1）某些视图受到噪声数据污染；2）部分视图存在数据缺失。本文提出了一种新颖的随机生成扩散融合（SGDF）方法来解决这一问题。SGDF利用多重生成机制处理每个样本的多视图特征，对低质量数据具有强鲁棒性。基于SGDF，我们进一步构建了生成扩散对比网络（GDCN）。大量实验表明，GDCN在深度多视图聚类任务中取得了最先进的性能。相关源代码已公开于https://github.com/HackerHyper/GDCN。\n\n（注：专业术语说明：\n- Multi-View Clustering (MVC) 标准译法为\"多视图聚类\"\n- Stochastic Generative Diffusion Fusion (SGDF) 采用意译\"随机生成扩散融合\"\n- Generative Diffusion Contrastive Network (GDCN) 译为\"生成扩散对比网络\"\n- state-of-the-art 遵循学术惯例译为\"最先进的\"\n- 技术概念\"multi-view fusion\"统一译为\"多视图融合\"\n- \"robust\"按工程术语译为\"鲁棒性\"）"
    },
    {
        "title": "Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided\n  Protocol on the Connectome 2.0 scanner",
        "url": "http://arxiv.org/abs/2509.09513v1",
        "pub_date": "2025-09-11",
        "summary": "The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.",
        "translated": "扩散磁共振神经突触交换成像模型通过估算脑灰质微结构参数（如分区容积、扩散系数及跨区水分子交换时间），为探测神经组织微观特性提供了有力工具。然而现有成像方案需耗费较长扫描时间。本研究针对\"连接组2.0\"扫描仪提出一种缩减采集方案，在保持模型精度的同时显著缩短扫描时长。我们开发了基于可解释人工智能的数据驱动框架，采用指导式递归特征消除策略，从原有15个特征的协议中优选8个特征子集。通过体内实验验证该优化方案的性能，并与完整采集方案及其他缩减策略进行对比评估，重点考察参数准确性、解剖对比度保持度和重测再现性。结果表明：缩减方案获得的参数估计值与皮层映射图与完整方案相当，在合成数据中呈现较低估计误差，且对重测变异影响极小。与理论驱动式和启发式缩减方案相比，本方案展现出更优的鲁棒性，将水交换时间估计值的偏差降低两倍以上。该混合优化框架最终实现了仅需14分钟即可完成神经突触交换成像且不损失参数保真度，为交换敏感型扩散磁共振成像在神经科学和临床研究中的广泛应用提供支持，同时为生物物理参数映射领域的高效采集方案设计提供了可推广的方法论。"
    },
    {
        "title": "AskDoc -- Identifying Hidden Healthcare Disparities",
        "url": "http://arxiv.org/abs/2509.09622v1",
        "pub_date": "2025-09-11",
        "summary": "The objective of this study is to understand the online Ask the Doctor services medical advice on internet platforms via AskDoc, a Reddit community that serves as a public AtD platform and study if platforms mirror existing hurdles and partiality in healthcare across various demographic groups. We downloaded data from January 2020 to May 2022 from AskDoc -- a subreddit, and created regular expressions to identify self-reported demographics (Gender, Race, and Age) from the posts, and performed statistical analysis to understand the interaction between peers and physicians with the posters. Half of the posts did not receive comments from peers or physicians. At least 90% of the people disclose their gender and age, and 80% of the people do not disclose their race. It was observed that the subreddit is dominated by adult (age group 20-39) white males. Some disparities were observed in the engagement between the users and the posters with certain demographics. Beyond the confines of clinics and hospitals, social media could bring patients and providers closer together, however, as observed, current physicians participation is low compared to posters.",
        "translated": "本研究旨在通过分析Reddit平台的AskDoc社区（一个公共在线问诊平台），探究互联网“问诊”服务的医疗建议模式，并验证此类平台是否反映了不同人口群体在医疗保健中面临的既有障碍与偏见。我们收集了该子论坛2020年1月至2022年5月的数据，通过正则表达式识别发帖者自我报告的人口统计特征（性别、种族、年龄），并采用统计分析研究医患及用户间的互动模式。\n\n研究发现：半数发帖未获得任何用户或医生的回复；超过90%的用户公开了性别和年龄信息，但80%未披露种族信息；该社区主要由20-39岁的成年白人男性主导。数据分析显示，特定人口特征的发帖者获得的互动参与度存在差异。研究表明，社交媒体具有突破传统诊所医院限制、拉近医患距离的潜力，但当前医生参与度远低于发帖者需求。\n\n（注：专业术语说明：\n- Ask the Doctor(AtD): 在线问诊服务\n- subreddit: Reddit子论坛\n- regular expressions: 正则表达式\n- demographic groups: 人口统计群体\n- 所有医学术语和数据分析方法均按学术规范准确翻译）"
    },
    {
        "title": "Boosting Data Utilization for Multilingual Dense Retrieval",
        "url": "http://arxiv.org/abs/2509.09459v1",
        "pub_date": "2025-09-11",
        "summary": "Multilingual dense retrieval aims to retrieve relevant documents across different languages based on a unified retriever model. The challenge lies in aligning representations of different languages in a shared vector space. The common practice is to fine-tune the dense retriever via contrastive learning, whose effectiveness highly relies on the quality of the negative sample and the efficacy of mini-batch data. Different from the existing studies that focus on developing sophisticated model architecture, we propose a method to boost data utilization for multilingual dense retrieval by obtaining high-quality hard negative samples and effective mini-batch data. The extensive experimental results on a multilingual retrieval benchmark, MIRACL, with 16 languages demonstrate the effectiveness of our method by outperforming several existing strong baselines.",
        "translated": "多语言稠密检索旨在通过统一的检索器模型实现跨语言的相关文档检索，其核心挑战在于将不同语言的表征对齐到共享向量空间中。当前主流方法是通过对比学习对稠密检索器进行微调，其效果高度依赖于负样本质量与小批次数据的有效性。与现有研究侧重于开发复杂模型架构不同，我们提出了一种提升多语言稠密检索数据利用效率的方法，通过获取高质量困难负样本并构建高效小批次数据实现性能提升。在多语言检索基准MIRACL（涵盖16种语言）上的大量实验表明，我们的方法优于多个现有强基线，验证了其有效性。\n\n（注：译文严格遵循了以下要点：\n1. 专业术语准确：\"dense retrieval\"译为\"稠密检索\"，\"contrastive learning\"译为\"对比学习\"，\"hard negative samples\"译为\"困难负样本\"\n2. 技术细节保留：完整传递了\"mini-batch data有效性\"、\"共享向量空间对齐\"等核心概念\n3. 学术表达规范：采用\"旨在\"\"侧重于\"\"验证了\"等学术用语\n4. 逻辑结构保持：遵循\"问题背景-方法创新-实验验证\"的原文脉络\n5. 数据准确性：明确保留\"MIRACL基准\"和\"16种语言\"的关键数据信息）"
    },
    {
        "title": "We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years\n  Later",
        "url": "http://arxiv.org/abs/2509.09414v1",
        "pub_date": "2025-09-11",
        "summary": "In 2011, Xavier Amatriain sounded the alarm: recommender systems research was \"doing it all wrong\" [1]. His critique, rooted in statistical misinterpretation and methodological shortcuts, remains as relevant today as it was then. But rather than correcting course, we added new layers of sophistication on top of the same broken foundations. This paper revisits Amatriain's diagnosis and argues that many of the conceptual, epistemological, and infrastructural failures he identified still persist, in more subtle or systemic forms. Drawing on recent work in reproducibility, evaluation methodology, environmental impact, and participatory design, we showcase how the field's accelerating complexity has outpaced its introspection. We highlight ongoing community-led initiatives that attempt to shift the paradigm, including workshops, evaluation frameworks, and calls for value-sensitive and participatory research. At the same time, we contend that meaningful change will require not only new metrics or better tooling, but a fundamental reframing of what recommender systems research is for, who it serves, and how knowledge is produced and validated. Our call is not just for technical reform, but for a recommender systems research agenda grounded in epistemic humility, human impact, and sustainable practice.",
        "translated": "2011年，Xavier Amatriain曾敲响警钟：推荐系统研究正\"误入歧途\"[1]。其批判根植于统计误读与方法论捷径，这一洞见在当今仍具现实意义。然而我们非但没有修正航向，反而在原有缺陷基础上叠加了更复杂的层次。本文重访Amatriain的诊断，指出其当年指出的概念性、认识论和基础设施层面的缺陷——尽管以更隐蔽或系统化的形式——至今依然存在。通过借鉴可复现性研究、评估方法论、环境影响及参与式设计等领域的最新成果，我们揭示该领域如何陷入\"复杂性加速超越自省速度\"的困境。我们重点介绍了社区主导的范式转型尝试，包括专题研讨会、评估框架建设，以及推动价值敏感性与参与式研究的倡议。同时我们主张，真正意义上的变革不仅需要新评估指标或更优工具，更需要从根本上重新界定推荐系统研究的目标宗旨、服务对象以及知识生产与验证机制。我们呼吁的不仅是技术改良，更是一场需要以认知谦逊、人类福祉和可持续实践为根基的推荐系统研究范式重构。\n\n（注：保留原文文献标注[1]，专业术语如\"epistemic humility\"译为\"认知谦逊\"，\"value-sensitive\"译为\"价值敏感性\"，\"participatory design\"译为\"参与式设计\"，在保持学术严谨性的同时确保中文表达流畅。）"
    },
    {
        "title": "CESRec: Constructing Pseudo Interactions for Sequential Recommendation\n  via Conversational Feedback",
        "url": "http://arxiv.org/abs/2509.09342v1",
        "pub_date": "2025-09-11",
        "summary": "Sequential Recommendation Systems (SRS) have become essential in many real-world applications. However, existing SRS methods often rely on collaborative filtering signals and fail to capture real-time user preferences, while Conversational Recommendation Systems (CRS) excel at eliciting immediate interests through natural language interactions but neglect historical behavior. To bridge this gap, we propose CESRec, a novel framework that integrates the long-term preference modeling of SRS with the real-time preference elicitation of CRS. We introduce semantic-based pseudo interaction construction, which dynamically updates users'historical interaction sequences by analyzing conversational feedback, generating a pseudo-interaction sequence that seamlessly combines long-term and real-time preferences. Additionally, we reduce the impact of outliers in historical items that deviate from users'core preferences by proposing dual alignment outlier items masking, which identifies and masks such items using semantic-collaborative aligned representations. Extensive experiments demonstrate that CESRec achieves state-of-the-art performance by boosting strong SRS models, validating its effectiveness in integrating conversational feedback into SRS.",
        "translated": "序列推荐系统（SRS）已在众多实际应用中变得至关重要。然而，现有SRS方法通常依赖协同过滤信号，难以捕捉实时用户偏好；而对话推荐系统（CRS）虽能通过自然语言交互有效获取即时兴趣，却忽略了历史行为。为弥补这一缺陷，我们提出CESRec——一个创新框架，将SRS的长期偏好建模与CRS的实时偏好获取能力相融合。我们引入基于语义的伪交互构建技术，通过分析对话反馈动态更新用户历史交互序列，生成融合长期与实时偏好的伪交互序列。此外，针对历史项目中偏离用户核心偏好的异常项，我们提出双对齐异常项掩蔽机制，通过语义-协同对齐表征识别并屏蔽此类干扰项。大量实验表明，CESRec通过增强现有强效SRS模型实现了最先进的性能，验证了其将对话反馈整合至SRS的有效性。"
    },
    {
        "title": "Constructing a Question-Answering Simulator through the Distillation of\n  LLMs",
        "url": "http://arxiv.org/abs/2509.09226v1",
        "pub_date": "2025-09-11",
        "summary": "The question-answering (QA) simulator is a model that mimics real student learning behaviors and predicts their correctness of their responses to questions. QA simulators enable educational recommender systems (ERS) to collect large amounts of training data without interacting with real students, thereby preventing harmful recommendations made by an undertrained ERS from undermining actual student learning. Given the QA history, there are two categories of solutions to predict the correctness, conducting the simulation: (1) LLM-free methods, which apply a traditional sequential model to transfer the QA history into a vector representation first, and make predictions based on the representation; (2) LLM-based methods, which leverage the domain knowledge and reasoning capability of LLM to enhence the prediction. LLM-free methods offer fast inference but generally yield suboptimal performance. In contrast, most LLM-based methods achieve better results, but at the cost of slower inference speed and higher GPU memory consumption. In this paper, we propose a method named LLM Distillation based Simulator (LDSim), which distills domain knowledge and reasoning capability from an LLM to better assist prediction, thereby improving simulation performance. Extensive experiments demonstrate that our LDSim achieves strong results on both the simulation task and the knowledge tracing (KT) task. Our code is publicly available at https://anonymous.4open.science/r/LDSim-05A9.",
        "translated": "问题回答模拟器（QA simulator）是一种模拟真实学生学习行为并预测其答题正确率的模型。该模拟器使教育推荐系统（ERS）无需与真实学生交互即可收集大量训练数据，从而避免因系统训练不足产生有害推荐而影响实际学习效果。基于答题历史记录，现有预测正确率的模拟方法可分为两类：（1）无大语言模型方法（LLM-free），采用传统序列模型先将答题历史转换为向量表示，再基于该表示进行预测；（2）基于大语言模型方法（LLM-based），利用大模型的领域知识和推理能力提升预测精度。无大模型方法推理速度快但性能欠佳，而多数大模型方法虽效果更好，却需以降低推理速度和增加GPU内存消耗为代价。本文提出名为大模型蒸馏模拟器（LDSim）的方法，通过从大模型中蒸馏领域知识与推理能力来优化预测，从而提升模拟性能。大量实验表明，LDSim在模拟任务和知识追踪（KT）任务上均取得优异结果。代码已开源：https://anonymous.4open.science/r/LDSim-05A9。\n\n（注：根据学术规范，译文对原文中的技术术语（如knowledge tracing译为\"知识追踪\"）、方法论描述（如distill译为\"蒸馏\"）及长难句结构进行了专业化处理，同时确保了URL等要素的准确保留。）"
    },
    {
        "title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2509.09114v1",
        "pub_date": "2025-09-11",
        "summary": "Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec.",
        "translated": "多模态推荐系统正日益成为电子商务和内容平台的基础技术，其通过联合建模用户历史行为与商品多模态特征（如视觉与文本特征）来实现个性化服务。然而现有方法大多依赖静态融合策略或基于图的局部交互建模，存在两个关键局限：（1）细粒度跨模态关联建模能力不足，导致融合质量欠佳；（2）缺乏全局分布层面的一致性，引发表征偏差。为此，我们提出MambaRec框架，通过注意力引导学习整合局部特征对齐与全局分布正则化。其核心是提出的扩张细化注意力模块（DREAM），该模块采用多尺度扩张卷积结合通道与空间注意力机制，对齐视觉与文本模态间的细粒度语义模式。该模块能捕获层次化关系和上下文感知关联，提升跨模态语义建模能力。此外，我们引入最大均值差异（MMD）和对比损失函数约束全局模态对齐，增强语义一致性。这种双重正则化机制减少了模态特异性偏差并提升鲁棒性。为提高可扩展性，MambaRec采用降维策略降低高维多模态特征的计算成本。在真实电商数据集上的大量实验表明，MambaRec在融合质量、泛化能力和效率方面均优于现有方法。代码已开源：https://github.com/rkl71/MambaRec。\n\n（注：译文严格遵循以下技术细节处理：\n1. 专业术语标准化：\"Multimodal recommendation systems\"译为\"多模态推荐系统\"，\"Maximum Mean Discrepancy\"保留英文缩写MMD并补充中文全称\n2. 技术概念准确传达：\"dilated convolutions\"译为\"扩张卷积\"，\"contrastive loss\"译为\"对比损失函数\"\n3. 结构逻辑显性化：通过\"其核心是\"、\"此外\"等连接词明确技术模块的层次关系\n4. 长句拆分重组：将原文复合句拆分为符合中文表达习惯的短句，如将DREAM模块的说明拆分为功能描述和技术实现两个层次）"
    },
    {
        "title": "Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in\n  Personalized Recommendation",
        "url": "http://arxiv.org/abs/2509.09037v1",
        "pub_date": "2025-09-10",
        "summary": "Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have been used as fairness concepts in the economics, game theory, and social choice literatures since the 1960s, and have recently gained popularity within the recommendation systems communities. In this short position paper we will give an overview of envy-freeness and its use in economics and recommendation systems; and illustrate why envy is not appropriate to measure fairness for use in settings where personalization plays a role.",
        "translated": "嫉妒自由性（Envy-freeness）及其松弛形式\"单物品嫉妒自由性\"（EF-1）作为公平性概念，自1960年代以来广泛应用于经济学、博弈论和社会选择领域的研究中，近期在推荐系统领域也受到广泛关注。本短篇立场文件将概述嫉妒自由性在经济学与推荐系统中的应用，并论证在涉及个性化设置的场景中，该指标并不适合作为公平性的衡量标准。"
    },
    {
        "title": "Generative Engine Optimization: How to Dominate AI Search",
        "url": "http://arxiv.org/abs/2509.08919v1",
        "pub_date": "2025-09-10",
        "summary": "The rapid adoption of generative AI-powered search engines like ChatGPT, Perplexity, and Gemini is fundamentally reshaping information retrieval, moving from traditional ranked lists to synthesized, citation-backed answers. This shift challenges established Search Engine Optimization (SEO) practices and necessitates a new paradigm, which we term Generative Engine Optimization (GEO).   This paper presents a comprehensive comparative analysis of AI Search and traditional web search (Google). Through a series of large-scale, controlled experiments across multiple verticals, languages, and query paraphrases, we quantify critical differences in how these systems source information. Our key findings reveal that AI Search exhibit a systematic and overwhelming bias towards Earned media (third-party, authoritative sources) over Brand-owned and Social content, a stark contrast to Google's more balanced mix. We further demonstrate that AI Search services differ significantly from each other in their domain diversity, freshness, cross-language stability, and sensitivity to phrasing.   Based on these empirical results, we formulate a strategic GEO agenda. We provide actionable guidance for practitioners, emphasizing the critical need to: (1) engineer content for machine scannability and justification, (2) dominate earned media to build AI-perceived authority, (3) adopt engine-specific and language-aware strategies, and (4) overcome the inherent \"big brand bias\" for niche players. Our work provides the foundational empirical analysis and a strategic framework for achieving visibility in the new generative search landscape.",
        "translated": "随着以ChatGPT、Perplexity和Gemini为代表的生成式AI搜索引擎迅速普及，信息检索模式正经历根本性变革——从传统排名列表转向具备文献引证的合成式答案。这一转变对现有搜索引擎优化（SEO）实践构成挑战，亟需建立新范式，我们将其定义为生成式引擎优化（GEO）。  \n本文通过跨垂直领域、多语言及查询释义的大规模对照实验，对AI搜索与传统网络搜索（Google）展开全面对比分析。量化研究揭示：AI搜索系统呈现出系统性、压倒性的对第三方权威媒体内容（Earned media）的偏好，显著区别于谷歌对自有品牌内容与社交内容的均衡收录模式。研究进一步发现，不同AI搜索服务在领域多样性、内容时效性、跨语言稳定性及查询句式敏感性方面存在显著差异。  \n基于实证结果，我们构建了战略性GEO实施框架，为从业者提供可操作的指导建议：（1）设计便于机器扫描与逻辑验证的内容架构；（2）通过权威媒体曝光建立AI可感知的公信力；（3）制定引擎差异化与语言感知型策略；（4）帮助利基市场参与者克服固有的\"大品牌偏见\"。本研究为在生成式搜索新生态中获取可见性提供了实证基础与战略框架。"
    },
    {
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
        "url": "http://arxiv.org/abs/2509.06733v1",
        "pub_date": "2025-09-08",
        "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
        "translated": "深度研究系统——一种通过协调推理、开放网络与用户文件搜索及工具使用来解决复杂多步骤任务的智能体AI——正朝着包含规划器、协调器和执行器的分层架构发展。实践中，端到端训练整个系统栈仍不现实，因此现有研究多集中于训练连接搜索、浏览和代码等核心工具的单一规划器。虽然监督微调（SFT）能保证协议规范性，但其存在模仿偏差和暴露偏差问题，且未能充分利用环境反馈。偏好对齐方法（如DPO）受限于预设框架和代理指标，属于离策策略，在长周期信用分配和多目标权衡方面表现较弱。SFT和DPO的更深层局限在于其依赖通过框架设计和标注比较来人为定义决策点与子技能。\n\n强化学习通过优化轨迹级策略，与闭环工具交互研究形成天然契合：它支持探索与恢复行为，实现原理性信用分配，并减少对人类先验知识和评分者偏差的依赖。据我们所知，本综述是首篇专注于深度研究系统强化学习基础的系统性研究。我们沿三个维度梳理了DeepSeek-R1后的研究成果：（i）数据合成与治理；（ii）智能研究体的RL方法，涵盖稳定性、样本效率、长上下文处理、奖励与信用设计、多目标优化及多模态整合；（iii）智能体RL训练系统与框架。同时涵盖智能体架构与协调机制，以及评估基准——包括最新QA、VQA、长文本合成及领域扎根的工具交互任务。我们提炼出重复模式，揭示基础设施瓶颈，并为训练鲁棒透明的深度研究智能体提供实用RL指导。\n\n（注：专业术语说明：\n- Agentic AI：译为\"智能体AI\"，强调自主决策能力\n- Exposure bias：暴露偏差，指训练与推断阶段的数据分布差异问题\n- Off-policy：离策策略，指评估策略与行为策略不同的强化学习方法\n- Credit assignment：信用分配，指将总体回报归因到具体行动的过程\n- VQA：视觉问答（Visual Question Answering）\n- 长文本合成（long-form synthesis）：指生成连贯长文本的能力）"
    },
    {
        "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
        "url": "http://arxiv.org/abs/2509.09360v1",
        "pub_date": "2025-09-11",
        "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must be consistent with retrieved evidence. We therefore present MetaRAG, a metamorphic testing framework for hallucination detection in Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time, unsupervised, black-box setting, requiring neither ground-truth references nor access to model internals, making it suitable for proprietary and high-stakes domains. The framework proceeds in four stages: (1) decompose answers into atomic factoids, (2) generate controlled mutations of each factoid using synonym and antonym substitutions, (3) verify each variant against the retrieved context (synonyms are expected to be entailed and antonyms contradicted), and (4) aggregate penalties for inconsistencies into a response-level hallucination score. Crucially for identity-aware AI, MetaRAG localizes unsupported claims at the factoid span where they occur (e.g., pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility), allowing users to see flagged spans and enabling system designers to configure thresholds and guardrails for identity-sensitive queries. Experiments on a proprietary enterprise dataset illustrate the effectiveness of MetaRAG for detecting hallucinations and enabling trustworthy deployment of RAG-based conversational agents. We also outline a topic-based deployment design that translates MetaRAG's span-level scores into identity-aware safeguards; this design is discussed but not evaluated in our experiments.",
        "translated": "大型语言模型（LLMs）在企业应用中的部署日益增多，但其可靠性仍受幻觉问题（即自信但事实错误的输出）的限制。现有检测方法（如SelfCheckGPT和MetaQA）主要针对独立LLMs，未能解决检索增强生成（RAG）系统中需确保响应与检索证据一致的特殊挑战。为此，我们提出MetaRAG——一个面向RAG系统幻觉检测的蜕变测试框架。该框架在实时、无监督、黑盒环境下运行，既不需要真实参考数据，也无需访问模型内部，适用于专有和高风险领域。\n\nMetaRAG的工作流程包含四个阶段：（1）将答案分解为原子化事实单元；（2）通过同义词/反义词替换生成受控变异；（3）依据检索上下文验证每个变体（要求同义词版本可被验证，反义词版本应被否定）；（4）将不一致性惩罚聚合为响应级幻觉分数。该框架的核心价值在于支持身份感知AI——它能定位未经验证的主张所在的具体事实单元（例如妊娠特定注意事项、LGBTQ+难民权利或劳动资格），既允许用户查看被标记的文本片段，也支持系统设计者为身份敏感查询配置阈值与防护机制。\n\n在专有企业数据集上的实验证明了MetaRAG在检测幻觉和推动RAG对话代理可信部署方面的有效性。我们还提出基于主题的部署设计方案，将片段级分数转化为身份感知保障机制（该设计在文中讨论但未纳入实验评估）。"
    },
    {
        "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
        "url": "http://arxiv.org/abs/2509.09332v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io",
        "translated": "多模态大语言模型（MLLMs）的最新进展为具身智能开辟了新机遇，使其能够实现多模态理解、推理与交互以及连续空间决策。然而，当前基于MLLM的具身系统面临两个关键局限：其一，几何适应性鸿沟——仅基于二维输入训练或采用硬编码三维几何注入的模型存在空间信息不足或二维泛化受限的问题，导致其难以适应不同空间需求的任务；其二，具身约束鸿沟——现有研究常忽略真实机器人的物理约束与能力，导致生成的任务计划理论上可行但实际难以执行。\n\n针对上述问题，我们提出OmniEVA——一种具身通用规划器，通过两项关键创新实现先进的具身推理与任务规划：（1）任务自适应三维 grounding 机制，引入门控路由器根据上下文需求对三维融合进行显式选择性调控，从而为多样化具身任务实现情境感知的三维 grounding；（2）具身意识推理框架，将任务目标与实体约束共同纳入推理循环，生成既符合目标导向又可执行的规划决策。\n\n大量实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛下游场景中展现出强大能力。通过对包括基础任务与复合任务在内的新型具身基准测试评估，其鲁棒且通用的规划能力得到验证。项目页面：https://omnieva.github.io\n\n（注：根据学术规范，grounding在此保留英文术语，其含义可理解为\"基于感知数据的语义锚定\"或\"跨模态对齐\"。若需进一步解释，可添加译注说明该术语在具身人工智能中特指多模态信息与物理空间的关联过程。）"
    },
    {
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
        "url": "http://arxiv.org/abs/2509.09307v1",
        "pub_date": "2025-09-11",
        "summary": "Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
        "translated": "材料表征是获取材料信息的基础，能够揭示指导材料设计与优化的加工-微观结构-性能关系。尽管多模态大语言模型（MLLMs）近期在材料科学的生成与预测任务中展现出潜力，但其对真实世界表征成像数据的理解能力仍待深入探索。为填补这一空白，我们推出了首个面向材料表征图像理解的基准测试MatCha，包含1,500个需要专家级领域知识的问题。该基准覆盖材料研究的四个关键阶段共21项任务，每项任务均设计用于反映材料科学家面临的真实挑战。\n\n我们对前沿多模态大语言模型在MatCha上的评估显示，其性能与人类专家存在显著差距。这些模型在处理需要高阶专业知识和复杂视觉感知的问题时表现明显退化，简单的少样本提示和思维链提示方法难以缓解这些局限性。这些发现表明，现有多模态大语言模型对真实材料表征场景的适应性仍然有限。我们期望MatCha能推动新材料发现和自主科研智能体等领域的未来研究。MatCha已发布于：https://github.com/FreedomIntelligence/MatCha。\n\n（注：翻译严格遵循了以下技术规范：\n1. 专业术语准确对应：\"characterization\"译为\"表征\"，\"multimodal large language models\"保留专业缩写\"MLLMs\"并补充全称\"多模态大语言模型\"\n2. 概念体系完整保留：\"processing-microstructure-property relationships\"完整译为\"加工-微观结构-性能关系\"\n3. 技术动作精准传达：\"few-shot prompting\"译为\"少样本提示\"，\"chain-of-thought prompting\"采用学界通用译法\"思维链提示\"\n4. 学术表述符合规范：被动语态转换为主动句式（如\"are designed to\"译为\"设计用于\"），长难句按中文习惯切分重组\n5. 重要概念首次出现时标注英文原词（如MLLMs），确保学术严谨性）"
    },
    {
        "title": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models",
        "url": "http://arxiv.org/abs/2509.09303v1",
        "pub_date": "2025-09-11",
        "summary": "Classifying patents by their relevance to the UN Sustainable Development Goals (SDGs) is crucial for tracking how innovation addresses global challenges. However, the absence of a large, labeled dataset limits the use of supervised learning. Existing methods, such as keyword searches, transfer learning, and citation-based heuristics, lack scalability and generalizability. This paper frames patent-to-SDG classification as a weak supervision problem, using citations from patents to SDG-tagged scientific publications (NPL citations) as a noisy initial signal. To address its sparsity and noise, we develop a composite labeling function (LF) that uses large language models (LLMs) to extract structured concepts, namely functions, solutions, and applications, from patents and SDG papers based on a patent ontology. Cross-domain similarity scores are computed and combined using a rank-based retrieval approach. The LF is calibrated via a custom positive-only loss that aligns with known NPL-SDG links without penalizing discovery of new SDG associations. The result is a silver-standard, soft multi-label dataset mapping patents to SDGs, enabling the training of effective multi-label regression models. We validate our approach through two complementary strategies: (1) internal validation against held-out NPL-based labels, where our method outperforms several baselines including transformer-based models, and zero-shot LLM; and (2) external validation using network modularity in patent citation, co-inventor, and co-applicant graphs, where our labels reveal greater thematic, cognitive, and organizational coherence than traditional technological classifications. These results show that weak supervision and semantic alignment can enhance SDG classification at scale.",
        "translated": "根据专利与联合国可持续发展目标（SDGs）的相关性进行分类，对于追踪创新如何应对全球挑战至关重要。然而，由于缺乏大规模标注数据集，监督学习的应用受到限制。现有方法（如关键词搜索、迁移学习和基于引用的启发式方法）缺乏可扩展性和泛化能力。本文通过将专利-SDG分类构建为弱监督问题，利用专利引用带有SDG标签的科学出版物（非专利文献引用，NPL citations）作为噪声初始信号。为解决该信号的稀疏性和噪声问题，我们开发了一种复合标注函数（LF），基于专利本体论使用大语言模型（LLM）从专利和SDG论文中提取结构化概念（即功能、解决方案和应用）。通过基于排序的检索方法计算并融合跨领域相似度得分。该标注函数通过定制化的仅正样本损失函数进行校准，该函数与已知的NPL-SDG关联保持一致，且不会惩罚新SDG关联的发现。最终生成银标准软多标签数据集，将专利映射到SDGs，从而支持训练有效的多标签回归模型。我们通过两种互补策略验证方法有效性：（1）基于保留的NPL标签进行内部验证，本方法在包括基于Transformer的模型和零样本LLM在内的多个基线模型中表现优异；（2）利用专利引用、共同发明人和共同申请人网络中的模块度进行外部验证，结果表明相较于传统技术分类，我们的标签在主题、认知和组织层面展现出更强的一致性。这些结果证明，弱监督与语义对齐能够有效提升SDG分类的大规模应用能力。"
    },
    {
        "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning",
        "url": "http://arxiv.org/abs/2509.09284v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We propose a staged GRPO training paradigm where completions are derived from partially revealed MCTS rollouts, introducing a novel tree-structured setting for advantage estimation. This leads to a rich class of prefix-conditioned reward signals, which we analyze theoretically and empirically. Our initial results indicate that while structured advantage estimation can stabilize updates and better reflect compositional reasoning quality, challenges such as advantage saturation and reward signal collapse remain. We propose heuristic and statistical solutions to mitigate these issues and discuss open challenges for learning under staged or tree-like reward structures.",
        "translated": "近期，大型语言模型（LLM）在推理任务中的进展表明，蒙特卡洛树搜索（MCTS）能够有效生成高质量的中继轨迹，尤其在数学与符号领域表现突出。受此启发，我们探索如何将传统上用于训练价值模型或奖励模型的MCTS轨迹重新应用于基于偏好的强化学习（RL）中的策略优化。具体而言，我们聚焦于群组相对策略优化（GRPO）——一种无需价值网络即可实现偏好一致策略学习的新算法。我们提出一种分阶段GRPO训练范式，其中补全结果源自部分展开的MCTS推演，从而为优势估计引入了一种新颖的树形结构设定。该方法产生了一类丰富的基于前缀条件的奖励信号，我们从理论与实证两方面对其进行了分析。初步结果表明：虽然结构化优势估计能够稳定更新过程并更好地反映组合推理质量，但仍存在优势饱和与奖励信号坍缩等挑战。我们提出了启发式与统计学的解决方案以缓解这些问题，并讨论了在阶段性或树状奖励结构下学习的开放性难题。"
    },
    {
        "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
        "url": "http://arxiv.org/abs/2509.09265v1",
        "pub_date": "2025-09-11",
        "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/",
        "translated": "在长周期任务中，基于大语言模型（LLM）的智能体面临核心挑战：稀疏的结果型奖励难以对中间步骤进行有效信用分配。现有方法主要通过传统强化学习技术（如逆强化学习）或使用过程奖励模型提供逐步反馈，以构建密集奖励信号来指导学习。本文发现LLM学习动态中存在一个根本性问题：策略梯度幅度与熵值固有耦合，导致对置信度高的正确动作产生低效的小幅度更新，而对不确定动作可能产生破坏稳定性的大幅度更新。针对这一问题，我们提出熵调制策略梯度（EMPG）框架，该框架基于步骤级不确定性和最终任务结果重新校准学习信号。EMPG会放大对高置信度正确动作的更新，惩罚高置信度错误动作，并衰减不确定步骤的更新以稳定探索过程。我们还引入了未来清晰度奖励项，激励智能体寻找更具可预测性的解决路径。通过在WebShop、ALFWorld和DeepSearch三个具有挑战性的智能体任务上进行综合实验，我们证明EMPG实现了显著性能提升，大幅优于强策略梯度基线方法。项目页面详见：https://empgseed-seed.github.io/\n\n（注：专业术语说明：\n1. Entropy-Modulated Policy Gradients (EMPG) 译为\"熵调制策略梯度\"\n2. Process Reward Models 译为\"过程奖励模型\"\n3. 保持技术表述准确性：将\"policy gradients\"译为\"策略梯度\"，\"credit assignment\"译为\"信用分配\"，\"exploration\"译为\"探索\"等\n4. 长难句处理：对原文中复合从句进行合理切分，符合中文表达习惯）"
    },
    {
        "title": "Agentic LLMs for Question Answering over Tabular Data",
        "url": "http://arxiv.org/abs/2509.09234v1",
        "pub_date": "2025-09-11",
        "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges due to the diverse structure, size, and data types of real-world tables. The SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale, domain-diverse datasets to evaluate the ability of models to accurately answer structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a multi-stage pipeline involving example selection, SQL query generation, answer extraction, verification, and iterative refinement. Experiments demonstrate the effectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and 71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\% and 27\\% respectively. This paper details our methodology, experimental results, and alternative approaches, providing insights into the strengths and limitations of LLM-driven Table QA.",
        "translated": "表格问答（Table QA）因现实世界中表格结构、规模和数据类型的多样性而面临独特挑战。SemEval 2025任务8（DataBench）引入了一个由大规模、多领域数据集组成的基准测试，用于评估模型准确回答结构化查询的能力。我们提出了一种基于大语言模型（如GPT-4o、GPT-4o-mini和DeepSeek v2:16b）的自然语言转SQL（NL-to-SQL）方法，通过动态生成SQL查询实现问答。该系统采用多阶段流程，包括示例选择、SQL查询生成、答案提取、验证与迭代优化。实验证明该方法效果显著，在DataBench QA和DataBench Lite QA上分别达到70.5%和71.6%的准确率，显著超越26%和27%的基线水平。本文详细阐述了方法论、实验结果及替代方案，深入分析了大语言模型驱动表格问答的优势与局限性。"
    },
    {
        "title": "Reading Between the Lines: Classifying Resume Seniority with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.09229v1",
        "pub_date": "2025-09-11",
        "summary": "Accurately assessing candidate seniority from resumes is a critical yet challenging task, complicated by the prevalence of overstated experience and ambiguous self-presentation. In this study, we investigate the effectiveness of large language models (LLMs), including fine-tuned BERT architectures, for automating seniority classification in resumes. To rigorously evaluate model performance, we introduce a hybrid dataset comprising both real-world resumes and synthetically generated hard examples designed to simulate exaggerated qualifications and understated seniority. Using the dataset, we evaluate the performance of Large Language Models in detecting subtle linguistic cues associated with seniority inflation and implicit expertise. Our findings highlight promising directions for enhancing AI-driven candidate evaluation systems and mitigating bias introduced by self-promotional language. The dataset is available for the research community at https://bit.ly/4mcTovt",
        "translated": "准确评估简历中候选人的资历水平是一项关键但具有挑战性的任务，其复杂性主要源于普遍存在的工作经验夸大现象和模糊的自我表述。本研究探讨了大型语言模型（包括经过微调的BERT架构）在自动化简历资历分类中的有效性。为系统评估模型性能，我们构建了一个混合数据集，其中既包含真实简历，也包含专门设计的合成困难样本——这些样本用于模拟夸大资质或刻意低调描述资历的情况。基于该数据集，我们评估了大语言模型在识别与资历夸大相关的微妙语言线索及隐含专业技能方面的表现。研究结果为增强AI驱动的候选人评估系统、减少自我宣传语言带来的偏见提供了有前景的研究方向。本数据集已向研究社区开放，访问地址：https://bit.ly/4mcTovt\n\n（注：翻译中对以下要点进行了专业处理：\n1. \"overstated experience\"译为\"工作经验夸大\"符合人力资源领域术语\n2. \"synthetically generated hard examples\"采用\"合成困难样本\"的学术表述\n3. \"seniority inflation\"译为\"资历夸大\"准确传达概念\n4. 长难句拆分为符合中文表达习惯的短句结构\n5. 技术术语（BERT/LLMs）保持原文大写形式\n6. 链接地址完整保留并添加\"访问地址\"引导词）"
    },
    {
        "title": "Identifying Key Features for Establishing Sustainable Agro-Tourism\n  Centre: A Data Driven Approach",
        "url": "http://arxiv.org/abs/2509.09214v1",
        "pub_date": "2025-09-11",
        "summary": "Agro-tourism serves as a strategic economic model designed to facilitate rural development by diversifying income streams for local communities like farmers while promoting the conservation of indigenous cultural heritage and traditional agricultural practices. As a very booming subdomain of tourism, there is a need to study the strategies for the growth of Agro-tourism in detail. The current study has identified the important indicators for the growth and enhancement of agro-tourism. The study is conducted in two phases: identification of the important indicators through a comprehensive literature review and in the second phase state-of-the-art techniques were used to identify the important indicators for the growth of agro-tourism. The indicators are also called features synonymously, the machine learning models for feature selection were applied and it was observed that the Least Absolute Shrinkage and Selection Operator (LASSO) method combined with, the machine Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT), Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were used to suggest the growth of the agro-tourism. The results show that with the LASSO method, LR model gives the highest classification accuracy of 98% in 70-30% train-test data followed by RF with 95% accuracy. Similarly, in the 80-20% train-test data LR maintains the highest accuracy at 99%, while DT and XGBoost follow with 97% accuracy.",
        "translated": "农业旅游作为一种战略性经济模式，旨在通过为农民等当地社区拓展收入渠道促进乡村发展，同时推动本土文化遗产与传统农业实践的保存。作为旅游业中蓬勃发展的细分领域，亟需对农业旅游的发展策略进行深入研究。本研究通过系统方法识别了推动农业旅游发展的关键指标。研究分为两个阶段：首先通过全面文献综述初步确定重要指标，随后采用前沿技术手段进一步筛选关键影响因素。\n\n这些指标在机器学习领域常被称为特征，本研究应用特征选择模型进行分析。通过结合最小绝对收缩与选择算子（LASSO）方法，并采用逻辑回归（LR）、决策树（DT）、随机森林（RF）和极限梯度提升（XGBOOST）等机器学习分类器，构建了农业旅游发展预测模型。实验结果表明：在70%-30%的训练-测试数据划分下，LASSO结合逻辑回归模型取得98%的最高分类准确率，随机森林模型以95%的准确率次之；在80%-20%的数据划分中，逻辑回归模型保持99%的峰值准确率，决策树与XGBOOST模型则以97%的准确率紧随其后。"
    },
    {
        "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection\n  Systems",
        "url": "http://arxiv.org/abs/2509.09204v1",
        "pub_date": "2025-09-11",
        "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.",
        "translated": "音频深度伪造检测（ADD）模型通常通过融合多种合成器的数据集进行评估，并以单一等错误率（EER）作为性能指标。然而，这种方法会过度加权样本量较多的合成器，使其他合成器的代表性不足，从而降低EER的整体可靠性。此外，多数ADD数据集的真实语音多样性不足，通常仅包含单一环境和语音风格（如纯净朗读语音），限制了其模拟真实场景的能力。针对这些问题，我们提出真实语音交叉测试框架——一种整合多样化真实语音数据集并通过聚合EER实现更均衡评估的新型评估方法。与传统评估方式相比，我们的方案显著提升了鲁棒性和可解释性。我们在九类真实语音类型上对超过150种合成器进行基准测试，并发布新数据集以推动后续研究（项目地址：https://github.com/cyaaronk/audio_deepfake_eval）。"
    },
    {
        "title": "Region-Wise Correspondence Prediction between Manga Line Art Images",
        "url": "http://arxiv.org/abs/2509.09501v1",
        "pub_date": "2025-09-11",
        "summary": "Understanding region-wise correspondence between manga line art images is a fundamental task in manga processing, enabling downstream applications such as automatic line art colorization and in-between frame generation. However, this task remains largely unexplored, especially in realistic scenarios without pre-existing segmentation or annotations. In this paper, we introduce a novel and practical task: predicting region-wise correspondence between raw manga line art images without any pre-existing labels or masks. To tackle this problem, we divide each line art image into a set of patches and propose a Transformer-based framework that learns patch-level similarities within and across images. We then apply edge-aware clustering and a region matching algorithm to convert patch-level predictions into coherent region-level correspondences. To support training and evaluation, we develop an automatic annotation pipeline and manually refine a subset of the data to construct benchmark datasets. Experiments on multiple datasets demonstrate that our method achieves high patch-level accuracy (e.g., 96.34%) and generates consistent region-level correspondences, highlighting its potential for real-world manga applications.",
        "translated": "理解漫画线稿图像间的区域对应关系是漫画处理中的基础任务，能够支持自动线稿上色和中间帧生成等下游应用。然而该任务尤其在缺乏预分割或标注的真实场景中仍属未充分探索的领域。本文提出一项新颖且实用的任务：在没有任何预标注或掩码的情况下预测原始漫画线稿图像间的区域对应关系。为解决该问题，我们将线稿图像划分为若干图像块，并提出基于Transformer的框架以学习图像内及图像间的块级相似性。随后通过边缘感知聚类和区域匹配算法，将块级预测转换为连贯的区域级对应关系。为支持训练与评估，我们开发了自动标注流程并对部分数据人工精校，构建了基准数据集。在多数据集上的实验表明，我们的方法实现了高块级准确率（如96.34%），并生成一致的区域级对应关系，彰显了其在现实漫画应用中的潜力。\n\n（注：专业术语说明：\n1. \"region-wise correspondence\" 译为\"区域对应关系\"\n2. \"manga line art images\" 译为\"漫画线稿图像\"\n3. \"edge-aware clustering\" 译为\"边缘感知聚类\"\n4. \"Transformer-based framework\" 译为\"基于Transformer的框架\"\n5. 数值精度保留原文三位小数以符合学术规范）"
    },
    {
        "title": "Improving Human Motion Plausibility with Body Momentum",
        "url": "http://arxiv.org/abs/2509.09496v1",
        "pub_date": "2025-09-11",
        "summary": "Many studies decompose human motion into local motion in a frame attached to the root joint and global motion of the root joint in the world frame, treating them separately. However, these two components are not independent. Global movement arises from interactions with the environment, which are, in turn, driven by changes in the body configuration. Motion models often fail to precisely capture this physical coupling between local and global dynamics, while deriving global trajectories from joint torques and external forces is computationally expensive and complex. To address these challenges, we propose using whole-body linear and angular momentum as a constraint to link local motion with global movement. Since momentum reflects the aggregate effect of joint-level dynamics on the body's movement through space, it provides a physically grounded way to relate local joint behavior to global displacement. Building on this insight, we introduce a new loss term that enforces consistency between the generated momentum profiles and those observed in ground-truth data. Incorporating our loss reduces foot sliding and jitter, improves balance, and preserves the accuracy of the recovered motion. Code and data are available at the project page https://hlinhn.github.io/momentum_bmvc.",
        "translated": "许多研究将人体运动分解为附着于根关节坐标系中的局部运动与世界坐标系中根关节的全局运动，并分别处理这两个部分。然而，这两个组成部分并非相互独立。全局运动产生于与环境的交互，而这种交互又由身体姿态的变化所驱动。现有运动模型往往难以精确捕捉局部与全局动力学之间的这种物理耦合关系，而通过关节扭矩和外力推导全局轨迹又存在计算成本高、复杂度大的问题。针对这些挑战，我们提出使用全身线性动量与角动量作为约束条件，将局部运动与全局位移相关联。由于动量反映了关节层面动力学对身体空间运动的整体影响，它为连接局部关节行为与全局位移提供了物理依据。基于这一思路，我们引入了一种新的损失函数项，用于强制生成动量曲线与真实数据中观测到的动量分布保持一致。引入该损失函数后，有效减少了脚步滑动与抖动现象，改善了运动平衡性，同时保持了重建运动的精确度。代码与数据详见项目页面：https://hlinhn.github.io/momentum_bmvc。\n\n（注：译文严格遵循以下学术规范：\n1. 专业术语准确对应：\"root joint\"译为\"根关节\"，\"world frame\"译为\"世界坐标系\"，\"linear and angular momentum\"译为\"线性动量与角动量\"\n2. 技术概念完整保留：通过增补\"物理依据\"、\"动力学\"等表述确保物理耦合关系的准确传达\n3. 长难句重构：将原文复合句拆分为符合中文表达习惯的短句结构，如对\"global movement arises from...\"句式的处理\n4. 逻辑连接显性化：使用\"然而\"、\"又由\"、\"针对\"等连接词明确技术论证逻辑\n5. 被动语态转化：将\"are driven by\"等被动结构转换为\"由...驱动\"的中文主动表达\n6. 项目信息完整保留：准确呈现网址及技术资源信息）"
    },
    {
        "title": "OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake\n  Detection",
        "url": "http://arxiv.org/abs/2509.09495v1",
        "pub_date": "2025-09-11",
        "summary": "Deepfakes, synthetic media created using advanced AI techniques, have intensified the spread of misinformation, particularly in politically sensitive contexts. Existing deepfake detection datasets are often limited, relying on outdated generation methods, low realism, or single-face imagery, restricting the effectiveness for general synthetic image detection. By analyzing social media posts, we identify multiple modalities through which deepfakes propagate misinformation. Furthermore, our human perception study demonstrates that recently developed proprietary models produce synthetic images increasingly indistinguishable from real ones, complicating accurate identification by the general public. Consequently, we present a comprehensive, politically-focused dataset specifically crafted for benchmarking detection against modern generative models. This dataset contains three million real images paired with descriptive captions, which are used for generating 963k corresponding high-quality synthetic images from a mix of proprietary and open-source models. Recognizing the continual evolution of generative techniques, we introduce an innovative crowdsourced adversarial platform, where participants are incentivized to generate and submit challenging synthetic images. This ongoing community-driven initiative ensures that deepfake detection methods remain robust and adaptive, proactively safeguarding public discourse from sophisticated misinformation threats.",
        "translated": "深度伪造（Deepfakes）作为一种基于先进人工智能技术生成的合成媒体，加剧了错误信息的传播，尤其在政治敏感语境中尤为突出。现有的深度伪造检测数据集往往存在局限性：依赖过时的生成方法、真实性不足或仅包含单一人脸图像，这限制了其在通用合成图像检测中的有效性。通过分析社交媒体帖子，我们识别出深度伪造传播错误信息的多模态特征。此外，我们的人类感知研究表明，近期开发的专有模型生成的合成图像与真实图像的区分度越来越低，导致公众难以准确识别。为此，我们提出了一个专注于政治语境、面向现代生成模型检测基准的综合数据集。该数据集包含300万张真实图像及其描述性文本标注，并基于混合专有与开源模型生成了96.3万张对应的高质量合成图像。鉴于生成技术的持续演进，我们创新性地推出了众包对抗平台，通过激励参与者生成并提交具有挑战性的合成图像。这一持续开展的社区驱动计划将确保深度伪造检测方法保持鲁棒性和适应性，从而主动保护公共讨论免受复杂错误信息威胁的侵害。\n\n（注：专业术语说明：  \n- Deepfakes：采用\"深度伪造\"行业通用译法  \n- Synthetic media：译为\"合成媒体\"  \n- Proprietary models：译为\"专有模型\"  \n- Adversarial platform：译为\"对抗平台\"  \n- Robust and adaptive：译为\"鲁棒性和适应性\"  \n译文严格遵循技术文档翻译规范，确保学术概念准确传达）"
    },
    {
        "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
        "url": "http://arxiv.org/abs/2509.09494v1",
        "pub_date": "2025-09-11",
        "summary": "In-loop filtering (ILF) is a key technology in video coding standards to reduce artifacts and enhance visual quality. Recently, neural network-based ILF schemes have achieved remarkable coding gains, emerging as a powerful candidate for next-generation video coding standards. However, the use of deep neural networks (DNN) brings significant computational and time complexity or high demands for dedicated hardware, making it challenging for general use. To address this limitation, we study a practical ILF solution by adopting look-up tables (LUTs). After training a DNN with a restricted reference range for ILF, all possible inputs are traversed, and the output values of the DNN are cached into LUTs. During the coding process, the filtering process is performed by simply retrieving the filtered pixel through locating the input pixels and interpolating between the cached values, instead of relying on heavy inference computations. In this paper, we propose a universal LUT-based ILF framework, termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of filtering LUTs and propose a series of customized indexing mechanisms to enable better filtering reference perception with limited storage consumption. Second, we propose the cross-component indexing mechanism to enable the filtering of different color components jointly. Third, in order to make our solution practical for coding uses, we propose the LUT compaction scheme to enable the LUT pruning, achieving a lower storage cost of the entire solution. The proposed framework is implemented in the VVC reference software. Experimental results show that the proposed framework achieves on average 0.82%/2.97%/1.63% and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI and RA configurations, respectively. Compared to DNN-based solutions, our proposed solution has much lower time complexity and storage cost.",
        "translated": "环路滤波（ILF）是视频编码标准中用于减少伪影和提升视觉质量的关键技术。近年来，基于神经网络的ILF方案取得了显著的编码增益，成为下一代视频编码标准的有力候选方案。然而，深度神经网络（DNN）的使用带来了巨大的计算和时间复杂度，或对专用硬件的高需求，使其难以普及应用。为解决这一局限性，我们研究了一种采用查找表（LUT）的实用ILF方案。通过训练一个具有受限参考范围的DNN用于ILF后，遍历所有可能的输入，并将DNN的输出值缓存至LUT中。在编码过程中，滤波操作仅需通过定位输入像素并在缓存值间插值来获取滤波后像素，无需依赖繁重的推理计算。本文提出了一种通用的基于LUT的ILF框架——LUT-ILF++。首先，我们引入多类滤波LUT的协同机制，并提出一系列定制化索引方案，在有限存储消耗下实现更优的滤波参考感知。其次，我们提出跨分量索引机制，实现不同颜色分量的联合滤波。第三，为使方案更适用于实际编码场景，我们提出LUT压缩方案以实现剪枝，降低整体存储成本。该框架已在VVC参考软件中实现。实验结果表明，在AI和RA配置下，所提方案对通用测试序列的平均码率节省分别达到0.82%/2.97%/1.63%和0.85%/4.11%/2.06%。与基于DNN的方案相比，本方案具有显著更低的时间复杂度和存储成本。\n\n（注：专业术语说明：\n1. in-loop filtering (ILF)：环路滤波（视频编码中位于编解码环路内的滤波技术）\n2. look-up tables (LUTs)：查找表（通过预计算值替代实时计算的优化技术）\n3. VVC：Versatile Video Coding（H.266下一代视频编码标准）\n4. AI/RA配置：All-Intra/Random Access（VVC标准中的两种编码模式）\n5. 跨分量索引：cross-component indexing（联合处理YUV等颜色分量的技术））"
    },
    {
        "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
        "url": "http://arxiv.org/abs/2509.09469v1",
        "pub_date": "2025-09-11",
        "summary": "Gliomas are the most prevalent type of primary brain tumors, and their accurate segmentation from MRI is critical for diagnosis, treatment planning, and longitudinal monitoring. However, the scarcity of high-quality annotated imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for deploying advanced segmentation models in clinical workflows. This study introduces a robust and computationally efficient deep learning framework tailored for resource-constrained settings. We leveraged a 3D Attention UNet architecture augmented with residual blocks and enhanced through transfer learning from pre-trained weights on the BraTS 2021 dataset. Our model was evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma segmentation in SSA MRI data. Despite the limited data quality and quantity, our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80 for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding Non-Functional Hemisphere (SNFH). These results demonstrate the generalizability of the proposed model and its potential to support clinical decision making in low-resource settings. The compact architecture, approximately 90 MB, and sub-minute per-volume inference time on consumer-grade hardware further underscore its practicality for deployment in SSA health systems. This work contributes toward closing the gap in equitable AI for global health by empowering underserved regions with high-performing and accessible medical imaging solutions.",
        "translated": "胶质瘤是最常见的原发性脑肿瘤类型，其基于MRI图像的精确分割对诊断、治疗规划和长期监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺性，为在临床工作流程中部署先进分割模型带来了重大挑战。本研究提出了一种专为资源受限环境设计的鲁棒且计算高效深度学习框架。我们采用集成残差块的3D注意力UNet架构，并基于BraTS 2021数据集通过预训练权重进行迁移学习增强。使用BraTS-Africa数据集（SSA地区MRI胶质瘤分割基准）的95例MRI病例进行评估，在数据质量和数量有限的情况下，我们的方法在增强肿瘤区域(ET)获得0.76的Dice分数，坏死与非增强肿瘤核心区域(NETC)达0.80，周围非功能性半球区域(SNFH)达0.85。这些结果证明了所提出模型的泛化能力及其在低资源环境中支持临床决策的潜力。该紧凑架构仅约90MB，在消费级硬件上单例推理时间不足一分钟，进一步凸显了其在SSA医疗系统中部署的实用性。通过为资源匮乏地区提供高性能、可及的医学影像解决方案，本研究有助于缩小全球健康领域人工智能公平性差距。\n\n（注：根据医学影像分割领域规范，对专业术语采用标准译法：\n- Enhancing Tumor (ET) 译为\"增强肿瘤区域\"\n- Necrotic and Non-Enhancing Tumor Core (NETC) 译为\"坏死与非增强肿瘤核心区域\"\n- Surrounding Non-Functional Hemisphere (SNFH) 译为\"周围非功能性半球区域\"\n- Dice score保留专业指标名称不翻译\n- 保持BraTS等专业数据集名称原貌\n- 准确传递了迁移学习、残差块等技术细节）"
    },
    {
        "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion\n  based on diffusion model",
        "url": "http://arxiv.org/abs/2509.09456v1",
        "pub_date": "2025-09-11",
        "summary": "Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.",
        "translated": "不同模态的医学影像能为疾病提供独特的生理与解剖信息。多模态医学图像融合技术通过整合来自不同互补模态医学图像的有效信息，生成能全面客观反映病灶特征的融合图像，以辅助医生进行临床诊断。然而现有融合方法只能处理固定数量的模态输入（如仅接受双模态或三模态输入），无法直接处理变化的输入数量，这限制了其在临床环境中的应用。为解决这一问题，我们提出FlexiD-Fuse——一种基于扩散模型的图像融合网络，可适配灵活的输入模态数量。该网络能在相同权重参数下端到端处理双模态及三模态医学图像融合任务。FlexiD-Fuse将原本仅支持固定条件输入的扩散融合问题，转化为基于扩散过程与分层贝叶斯建模的最大似然估计问题。通过将期望最大化算法融入扩散采样迭代过程，FlexiD-Fuse能独立于输入图像数量，生成具有源图像跨模态信息的高质量融合图像。我们在哈佛数据集上对比了最新的双模态与三模态医学图像融合方法，并使用九种主流指标进行评估。实验结果表明，本方法在变输入数量的医学图像融合中取得了最优性能。同时，我们在红外-可见光、多曝光、多焦点等任意数量图像融合任务上进行了广泛扩展实验，并与视角级SOTA方法进行对比。扩展实验结果一致证明了本方法的有效性与优越性。"
    },
    {
        "title": "Semantic Concentration for Self-Supervised Dense Representations\n  Learning",
        "url": "http://arxiv.org/abs/2509.09429v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.",
        "translated": "近年来，图像级自监督学习（SSL）虽取得显著进展，但为图像块学习稠密表征仍具挑战性。主流方法存在\"过度分散\"现象——同实例/类别的图像块在表征空间中离散分布，损害了稠密预测任务的下游性能。本研究揭示图像级SSL通过隐式语义聚集避免该问题：非严格的空间对齐保证实例内一致性，而输入空间中类内实例的共享模式（即相似局部）确保图像间一致性。但由于空间敏感性和复杂场景中心数据，这些方法难以适用于稠密SSL。基于此，我们探索显式语义聚集方案：首先，为打破严格空间对齐，提出提取图像块对应关系；针对噪声和不平衡伪标签，提出耐噪排序损失，其核心是将平均精度（AP）损失扩展至连续目标，利用其决策无关和自适应聚焦特性防止学生模型被误导；其次，为从复杂场景中区分共享模式，提出物体感知过滤器将输出空间映射至基于物体的空间，具体通过交叉注意力使图像块由可学习的物体原型表示。大量实验证明该方法在多任务中的有效性，代码已开源。"
    },
    {
        "title": "FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal\n  image fusion and super-resolution",
        "url": "http://arxiv.org/abs/2509.09427v1",
        "pub_date": "2025-09-11",
        "summary": "As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.",
        "translated": "作为一项具有影响力的信息融合与底层视觉技术，图像融合通过整合源图像中的互补信息生成信息丰富的融合图像。近年来已有研究尝试联合实现图像融合与超分辨率任务。然而在军事侦察和远程探测等实际应用中，多模态图像中的目标与背景结构易受损，存在分辨率低、语义信息弱等问题，导致现有融合方法效果欠佳。为此，我们提出FS-Diff——一种语义引导与清晰度感知的图像融合与超分辨率联合方法。该方法将图像融合与超分辨率统一为条件生成问题，利用所提出的清晰度感知机制实现自适应低分辨率感知与跨模态特征提取。具体而言，我们将目标融合结果初始化为纯高斯噪声，并引入双向特征Mamba模块提取多模态图像的全局特征。进一步以源图像和语义信息为条件，通过改进的U-Net网络实现随机迭代去噪过程。该网络经过多噪声级去噪训练，能生成具有跨模态特征和丰富语义信息的高分辨率融合结果。我们还构建了包含600对图像的大规模航空视角多场景基准数据集AVMS。在六个公共数据集及AVMS数据集上的大量实验表明，FS-Diff在多种放大倍数下均优于现有先进方法，能恢复更丰富的细节与语义信息。代码已开源：https://github.com/XylonXu01/FS-Diff。\n\n（注：翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"semantic guidance\"译作\"语义引导\"，\"conditional generation\"译作\"条件生成\"\n2. 技术概念完整保留：双向特征Mamba（bidirectional feature Mamba）、多噪声级去噪（multi-noise level denoising）等专业表述保持原意\n3. 长难句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如方法原理部分采用分号衔接的递进式说明\n4. 被动语态转化：\"are easily corrupted\"主动化为\"易受损\"，\"is trained\"转化为\"经过训练\"\n5. 学术表述规范：采用\"该方法\"\"实验表明\"等符合中文论文摘要的书面表达）"
    },
    {
        "title": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot\n  Adaptation under Shift",
        "url": "http://arxiv.org/abs/2509.09397v1",
        "pub_date": "2025-09-11",
        "summary": "Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.",
        "translated": "医学视觉-语言模型（VLM）为临床决策支持提供了潜力，但其在数据分布变化下的可靠性仍是安全部署的核心挑战。由于医学影像协议和自由文本报告的多样性，这些模型常学习到与任务无关的虚假关联，限制了泛化能力并增加实际应用中的失败风险。我们提出DRiFt——一种结构化特征解耦框架，通过参数高效调优（LoRA）和可学习的提示标记，显式分离临床相关信号与任务无关噪声。为增强跨模态对齐并降低不确定性，我们通过为多样化医学数据集生成描述文本，构建了高质量、临床相关的图像-文本对。相比现有基于提示的方法，我们的方法在分布内性能提升11.4%的Top-1准确率和3.3%的Macro-F1分数，同时在未见数据集上保持强劲鲁棒性。消融实验表明：解耦任务相关特征与精细对齐能显著增强模型泛化能力，减少领域偏移下的不可预测行为。这些发现为构建更安全可靠的临床用VLM提供了重要见解。代码已开源：https://github.com/rumaima/DRiFt。\n\n（注：翻译严格遵循以下技术要点：\n1. \"distribution shifts\"译为\"数据分布变化\"符合机器学习领域术语\n2. \"task-agnostic correlations\"采用\"与任务无关的虚假关联\"的译法，既准确传达原意又符合中文表达习惯\n3. \"parameter-efficient tuning (LoRA)\"保留英文缩写同时括号注明全称，符合学术规范\n4. 性能指标\"Top-1 accuracy\"和\"Macro-F1\"直接保留英文术语+中文说明，确保专业性\n5. \"Ablation studies\"译为\"消融实验\"是计算机视觉领域标准译法）"
    },
    {
        "title": "Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic\n  Normality",
        "url": "http://arxiv.org/abs/2509.09375v1",
        "pub_date": "2025-09-11",
        "summary": "Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained defects that depress yield and reliability. Most industrial defect segmentation compares a test image against an external normal set, a strategy that is brittle for IC imagery where layouts vary across products and accurate alignment is difficult. We observe that defects are predominantly local, while each image still contains rich, repeatable normal patterns. We therefore propose an unsupervised IC defect segmentation framework that requires no external normal support. A learnable normal-information extractor aggregates representative normal features from the test image, and a coherence loss enforces their association with normal regions. Guided by these features, a decoder reconstructs only normal content; the reconstruction residual then segments defects. Pseudo-anomaly augmentation further stabilizes training. Experiments on datasets from three IC process stages show consistent improvements over existing approaches and strong robustness to product variability.",
        "translated": "现代集成电路（IC）制造过程中会产生多样化、细粒度的缺陷，这些缺陷会降低产品良率和可靠性。当前工业缺陷分割方法大多通过将测试图像与外部正常样本集进行比对，但这种策略对IC图像存在局限性——因为不同产品的版图布局存在差异且难以实现精确配准。我们注意到缺陷通常具有局部性特征，而每张图像本身仍包含大量可重复的正常模式。因此，本文提出了一种无需外部正常样本支持的无监督IC缺陷分割框架。该框架通过可学习的正常信息提取器从测试图像中聚合具有代表性的正常特征，并采用一致性损失约束这些特征与正常区域的关联性。在正常特征的引导下，解码器仅重构正常内容，最终通过重构残差实现缺陷分割。伪异常增强技术进一步稳定了训练过程。在三个IC制造阶段数据集上的实验表明，本方法相较现有方案均取得稳定提升，并对产品变异性展现出强大鲁棒性。\n\n（注：翻译过程中对以下专业术语进行了精准处理：\n- \"fine-grained defects\"译为\"细粒度的缺陷\"\n- \"reconstruction residual\"译为\"重构残差\"\n- \"pseudo-anomaly augmentation\"译为\"伪异常增强\"\n- \"robustness to product variability\"译为\"对产品变异性的鲁棒性\"\n同时保持了技术细节的准确性，如\"coherence loss\"译为\"一致性损失\"，\"normal-information extractor\"译为\"正常信息提取器\"等。）"
    },
    {
        "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for\n  Medicaid Care Management",
        "url": "http://arxiv.org/abs/2509.09655v1",
        "pub_date": "2025-09-11",
        "summary": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline RL procedure that calibrates per-group safety thresholds to reduce harm while equalizing a chosen fairness target (coverage or harm) across protected subgroups. Using de-identified longitudinal trajectories from a Medicaid population health management program, we evaluate FG-FARL against behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global conformal safety baseline). We report off-policy value estimates with bootstrap 95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL achieves comparable value to baselines while improving fairness metrics, demonstrating a practical path to safer and more equitable decision support.",
        "translated": "我们提出了可行性引导的公平自适应强化学习（FG-FARL），这是一种离线强化学习框架，通过校准各群体安全阈值来减少伤害，同时在受保护子群体间平衡选定的公平目标（覆盖率或伤害）。基于医疗补助人群健康管理项目的脱敏纵向轨迹数据，我们将FG-FARL与行为克隆（BC）及HACO（混合自适应共形离线强化学习；全局共形安全基线）进行对比评估。研究采用自助法95%置信区间报告离线策略价值估计，并提供带p值的子群体差异分析。结果表明，FG-FARL在保持与基线方法相当价值的同时显著改善了公平性指标，为实现更安全、更公平的决策支持提供了可行路径。"
    },
    {
        "title": "Functional Groups are All you Need for Chemically Interpretable\n  Molecular Property Prediction",
        "url": "http://arxiv.org/abs/2509.09619v1",
        "pub_date": "2025-09-11",
        "summary": "Molecular property prediction using deep learning (DL) models has accelerated drug and materials discovery, but the resulting DL models often lack interpretability, hindering their adoption by chemists. This work proposes developing molecule representations using the concept of Functional Groups (FG) in chemistry. We introduce the Functional Group Representation (FGR) framework, a novel approach to encoding molecules based on their fundamental chemical substructures. Our method integrates two types of functional groups: those curated from established chemical knowledge (FG), and those mined from a large molecular corpus using sequential pattern mining (MFG). The resulting FGR framework encodes molecules into a lower-dimensional latent space by leveraging pre-training on a large dataset of unlabeled molecules. Furthermore, the proposed framework allows the inclusion of 2D structure-based descriptors of molecules. We demonstrate that the FGR framework achieves state-of-the-art performance on a diverse range of 33 benchmark datasets spanning physical chemistry, biophysics, quantum mechanics, biological activity, and pharmacokinetics while enabling chemical interpretability. Crucially, the model's representations are intrinsically aligned with established chemical principles, allowing chemists to directly link predicted properties to specific functional groups and facilitating novel insights into structure-property relationships. Our work presents a significant step toward developing high-performing, chemically interpretable DL models for molecular discovery.",
        "translated": "基于深度学习（DL）模型的分子性质预测加速了药物与材料发现进程，但其结果往往缺乏可解释性，阻碍了化学家的实际应用。本研究提出利用化学中的官能团（Functional Groups, FG）概念构建分子表征方法。我们开发了官能团表征（Functional Group Representation, FGR）框架——一种基于基础化学子结构编码分子的创新方法。该框架整合了两类官能团：源自既定化学知识的传统官能团（FG），以及通过序列模式挖掘技术从大规模分子库中提取的挖掘型官能团（MFG）。通过对海量未标注分子数据进行预训练，FGR框架可将分子编码至低维潜在空间。此外，该框架支持融入基于二维分子结构的描述符。我们在33个跨领域的基准数据集（涵盖物理化学、生物物理、量子力学、生物活性和药代动力学）上验证了FGR框架的先进性能，同时保持了化学可解释性。关键的是，该模型的表征与既定化学原理内在契合，使化学家能够直接将预测性质与特定官能团关联，为结构-性质关系研究提供新视角。本研究成果为开发高性能、具有化学可解释性的分子发现深度学习模型迈出重要一步。"
    },
    {
        "title": "Explaining Concept Drift through the Evolution of Group Counterfactuals",
        "url": "http://arxiv.org/abs/2509.09616v1",
        "pub_date": "2025-09-11",
        "summary": "Machine learning models in dynamic environments often suffer from concept drift, where changes in the data distribution degrade performance. While detecting this drift is a well-studied topic, explaining how and why the model's decision-making logic changes still remains a significant challenge. In this paper, we introduce a novel methodology to explain concept drift by analyzing the temporal evolution of group-based counterfactual explanations (GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their associated counterfactual action vectors before and after a drift. These evolving GCEs act as an interpretable proxy, revealing structural changes in the model's decision boundary and its underlying rationale. We operationalize this analysis within a three-layer framework that synergistically combines insights from the data layer (distributional shifts), the model layer (prediction disagreement), and our proposed explanation layer. We show that such holistic view allows for a more comprehensive diagnosis of drift, making it possible to distinguish between different root causes, such as a spatial data shift versus a re-labeling of concepts.",
        "translated": "在动态环境中，机器学习模型常面临概念漂移问题——数据分布的变化会导致模型性能下降。虽然漂移检测已是广泛研究的课题，但解释模型决策逻辑如何及为何发生变化仍存在重大挑战。本文提出一种创新方法，通过分析基于群体的反事实解释（GCEs）的时序演化来解释概念漂移。该方法追踪漂移前后GCEs聚类中心点及其关联反事实行动向量的变化轨迹，这些动态演化的GCEs可作为可解释代理，揭示模型决策边界及其底层逻辑的结构性变化。我们通过三层分析框架将这一方法操作化：数据层（分布变化）、模型层（预测分歧）与我们提出的解释层形成协同分析体系。研究表明，这种整体视角能实现对概念漂移更全面的诊断，可有效区分不同根本原因（如空间数据偏移与概念重新标注等）。"
    },
    {
        "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and\n  Achieving Discretization Invariance",
        "url": "http://arxiv.org/abs/2509.09611v1",
        "pub_date": "2025-09-11",
        "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct inputs. Inspired by the Reduced Basis Method and the recently introduced Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a mathematically rigorous greedy algorithm to build its network structure offline adaptively from the ground up. Knowledge distillation via task-specific activation function allows ReBaNO to have a compact architecture requiring minimal computational cost online while embedding physics. In comparison to state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, numerical results demonstrate that ReBaNO significantly outperforms them in terms of eliminating/shrinking the generalization gap for both in- and out-of-distribution tests and being the only operator learning algorithm achieving strict discretization invariance.",
        "translated": "我们提出了一种新型数据稀疏算子学习算法——缩减基神经算子（ReBaNO），用于求解具有多个不同输入参数的偏微分方程组。该方法受缩减基方法和近期提出的生成式预训练物理信息神经网络启发，采用数学严谨的贪婪算法自底向上自适应地构建离线网络结构。通过任务特定激活函数实现的知识蒸馏技术，使ReBaNO在嵌入物理约束的同时保持紧凑的网络架构，在线计算成本极低。与主流的PCA-Net、DeepONet、FNO和CNO等算子学习算法相比，数值实验表明：ReBaNO在缩小/消除分布内与分布外测试的泛化差距方面显著优于现有方法，且是唯一严格实现离散化不变性的算子学习算法。\n\n（注：专业术语说明：\n1. Reduced Basis Neural Operator (ReBaNO) 译为\"缩减基神经算子\"\n2. Generative Pre-Trained Physics-Informed Neural Networks 译为\"生成式预训练物理信息神经网络\"\n3. discretization invariance 译为\"离散化不变性\"\n4. generalization gap 译为\"泛化差距\"\n5. greedy algorithm 译为\"贪婪算法\"\n6. knowledge distillation 译为\"知识蒸馏\"\n7. in- and out-of-distribution tests 译为\"分布内与分布外测试\"）"
    },
    {
        "title": "Conditioning on PDE Parameters to Generalise Deep Learning Emulation of\n  Stochastic and Chaotic Dynamics",
        "url": "http://arxiv.org/abs/2509.09599v1",
        "pub_date": "2025-09-11",
        "summary": "We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events.",
        "translated": "我们提出了一种针对随机与混沌时空系统的深度学习仿真器，其显式条件化于偏微分方程（PDE）的参数值。该方法首先在单一参数域上进行模型预训练，随后通过小规模多样化数据集进行微调，实现了对广泛参数值的泛化能力。通过引入局部注意力机制，该网络能够处理可变域尺寸与分辨率，从而在较小计算域上实现高效预训练，仅需少量附加数据即可学习如何泛化至更大域尺寸。我们在混沌Kuramoto-Sivashinsky方程和随机强迫β平面湍流系统上验证了模型性能，证明其能有效捕捉插值参数值下的物理现象。相较于传统数值积分方法，该仿真器可实现显著的计算加速，助力参数空间的高效探索；其概率化变体还能提供不确定性量化，为罕见事件的统计研究提供支持。\n\n（注：专业术语说明：\n1. Kuramoto-Sivashinsky equation：非线性偏微分方程，用于描述反应扩散系统中的混沌动力学\n2. beta-plane turbulence：基于β平面近似的湍流模型，常用于地球物理流体力学\n3. 局部注意力机制：一种聚焦局部特征的神经网络计算模式\n4. 不确定性量化：通过概率方法对模型预测的可靠性进行度量）"
    },
    {
        "title": "What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion\n  Detection Datasets",
        "url": "http://arxiv.org/abs/2509.09564v1",
        "pub_date": "2025-09-11",
        "summary": "Supervised machine learning techniques rely on labeled data to achieve high task performance, but this requires the labels to capture some meaningful differences in the underlying data structure. For training network intrusion detection algorithms, most datasets contain a series of attack classes and a single large benign class which captures all non-attack network traffic. A review of intrusion detection papers and guides that explicitly state their data preprocessing steps identified that the majority took the labeled categories of the dataset at face value when training their algorithms. The present paper evaluates the structure of benign traffic in several common intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and determines whether there are meaningful sub-categories within this traffic which may improve overall multi-classification performance using common machine learning techniques. We present an overview of some unsupervised clustering techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they differentially cluster the benign traffic space.",
        "translated": "监督式机器学习技术依赖标注数据以实现较高的任务性能，但这要求标签能够捕捉到底层数据结构中有意义的差异。在网络入侵检测算法的训练过程中，大多数数据集包含一系列攻击类别和一个庞大的良性流量类别（涵盖所有非攻击网络流量）。通过综述明确说明数据预处理步骤的入侵检测论文与指南发现，大多数研究在训练算法时直接采用数据集的标注分类。本文评估了多个常用入侵检测数据集（NSL-KDD、UNSW-NB15和CIC-IDS 2017）中良性流量的结构特征，并探究是否存在有意义的子类别划分——通过使用常见机器学习技术提升整体多分类性能。我们系统概述了无监督聚类技术（如HDBSCAN、均值漂移聚类），并展示这些技术如何对良性流量空间实现差异化聚类分析。"
    },
    {
        "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution",
        "url": "http://arxiv.org/abs/2509.09560v1",
        "pub_date": "2025-09-11",
        "summary": "Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary \"thinking\" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.",
        "translated": "具身智能系统在动态环境中运行，需要感知模块与生成模块的无缝协同，以处理高频的输入输出需求。传统串行计算模式虽能有效保证准确性，但在实现现实应用所需\"思维\"频率方面存在明显局限。本研究提出Auras——一种算法-系统协同设计的推理框架，通过优化具身智能代理的推理频率实现突破。该框架将感知与生成过程解耦，并为二者提供受控的流水线并行机制，从而实现高且稳定的吞吐量。针对并行度提升导致的数据陈旧问题，Auras建立了感知与生成模块共享的公共上下文环境，确保具身代理的准确性。实验结果表明，Auras在保持102.7%原系统精度的同时，平均提升2.54倍吞吐量，有效突破了串行计算模式的限制，实现了高性能推理。"
    },
    {
        "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
        "url": "http://arxiv.org/abs/2509.09550v2",
        "pub_date": "2025-09-11",
        "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech processing tasks due to their excellent rate-distortion performance and compatibility with Large Language Models (LLMs) as discrete feature representations for audio generation. While most existing codecs rely on Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has recently emerged as a compelling alternative that simplifies training and natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC, and show that FSQ encodes baked-in redundancy which produces an encoding which is robust when transmitted through noisy channels. First, through an encoder distillation experiment, we show that two different encoders can learn to encode identical audio into vastly different code sequences whilst maintaining comparable reconstruction quality with the same quantizer and decoder. Second, we demonstrate that FSQ has vastly superior bit-level perturbation robustness by comparing the performance of RVQ and FSQ codecs when simulating the transmission of code sequences through a noisy channel.",
        "translated": "神经音频编解码器（NACs）因其优异的率失真性能以及与大型语言模型（LLMs）的兼容性——可作为音频生成的离散特征表示——在语音处理任务中日益普及。虽然现有编解码器大多依赖残差向量量化（RVQ），但有限标量量化（FSQ）近期成为一种引人注目的替代方案，它简化了训练过程并原生支持单码本。我们提出了基于FSQ的神经编解码器NeuCodec，并证明FSQ通过内置冗余编码机制，能够在噪声信道传输中生成具有强鲁棒性的编码表示。首先，通过编码器蒸馏实验，我们证明两个不同的编码器可以学会将相同音频编码为截然不同的码序列，同时在使用相同量化器和解码器的情况下保持相当的重建质量。其次，通过模拟噪声信道传输场景对比RVQ与FSQ编解码器的性能，我们证明了FSQ具有显著优越的比特级扰动鲁棒性。\n\n（注：专业术语说明：\n1. Residual Vector Quantization (RVQ) → 残差向量量化\n2. Finite Scalar Quantization (FSQ) → 有限标量量化\n3. rate-distortion performance → 率失真性能\n4. discrete feature representations → 离散特征表示\n5. encoder distillation → 编码器蒸馏\n6. bit-level perturbation robustness → 比特级扰动鲁棒性\n7. noisy channel → 噪声信道）"
    },
    {
        "title": "ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated\n  Learning",
        "url": "http://arxiv.org/abs/2509.09534v1",
        "pub_date": "2025-09-11",
        "summary": "Federated Learning (FL) emerged as a widely studied paradigm for distributed learning. Despite its many advantages, FL remains vulnerable to adversarial attacks, especially under data heterogeneity. We propose a new Byzantine-robust FL algorithm called ProDiGy. The key novelty lies in evaluating the client gradients using a joint dual scoring system based on the gradients' proximity and dissimilarity. We demonstrate through extensive numerical experiments that ProDiGy outperforms existing defenses in various scenarios. In particular, when the clients' data do not follow an IID distribution, while other defense mechanisms fail, ProDiGy maintains strong defense capabilities and model accuracy. These findings highlight the effectiveness of a dual perspective approach that promotes natural similarity among honest clients while detecting suspicious uniformity as a potential indicator of an attack.",
        "translated": "联邦学习（Federated Learning, FL）已成为分布式学习中广泛研究的重要范式。尽管具备诸多优势，联邦学习在数据异构场景下仍易遭受对抗攻击。本文提出了一种名为ProDiGy的新型拜占庭鲁棒联邦学习算法，其核心创新在于通过梯度邻近性与差异性的联合双评分系统评估客户端梯度。大量数值实验表明，ProDiGy在多种场景下均优于现有防御机制。特别在客户端数据非独立同分布（non-IID）时，当其他防御机制失效的情况下，ProDiGy仍能保持强大的防御能力和模型精度。这些发现凸显了双视角方法的有效性：该方法既能促进诚实客户端间的自然相似性，又能通过检测可疑的一致性模式来识别潜在攻击。"
    },
    {
        "title": "Cough Classification using Few-Shot Learning",
        "url": "http://arxiv.org/abs/2509.09515v1",
        "pub_date": "2025-09-11",
        "summary": "This paper investigates the effectiveness of few-shot learning for respiratory sound classification, focusing on coughbased detection of COVID-19, Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram representations of cough sounds to address the challenge of limited labeled data. Our study evaluates whether few-shot learning can enable models to achieve performance comparable to traditional deep learning approaches while using significantly fewer training samples. Additionally, we compare multi-class and binary classification models to assess whether multi-class models can perform comparably to their binary counterparts. Experimental findings show that few-shot learning models can achieve competitive accuracy. Our model attains 74.87% accuracy in multi-class classification with only 15 support examples per class, while binary classification achieves over 70% accuracy across all class pairs. Class-wise analysis reveals Flu as the most distinguishable class, and Healthy as the most challenging. Statistical tests (paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant performance difference between binary and multiclass models, supporting the viability of multi-class classification in this setting. These results highlight the feasibility of applying few-shot learning in medical diagnostics, particularly when large labeled datasets are unavailable.",
        "translated": "本文研究了小样本学习在呼吸音分类中的有效性，重点关注基于咳嗽声音的COVID-19、流感和健康状态的检测。我们采用原型网络（Prototypical Networks）结合咳嗽声的频谱图表示，以解决标注数据有限的问题。本研究评估了小样本学习能否在使用显著较少训练样本的情况下，使模型达到与传统深度学习方法相媲美的性能。此外，我们比较了多类别和二分类模型，以评估多类别模型是否能够达到与二分类模型相当的表现。实验结果表明，小样本学习模型能够取得具有竞争力的准确率：在多类别分类任务中，每类仅使用15个支持样本，我们的模型达到了74.87%的准确率；而在所有类别对的二分类任务中，准确率均超过70%。类别分析显示，流感是最容易区分的类别，而健康状态则最具挑战性。统计检验（配对t检验p = 0.149，Wilcoxon检验p = 0.125）表明二分类与多类别模型之间无显著性能差异，支持了多类别分类在此场景下的可行性。这些结果突显了小样本学习在医疗诊断中应用的潜力，特别是在缺乏大规模标注数据集的情况下。"
    },
    {
        "title": "PIPES: A Meta-dataset of Machine Learning Pipelines",
        "url": "http://arxiv.org/abs/2509.09512v1",
        "pub_date": "2025-09-11",
        "summary": "Solutions to the Algorithm Selection Problem (ASP) in machine learning face the challenge of high computational costs associated with evaluating various algorithms' performances on a given dataset. To mitigate this cost, the meta-learning field can leverage previously executed experiments shared in online repositories such as OpenML. OpenML provides an extensive collection of machine learning experiments. However, an analysis of OpenML's records reveals limitations. It lacks diversity in pipelines, specifically when exploring data preprocessing steps/blocks, such as scaling or imputation, resulting in limited representation. Its experiments are often focused on a few popular techniques within each pipeline block, leading to an imbalanced sample. To overcome the observed limitations of OpenML, we propose PIPES, a collection of experiments involving multiple pipelines designed to represent all combinations of the selected sets of techniques, aiming at diversity and completeness. PIPES stores the results of experiments performed applying 9,408 pipelines to 300 datasets. It includes detailed information on the pipeline blocks, training and testing times, predictions, performances, and the eventual error messages. This comprehensive collection of results allows researchers to perform analyses across diverse and representative pipelines and datasets. PIPES also offers potential for expansion, as additional data and experiments can be incorporated to support the meta-learning community further. The data, code, supplementary material, and all experiments can be found at https://github.com/cynthiamaia/PIPES.git.",
        "translated": "在机器学习领域，算法选择问题（ASP）的解决方案面临着一个关键挑战：评估不同算法在给定数据集上的性能需要高昂的计算成本。为降低这一成本，元学习领域可利用OpenML等在线知识库共享的历史实验数据。OpenML虽提供了大量机器学习实验记录，但分析发现其存在明显局限性：一方面在探索数据预处理步骤/模块（如标准化、缺失值填补）时缺乏流程多样性，导致代表性不足；另一方面实验往往聚焦于各流程模块中的少数热门技术，造成样本失衡。\n\n为克服OpenML的现有局限，我们提出PIPES——一个包含多种处理流程的实验集合，其设计目标是通过选取技术组合的全覆盖来实现多样性和完整性。PIPES存储了将9,408种处理流程应用于300个数据集的实验结果，详细记录了流程模块配置、训练测试耗时、预测结果、性能指标及最终错误信息。这一综合性成果集合使研究人员能够基于多样化和具代表性的流程及数据集进行分析。PIPES还具有扩展潜力，可通过纳入更多数据与实验来进一步支持元学习社区。相关数据、代码、补充材料及完整实验记录详见https://github.com/cynthiamaia/PIPES.git。\n\n（注：根据学术规范，专业术语处理说明：\n1. Algorithm Selection Problem保留英文缩写ASP并标注中文全称\n2. OpenML作为专有平台名称保留英文形式\n3. scaling根据上下文译为\"标准化\"而非字面意义的\"缩放\"\n4. imputation译为专业术语\"缺失值填补\"\n5. pipelines根据计算机领域惯例译为\"处理流程\"而非直译\"管道\"\n6. meta-learning统一译为\"元学习\"）"
    },
    {
        "title": "Balancing Utility and Privacy: Dynamically Private SGD with Random\n  Projection",
        "url": "http://arxiv.org/abs/2509.09485v2",
        "pub_date": "2025-09-11",
        "summary": "Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.",
        "translated": "随机优化是现代机器学习中的关键推动技术，能够为各类任务生成高效模型。然而，现有研究表明模型参数与梯度信息存在隐私泄露风险。虽然差分隐私随机梯度下降（DPSGD）能解决隐私问题，但其静态噪声机制会影响模型性能的误差边界。此外，随着模型参数量的指数级增长，使用随机优化器高效学习这些模型变得更具挑战性。为此，我们提出动态差分隐私投影随机梯度下降（D2P2-SGD）优化器。该优化器融合两大核心思想：（i）采用自动梯度裁剪的动态差分隐私（DDP）机制；（ii）结合随机投影的SGD方法，实现模型效用与隐私保护平衡的动态调节。理论证明表明，该算法在不同目标函数上均具有可证明的次线性收敛速率，且达到了现有最佳收敛率。理论分析进一步揭示：动态差分隐私能以隐私代价换取更优的模型效用，而随机投影技术可提升模型学习效率。在多组数据集上的大量实验表明，D2P2-SGD在保持隐私保护的同时显著提升了模型精度。代码已开源。"
    },
    {
        "title": "Database Views as Explanations for Relational Deep Learning",
        "url": "http://arxiv.org/abs/2509.09482v1",
        "pub_date": "2025-09-11",
        "summary": "In recent years, there has been significant progress in the development of deep learning models over relational databases, including architectures based on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph transformers. In effect, such architectures state how the database records and links (e.g., foreign-key references) translate into a large, complex numerical expression, involving numerous learnable parameters. This complexity makes it hard to explain, in human-understandable terms, how a model uses the available data to arrive at a given prediction. We present a novel framework for explaining machine-learning models over relational databases, where explanations are view definitions that highlight focused parts of the database that mostly contribute to the model's prediction. We establish such global abductive explanations by adapting the classic notion of determinacy by Nash, Segoufin, and Vianu (2010). In addition to tuning the tradeoff between determinacy and conciseness, the framework allows controlling the level of granularity by adopting different fragments of view definitions, such as ones highlighting whole columns, foreign keys between tables, relevant groups of tuples, and so on. We investigate the realization of the framework in the case of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive search over the space of all databases. We propose techniques that are model-agnostic, and others that are tailored to hetero-GNNs via the notion of learnable masking. Our approach is evaluated through an extensive empirical study on the RelBench collection, covering a variety of domains and different record-level tasks. The results demonstrate the usefulness of the proposed explanations, as well as the efficiency of their generation.",
        "translated": "近年来，基于关系数据库的深度学习模型取得了显著进展，包括基于异构图神经网络（hetero-GNNs）和异构图变换器的架构。这类架构实质上揭示了数据库记录与关联（如外键引用）如何转化为包含大量可学习参数的复杂数值表达式。这种复杂性使得难以用人类可理解的方式解释模型如何利用数据得出特定预测。本文提出了一种新颖的关系数据库机器学习模型解释框架，其解释结果以视图定义的形式呈现，突出显示对模型预测贡献最大的数据库核心部分。我们通过调整Nash、Segoufin和Vianu（2010）提出的确定性概念来建立这种全局溯因解释。该框架不仅可权衡确定性与简洁性，还能通过采用不同的视图定义片段（如突出整列、表间外键、相关元组群等）来控制粒度层级。我们重点研究了该框架在异构图神经网络中的实现：开发了避免全数据库空间穷举搜索的启发式算法，提出了模型无关的通用技术，以及通过可学习掩码概念为hetero-GNNs定制的专项技术。通过在RelBench数据集上进行涵盖多领域多任务的实证研究，结果表明所提出的解释方法兼具实用性和生成高效性。\n\n（注：专业术语说明：\n1. hetero-GNNs：异构图神经网络，专用于处理多种类型节点和边的图结构数据\n2. 全局溯因解释：通过反推数据中哪些部分对结果产生决定性影响的解释方法\n3. 可学习掩码：通过训练过程自动识别重要特征的掩码技术\n4. RelBench：专门用于评估关系数据库机器学习模型的基准数据集）"
    },
    {
        "title": "CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph\n  Forecasting",
        "url": "http://arxiv.org/abs/2509.09474v1",
        "pub_date": "2025-09-11",
        "summary": "We address the task of temporal knowledge graph (TKG) forecasting by introducing a fully explainable method based on temporal rules. Motivated by recent work proposing a strong baseline using recurrent facts, our approach learns four simple types of rules with a confidence function that considers both recency and frequency. Evaluated on nine datasets, our method matches or surpasses the performance of eight state-of-the-art models and two baselines, while providing fully interpretable predictions.",
        "translated": "我们针对时序知识图谱（TKG）预测任务，提出了一种基于时序规则的完全可解释方法。受近期利用循环事实构建强基线研究的启发，本方法通过学习四种简单规则类型，并采用同时考虑时效性和频次的可信度函数进行推理。在九个数据集上的实验表明，该方法在匹配或超越八个前沿模型和两个基线模型性能的同时，能够提供完全可解释的预测结果。\n\n（注：专业术语说明：\n1. \"temporal knowledge graph (TKG)\" 译为\"时序知识图谱\"，是知识图谱中引入时间维度的专业表述\n2. \"recurrent facts\" 译为\"循环事实\"，特指在时间维度上周期性出现的事实关系\n3. \"recency and frequency\" 译为\"时效性和频次\"，准确表达时间新近性和发生频率的双重考量\n4. \"state-of-the-art models\" 采用学界通用译法\"前沿模型\"\n5. \"fully interpretable predictions\" 译为\"完全可解释的预测\"，强调模型的可解释性特性）"
    },
    {
        "title": "AEGIS: An Agent for Extraction and Geographic Identification in\n  Scholarly Proceedings",
        "url": "http://arxiv.org/abs/2509.09470v1",
        "pub_date": "2025-09-11",
        "summary": "Keeping pace with the rapid growth of academia literature presents a significant challenge for researchers, funding bodies, and academic societies. To address the time-consuming manual effort required for scholarly discovery, we present a novel, fully automated system that transitions from data discovery to direct action. Our pipeline demonstrates how a specialized AI agent, 'Agent-E', can be tasked with identifying papers from specific geographic regions within conference proceedings and then executing a Robotic Process Automation (RPA) to complete a predefined action, such as submitting a nomination form. We validated our system on 586 papers from five different conferences, where it successfully identified every target paper with a recall of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the potential of task-oriented AI agents to not only filter information but also to actively participate in and accelerate the workflows of the academic community.",
        "translated": "面对学术文献的快速增长，如何及时跟进已成为研究者、资助机构和学术团体面临的重要挑战。为减轻学术发现过程中繁重的人工负担，我们开发了一种全新的全自动化系统，实现了从数据发现到直接执行的无缝衔接。本研究展示了一个名为\"Agent-E\"的专用AI代理如何完成以下任务：首先从会议论文集中识别特定地区的论文，随后通过机器人流程自动化（RPA）执行预定操作（如提交提名表格）。我们在五个国际会议的586篇论文上验证系统性能，成功实现100%的召回率与99.4%的准召率。该成果表明：面向任务的AI代理不仅能有效过滤信息，更可主动参与并加速学术工作流程的运行。"
    },
    {
        "title": "AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed\n  Multi-Input Transformer",
        "url": "http://arxiv.org/abs/2509.09458v1",
        "pub_date": "2025-09-11",
        "summary": "This work addresses the challenge of forecasting urban water dynamics by developing a multi-input, multi-output deep learning model that incorporates both endogenous variables (e.g., water height or discharge) and exogenous factors (e.g., precipitation history and forecast reports). Unlike conventional forecasting, the proposed model, AquaCast, captures both inter-variable and temporal dependencies across all inputs, while focusing forecast solely on endogenous variables. Exogenous inputs are fused via an embedding layer, eliminating the need to forecast them and enabling the model to attend to their short-term influences more effectively. We evaluate our approach on the LausanneCity dataset, which includes measurements from four urban drainage sensors, and demonstrate state-of-the-art performance when using only endogenous variables. Performance also improves with the inclusion of exogenous variables and forecast reports. To assess generalization and scalability, we additionally test the model on three large-scale synthesized datasets, generated from MeteoSwiss records, the Lorenz Attractors model, and the Random Fields model, each representing a different level of temporal complexity across 100 nodes. The results confirm that our model consistently outperforms existing baselines and maintains a robust and accurate forecast across both real and synthetic datasets.",
        "translated": "本研究致力于解决城市水动态预测的挑战，提出了一种多输入多输出的深度学习模型——AquaCast。该模型同时融合了内生变量（如水位高度或流量）与外生因素（如历史降水数据和预报报告）。与传统预测方法不同，AquaCast能够捕捉所有输入变量间的相互依赖关系以及时间维度上的关联性，同时仅针对内生变量进行预测。外生输入通过嵌入层进行融合，无需对其单独预测，使模型能更有效地关注其短期影响。我们在洛桑城市数据集（包含四个城市排水传感器的监测数据）上评估了该方法，结果表明仅使用内生变量时模型已达到领先性能，而引入外生变量和预报报告后性能进一步提升。为评估泛化能力与可扩展性，我们额外在三个大规模合成数据集上进行了测试：这些数据集分别基于瑞士气象局记录、洛伦兹吸引子模型和随机场模型生成，每个数据集包含100个节点并呈现不同层次的时间复杂性。实验结果表明，我们的模型在真实与合成数据集中均持续优于现有基线方法，展现出稳健且精确的预测能力。\n\n（注：专业术语说明：\n1. 内生变量（endogenous variables）：指系统内部产生的变量\n2. 外生因素（exogenous factors）：指外部输入的影响因素\n3. 嵌入层（embedding layer）：深度学习中对离散变量进行稠密向量表示的神经网络层\n4. 洛伦兹吸引子（Lorenz Attractors）：描述混沌系统行为的经典数学模型\n5. 随机场（Random Fields）：具有空间相关性的随机变量集合）"
    },
    {
        "title": "Composable Score-based Graph Diffusion Model for Multi-Conditional\n  Molecular Generation",
        "url": "http://arxiv.org/abs/2509.09451v1",
        "pub_date": "2025-09-11",
        "summary": "Controllable molecular graph generation is essential for material and drug discovery, where generated molecules must satisfy diverse property constraints. While recent advances in graph diffusion models have improved generation quality, their effectiveness in multi-conditional settings remains limited due to reliance on joint conditioning or continuous relaxations that compromise fidelity. To address these limitations, we propose Composable Score-based Graph Diffusion model (CSGD), the first model that extends score matching to discrete graphs via concrete scores, enabling flexible and principled manipulation of conditional guidance. Building on this foundation, we introduce two score-based techniques: Composable Guidance (CoG), which allows fine-grained control over arbitrary subsets of conditions during sampling, and Probability Calibration (PC), which adjusts estimated transition probabilities to mitigate train-test mismatches. Empirical results on four molecular datasets show that CSGD achieves state-of-the-art performance, with a 15.3% average improvement in controllability over prior methods, while maintaining high validity and distributional fidelity. Our findings highlight the practical advantages of score-based modeling for discrete graph generation and its capacity for flexible, multi-property molecular design.",
        "translated": "可控分子图生成对于材料与药物发现至关重要，生成的分子必须满足多样化的属性约束。尽管图扩散模型的最新进展提升了生成质量，但由于依赖联合条件作用或损害保真度的连续松弛方法，其在多条件设定下的有效性仍受限。为解决这些局限性，我们提出可组合基于分数的图扩散模型（CSGD），这是首个通过具体分数将分数匹配扩展至离散图的模型，实现了条件引导的灵活且原则性调控。基于此，我们引入两种基于分数的技术：可组合引导（CoG）——支持在采样过程中对任意条件子集进行细粒度控制；概率校准（PC）——通过调整估计的转移概率缓解训练-测试失配问题。在四个分子数据集上的实验结果表明，CSGD实现了最先进的性能，其可控性较现有方法平均提升15.3%，同时保持高有效性和分布保真度。我们的研究发现凸显了基于分数建模在离散图生成中的实践优势及其在多属性分子灵活设计方面的潜力。"
    },
    {
        "title": "Fused Lasso Improves Accuracy of Co-occurrence Network Inference in\n  Grouped Samples",
        "url": "http://arxiv.org/abs/2509.09413v1",
        "pub_date": "2025-09-11",
        "summary": "Co-occurrence network inference algorithms have significantly advanced our understanding of microbiome communities. However, these algorithms typically analyze microbial associations within samples collected from a single environmental niche, often capturing only static snapshots rather than dynamic microbial processes. Previous studies have commonly grouped samples from different environmental niches together without fully considering how microbial communities adapt their associations when faced with varying ecological conditions. Our study addresses this limitation by explicitly investigating both spatial and temporal dynamics of microbial communities. We analyzed publicly available microbiome abundance data across multiple locations and time points, to evaluate algorithm performance in predicting microbial associations using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates algorithms in two distinct scenarios: training and testing within the same environmental niche (Same), and training and testing on combined data from multiple environmental niches (All). To overcome the limitations of conventional algorithms, we propose fuser, an algorithm that, while not entirely new in machine learning, is novel for microbiome community network inference. It retains subsample-specific signals while simultaneously sharing relevant information across environments during training. Unlike standard approaches that infer a single generalized network from combined data, fuser generates distinct, environment-specific predictive networks. Our results demonstrate that fuser achieves comparable predictive performance to existing algorithms such as glmnet when evaluated within homogeneous environments (Same), and notably reduces test error compared to baseline algorithms in cross-environment (All) scenarios.",
        "translated": "共现网络推断算法显著推动了我们对微生物群落的理解。然而，这些算法通常仅分析来自单一环境样本中的微生物关联，往往只能捕捉静态快照而非动态的微生物过程。以往研究常将不同环境样本混合分析，未能充分考虑微生物群落在面对不同生态条件时如何调整其关联模式。本研究通过明确考察微生物群落的空间和时间动态特征，解决了这一局限性。我们分析了跨多个地点和时间的公开微生物组丰度数据，并采用我们提出的同源-混合交叉验证（SAC）框架来评估算法预测微生物关联的性能。SAC在两种场景下验证算法：相同环境内的训练与测试（同源模式），以及跨环境混合数据的训练与测试（混合模式）。\n\n为突破传统算法的限制，我们提出了fuser算法——该方法在机器学习领域虽非全新，但在微生物群落网络推断中具有创新性。它能在训练过程中保留特定子样本信号的同时，实现跨环境的信息共享。与从混合数据推断单一通用网络的标准方法不同，fuser能生成具有环境特异性的预测网络。实验结果表明：在同质环境（同源模式）下，fuser与glmnet等现有算法预测性能相当；而在跨环境（混合模式）场景中，其测试误差较基线算法显著降低。\n\n（注：glmnet保留英文原名因该名称在机器学习领域为特定算法标识；SAC框架名称采用中文释义+英文缩写组合方式以实现技术准确性与可读性的平衡）"
    },
    {
        "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers",
        "url": "http://arxiv.org/abs/2509.10452v1",
        "pub_date": "2025-09-12",
        "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.",
        "translated": "尽管Whisper等预训练自动语音识别（ASR）模型表现优异，但仍需领域自适应以处理未见过的新词汇和特定表达方式。在许多实际场景中，采集语音数据存在困难，因此需要纯文本自适应方法。我们提出WhisTLE——一种针对预训练编码器-解码器ASR模型的深度监督式纯文本自适应方法。该方法通过变分自编码器（VAE）对文本生成的编码器输出进行建模，并利用学习到的文本-潜在编码器对解码器进行微调，还可结合文本转语音（TTS）技术进行联合适配。在推理阶段，系统将恢复使用原始编码器，不会产生额外计算开销。在四个跨领域数据集和四种ASR模型上的实验表明：结合TTS的WhisTLE方法相较于纯TTS自适应将词错误率（WER）相对降低12.3%，在32个测试场景中的27个场景性能超越所有非WhisTLE基线方法。\n\n（译文说明：\n1. 专业术语处理：\"variational autoencoder\"译为\"变分自编码器\"，\"encoder-decoder\"保留编码器-解码器结构表述\n2. 技术概念转化：\"deeply supervised\"译为\"深度监督式\"，\"text-only adaptation\"译为\"纯文本自适应\"\n3. 长句拆分：将原文复合长句按中文表达习惯拆分为多个短句，如将推理阶段的说明单独成句\n4. 数据呈现：精确转换\"12.3% relative\"为\"相对降低12.3%\"，保留\"27 of 32\"的原始数据表述\n5. 逻辑衔接：使用\"尽管...但仍需...\"、\"因此\"等连接词保持论证逻辑的连贯性）"
    },
    {
        "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
        "url": "http://arxiv.org/abs/2509.10446v1",
        "pub_date": "2025-09-12",
        "summary": "Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at https://github.com/THUDM/DeepDive.",
        "translated": "通过为大型语言模型（LLMs）配备浏览工具，可显著增强其作为深度搜索代理解决复杂现实任务的潜力。然而，由于现有开源LLMs在使用浏览工具时存在长程推理能力不足，且缺乏足够难度的监督数据，其在此类场景中表现仍不理想。针对这些挑战，我们提出DeepDive系统以推进深度搜索代理的发展。首先，我们设计了一种从开放知识图谱自动合成复杂、困难且难以查找问题的方法。其次，我们采用端到端多轮强化学习（RL）来增强LLMs在深度搜索中的长程推理能力。实验表明，DeepDive-32B在BrowseComp基准测试中创下开源模型的新竞争力记录，性能超越WebSailor、DeepSeek-R1-Browse和Search-o1。我们证实多轮RL训练能有效提升深度搜索能力，并在多个基准测试中显著推动性能提升。实验还观察到DeepDive支持测试时工具调用的规模扩展和并行采样。所有数据集、模型和代码均已开源：https://github.com/THUDM/DeepDive。\n\n（注：专业术语处理说明：\n1. \"long-horizon reasoning\" 译为\"长程推理\"（认知科学领域标准译法）\n2. \"multi-turn reinforcement learning\" 译为\"多轮强化学习\"（符合RL社区命名惯例）\n3. \"BrowseComp\" 保留原名（作为基准测试名称）\n4. \"test-time scaling\" 译为\"测试时扩展\"（机器学习领域通用译法）\n5. \"parallel sampling\" 译为\"并行采样\"（符合分布式计算术语规范））"
    },
    {
        "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment",
        "url": "http://arxiv.org/abs/2509.10436v1",
        "pub_date": "2025-09-12",
        "summary": "To optimize the reasoning and problem-solving capabilities of Large Language Models (LLMs), we propose a novel cloud-edge collaborative architecture that enables a structured, multi-agent prompting framework. This framework comprises three specialized components: GuideLLM, a lightweight model deployed at the edge to provide methodological guidance; SolverLLM, a more powerful model hosted in the cloud responsible for generating code solutions; and JudgeLLM, an automated evaluator for assessing solution correctness and quality. To evaluate and demonstrate the effectiveness of this architecture in realistic settings, we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate and enhance the performance of Large Language Models (LLMs) across multi-domain coding tasks. Motivated by the limitations of existing benchmarks, RefactorCoderQA systematically covers various technical domains, including Software Engineering, Data Science, Machine Learning, and Natural Language Processing, using authentic coding challenges from Stack Overflow. Extensive experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves state-of-the-art performance, significantly outperforming leading open-source and commercial baselines with an overall accuracy of 76.84%. Human evaluations further validate the interpretability, accuracy, and practical relevance of the generated solutions. In addition, we evaluate system-level metrics, such as throughput and latency, to gain deeper insights into the performance characteristics and trade-offs of the proposed architecture.",
        "translated": "为优化大语言模型（LLMs）的推理与问题解决能力，我们提出了一种新颖的云-边协同架构，该架构实现了结构化的多智能体提示框架。该框架包含三个专业化组件：部署在边缘侧的轻量化模型GuideLLM（用于提供方法学指导）、云端部署的强大模型SolverLLM（负责生成代码解决方案）以及自动评估器JudgeLLM（用于检验解决方案的正确性与质量）。为评估该架构在真实场景中的有效性，我们构建了RefactorCoderQA基准测试集——一个通过Stack Overflow真实编程挑战系统覆盖软件工程、数据科学、机器学习和自然语言处理等多技术领域，专门用于评估和提升大语言模型多领域编程任务性能的综合基准。大量实验表明，经微调的RefactorCoder-MoE模型实现了最先进的性能，以76.84%的整体准确率显著超越主流开源和商业基线模型。人工评估进一步验证了生成方案的可解释性、准确性和实践相关性。此外，我们还评估了吞吐量和延迟等系统级指标，以深入探究所提出架构的性能特征与权衡关系。\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语标准化：LLMs统一译为\"大语言模型\"，\"cloud-edge\"译为\"云-边\"，\"multi-agent\"译为\"多智能体\"\n2. 技术概念准确传达：如\"prompting framework\"译为\"提示框架\"，\"fine-tuned\"译为\"微调\"\n3. 长句拆分与语序调整：将英文复合句按中文表达习惯分解为多个短句，如对基准测试集的描述\n4. 被动语态转化：\"are designed to\"等被动结构转换为主动语态\n5. 数字精度保留：准确呈现76.84%等数据指标\n6. 技术指标完整传达：完整保留\"throughput and latency\"（吞吐量与延迟）等系统性能指标）"
    },
    {
        "title": "Long Context Automated Essay Scoring with Language Models",
        "url": "http://arxiv.org/abs/2509.10417v1",
        "pub_date": "2025-09-12",
        "summary": "Transformer-based language models are architecturally constrained to process text of a fixed maximum length. Essays written by higher-grade students frequently exceed the maximum allowed length for many popular open-source models. A common approach to addressing this issue when using these models for Automated Essay Scoring is to truncate the input text. This raises serious validity concerns as it undermines the model's ability to fully capture and evaluate organizational elements of the scoring rubric, which requires long contexts to assess. In this study, we evaluate several models that incorporate architectural modifications of the standard transformer architecture to overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models considered in this study include fine-tuned versions of XLNet, Longformer, ModernBERT, Mamba, and Llama models.",
        "translated": "基于Transformer架构的语言模型在处理文本时存在固定的最大长度限制。高年级学生撰写的论文经常超出许多主流开源模型的最大允许长度。在使用这些模型进行自动作文评分时，常见的解决方案是对输入文本进行截断处理。这种做法引发了严重的效度问题，因为它削弱了模型完整捕捉和评估评分标准中组织结构要素的能力——这些要素往往需要通过长上下文来进行判断。本研究采用Kaggle ASAP 2.0数据集，评估了若干对标准Transformer架构进行改进的模型以突破长度限制。具体研究的模型包括经过微调的XLNet、Longformer、ModernBERT、Mamba以及Llama等模型的变体。"
    },
    {
        "title": "Is In-Context Learning Learning?",
        "url": "http://arxiv.org/abs/2509.10414v1",
        "pub_date": "2025-09-12",
        "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.",
        "translated": "上下文学习（ICL）使某些自回归模型能够通过下一词元预测来完成任务，而无需进一步训练。这引发了关于此类模型仅需提示中少量示例（样本）即可解决（学习）未见任务能力的讨论。然而，推理并不总是意味着学习，因为ICL并未显式编码给定观测数据。相反，模型依赖于其先验知识以及可能提供的示例样本。我们从数学角度论证ICL确实构成一种学习行为，但其完整特性仍需通过实证研究来刻画。随后我们开展了大规模ICL分析，通过消融实验控制记忆效应、预训练数据、分布偏移及提示样式与表述等因素。研究发现：ICL是一种有效的学习范式，但在学习未见任务并实现泛化方面存在局限。值得注意的是，当示例数量趋于充足时，模型准确率对示例分布、模型架构、提示样式及输入语言特征均呈现不敏感性。其本质是从提示的规律性中推断模式，这导致模型对数据分布敏感——尤其是思维链等提示方式。鉴于模型在形式相似任务上表现出的精度差异，我们得出结论：自回归机制的临时编码并非鲁棒的学习机制，其通用泛化能力存在局限性。\n\n（注：译文严格遵循学术规范，对关键术语如\"in-context learning\"译为\"上下文学习\"、\"autoregressive models\"译为\"自回归模型\"、\"chain-of-thought\"译为\"思维链\"等均采用学界通用译法。通过拆分英文长句为符合中文表达习惯的短句结构，在保持原文逻辑严密性的同时确保可读性。针对\"exemplars\"根据上下文分别处理为\"示例样本\"和\"示例\"，\"distributional sensitivity\"译为\"对数据分布敏感\"等细节均体现专业领域翻译的准确性。）"
    },
    {
        "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems",
        "url": "http://arxiv.org/abs/2509.10401v1",
        "pub_date": "2025-09-12",
        "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\\&amp;When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's 12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution.",
        "translated": "多智能体系统中的故障归因——精确定位决定性错误发生的具体步骤——是一个关键却尚未解决的挑战。现有方法将其视为对长对话日志的模式识别任务，导致步骤级准确率极低（低于17%），难以应用于复杂系统的调试。这些方法的根本缺陷在于无法执行可靠的反事实推理：即无法判断修正单个动作是否真能避免任务失败。为弥补这一反事实推理缺口，我们提出溯因-行动-预测（A2P）框架，这是一种将故障归因从模式识别转化为结构化因果推理任务的新型智能体框架。A2P通过单次推理过程，引导大语言模型执行严格的三步推理：（1）溯因：推断智能体行为背后的隐藏根本原因；（2）行动：定义最小修正干预；（3）预测：模拟后续轨迹并验证干预是否解决故障。该结构化方法在利用完整对话上下文的同时，对模型分析施加严谨的因果逻辑。我们在Who&When基准上的大量实验证明了其有效性：在算法生成数据集上，A2P达到47.46%的步骤级准确率，较基线方法的16.67%提升2.85倍；在更复杂的手工构建数据集上，实现29.31%的步骤准确率，较基线的12.07%提升2.43倍。通过因果视角重构问题，A2P框架为自动化故障归因提供了更可靠、可验证且显著更精准的解决方案。"
    },
    {
        "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for\n  Sparse Mixture-of-Experts LLMs",
        "url": "http://arxiv.org/abs/2509.10377v1",
        "pub_date": "2025-09-12",
        "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large language models (LLMs) due to their computational efficiency. However, though only a few experts are activated for each token, SMoE still requires loading all expert parameters, leading to high memory usage and challenges in deployment. Previous work has tried to reduce the overhead by pruning and merging experts, but primarily focused on expert-level operations, leaving neuron-level structure underexplored. We propose DERN (Dropping Experts, Recombining Neurons), a task-agnostic and retraining-free framework for expert pruning and reconstruction. We observe that experts are often misaligned and contain semantic conflicts at the neuron level, which poses challenges for direct merging. To solve this, DERN works in three steps: it first prunes redundant experts using router statistics; then it decomposes them into neuron-level expert segments, assigning each segment to its most compatible retained expert; and finally, it merges segments within each retained expert to build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE models show that DERN improves performance by more than 5% on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, without extra training. It also greatly reduces the number of experts and memory usage, making SMoE LLMs easier to deploy in practice.",
        "translated": "稀疏专家混合（SMoE）架构因其计算效率在大语言模型（LLM）中得到广泛应用。然而，尽管每个token仅激活少量专家，SMoE仍需加载全部专家参数，导致内存占用过高并带来部署挑战。先前研究尝试通过剪枝与合并专家来降低开销，但主要聚焦于专家级操作，对神经元级结构的探索仍显不足。我们提出DERN（专家丢弃与神经元重组）框架——一种任务无关且无需重训练的专家剪枝与重构方法。我们发现专家间常存在神经元级的语义冲突与错位，这为直接合并带来困难。为此，DERN分三步操作：首先基于路由统计剪枝冗余专家；随后将剪枝专家分解为神经元级的专家片段，并将每个片段分配给兼容性最高的保留专家；最后在保留专家内部合并片段以构建紧凑表征。在Mixtral、Qwen和DeepSeek的SMoE模型实验表明，在50%专家稀疏度下，DERN无需额外训练即可在常识推理和MMLU基准上提升超过5%的性能。该方法显著减少专家数量与内存占用，极大提升了SMoE大语言模型的实际部署可行性。\n\n（注：专业术语说明：\n- SMoE：稀疏专家混合（Sparse Mixture-of-Experts）\n- token：文本处理最小单元（保留英文术语）\n- neuron-level：神经元级\n- router：路由机制（模型中的门控系统）\n- MMLU：大规模多任务语言理解基准（Massive Multitask Language Understanding））"
    },
    {
        "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning",
        "url": "http://arxiv.org/abs/2509.10208v1",
        "pub_date": "2025-09-12",
        "summary": "Large Language Models often generate unfaithful responses in knowledge intensive tasks due to knowledge conflict,that is,a preference for relying on internal parametric knowledge rather than the provided context.To address this issue,we propose a novel self improving framework,Self Improving Faithfulness Aware Contrastive Tuning.The framework uses a self instruct mechanism that allows the base LLM to automatically generate high quality,structured contrastive learning data,including anchor samples,semantically equivalent positive samples,and negative samples simulating unfaithful scenarios.This approach significantly reduces the cost of manual annotation.Subsequently,contrastive learning is applied to train the model,enabling it to pull faithful responses closer and push unfaithful responses farther apart in the representation space.Experiments on knowledge conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2% over the best baseline method,while significantly reducing dependence on internal memory.The results indicate that SI FACT provides strong effectiveness and high data efficiency in enhancing the contextual faithfulness of LLMs,offering a practical pathway toward building more proactive and trustworthy language models.",
        "translated": "在知识密集型任务中，大语言模型常因知识冲突问题产生不可靠回答——即倾向于依赖内部参数化知识而非给定上下文。为解决该问题，我们提出了一种新型自改进框架：自改进式忠实度感知对比调优（SI FACT）。该框架采用自指令机制，使基础大语言模型能自动生成高质量的结构化对比学习数据，包括锚点样本、语义等效的正样本以及模拟不可靠场景的负样本，显著降低了人工标注成本。随后通过对比学习训练模型，使其在表征空间中拉近可靠回答、推远不可靠回答。在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct构建的SI FACT模型将上下文召回率较最佳基线方法提升6.2%，同时显著降低了对内部记忆的依赖。结果表明，SI FACT在增强大语言模型上下文忠实度方面具有强有效性和高数据效率，为构建更具主动性和可信度的语言模型提供了实用路径。\n\n（注：专业术语说明：\n1. \"faithful responses\" 译为\"可靠回答\"以符合中文NLP领域表述习惯\n2. \"Contextual Recall Rate\" 采用通用译法\"上下文召回率\"\n3. \"self instruct mechanism\" 译为\"自指令机制\"以区别于\"self-attention\"\n4. \"parametric knowledge\" 译为\"参数化知识\"保持技术准确性\n5. 框架名称SI FACT保留英文缩写并补充中文全称）"
    },
    {
        "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text\n  Classification",
        "url": "http://arxiv.org/abs/2509.10199v1",
        "pub_date": "2025-09-12",
        "summary": "The most widely used large language models in the social sciences (such as BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text length that they can process to produce predictions. This is a particularly pressing issue for some classification tasks, where the aim is to handle long input texts. One such area deals with laws and draft laws (bills), which can have a length of multiple hundred pages and, therefore, are not particularly amenable for processing with models that can only handle e.g. 512 tokens. In this paper, we show results from experiments covering 5 languages with XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass classification task of the Comparative Agendas Project, which has a codebook of 21 policy topic labels from education to health care. Results show no particular advantage for the Longformer model, pre-trained specifically for the purposes of handling long inputs. The comparison between the GPT variants and the best-performing open model yielded an edge for the latter. An analysis of class-level factors points to the importance of support and substance overlaps between specific categories when it comes to performance on long text inputs.",
        "translated": "社会科学领域最广泛使用的大型语言模型（如BERT及其衍生模型RoBERTa）存在输入文本长度的处理限制，这对需要处理长文本的分类任务构成了显著挑战。法律条文及法律草案（法案）分析便是典型领域——这类文本可达数百页，无法被仅能处理512个标记的模型有效分析。本文针对比较议程项目的多类别分类任务（包含从教育到医疗保健等21个政策主题标签），在五种语言环境下对XLM-RoBERTa、Longformer、GPT-3.5和GPT-4模型进行了实验测试。结果表明：专为长文本处理预训练的Longformer模型并未展现显著优势；在GPT系列模型与最佳开源模型的对比中，后者表现更优。通过类别层面因素分析发现，特定政策类别间的支持度与内容重叠度对长文本处理性能具有重要影响。"
    },
    {
        "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online\n  Supportive Conversations",
        "url": "http://arxiv.org/abs/2509.10184v1",
        "pub_date": "2025-09-12",
        "summary": "In emotionally supportive conversations, well-intended positivity can sometimes misfire, leading to responses that feel dismissive, minimizing, or unrealistically optimistic. We examine this phenomenon of incongruent positivity as miscalibrated expressions of positive support in both human and LLM generated responses. To this end, we collected real user-assistant dialogues from Reddit across a range of emotional intensities and generated additional responses using large language models for the same context. We categorize these conversations by intensity into two levels: Mild, which covers relationship tension and general advice, and Severe, which covers grief and anxiety conversations. This level of categorization enables a comparative analysis of how supportive responses vary across lower and higher stakes contexts. Our analysis reveals that LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. To further study the underlying dimensions of this phenomenon, we finetune LLMs on datasets with strong and weak emotional reactions. Moreover, we developed a weakly supervised multilabel classifier ensemble (DeBERTa and MentalBERT) that shows improved detection of incongruent positivity types across two sorts of concerns (Mild and Severe). Our findings shed light on the need to move beyond merely generating generic positive responses and instead study the congruent support measures to balance positive affect with emotional acknowledgment. This approach offers insights into aligning large language models with affective expectations in the online supportive dialogue, paving the way toward context-aware and trust preserving online conversation systems.",
        "translated": "在情感支持对话中，善意的积极回应有时可能适得其反，导致回复显得敷衍、弱化问题或过度乐观。本研究针对人类和大型语言模型（LLM）生成回复中出现的这种\"失调型积极回应\"现象展开分析。我们收集了Reddit平台上不同情感强度的真实用户-助手对话，并使用大模型为相同语境生成补充回复。根据情感强度将这些对话分为两个层级：轻度（涉及关系紧张和一般性建议）和重度（涉及悲痛与焦虑对话）。这种分级方式使得我们能够比较不同风险等级语境中支持性回复的差异。\n\n分析表明，大型语言模型更容易通过敷衍和弱化问题的语调表现出不切实际的积极性，在高风险语境中尤为明显。为深入探究该现象的内在维度，我们在具有强/弱情绪反应的数据集上对LLM进行了微调。此外，我们开发了基于DeBERTa和MentalBERT的弱监督多标签分类器集成模型，该模型在两种关切类型（轻度和重度）中实现了对失调型积极回应更精准的检测。\n\n研究结果揭示：需要超越生成泛化积极回复的层面，转而研究如何通过契合语境的支持措施来平衡积极情感与情绪认同。该方法为调整大语言模型以适应在线支持性对话中的情感预期提供了新思路，为构建情境感知且维护信任的在线对话系统指明了方向。"
    },
    {
        "title": "Benchmark of stylistic variation in LLM-generated texts",
        "url": "http://arxiv.org/abs/2509.10179v1",
        "pub_date": "2025-09-12",
        "summary": "This study investigates the register variation in texts written by humans and comparable texts produced by large language models (LLMs). Biber's multidimensional analysis (MDA) is applied to a sample of human-written texts and AI-created texts generated to be their counterparts to find the dimensions of variation in which LLMs differ most significantly and most systematically from humans. As textual material, a new LLM-generated corpus AI-Brown is used, which is comparable to BE-21 (a Brown family corpus representing contemporary British English). Since all languages except English are underrepresented in the training data of frontier LLMs, similar analysis is replicated on Czech using AI-Koditex corpus and Czech multidimensional model. Examined were 16 frontier models in various settings and prompts, with emphasis placed on the difference between base models and instruction-tuned models. Based on this, a benchmark is created through which models can be compared with each other and ranked in interpretable dimensions.",
        "translated": "本研究针对人类写作文本与大型语言模型（LLM）生成的对应文本进行语域变异分析。通过应用Biber多维度分析法（MDA），对人工撰写文本和AI生成文本样本进行对比研究，系统性地揭示LLM与人类写作最具显著差异的变异维度。研究采用新构建的AI-Brown语料库（与代表当代英式英语的BE-21布朗家族语料库形成对照）作为文本材料。鉴于前沿大语言模型的训练数据中非英语语种代表性不足，本研究还基于捷克语多维分析模型和AI-Koditex语料库对捷克语进行了平行分析。实验涵盖16种前沿模型在不同参数设置与提示策略下的表现，重点对比了基础模型与指令微调模型的差异。基于上述分析，研究构建了一个可解释的多维基准测试体系，用于实现模型间的量化比较与排名评估。"
    },
    {
        "title": "Error Analysis in a Modular Meeting Transcription System",
        "url": "http://arxiv.org/abs/2509.10143v1",
        "pub_date": "2025-09-12",
        "summary": "Meeting transcription is a field of high relevance and remarkable progress in recent years. Still, challenges remain that limit its performance. In this work, we extend a previously proposed framework for analyzing leakage in speech separation with proper sensitivity to temporal locality. We show that there is significant leakage to the cross channel in areas where only the primary speaker is active. At the same time, the results demonstrate that this does not affect the final performance much as these leaked parts are largely ignored by the voice activity detection (VAD). Furthermore, different segmentations are compared showing that advanced diarization approaches are able to reduce the gap to oracle segmentation by a third compared to a simple energy-based VAD. We additionally reveal what factors contribute to the remaining difference. The results represent state-of-the-art performance on LibriCSS among systems that train the recognition module on LibriSpeech data only.",
        "translated": "会议转录是近年来备受关注且取得显著进展的研究领域，但仍存在制约其性能的技术挑战。本研究扩展了先前提出的语音分离泄漏分析框架，增强了对时间局部性的敏感度分析。实验表明，在主说话人单独发声的时段存在显著的跨通道语音泄漏现象。但值得注意的是，这些泄漏内容大部分被语音活动检测（VAD）系统忽略，因此对最终转录性能影响有限。通过对比不同分割方法，研究发现先进的说话人日志系统能够将与传统能量检测VAD的性能差距缩小三分之一。此外，本文还揭示了导致剩余性能差异的关键因素。在仅使用LibriSpeech数据训练识别模块的系统中，本研究成果在LibriCSS数据集上实现了当前最先进的性能表现。\n\n（注：LibriCSS和LibriSpeech均为语音处理领域常用数据集，其中LibriCSS专门用于会议场景语音分离任务评估）"
    },
    {
        "title": "Towards Reliable and Interpretable Document Question Answering via VLMs",
        "url": "http://arxiv.org/abs/2509.10129v1",
        "pub_date": "2025-09-12",
        "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document understanding, particularly in identifying and extracting textual information from complex documents. Despite this, accurately localizing answers within documents remains a major challenge, limiting both interpretability and real-world applicability. To address this, we introduce \\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that decouples answer generation from spatial localization. This design makes it applicable to existing VLMs, including proprietary systems where fine-tuning is not feasible. Through systematic evaluation, we provide quantitative insights into the gap between textual accuracy and spatial grounding, showing that correct answers often lack reliable localization. Our standardized framework highlights these shortcomings and establishes a benchmark for future research toward more interpretable and robust document information extraction VLMs.",
        "translated": "视觉-语言模型（VLM）在文档理解方面展现出强大能力，尤其在从复杂文档中识别和提取文本信息方面表现突出。然而，在文档中精确定位答案仍然是一个重大挑战，这限制了模型的可解释性和实际应用价值。为解决这一问题，我们提出\\textit{DocExplainerV0}——一种即插即用的边界框预测模块，该模块将答案生成与空间定位解耦。这种设计使其能够适配现有视觉-语言模型（包括无法进行微调的私有系统）。通过系统性评估，我们量化分析了文本准确性与空间定位之间的差距，证明正确答案往往缺乏可靠的空间定位。我们提出的标准化框架不仅揭示了这些缺陷，还为未来构建更具可解释性和鲁棒性的文档信息提取视觉-语言模型建立了基准。"
    },
    {
        "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
        "url": "http://arxiv.org/abs/2509.10127v1",
        "pub_date": "2025-09-12",
        "summary": "Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.",
        "translated": "近年来，大语言模型（LLM）的突破使得人类级社会模拟实现了前所未有的规模与拟真度，为计算社会科学带来新机遇。然而核心挑战在于如何构建能真实反映现实人群多样性及分布特征的角色画像集合。现有基于大语言模型的社会模拟研究多聚焦于智能体框架与仿真环境设计，往往忽视角色生成过程的复杂性以及非代表性角色集可能引入的偏差。本文提出系统性框架，专门为大语言模型驱动的社会模拟合成高质量、符合总体分布特征的角色画像集。该方法首先利用大语言模型从长期社交媒体数据中生成叙事型角色画像，通过严格质量评估筛选低拟真度样本；继而采用重要性采样技术实现与参考心理测量分布（如大五人格特质）的全局对齐；针对特定模拟场景需求，我们进一步引入任务适配模块，将全局对齐后的角色集调整至目标子群体。大量实验表明，本方法能显著降低群体级偏差，并为广泛的研究与政策应用提供精准灵活的社会模拟支持。"
    },
    {
        "title": "Prominence-aware automatic speech recognition for conversational speech",
        "url": "http://arxiv.org/abs/2509.10116v1",
        "pub_date": "2025-09-12",
        "summary": "This paper investigates prominence-aware automatic speech recognition (ASR) by combining prominence detection and speech recognition for conversational Austrian German. First, prominence detectors were developed by fine-tuning wav2vec2 models to classify word-level prominence. The detector was then used to automatically annotate prosodic prominence in a large corpus. Based on those annotations, we trained novel prominence-aware ASR systems that simultaneously transcribe words and their prominence levels. The integration of prominence information did not change performance compared to our baseline ASR system, while reaching a prominence detection accuracy of 85.53% for utterances where the recognized word sequence was correct. This paper shows that transformer-based models can effectively encode prosodic information and represents a novel contribution to prosody-enhanced ASR, with potential applications for linguistic research and prosody-informed dialogue systems.",
        "translated": "本文研究了针对奥地利德语会话的显著性感知自动语音识别（ASR），通过结合显著性检测与语音识别技术。首先，通过微调wav2vec2模型开发了词汇级显著性分类器，用于检测语句中的重音突出部分。随后利用该检测器对大规模语料库进行韵律显著性的自动标注。基于这些标注数据，我们训练了新型的显著性感知ASR系统，能够同步转写文本及其韵律突出层级。实验表明，在保持基线ASR系统性能不变的前提下，当词序列识别正确时，该系统达到85.53%的显著性检测准确率。本研究证实基于Transformer的模型能有效编码韵律信息，为韵律增强型语音识别提供了创新方案，在语言学研究及韵律感知对话系统领域具有应用潜力。"
    },
    {
        "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing\n  Generative AI with Synthetic Patient Records",
        "url": "http://arxiv.org/abs/2509.10108v1",
        "pub_date": "2025-09-12",
        "summary": "The development of medical chatbots in Arabic is significantly constrained by the scarcity of large-scale, high-quality annotated datasets. While prior efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from social media to fine-tune large language models (LLMs), model scalability and generalization remained limited. In this study, we propose a scalable synthetic data augmentation strategy to expand the training corpus to 100,000 records. Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated 80,000 contextually relevant and medically coherent synthetic question-answer pairs grounded in the structure of the original dataset. These synthetic samples were semantically filtered, manually validated, and integrated into the training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2, and evaluated their performance using BERTScore metrics and expert-driven qualitative assessments. To further analyze the effectiveness of synthetic sources, we conducted an ablation study comparing ChatGPT-4o and Gemini-generated data independently. The results showed that ChatGPT-4o data consistently led to higher F1-scores and fewer hallucinations across all models. Overall, our findings demonstrate the viability of synthetic augmentation as a practical solution for enhancing domain-specific language models in-low resource medical NLP, paving the way for more inclusive, scalable, and accurate Arabic healthcare chatbot systems.",
        "translated": "阿拉伯语医疗聊天机器人的发展受到大规模高质量标注数据集稀缺的显著制约。尽管先前研究通过整合社交媒体上的20,000条阿拉伯医患对话数据对大型语言模型（LLM）进行微调，但模型的可扩展性与泛化能力仍存在局限。本研究提出一种可扩展的合成数据增强策略，将训练语料库扩展至10万条记录。基于原始数据集的结构框架，我们采用ChatGPT-4o和Gemini 2.5 Pro等先进生成式AI系统，构建了80,000个上下文相关且医学逻辑自洽的合成问答对。这些合成样本经过语义过滤、人工验证并整合至训练流程。我们对Mistral-7B和AraGPT2等五个LLM进行微调，采用BERTScore指标和专家驱动的定性评估进行性能验证。为进一步分析不同合成数据源的有效性，我们开展了对比ChatGPT-4o与Gemini生成数据的消融实验。结果表明：在所有模型中，基于ChatGPT-4o生成的数据始终能获得更高的F1分数且产生更少幻觉现象。本研究证实了合成数据增强作为低资源医疗NLP领域专用语言模型优化方案的可行性，为构建更具包容性、可扩展性且精准的阿拉伯语医疗聊天机器人系统奠定了基础。"
    },
    {
        "title": "VARCO-VISION-2.0 Technical Report",
        "url": "http://arxiv.org/abs/2509.10105v1",
        "pub_date": "2025-09-12",
        "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model (VLM) for Korean and English with improved capabilities compared to the previous model VARCO-VISION-14B. The model supports multi-image understanding for complex inputs such as documents, charts, and tables, and delivers layoutaware OCR by predicting both textual content and its spatial location. Trained with a four-stage curriculum with memory-efficient techniques, the model achieves enhanced multimodal alignment, while preserving core language abilities and improving safety via preference optimization. Extensive benchmark evaluations demonstrate strong spatial grounding and competitive results for both languages, with the 14B model achieving 8th place on the OpenCompass VLM leaderboard among models of comparable scale. Alongside the 14B-scale model, we release a 1.7B version optimized for on-device deployment. We believe these models advance the development of bilingual VLMs and their practical applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a full-scale 14B model and a lightweight 1.7B model.",
        "translated": "我们推出VARCO-VISION-2.0——一个开放权重的韩英双语视觉语言模型（VLM），其性能较前代模型VARCO-VISION-14B有显著提升。该模型支持对文档、图表等复杂输入进行多图像理解，通过同步预测文本内容及其空间位置实现布局感知的OCR功能。采用四阶段课程训练结合内存优化技术，模型在保持核心语言能力的同时增强了多模态对齐能力，并通过偏好优化提升了安全性。大量基准测试表明，该模型在两种语言上均展现出强大的空间定位能力和竞争力，其中140亿参数版本在OpenCompass VLM排行榜同规模模型中位列第八。除140亿参数版本外，我们还发布了专为设备端部署优化的17亿参数版本。我们相信这些模型将推动双语VLM的发展及其实际应用。VARCO-VISION-2.0的两个变体已发布于Hugging Face平台：完整版140亿参数模型和轻量版17亿参数模型。\n\n（注：根据技术文档翻译规范，对以下术语进行标准化处理：\n1. \"open-weight\"译为\"开放权重\"而非字面直译\n2. \"layout-aware OCR\"采用行业通用译法\"布局感知OCR\"\n3. \"preference optimization\"保留技术概念译为\"偏好优化\"\n4. 模型规模表述统一为\"XX亿参数\"以符合中文技术文献惯例\n5. 保持\"VLM\"首字母缩写与英文原文对应）"
    },
    {
        "title": "Arabic Large Language Models for Medical Text Generation",
        "url": "http://arxiv.org/abs/2509.10095v1",
        "pub_date": "2025-09-12",
        "summary": "Efficient hospital management systems (HMS) are critical worldwide to address challenges such as overcrowding, limited resources, and poor availability of urgent health care. Existing methods often lack the ability to provide accurate, real-time medical advice, particularly for irregular inputs and underrepresented languages. To overcome these limitations, this study proposes an approach that fine-tunes large language models (LLMs) for Arabic medical text generation. The system is designed to assist patients by providing accurate medical advice, diagnoses, drug recommendations, and treatment plans based on user input. The research methodology required the collection of a unique dataset from social media platforms, capturing real-world medical conversations between patients and doctors. The dataset, which includes patient complaints together with medical advice, was properly cleaned and preprocessed to account for multiple Arabic dialects. Fine-tuning state-of-the-art generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2 Medium, optimized the system's ability to generate reliable medical text. Results from evaluations indicate that the fine-tuned Mistral-7B model outperformed the other models, achieving average BERT (Bidirectional Encoder Representations from Transformers) Score values in precision, recall, and F1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative benchmarking and qualitative assessments validate the system's ability to produce coherent and relevant medical replies to informal input. This study highlights the potential of generative artificial intelligence (AI) in advancing HMS, offering a scalable and adaptable solution for global healthcare challenges, especially in linguistically and culturally diverse environments.",
        "translated": "高效的医院管理系统（HMS）对于应对全球范围内的医疗资源紧张、患者过度拥挤和紧急医疗服务可及性不足等挑战至关重要。现有方法往往无法提供准确、实时的医疗建议，尤其对于非规范化输入及资源稀缺语言的支持存在局限。为突破这些限制，本研究提出通过微调大语言模型（LLM）来实现阿拉伯语医疗文本生成的新方法。该系统可根据用户输入提供精准的医疗建议、诊断结论、药物推荐及治疗方案，旨在为患者提供辅助支持。\n\n研究方法通过从社交媒体平台收集真实医患对话，构建了独特的医疗对话数据集。该数据集包含患者主诉与对应的医疗建议，并针对阿拉伯语多种方言进行了数据清洗与预处理。通过微调Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等前沿生成模型，显著提升了系统生成可靠医疗文本的能力。评估结果表明，微调后的Mistral-7B模型表现最优，其在精确率、召回率和F1分数上的BERT平均得分分别达到68.5%、69.08%和68.5%。对比基准测试与定性评估验证了系统对非规范化输入生成连贯相关医疗回复的能力。\n\n本研究凸显了生成式人工智能（AI）在推进医院管理系统发展方面的潜力，为全球医疗挑战——尤其是在语言文化多样化的环境中——提供了可扩展且适应性强的解决方案。"
    },
    {
        "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery",
        "url": "http://arxiv.org/abs/2509.10087v1",
        "pub_date": "2025-09-12",
        "summary": "The growing complexity and volume of climate science literature make it increasingly difficult for researchers to find relevant information across models, datasets, regions, and variables. This paper introduces a domain-specific Knowledge Graph (KG) built from climate publications and broader scientific texts, aimed at improving how climate knowledge is accessed and used. Unlike keyword based search, our KG supports structured, semantic queries that help researchers discover precise connections such as which models have been validated in specific regions or which datasets are commonly used with certain teleconnection patterns. We demonstrate how the KG answers such questions using Cypher queries, and outline its integration with large language models in RAG systems to improve transparency and reliability in climate-related question answering. This work moves beyond KG construction to show its real world value for climate researchers, model developers, and others who rely on accurate, contextual scientific information.",
        "translated": "随着气候科学文献的复杂性和数量不断增长，研究人员在跨模型、数据集、区域和变量查找相关信息时面临日益严峻的挑战。本文提出了一种基于气候领域出版物及更广泛科学文献构建的领域知识图谱（KG），旨在改进气候知识的获取与使用方式。与基于关键词的检索不同，该知识图谱支持结构化语义查询，可帮助研究人员发现精确关联，例如哪些模型在特定区域经过验证，或哪些数据集常与特定遥相关模式结合使用。我们通过Cypher查询演示了知识图谱如何响应此类问题，并概述了其与RAG系统中大型语言模型的集成方案，以提升气候领域问答的透明度与可靠性。这项工作不仅构建了知识图谱，更展现了其对气候研究人员、模型开发者等依赖精准情境化科学信息的群体的实际应用价值。\n\n（注：根据学术规范，术语处理说明：\n1. Knowledge Graph (KG) 保留\"知识图谱\"标准译法\n2. Cypher queries 采用技术界通用译法\"Cypher查询\"\n3. RAG systems 译为\"RAG系统\"（Retrieval-Augmented Generation的通用缩写）\n4. teleconnection patterns 采用气象学标准术语\"遥相关模式\"\n5. 专业表述如\"结构化语义查询\"\"情境化科学信息\"等符合中文科技文献表达习惯）"
    },
    {
        "title": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models",
        "url": "http://arxiv.org/abs/2509.10078v1",
        "pub_date": "2025-09-12",
        "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",
        "translated": "研究人员常采用成熟的心理测量问卷（如BFI、PVQ）来评估大语言模型（LLM）响应中反映的个性特征与价值观。然而，将这类为人设计的问卷直接应用于LLM存在诸多问题，其中一个核心缺陷是缺乏生态效度——即调查问题未能充分反映LLM在真实场景中回应用户查询的文本生成语境。目前尚不明确传统问卷与生态效度问卷在测量结果上的差异及其深层含义。本文对两类问卷进行了系统性对比分析，发现传统问卷存在以下问题：(1) 所获LLM特征轮廓与生态效度问卷存在显著差异，偏离了模型在用户查询语境中表现出的心理特征；(2) 题目数量不足导致测量稳定性欠缺；(3) 易造成\"LLM具有稳定心理建构\"的误导性认知；(4) 对角色设定提示下的LLM会产生夸张的特征描述。本研究警示直接沿用传统心理问卷评估LLM的局限性。相关代码将在论文发表时开源。\n\n（注：翻译过程中对以下术语进行了专业处理：\n1. \"ecological validity\"译为\"生态效度\"（心理学标准术语）\n2. \"persona-prompted LLMs\"译为\"角色设定提示下的LLM\"\n3. \"stable constructs\"译为\"稳定心理建构\"\n4. 保持\"BFI/PVQ\"等专业量表名称原缩写\n5. 采用\"测量稳定性\"\"特征轮廓\"等心理测量学专业表述）"
    },
    {
        "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for\n  Readability Assessment",
        "url": "http://arxiv.org/abs/2509.10040v1",
        "pub_date": "2025-09-12",
        "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained Arabic readability assessment, achieving first place in six of six tracks. Our approach is a confidence-weighted ensemble of four complementary transformer models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with distinct loss functions to capture diverse readability signals. To tackle severe class imbalance and data scarcity, we applied weighted training, advanced preprocessing, SAMER corpus relabeling with our strongest model, and synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level samples. A targeted post-processing step corrected prediction distribution skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system reached 87.5 percent QWK at the sentence level and 87.4 percent at the document level, demonstrating the power of model and loss diversity, confidence-informed fusion, and intelligent augmentation for robust Arabic readability prediction.",
        "translated": "我们介绍了在BAREC 2025细粒度阿拉伯语可读性评估共享任务中夺冠的MSA系统，该方案在全部六个赛道中均获得第一名。我们的方法采用置信度加权集成策略，融合了四个互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT和CAMeLBERT），每个模型通过不同的损失函数进行微调以捕捉多样化的可读性特征。针对严重的类别不平衡和数据稀缺问题，我们采用了加权训练、高级文本预处理、使用最强模型对SAMER语料进行重新标注，并通过Gemini 2.5 Flash生成约10,000个稀有难度级别的合成数据样本。通过针对性后处理步骤修正预测分布偏差，使二次加权卡帕系数（QWK）提升6.3%。我们的系统在句子级别达到87.5%的QWK，文档级别达到87.4%的QWK，证明了模型与损失函数的多样性设计、置信度感知融合机制以及智能数据增强对阿拉伯语可读性预测的有效性。"
    },
    {
        "title": "Linguistic trajectories of bipolar disorder on social media",
        "url": "http://arxiv.org/abs/2509.10035v1",
        "pub_date": "2025-09-12",
        "summary": "Language provides valuable markers of affective disorders such as bipolar disorder (BD), yet clinical assessments remain limited in scale. In response, analyses of social media (SM) language have gained prominence due to their high temporal resolution and longitudinal scope. Here, we introduce a method to determine the timing of users' diagnoses and apply it to study language trajectories from 3 years before to 21 years after BD diagnosis - contrasted with uses reporting unipolar depression (UD) and non-affected users (HC). We show that BD diagnosis is accompanied by pervasive linguistic alterations reflecting mood disturbance, psychiatric comorbidity, substance abuse, hospitalization, medical comorbidities, unusual thought content, and disorganized thought. We further observe recurring mood-related language changes across two decades after the diagnosis, with a pronounced 12-month periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence suggests an increased periodicity in users estimated to be female. In sum, our findings provide evidence for language alterations in the acute and chronic phase of BD. This validates and extends recent efforts leveraging SM for scalable monitoring of mental health.",
        "translated": "语言为双相情感障碍（BD）等情感障碍提供了重要的标记物，但临床评估的规模仍然有限。为此，社交媒体（SM）语言分析因其高时间分辨率和纵向跨度而日益受到重视。本研究提出一种确定用户诊断时间的方法，并应用该方法追踪从BD诊断前3年至诊断后21年的语言演变轨迹——同时与单相抑郁（UD）用户及未受影响用户（HC）进行对比。我们发现BD诊断伴随着广泛的语言特征变化，这些变化反映了情绪紊乱、精神共病、物质滥用、住院治疗、医学并发症、异常思维内容及思维混乱。进一步观察到诊断后二十年中反复出现的情绪相关语言变化，其具有显著的12个月周期特征，暗示季节性情绪发作。趋势性证据还表明，被判定为女性的用户群体呈现更强的周期性特征。总之，我们的研究结果证明了BD在急性期和慢性期均存在语言特征改变，这验证并拓展了近期利用社交媒体实现可扩展心理健康监测的研究方向。\n\n（注：专业术语说明：\n1. bipolar disorder (BD) 译为\"双相情感障碍\"\n2. unipolar depression (UD) 译为\"单相抑郁\"\n3. non-affected users (HC) 译为\"未受影响用户\"，HC通常指健康对照组\n4. psychiatric comorbidity 译为\"精神共病\"\n5. substance abuse 译为\"物质滥用\"\n6. 保留SM（社交媒体）和BD（双相障碍）等专业领域常用缩写形式，符合学术翻译惯例）"
    },
    {
        "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR",
        "url": "http://arxiv.org/abs/2509.10031v1",
        "pub_date": "2025-09-12",
        "summary": "Neural front-ends represent a promising approach to feature extraction for automatic speech recognition (ASR) systems as they enable to learn specifically tailored features for different tasks. Yet, many of the existing techniques remain heavily influenced by classical methods. While this inductive bias may ease the system design, our work aims to develop a more generic front-end for feature extraction. Furthermore, we seek to unify the front-end architecture contrasting with existing approaches that apply a composition of several layer topologies originating from different sources. The experiments systematically show how to reduce the influence of existing techniques to achieve a generic front-end. The resulting 2D convolutional front-end is parameter-efficient and suitable for a scenario with limited computational resources unlike large models pre-trained on unlabeled audio. The results demonstrate that this generic unified approach is not only feasible but also matches the performance of existing supervised learnable feature extractors.",
        "translated": "神经前端为自动语音识别（ASR）系统的特征提取提供了一种前景广阔的方法，其能够针对不同任务学习专门优化的特征。然而现有技术仍深受传统方法影响，这种归纳偏置虽可简化系统设计，但本研究致力于开发更具通用性的特征提取前端。与现有采用多源混合层拓扑结构的方案不同，我们进一步寻求前端架构的统一化。通过系统化实验，我们展示了如何削弱现有技术的影响以实现通用前端。最终构建的二维卷积前端具有参数高效性，适用于计算资源有限的场景——这与基于未标注音频预训练的大模型形成鲜明对比。实验结果表明，这种通用统一方法不仅可行，其性能更可媲美现有监督式可学习特征提取器。"
    },
    {
        "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between\n  Smaller Open-Source LLMs",
        "url": "http://arxiv.org/abs/2509.10010v1",
        "pub_date": "2025-09-12",
        "summary": "In this paper, we provide an extensive analysis of multi-label intent classification using Large Language Models (LLMs) that are open-source, publicly available, and can be run in consumer hardware. We use the MultiWOZ 2.1 dataset, a benchmark in the dialogue system domain, to investigate the efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf, Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot setup, giving 20 examples in the prompt with some instructions. Our approach focuses on the differences in performance of these models across several performance metrics by methodically assessing these models on multi-label intent classification tasks. Additionally, we compare the performance of the instruction-based fine-tuning approach with supervised learning using the smaller transformer model BertForSequenceClassification as a baseline. To evaluate the performance of the models, we use evaluation metrics like accuracy, precision, and recall as well as micro, macro, and weighted F1 score. We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1 outperforms two other generative models on 11 intent classes out of 14 in terms of F-Score, with a weighted average of 0.50. It also has relatively lower Humming Loss and higher Jaccard Similarity, making it the winning model in the few-shot setting. We find BERT based supervised classifier having superior performance compared to the best performing few-shot generative LLM. The study provides a framework for small open-source LLMs in detecting complex multi-intent dialogues, enhancing the Natural Language Understanding aspect of task-oriented chatbots.",
        "translated": "本文针对基于开源、可公开获取且能在消费级硬件上运行的大语言模型（LLMs）进行多标签意图分类的深入分析。研究采用对话系统领域的基准数据集MultiWOZ 2.1，评估了三种主流开源预训练大模型（LLama2-7B-hf、Mistral-7B-v0.1和Yi-6B）在小样本学习场景下的性能。通过提示模板注入20个示例与指令，系统比较了这些模型在多标签意图分类任务中各项性能指标的差异。同时，研究以基于小规模Transformer的BertForSequenceClassification监督学习模型为基线，对比了指令微调方法的性能表现。\n\n模型评估采用准确率、精确率、召回率以及微观/宏观/加权F1值等指标，并统计了推理时间和显存需求。实验结果表明：在14个意图类别中，Mistral-7B-v0.1在11个类别的F值上优于其他生成模型，加权平均F值达到0.50，同时具有较低的汉明损失和较高的杰卡德相似系数，成为小样本设置下的最优模型。但研究发现基于BERT的监督分类器性能仍显著优于最佳的小样本生成式LLM。本研究为开源小参数模型检测复杂多意图对话提供了实践框架，对提升任务型对话系统的自然语言理解能力具有推进意义。\n\n（注：专业术语说明：\n1. Humming Loss保留原意译为\"汉明损失\"\n2. Jaccard Similarity采用学界通用译名\"杰卡德相似系数\"\n3. Few-shot setting译为\"小样本设置\"\n4. VRAM译为\"显存\"\n5. 模型名称保持英文原称）"
    },
    {
        "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
        "url": "http://arxiv.org/abs/2509.10004v1",
        "pub_date": "2025-09-12",
        "summary": "Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection.",
        "translated": "无监督幻觉检测旨在无需依赖标注数据的情况下，识别大语言模型（LLM）生成的虚假内容。尽管无监督方法因省去费时费力的人工标注而受到关注，但这些方法往往依赖与事实正确性无关的代理信号。这种偏差导致检测探针偏向表面特征或与真实性无关的方面，限制了其在跨数据集和场景中的泛化能力。为突破这些局限，我们提出IRIS——一种基于事实正确性内在表征的无监督幻觉检测框架。IRIS通过提示大语言模型对给定陈述进行严谨真实性验证，并获取其情境化嵌入向量作为训练特征。同时，将每个响应的不确定性视为真实性的软伪标签。实验结果表明，IRIS在多个基准上持续优于现有无监督方法。本方法完全无需监督、计算成本低，即使在少量训练数据下仍能有效工作，适用于实时检测场景。\n\n（注：专业术语说明：\n1. \"hallucination detection\" 译为\"幻觉检测\"，特指LLM生成虚假内容的检测任务\n2. \"contextualized embedding\" 译为\"情境化嵌入向量\"，保留NLP领域专业表述\n3. \"soft pseudolabel\" 译为\"软伪标签\"，体现半监督学习中的概率标注特性\n4. 保持\"LLM\"、\"无监督\"等技术术语中英文一致性，符合学术翻译规范）"
    },
    {
        "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority\n  Languages in China",
        "url": "http://arxiv.org/abs/2509.09990v1",
        "pub_date": "2025-09-12",
        "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks.",
        "translated": "中国少数民族语言（如藏语、维吾尔语和传统蒙古语）因其独特的文字体系与国际标准存在差异，面临着重大挑战。这种差异导致相关语料库严重匮乏，尤其在标题生成等监督任务中更为明显。为填补这一空白，我们推出了新颖的中文少数民族标题生成数据集（CMHG），其中包含10万条藏语条目，维吾尔语和蒙古语各5万条，专门为标题生成任务构建。此外，我们还提出了由母语者标注的高质量测试集，旨在为该领域的后续研究提供基准。我们希望该数据集能成为推动中文少数民族语言标题生成研究的重要资源，并为相关基准的发展作出贡献。"
    },
    {
        "title": "Whisper Has an Internal Word Aligner",
        "url": "http://arxiv.org/abs/2509.09987v1",
        "pub_date": "2025-09-12",
        "summary": "There is an increasing interest in obtaining accurate word-level timestamps from strong automatic speech recognizers, in particular Whisper. Existing approaches either require additional training or are simply not competitive. The evaluation in prior work is also relatively loose, typically using a tolerance of more than 200 ms. In this work, we discover attention heads in Whisper that capture accurate word alignments and are distinctively different from those that do not. Moreover, we find that using characters produces finer and more accurate alignments than using wordpieces. Based on these findings, we propose an unsupervised approach to extracting word alignments by filtering attention heads while teacher forcing Whisper with characters. Our approach not only does not require training but also produces word alignments that are more accurate than prior work under a stricter tolerance between 20 ms and 100 ms.",
        "translated": "目前，学术界对从高性能自动语音识别系统（尤其是Whisper模型）中获取精确词级时间戳的兴趣日益增长。现有方法要么需要额外训练，要么在性能上缺乏竞争力。此前研究的评估标准也相对宽松，通常允许超过200毫秒的误差容限。本研究发现，Whisper模型中存在能捕捉精确词语对齐的注意力头，这些注意力头与无效注意力头具有显著差异。此外，我们发现使用字符级单元比使用词片段能生成更精细、更准确的对齐结果。基于这些发现，我们提出一种无监督方法：通过筛选注意力头，在字符级教师强制模式下提取词语对齐。该方法不仅无需训练，而且在20毫秒至100毫秒的严格容差范围内，产生的词语对齐结果比现有研究更加精确。\n\n（注：教师强制（teacher forcing）指在自回归模型训练过程中使用真实标签而非预测结果作为下一步输入的技术）"
    },
    {
        "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey",
        "url": "http://arxiv.org/abs/2509.09969v1",
        "pub_date": "2025-09-12",
        "summary": "Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.",
        "translated": "近年来，大语言模型（LLMs）显著推动了法律人工智能（Legal AI）的发展，有效提升了法律任务的效率与准确性。为促进基于大语言模型的法律领域方法的研究与应用，本文系统综述了16个法律大语言模型系列和47种面向法律任务的LLM框架，同时汇集了15个基准测试和29个数据集以评估不同法律能力。此外，我们分析了当前法律领域大语言模型方法面临的挑战，并探讨了未来发展方向。本文旨在为初学者提供系统性介绍，并推动该领域的后续研究。相关资源已发布于：https://github.com/ZhitianHou/LLMs4LegalAI。\n\n（注：译文严格遵循了学术文本的规范表述，确保专业术语的准确性，如\"benchmarks\"译为\"基准测试\"、\"frameworks\"译为\"框架\"；同时采用中文长句拆分、被动语态转化等策略，例如将英文被动结构\"are gathered\"转化为中文主动态\"汇集\"，符合中文学术写作习惯。资源链接和专有名词（如LLMs/Legal AI）均保留原格式，确保技术信息的精确传递。）"
    },
    {
        "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic\n  Survey Responses for the Chilean Case",
        "url": "http://arxiv.org/abs/2509.09871v1",
        "pub_date": "2025-09-11",
        "summary": "Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy &gt; 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.",
        "translated": "大型语言模型（LLMs）通过生成合成受访者模拟人类答案与行为，为调查研究的方法论和应用创新提供了新路径，有望减少测量误差与表征偏差。然而，LLMs在还原总体项目分布方面的有效性尚不明确，且下游应用存在复现训练数据中社会刻板印象与偏见的风险。本研究以智利概率抽样民意调查的真实人类回答为基准，评估了LLM生成合成调查回复的可靠性。具体而言，我们构建了128组提示-模型-问题三元组测试单元，生成189,696份合成受访档案，并通过128个问题-子样本对的元分析汇集性能指标（准确率、精确率、召回率和F1分数），以检验关键社会人口维度上的偏差。评估涵盖OpenAI的GPT系列及o系列推理模型，以及Llama和Qwen的多个版本。主要发现有三：首先，合成回复在信任类问题上表现优异（F1分数与准确率＞0.90）；其次，GPT-4o、GPT-4o-mini与Llama 4 Maverick在此任务中性能相当；第三，45-59岁受访者群体的合成-人类回答对齐度最高。总体而言，基于LLM的合成样本可近似概率抽样样本的回答，但存在显著的项目级异质性。要完整捕捉公众意见的细微差别仍具挑战，需通过精细校准与额外分布测试来确保算法保真度并降低误差。\n\n（翻译说明：专业术语如\"probabilistic survey\"译为\"概率抽样调查\"，\"synthetic respondents\"译为\"合成受访者\"，\"meta-analysis\"译为\"元分析\"等均符合学术规范；长难句如\"pool performance metrics...sociodemographic dimensions\"通过拆分重组保持中文表达习惯；技术指标名称（F1-score等）保留英文原格式符合国内学术惯例；关键结论采用\"主要发现有三\"等中文摘要常用句式增强可读性。）"
    },
    {
        "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational\n  Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks",
        "url": "http://arxiv.org/abs/2509.09870v1",
        "pub_date": "2025-09-11",
        "summary": "Large language models (LLMs) enable conversational agents (CAs) to express distinctive personalities, raising new questions about how such designs shape user perceptions. This study investigates how personality expression levels and user-agent personality alignment influence perceptions in goal-oriented tasks. In a between-subjects experiment (N=150), participants completed travel planning with CAs exhibiting low, medium, or high expression across the Big Five traits, controlled via our novel Trait Modulation Keys framework. Results revealed an inverted-U relationship: medium expression produced the most positive evaluations across Intelligence, Enjoyment, Anthropomorphism, Intention to Adopt, Trust, and Likeability, significantly outperforming both extremes. Personality alignment further enhanced outcomes, with Extraversion and Emotional Stability emerging as the most influential traits. Cluster analysis identified three distinct compatibility profiles, with \"Well-Aligned\" users reporting substantially positive perceptions. These findings demonstrate that personality expression and strategic trait alignment constitute optimal design targets for CA personality, offering design implications as LLM-based CAs become increasingly prevalent.",
        "translated": "大型语言模型（LLMs）使会话代理（CAs）能够展现独特个性，这引发了关于此类设计如何影响用户感知的新问题。本研究探讨了在目标导向任务中，个性表达水平与用户-代理个性匹配如何影响用户感知。通过一项组间实验（N=150），参与者与呈现低、中、高三种大五人格特质表达水平的会话代理完成旅行规划任务，该表达水平通过我们创新的特质调节密钥框架进行控制。研究结果呈现出倒U型关系：中等表达水平在智力感知、愉悦度、拟人化、使用意愿、信任度和好感度六个维度上获得最积极评价，显著优于两个极端水平。个性匹配进一步提升了效果，其中外向性和情绪稳定性被发现是最具影响力的特质。聚类分析识别出三种不同的兼容性特征群体，\"良好匹配\"用户群体呈现出显著积极的感知体验。这些发现表明，个性表达与策略性特质匹配构成了会话代理个性设计的优化目标，为基于大语言模型的会话代理日益普及的时代提供了设计启示。"
    },
    {
        "title": "GC-VLN: Instruction as Graph Constraints for Training-free\n  Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2509.10454v1",
        "pub_date": "2025-09-12",
        "summary": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.",
        "translated": "本文提出了一种免训练的视觉语言导航（VLN）框架。现有的零样本VLN方法主要针对离散环境设计，或在连续仿真环境中进行无监督训练，这导致其在真实场景中的泛化与应用存在挑战。为实现连续环境中的免训练导航，本框架通过将导航指令分解为显式空间约束，将导航引导问题转化为图约束优化任务。这种约束驱动范式通过约束求解解析空间语义，实现了对未见环境的零样本适应。具体而言，我们构建了覆盖VLN指令中所有空间关系类型的空间约束库，将人类指令解构为包含路径点节点、物体节点及边的有向无环图，并通过查询约束库构建图约束。利用约束求解器对图约束进行优化以确定路径点位置，最终生成机器人的导航路径与目标位置。针对无解或多解情况，我们设计了导航树结构与回溯机制。在标准基准测试上的实验表明，该方法在成功率和导航效率上显著优于现有零样本VLN方法。进一步的真实环境实验验证了本框架能够有效泛化至新环境和指令集，为构建更鲁棒、自主的导航框架奠定了基础。\n\n（译文说明：  \n1. 专业术语处理：\"training-free\"译为\"免训练\"，\"zero-shot\"保留学术通用译法\"零样本\"，\"constraint solver\"译为\"约束求解器\"  \n2. 技术细节还原：完整保留\"有向无环图\"、\"回溯机制\"等关键概念  \n3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句  \n4. 逻辑显化：通过\"具体而言\"、\"针对\"等连接词明确技术流程的层次关系  \n5. 学术风格保持：使用\"范式\"、\"泛化\"等符合计算机学术论文表达的词汇）"
    },
    {
        "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n  Adaptability Across Alzheimer's Prediction Tasks and Datasets",
        "url": "http://arxiv.org/abs/2509.10453v1",
        "pub_date": "2025-09-12",
        "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes memory loss and cognitive decline. While there has been extensive research in applying deep learning models to Alzheimer's prediction tasks, these models remain limited by lack of available labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals between scans. In this study, we adapt three state-of-the-art temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis, and add novel extensions designed to handle variable-length inputs and learn robust spatial features. We aggregate four publicly available datasets comprising 3,161 patients for pre-training, and show the performance of our model across multiple Alzheimer's prediction tasks including diagnosis classification, conversion detection, and future conversion prediction. Importantly, our SSL model implemented with temporal order prediction and contrastive learning outperforms supervised learning on six out of seven downstream tasks. It demonstrates adaptability and generalizability across tasks and number of input images with varying time intervals, highlighting its capacity for robust performance across clinical applications. We release our code and model publicly at https://github.com/emilykaczmarek/SSL-AD.",
        "translated": "阿尔茨海默病是一种进行性神经退行性疾病，会导致记忆丧失和认知能力下降。尽管深度学习模型在阿尔茨海默病预测任务中的应用已有广泛研究，但这些模型仍受限于标注数据匮乏、跨数据集泛化能力差以及对不同数量输入扫描和扫描时间间隔的适应性不足。本研究采用三种最先进的时序自监督学习（SSL）方法进行3D脑部MRI分析，并新增了处理变长输入和学习鲁棒空间特征的创新扩展模块。我们整合了包含3,161名患者的四个公开数据集进行预训练，并在多项阿尔茨海默病预测任务（包括诊断分类、转化检测和未来转化预测）中验证模型性能。值得注意的是，采用时序预测和对比学习构建的自监督学习模型在七项下游任务中有六项性能超越监督学习。该模型展现出对不同任务、不同输入图像数量及不同时间间隔的适应性与泛化能力，凸显了其在临床应用中具有鲁棒性能的优势。我们已通过https://github.com/emilykaczmarek/SSL-AD公开代码与模型。\n\n（注：翻译过程中对以下专业术语进行了标准化处理：\n1. \"temporal self-supervised learning\" 译为 \"时序自监督学习\"\n2. \"contrastive learning\" 译为 \"对比学习\"\n3. \"generalization across datasets\" 译为 \"跨数据集泛化\"\n4. \"variable-length inputs\" 译为 \"变长输入\"\n5. \"conversion detection\" 译为 \"转化检测\"（特指轻度认知障碍向阿尔茨海默病的转化））"
    },
    {
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "url": "http://arxiv.org/abs/2509.10441v1",
        "pub_date": "2025-09-12",
        "summary": "Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
        "translated": "任意分辨率图像生成技术能够为不同设备提供一致的视觉体验，在内容生产与消费领域具有广泛应用。当前扩散模型的计算需求随分辨率呈平方级增长，导致生成4K图像耗时超过100秒。为解决此问题，我们基于潜在扩散模型展开第二代研究：将扩散模型生成的固定潜在表示作为内容表征，并提出通过单步生成器从紧凑潜在表示解码任意分辨率图像。据此我们提出\\textbf{InfGen}框架——用新型生成器替代VAE解码器，无需重新训练扩散模型即可从固定尺寸潜在表示生成任意分辨率图像。该方案不仅简化流程、降低计算复杂度，还可应用于所有使用相同潜在空间的模型。实验表明，InfGen能将多种模型升级至任意高分辨率时代，同时将4K图像生成时间压缩至10秒以内。\n\n（注：译文采用技术论文摘要的标准表述方式，对专业术语如\"latent diffusion models\"译为\"潜在扩散模型\"、\"VAE decoder\"译为\"VAE解码器\"等保持学术一致性。通过拆分英文长句为符合中文表达习惯的短句结构，如将\"which simplifies...\"独立成句处理。保留关键技术指标\"4K图像生成时间\"的精确数值表述，并采用\"升级至...时代\"等符合中文技术文献风格的比喻修辞。）"
    },
    {
        "title": "Multimodal SAM-adapter for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2509.10408v1",
        "pub_date": "2025-09-12",
        "summary": "Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.",
        "translated": "语义分割作为计算机视觉中的关键任务，在自动驾驶、医学成像和机器人等领域具有广泛应用。随着深度学习的发展，该领域已取得显著进展，但现有方法在光照不足、遮挡和恶劣天气等挑战性条件下仍存在局限性。为解决这些问题，近期出现了融合辅助传感器数据（如激光雷达、红外）的多模态方法，通过提供互补信息增强模型鲁棒性。本文提出MM SAM-adapter这一新型框架，扩展了Segment Anything Model（SAM）在多模态语义分割中的能力。该方法采用适配器网络，将融合的多模态特征注入SAM丰富的RGB特征中。该设计使模型既能保持RGB特征的强泛化能力，又可仅在辅助模态提供额外信息时选择性融合，从而实现多模态信息的高效平衡利用。我们在三个挑战性基准数据集（DeLiVER、FMB和MUSES）上评估本方法，MM SAM-adapter均取得了最先进的性能。为深入分析模态贡献，我们将DeLiVER和FMB划分为RGB简单子集和RGB困难子集。实验结果表明，该框架在优条件和恶劣条件下均优于现有方法，凸显了多模态适配对鲁棒场景理解的有效性。代码已开源：https://github.com/iacopo97/Multimodal-SAM-Adapter。\n\n（注：根据学术规范，翻译中对专业术语进行了标准化处理：  \n- \"Segment Anything Model\"保留英文缩写SAM并补充中文全称\"分割万物模型\"  \n- \"adapter network\"译为\"适配器网络\"  \n- \"state-of-the-art\"译为\"最先进的\"  \n- 数据集名称DeLiVER/FMB/MUSES保留英文原名  \n- 技术概念如\"泛化能力\"、\"多模态特征融合\"等采用计算机视觉领域通用译法）"
    },
    {
        "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards",
        "url": "http://arxiv.org/abs/2509.10407v1",
        "pub_date": "2025-09-12",
        "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC. While deep learning based CVQE has driven significant progress, existing surveys still suffer from limitations: lack of systematic classification linking methods to specific standards and artifacts, insufficient comparative analysis of architectural paradigms across coding types, and underdeveloped benchmarking practices. To address these gaps, this paper presents three key contributions. First, it introduces a novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization. Second, it proposes a unified benchmarking framework integrating modern compression protocols and standard test sequences for fair multi-criteria evaluation. Third, it provides a systematic analysis of the critical trade-offs between reconstruction performance and computational complexity observed in state-of-the-art methods and highlighting promising directions for future research. This comprehensive review aims to establish a foundation for consistent assessment and informed model selection in CVQE research and deployment.",
        "translated": "压缩视频质量增强（CVQE）对于提升H.264/AVC、H.265/HEVC和H.266/VVC等有损视频编解码器的用户体验至关重要。尽管基于深度学习的CVQE技术已取得显著进展，现有综述研究仍存在三大局限：缺乏将方法与特定编码标准及失真类型相连接的系统性分类；对不同编码类型的架构范式对比分析不足；基准测试实践发展不充分。针对这些不足，本文提出三项核心贡献：首先，建立了涵盖架构范式、编码标准和压缩域特征利用的新型CVQE方法分类体系；其次，设计了集成现代压缩协议与标准测试序列的统一基准测试框架，支持多指标公平评估；最后，系统分析了现有最优方法中重建性能与计算复杂度的关键权衡关系，并指明未来研究的重点方向。本综述旨在为CVQE研究与应用提供一致性评估基础和模型选择指导。"
    },
    {
        "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image\n  Decomposition",
        "url": "http://arxiv.org/abs/2509.10388v1",
        "pub_date": "2025-09-12",
        "summary": "Decomposing an image into its intrinsic photometric factors--shading and reflectance--is a long-standing challenge due to the lack of extensive ground-truth data for real-world scenes. Recent methods rely on synthetic data or sparse annotations for limited indoor and even fewer outdoor scenes. We introduce a novel training-free approach for intrinsic image decomposition using only a pair of visible and thermal images. We leverage the principle that light not reflected from an opaque surface is absorbed and detected as heat by a thermal camera. This allows us to relate the ordinalities between visible and thermal image intensities to the ordinalities of shading and reflectance, which can densely self-supervise an optimizing neural network to recover shading and reflectance. We perform quantitative evaluations with known reflectance and shading under natural and artificial lighting, and qualitative experiments across diverse outdoor scenes. The results demonstrate superior performance over recent learning-based models and point toward a scalable path to curating real-world ordinal supervision, previously infeasible via manual labeling.",
        "translated": "图像本征光度因子分解——即将图像分解为 shading（明暗）与 reflectance（反射率）两个组成部分——长期以来因缺乏真实场景的大规模真实标注数据而极具挑战。现有方法多依赖合成数据或针对有限室内场景（室外场景更少）的稀疏标注。本文提出一种无需训练的新方法，仅利用可见光与热成像图像对即可实现本征图像分解。我们基于这样一个原理：不透明表面未反射的光会被吸收，并被热成像相机检测为热量。这使得我们可以通过可见光与热成像强度之间的序数关系，推导出 shading 与 reflectance 的序数关系，进而以此为密集自监督信号优化神经网络，以恢复 shading 和 reflectance。我们在自然光与人造光环境下已知反射率与明暗条件的场景中进行定量评估，并在多样化室外场景中开展定性实验。结果表明，该方法性能优于近期基于学习的模型，并为构建以往通过人工标注难以实现的大规模真实场景序数监督提供了可行路径。"
    },
    {
        "title": "Efficient Learned Image Compression Through Knowledge Distillation",
        "url": "http://arxiv.org/abs/2509.10366v1",
        "pub_date": "2025-09-12",
        "summary": "Learned image compression sits at the intersection of machine learning and image processing. With advances in deep learning, neural network-based compression methods have emerged. In this process, an encoder maps the image to a low-dimensional latent space, which is then quantized, entropy-coded into a binary bitstream, and transmitted to the receiver. At the receiver end, the bitstream is entropy-decoded, and a decoder reconstructs an approximation of the original image. Recent research suggests that these models consistently outperform conventional codecs. However, they require significant processing power, making them unsuitable for real-time use on resource-constrained platforms, which hinders their deployment in mainstream applications. This study aims to reduce the resource requirements of neural networks used for image compression by leveraging knowledge distillation, a training paradigm where smaller neural networks, partially trained on the outputs of larger, more complex models, can achieve better performance than when trained independently. Our work demonstrates that knowledge distillation can be effectively applied to image compression tasks: i) across various architecture sizes, ii) to achieve different image quality/bit rate tradeoffs, and iii) to save processing and energy resources. This approach introduces new settings and hyperparameters, and future research could explore the impact of different teacher models, as well as alternative loss functions. Knowledge distillation could also be extended to transformer-based models. The code is publicly available at: https://github.com/FABallemand/PRIM .",
        "translated": "学习型图像压缩技术处于机器学习与图像处理的交叉领域。随着深度学习的进步，基于神经网络的压缩方法逐渐兴起。该过程中，编码器将图像映射到低维潜在空间，随后进行量化、熵编码生成二进制比特流并传输至接收端。接收端通过熵解码后，由解码器重建原始图像的近似版本。近期研究表明，这类模型持续超越传统编解码器性能，但其需要大量计算资源，难以在资源受限平台上实现实时应用，这阻碍了其在主流场景中的部署。本研究旨在通过知识蒸馏技术降低图像压缩神经网络的资源需求——该训练范式使较小规模的神经网络通过部分学习大型复杂模型的输出，能获得优于独立训练的性能。我们的工作证明知识蒸馏可有效应用于图像压缩任务：i)适用于不同架构规模，ii)实现多样化的图像质量/码率权衡，iii)节省处理能耗。该方法引入了新的配置与超参数，未来研究可探索不同教师模型的影响以及替代损失函数。知识蒸馏技术还可扩展至基于Transformer的模型。代码已开源：https://github.com/FABallemand/PRIM。\n\n（注：根据学术规范，术语\"knowledge distillation\"统一译为\"知识蒸馏\"，\"transformer\"保留英文首字母大写形式表示特定模型架构，\"entropy-coded\"译为\"熵编码\"符合信息论术语标准。译文在保持技术准确性的同时，采用中文科技论文常用的被动语态与长句结构，确保专业性与可读性的平衡。）"
    },
    {
        "title": "Immunizing Images from Text to Image Editing via Adversarial\n  Cross-Attention",
        "url": "http://arxiv.org/abs/2509.10359v1",
        "pub_date": "2025-09-12",
        "summary": "Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.",
        "translated": "近年来，基于文本的图像编辑技术实现了自然语言引导下的精细化视觉内容操控。然而，此类方法易受对抗性攻击的影响。本研究提出了一种针对图像编辑方法视觉组件的新型攻击策略——注意力攻击（Attention Attack）。该攻击通过使用源图像的自动生成描述作为编辑指令的代理，破坏文本提示与图像视觉表征之间的交叉注意力机制。这种方法无需了解具体编辑方法或编辑指令，即可打破图像内容与其文本描述之间的对齐关系。\n\n针对现有免疫成功率评估指标的可靠性问题，我们提出了两种新颖的评估策略：描述相似度（Caption Similarity）——量化原始编辑与对抗编辑之间的语义一致性，以及语义交并比（IoU）——通过分割掩码衡量空间布局的破坏程度。在TEDBench++基准测试上的实验表明，我们的攻击在保持视觉不可感知性的同时，能显著降低图像编辑性能。"
    },
    {
        "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms",
        "url": "http://arxiv.org/abs/2509.10348v1",
        "pub_date": "2025-09-12",
        "summary": "Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings.",
        "translated": "在医疗影像高风险任务中，深度学习模型的过度自信问题尤为突出，尤其在需要同时检测多种共存病变的胸部X光多标签分类场景。本研究提出了一种基于DenseNet-121架构的不确定性感知胸部X光诊断框架，通过集成两种选择性预测机制——基于信息熵的拒斥和基于置信区间的拒斥——使模型能够主动放弃不确定的预测，将存疑病例交由临床专家处理，从而提升系统可靠性。采用分位数校准程序，通过全局或类别特定策略调整拒斥阈值。在三大公共数据集（PadChest、NIH ChestX-ray14和MIMIC-CXR）上的实验表明：选择性拒斥机制优化了诊断准确率与覆盖范围之间的平衡，其中基于信息熵的拒斥方法在所有病理类型中取得了最高平均AUC值。这些研究成果为将选择性预测整合至AI辅助诊断流程提供了支撑，为推动深度学习在临床环境中实现更安全、具有不确定性感知能力的部署迈出实践性一步。\n\n（注：专业术语说明：\n1. AUC：Area Under Curve，指ROC曲线下面积，用于评估分类模型性能\n2. 校准程序：通过调整模型输出概率与真实概率一致性的方法\n3. 多标签分类：每个样本可同时属于多个类别的分类任务\n4. 覆盖范围：模型做出预测的样本比例）"
    },
    {
        "title": "Towards Understanding Visual Grounding in Visual Language Models",
        "url": "http://arxiv.org/abs/2509.10345v1",
        "pub_date": "2025-09-12",
        "summary": "Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research.",
        "translated": "视觉定位指模型根据文本描述在视觉输入中识别对应区域的能力。具备该能力的模型可广泛应用于多个领域：指代表达理解、图像/视频细粒度问答、显式指代实体的视觉描述生成，以及模拟环境与真实环境中的高低层级控制。本综述系统回顾了现代通用视觉语言模型（VLM）在各核心研究方向上的代表性工作。首先阐释视觉定位在VLM中的重要性，进而解析当代 grounded 模型开发范式的核心构成，考察其实际应用场景（包括基准测试与多模态生成评估指标）。同时深入探讨视觉定位、多模态思维链与VLM推理能力之间的多维关联。最后分析视觉定位面临的核心挑战，并展望未来研究的潜在发展方向。"
    },
    {
        "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography",
        "url": "http://arxiv.org/abs/2509.10344v1",
        "pub_date": "2025-09-12",
        "summary": "Mammography screening is an essential tool for early detection of breast cancer. The speed and accuracy of mammography interpretation have the potential to be improved with deep learning methods. However, the development of a foundation visual language model (VLM) is hindered by limited data and domain differences between natural and medical images. Existing mammography VLMs, adapted from natural images, often ignore domain-specific characteristics, such as multi-view relationships in mammography. Unlike radiologists who analyze both views together to process ipsilateral correspondence, current methods treat them as independent images or do not properly model the multi-view correspondence learning, losing critical geometric context and resulting in suboptimal prediction. We propose GLAM: Global and Local Alignment for Multi-view mammography for VLM pretraining using geometry guidance. By leveraging the prior knowledge about the multi-view imaging process of mammograms, our model learns local cross-view alignments and fine-grained local features through joint global and local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED [14], one of the largest open mammography datasets, our model outperforms baselines across multiple datasets under different settings.",
        "translated": "乳腺X线摄影筛查是早期检测乳腺癌的重要工具。借助深度学习方法，有望提升乳腺X线影像判读的速度与准确性。然而，基础视觉语言模型（VLM）的发展受限于数据稀缺以及自然图像与医学图像间的领域差异。现有基于自然图像适配的乳腺X线VLM常忽略领域特异性特征，例如影像中的多视角关联性。与放射科医生同时分析双侧视图以处理同侧对应关系的诊断方式不同，当前方法或将不同视图视为独立图像，或未能有效建模多视角对应学习，导致几何上下文信息丢失及预测性能欠佳。我们提出GLAM模型：基于几何引导的多视角乳腺X线VLM预训练框架，通过全局与局部对齐机制实现多模态学习。该方法利用乳腺X线多视角成像过程的先验知识，通过联合全局-局部、视觉-视觉及视觉-语言的对比学习，实现跨视角局部对齐与细粒度局部特征学习。在最大公开乳腺X线数据集EMBED[14]上完成预训练后，本模型在多种设置下的多个数据集评测中均超越基线方法。\n\n（注：[14]为原文引用的参考文献索引，需保留原始标注格式）"
    },
    {
        "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT",
        "url": "http://arxiv.org/abs/2509.10341v1",
        "pub_date": "2025-09-12",
        "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.",
        "translated": "光学相干断层扫描（OCT）是诊断和监测视网膜疾病的重要成像技术，但其图像本身会受到散斑噪声的干扰，导致细微结构模糊并影响诊断准确性。尽管现有多种去噪方法，但多数难以在噪声抑制与关键解剖结构保留之间取得平衡。本文提出GARD（基于伽马分布的解剖结构恢复与去噪）——一种基于扩散概率模型的新型深度学习OCT图像去噪方法。与传统假设高斯噪声的扩散模型不同，GARD采用去噪扩散伽马模型更精确地匹配散斑噪声的统计特性。此外，我们引入噪声抑制保真项，利用预处理获得的低噪声图像指导去噪过程，这一关键设计能有效避免高频噪声的二次引入。通过将去噪扩散隐式模型框架适配至伽马模型，我们实现了推理过程的加速。在包含噪声与低噪声配对OCT B扫描数据集上的实验表明，GARD在PSNR、SSIM和MSE指标上显著优于传统去噪方法和前沿深度学习模型。定性分析结果证实，GARD能生成更清晰的边缘特征并更好地保留细微解剖结构。\n\n（译文说明：  \n1. 专业术语处理：\"speckle noise\"译为\"散斑噪声\"，\"anatomical structures\"译为\"解剖结构\"，\"diffusion probabilistic models\"译为\"扩散概率模型\"等均采用领域标准译法  \n2. 技术概念转化：\"Denoising Diffusion Gamma Model\"保留核心概念译为\"去噪扩散伽马模型\"，\"Noise-Reduced Fidelity Term\"意译为\"噪声抑制保真项\"  \n3. 长句拆分：将原文复合长句按中文表达习惯拆分为多个短句，如对GARD方法的介绍采用破折号进行补充说明  \n4. 逻辑显化：通过\"这一关键设计\"等表述明确技术改进点的因果关系  \n5. 指标保留：PSNR/SSIM/MSE等专业指标保留英文缩写，符合学术惯例）"
    },
    {
        "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation",
        "url": "http://arxiv.org/abs/2509.10334v1",
        "pub_date": "2025-09-12",
        "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effective strategy to improve efficiency, but ViT-based segmentation models are notoriously fragile under low precision, as quantization errors accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the first fully integer-only ViT segmentation framework. Building on the Segmenter architecture, I-Segmenter systematically replaces floating-point operations with integer-only counterparts. To further stabilize both training and inference, we propose $\\lambda$-ShiftGELU, a novel activation function that mitigates the limitations of uniform quantization in handling long-tailed activation distributions. In addition, we remove the L2 normalization layer and replace bilinear interpolation in the decoder with nearest neighbor upsampling, ensuring integer-only execution throughout the computational graph. Extensive experiments show that I-Segmenter achieves accuracy within a reasonable margin of its FP32 baseline (5.1 % on average), while reducing model size by up to 3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably, even in one-shot PTQ with a single calibration image, I-Segmenter delivers competitive accuracy, underscoring its practicality for real-world deployment.",
        "translated": "视觉变换器（ViTs）在语义分割领域近期取得了显著成果，但由于其高内存占用和计算成本，在资源受限设备上的部署仍受限。量化是提升效率的有效策略，但基于ViT的分割模型在低精度条件下表现脆弱，量化误差会在深度编码器-解码器管道中累积。我们提出I-Segmenter——首个完全整型化的ViT分割框架。该框架基于Segmenter架构，系统地将浮点运算替换为纯整数运算。为进一步稳定训练和推理过程，我们提出新型激活函数$\\lambda$-ShiftGELU，有效缓解均匀量化在处理长尾激活分布时的局限性。此外，我们移除了L2归一化层，并将解码器中的双线性插值替换为最近邻上采样，确保整个计算图实现纯整数运算。大量实验表明，I-Segmenter在保持与FP32基线合理精度差距的前提下（平均差距5.1%），模型尺寸最大减少3.8倍，推理速度提升1.2倍。值得注意的是，即使使用单张校准图像进行一次性后训练量化（PTQ），I-Segmenter仍能保持竞争力精度，凸显其在实际部署中的实用性。\n\n（注：专业术语说明：\n1. PTQ（Post-Training Quantization）：后训练量化\n2. FP32：单精度浮点格式\n3. $\\lambda$-ShiftGELU：保持数学符号原格式\n4. 长尾激活分布：long-tailed activation distributions的标准译法\n5. 整型化：integer-only的规范技术译法）"
    },
    {
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
        "url": "http://arxiv.org/abs/2509.10312v1",
        "pub_date": "2025-09-12",
        "summary": "Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.",
        "translated": "近年来，扩散变换器因其生成高质量图像和视频的能力受到广泛关注，但迭代去噪过程导致其计算成本居高不下。近期提出的特征缓存技术通过缓存前序时间步的特征计算结果并在后续时间步复用，虽利用了扩散模型的时间相似性，却忽略了空间维度的相似性。本文提出聚类驱动特征缓存（ClusCa）方法，作为现有特征缓存技术的正交互补方案。具体而言，ClusCa对每个时间步的令牌进行空间聚类，仅计算每个簇中的一个令牌并将其信息传播至簇内所有其他令牌，可实现令牌数量减少90%以上。在DiT、FLUX和HunyuanVideo上的大量实验表明，该方法在文生图与文生视频任务中均具显著效果。此外，该方案无需训练即可直接应用于任何扩散变换器。例如在FLUX模型上实现4.96倍加速的同时保持99.49%的ImageReward评分，较原始模型提升0.51%。代码已开源：https://github.com/Shenyi-Z/Cache4Diffusion。\n\n（注：译文严格遵循学术论文摘要的规范表述，关键技术术语如\"diffusion transformers\"译为\"扩散变换器\"、\"feature caching\"译为\"特征缓存\"、\"temporal/spatial similarity\"译为\"时间/空间相似性\"等均采用领域内标准译法。长难句按中文习惯拆分重组，如将\"which leverage...\"定语从句转为独立分句。数值精度与超链接等细节均完整保留，符合机器之心等专业科技媒体的技术报道风格。）"
    },
    {
        "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation\n  in Urban Environments",
        "url": "http://arxiv.org/abs/2509.10310v1",
        "pub_date": "2025-09-12",
        "summary": "In this paper we address the problem of precise geolocation of street furniture in complex urban environments, which is a critical task for effective monitoring and maintenance of public infrastructure by local authorities and private stakeholders. To this end, we propose a probabilistic framework based on energy maps that encode the spatial likelihood of object locations. Representing the energy in a map-based geopositioned format allows the optimisation process to seamlessly integrate external geospatial information, such as GIS layers, road maps, or placement constraints, which improves contextual awareness and localisation accuracy. A stochastic birth-and-death optimisation algorithm is introduced to infer the most probable configuration of assets. We evaluate our approach using a realistic simulation informed by a geolocated dataset of street lighting infrastructure in Dublin city centre, demonstrating its potential for scalable and accurate urban asset mapping. The implementation of the algorithm will be made available in the GitHub repository https://github.com/EMurphy0108/SBD_Street_Furniture.",
        "translated": "本文针对复杂城市环境中街道家具的精确定位问题展开研究，该任务对地方政府和私营部门有效监控和维护公共基础设施至关重要。为此，我们提出了一种基于能量地图的概率框架，该框架通过空间似然编码物体位置信息。采用地图式地理定位格式表示能量分布，使优化过程能够无缝集成外部地理空间信息（如GIS图层、道路地图或布局约束），从而提升上下文感知能力与定位精度。我们引入随机生死优化算法来推断最可能的资产配置方案。通过都柏林市中心地理标注路灯基础设施数据集驱动的仿真实验表明，该方法具有可扩展且精准的城市资产测绘潜力。算法实现已开源发布于GitHub仓库：https://github.com/EMurphy0108/SBD_Street_Furniture。\n\n（注：专业术语说明：\n- Street furniture：街道家具（指路灯、长椅等城市街道设施）\n- Energy maps：能量地图（概率模型中表征空间似然分布的视觉化表达）\n- Stochastic birth-and-death：随机生死过程（一种模拟粒子产生与消亡的随机优化方法）\n- GIS layers：地理信息系统图层）"
    },
    {
        "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in\n  Neural Networks",
        "url": "http://arxiv.org/abs/2509.10298v1",
        "pub_date": "2025-09-12",
        "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art performance in computer vision but are highly vulnerable to adversarial perturbations. Standard defenses often incur high computational cost or lack formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath) method, where drop probabilities increase with depth to control the effective Lipschitz constant of the network. This approach regularizes deeper layers, improving robustness while preserving clean accuracy and reducing computation. Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent schedule maintains near-baseline clean accuracy, enhances robustness under FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to baseline and linear DropPath schedules.",
        "translated": "深度神经网络与视觉Transformer在计算机视觉领域已达到最先进性能，但其对对抗性扰动具有高度脆弱性。现有标准防御方法往往伴随高计算成本或缺乏形式化保证。我们提出一种利普希茨约束的随机深度（DropPath）方法，通过随网络深度增加丢弃概率来控制网络的有效利普希茨常数。该方法对深层进行正则化处理，在保持原始精度的同时提升鲁棒性并降低计算量。在CIFAR-10数据集上使用ViT-Tiny架构的实验表明：我们提出的深度自适应丢弃策略在保持接近基线水平的原始精度前提下，显著提升了模型在FGSM、PGD-20和AutoAttack攻击下的鲁棒性，且相比基线方法和线性DropPath策略，计算量（FLOPs）显著降低。\n\n（注：专业术语说明：\n1. Lipschitz constant：利普希茨常数，衡量函数敏感度的数学概念\n2. FGSM/PGD-20/AutoAttack：主流的对抗攻击方法\n3. FLOPs：浮点运算次数，计算复杂度衡量指标\n4. ViT-Tiny：轻量级视觉Transformer架构）"
    },
    {
        "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly\n  Detection",
        "url": "http://arxiv.org/abs/2509.10282v1",
        "pub_date": "2025-09-12",
        "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects without relying on labeled training data, making it especially valuable in scenarios constrained by data scarcity, privacy, or high annotation cost. However, most existing methods focus exclusively on point clouds, neglecting the rich semantic cues available from complementary modalities such as RGB images and texts priors. This paper introduces MCL-AD, a novel framework that leverages multimodal collaboration learning across point clouds, RGB images, and texts semantics to achieve superior zero-shot 3D anomaly detection. Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that enhances the intra-modal representation capability and inter-modal collaborative learning by introducing an object-agnostic decoupled text prompt and a multimodal contrastive loss. In addition, a collaborative modulation mechanism (CMM) is proposed to fully leverage the complementary representations of point clouds and RGB images by jointly modulating the RGB image-guided and point cloud-guided branches. Extensive experiments demonstrate that the proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D anomaly detection.",
        "translated": "零样本三维（ZS-3D）异常检测旨在无需依赖标注训练数据的情况下识别三维物体缺陷，这一特性使其在数据稀缺、隐私受限或标注成本高昂的场景中具有重要价值。然而现有方法大多仅关注点云数据，忽略了RGB图像和文本先验等互补模态所提供的丰富语义线索。本文提出MCL-AD——一种通过点云、RGB图像与文本语义的多模态协作学习来实现卓越零样本三维异常检测的新框架。具体而言，我们设计了多模态提示学习机制（MPLM），通过引入对象无关的解耦文本提示和多模态对比损失，有效增强了模态内表征能力与模态间协作学习。此外，我们提出协作调制机制（CMM），通过联合调控RGB图像引导分支与点云引导分支，充分挖掘点云与RGB图像的互补表征优势。大量实验表明，所提出的MCL-AD框架在零样本三维异常检测任务中达到了最先进的性能水平。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语标准化：\"zero-shot\"译为\"零样本\"，\"point clouds\"译为\"点云\"，\"multimodal contrastive loss\"译为\"多模态对比损失\"\n2. 技术概念准确传达：\"object-agnostic decoupled text prompt\"译为\"对象无关的解耦文本提示\"以保持技术精确性\n3. 学术论文句式规范：采用中文科技论文常用的被动语态和长句结构\n4. 逻辑连接词处理：将\"Specifically\"、\"In addition\"等衔接词自然转化为\"具体而言\"、\"此外\"等中文对应表达\n5. 技术动作描述：\"jointly modulating\"译为\"联合调控\"以符合控制领域的术语习惯）"
    },
    {
        "title": "Detecting Text Manipulation in Images using Vision Language Models",
        "url": "http://arxiv.org/abs/2509.10278v1",
        "pub_date": "2025-09-12",
        "summary": "Recent works have shown the effectiveness of Large Vision Language Models (VLMs or LVLMs) in image manipulation detection. However, text manipulation detection is largely missing in these studies. We bridge this knowledge gap by analyzing closed- and open-source VLMs on different text manipulation datasets. Our results suggest that open-source models are getting closer, but still behind closed-source ones like GPT- 4o. Additionally, we benchmark image manipulation detection-specific VLMs for text manipulation detection and show that they suffer from the generalization problem. We benchmark VLMs for manipulations done on in-the-wild scene texts and on fantasy ID cards, where the latter mimic a challenging real-world misuse.",
        "translated": "近期研究表明，大型视觉语言模型（VLM或LVLM）在图像篡改检测领域展现出显著成效。然而，现有研究对文本篡改检测的关注明显不足。为填补这一知识空白，我们针对不同文本篡改数据集对开源与闭源VLM进行了系统性分析。实验结果表明，开源模型虽正逐步逼近，但仍落后于GPT-4o等闭源模型。此外，我们首次对专攻图像篡改检测的VLM进行文本篡改检测能力评估，发现其存在泛化能力不足的问题。我们进一步测试了VLM在自然场景文本篡改和仿造身份证篡改场景下的表现，后者模拟了现实世界中极具挑战性的恶意篡改行为。"
    },
    {
        "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by\n  Multimodal Contrastive Fusion",
        "url": "http://arxiv.org/abs/2509.10266v1",
        "pub_date": "2025-09-12",
        "summary": "Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
        "translated": "手语翻译（SLT）旨在从手语视频中还原自然语言，是促进包容性沟通的重要桥梁。尽管当前研究利用强大的视觉主干网络和大语言模型取得了进展，但现有方法主要关注手动信号（手势），往往忽略了嘴部动作等非手动线索。事实上，嘴部动作承载着手语中关键的语言学信息，对于消除视觉相似手势的歧义具有重要作用。本文提出创新框架SignClip，通过融合手动与非手动线索（具体表现为空间手势特征与唇部运动特征）来提升手语翻译的准确性。此外，SignClip引入了具有多层级对齐目标的分层对比学习框架，确保手语-唇部模态与视觉-文本模态间的语义一致性。在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了本方法的优越性。以PHOENIX14T为例，在无注释文本（Gloss-free）的设置下，SignClip超越了此前最优模型SpaMo，将BLEU-4指标从24.32提升至24.71，ROUGE指标从46.57提升至48.38。\n\n（注：根据学术规范，关键术语保留英文原名：SignClip, PHOENIX14T, How2Sign, SpaMo, BLEU-4, ROUGE；专业表述如\"Gloss-free\"采用\"无注释文本\"的译法并括号标注原词；技术概念\"分层对比学习框架\"、\"多层级对齐目标\"等均按计算机领域术语标准进行准确转译。）"
    },
    {
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained\n  Artifacts Assessment in Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2509.10260v1",
        "pub_date": "2025-09-12",
        "summary": "Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.",
        "translated": "文本到图像（T2I）生成技术在指令遵循与美学表现方面取得了显著进展，但始终存在的物理伪影问题——如解剖结构和形态缺陷——严重降低了感知质量并限制了实际应用。鉴于这类伪影的多样性和复杂性，当前缺乏系统化的细粒度评估框架。为此，我们推出MagicMirror综合伪影评估框架：首先建立了生成图像伪影的细粒度分类体系；基于该体系，人工标注了首个大规模人工标注数据集MagicData340K，包含34万张生成图像及其细粒度伪影标签；在此基础上训练了视觉语言模型MagicAssessor，可提供详细评估结果与对应标签。针对类别不平衡和奖励破解等挑战，我们设计了新颖的数据采样策略和基于群组相对策略优化（GRPO）的多级奖励机制。最终利用MagicAssessor构建了自动化评测基准MagicBench，用于评估现有T2I模型的图像伪影问题。通过MagicBench的测试发现，即使如GPT-image-1等顶级模型仍持续存在显著伪影，这表明伪影消减将是T2I技术未来发展的重要攻坚方向。项目页面：https://wj-inf.github.io/MagicMirror-page/。\n\n（注：根据学术惯例，对模型名称\"GPT-image-1\"保留英文原文，因该名称可能指代特定技术版本，直译可能导致歧义。其余专业术语如GRPO、VLM等均采用学界通用译法。）"
    },
    {
        "title": "Mask Consistency Regularization in Object Removal",
        "url": "http://arxiv.org/abs/2509.10259v1",
        "pub_date": "2025-09-12",
        "summary": "Object removal, a challenging task within image inpainting, involves seamlessly filling the removed region with content that matches the surrounding context. Despite advancements in diffusion models, current methods still face two critical challenges. The first is mask hallucination, where the model generates irrelevant or spurious content inside the masked region, and the second is mask-shape bias, where the model fills the masked area with an object that mimics the mask's shape rather than surrounding content. To address these issues, we propose Mask Consistency Regularization (MCR), a novel training strategy designed specifically for object removal tasks. During training, our approach introduces two mask perturbations: dilation and reshape, enforcing consistency between the outputs of these perturbed branches and the original mask. The dilated masks help align the model's output with the surrounding content, while reshaped masks encourage the model to break the mask-shape bias. This combination of strategies enables MCR to produce more robust and contextually coherent inpainting results. Our experiments demonstrate that MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal.",
        "translated": "图像修复中的目标移除任务具有较高挑战性，其核心在于如何使填充区域与周边语境无缝融合。尽管扩散模型已取得显著进展，现有方法仍面临两大关键难题：一是掩码幻觉问题（模型在掩码区域内生成无关或虚假内容），二是掩码形状偏差（模型倾向于生成与掩码形状相似而非与周边内容协调的对象）。为解决这些问题，我们提出掩码一致性正则化（MCR）——一种专为目标移除任务设计的新型训练策略。在训练过程中，我们通过扩张和变形两种掩码扰动方式，强制要求扰动分支的输出与原始掩码保持一致性。扩张掩码促使模型输出与周边内容对齐，而变形掩码则帮助打破掩码形状偏差。这种组合策略使MCR能够生成更具鲁棒性和语境连贯性的修复结果。实验表明，MCR能显著减少幻觉现象和掩码形状偏差，有效提升目标移除任务的性能表现。"
    },
    {
        "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain\n  MRI",
        "url": "http://arxiv.org/abs/2509.10257v1",
        "pub_date": "2025-09-12",
        "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (&gt;90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.",
        "translated": "胎儿脑部磁共振成像（MRI）依赖快速多视角二维切片采集以减少胎儿运动导致的运动伪影。然而这些图像堆栈通常分辨率较低，可能受运动伪影影响，且无法充分呈现三维解剖结构。超分辨率重建（SRR）方法通过结合切片-体积配准与超分辨率技术，旨在生成高分辨率（HR）三维体积图像以解决这些局限性。虽然已有多种SRR方法被提出，但其在病理案例中的性能对比及其对下游体积分析与诊断任务的影响仍未得到充分探索。本研究将三种先进SRR方法——NiftyMIC、SVRTK和NeSVoR——应用于140例胎儿脑部MRI扫描数据（包含健康对照组HC和脑室扩大VM病理组PC）。通过BoUNTi算法对每个HR重建结果进行分割，提取九个主要脑结构的体积数据。我们评估了视觉质量、SRR成功率、体积测量一致性及诊断分类性能。NeSVoR在HC和PC组均展现出最高且最稳定的重建成功率（＞90%）。尽管不同SRR方法间的体积估计存在显著差异，但VM的诊断分类性能未受SRR方法选择的影响。这些发现凸显了NeSVoR的鲁棒性，以及诊断性能对SRR引起的体积变异具有耐受性。\n\n（注：专业术语说明：\n1. SRR (Super-resolution reconstruction): 超分辨率重建\n2. NiftyMIC/SVRTK/NeSVoR: 保留原始算法名称\n3. Ventriculomegaly (VM): 脑室扩大\n4. BoUNTi: 专用分割算法名称\n5. Volumetric measurement: 体积测量\n6. Reconstruction success rate: 重建成功率）"
    },
    {
        "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented\n  Training for AI-Generated Image Detection",
        "url": "http://arxiv.org/abs/2509.10250v1",
        "pub_date": "2025-09-12",
        "summary": "With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.",
        "translated": "随着生成模型日益复杂多样，AI生成图像的检测面临严峻挑战。现有检测器虽然在分布内生成图像上表现良好，但对未知生成模型的泛化能力仍显不足，这主要源于其对生成特异性伪影（如风格先验和压缩模式）的过度依赖。为解决这一局限，我们提出GAMMA训练框架，通过降低域偏差和增强语义对齐来提升泛化能力。该框架引入多样化操纵策略，包括基于修复的图像操纵和语义保持扰动，确保被操纵内容与真实内容的一致性。我们采用双分割头与分类头的多任务监督机制，实现跨生成域的像素级来源归因。此外，通过引入反向交叉注意力机制，使分割头能够引导并校正分类分支中的偏差表征。本方法在GenImage基准测试中取得了最先进的泛化性能，准确率提升5.8%，并对GPT-4o等最新生成模型保持强劲的鲁棒性。\n\n（注：译文严格遵循学术论文摘要的规范表述，关键技术术语如\"inpainting-based manipulation\"译为\"基于修复的图像操纵\"，\"semantics-preserving perturbations\"译为\"语义保持扰动\"，\"reverse cross-attention mechanism\"译为\"反向交叉注意力机制\"等均采用计算机视觉领域标准译法。在保持原文信息密度的同时，通过拆分长句、调整语序确保中文表达流畅性，如将英文复合句\"While existing...\"转化为\"虽然...但...\"的中文转折结构。）"
    },
    {
        "title": "On the Geometric Accuracy of Implicit and Primitive-based\n  Representations Derived from View Rendering Constraints",
        "url": "http://arxiv.org/abs/2509.10241v1",
        "pub_date": "2025-09-12",
        "summary": "We present the first systematic comparison of implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction, evaluating the role of appearance embeddings. While embeddings improve photometric fidelity by modeling lighting variation, we show they do not translate into meaningful gains in geometric accuracy - a critical requirement for space robotics applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian Splatting, and Convex Splatting, and demonstrate that embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Moreover, convex splatting achieves more compact and clutter-free representations than Gaussian splatting, offering advantages for safety-critical applications such as interaction and collision avoidance. Our findings clarify the limits of appearance embeddings for geometry-centric tasks and highlight trade-offs between reconstruction quality and representation efficiency in space scenarios.",
        "translated": "我们首次系统比较了基于隐式与显式表示的新视角合成方法在太空三维物体重建中的表现，重点评估了外观嵌入向量的作用。研究发现，虽然嵌入向量通过建模光照变化提升了光度保真度，但并未转化为几何精度的实质性提升——这对太空机器人应用至关重要。基于SPEED+数据集，我们对K-Planes、高斯泼溅和凸泼溅三种方法进行对比，证明嵌入向量主要作用是减少显式方法所需的图元数量，而非提升几何精度。值得注意的是，凸泼溅相较于高斯泼溅能实现更紧凑且无杂波的表示，为交互和避碰等安全关键场景提供优势。本研究阐明了外观嵌入在几何中心任务中的局限性，并揭示了太空场景下重建质量与表示效率之间的权衡关系。"
    },
    {
        "title": "LayerLock: Non-collapsing Representation Learning with Progressive\n  Freezing",
        "url": "http://arxiv.org/abs/2509.10156v1",
        "pub_date": "2025-09-12",
        "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from \"representation collapse\". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.",
        "translated": "我们提出LayerLock——一种简单而高效的自监督视觉表示学习方法，该方法通过渐进式层级冻结实现从像素预测到潜在预测的平滑过渡。首先，我们观察到在视频掩码自编码（MAE）模型训练过程中，ViT网络层会按其深度顺序收敛：浅层较早收敛，深层较晚收敛。基于这一发现，我们通过在整个训练过程中按照明确计划逐步冻结模型参数，成功加速了标准MAE的训练过程。此外，该冻结计划可应用于简单可扩展的潜在预测方法，且能有效避免\"表示坍塌\"问题。我们将所提出的LayerLock方法应用于参数量高达40亿的大型模型，在4DS感知评估体系上的表现超越了非潜在掩码预测方法。\n\n（注：根据学术规范，\"4B parameters\"译为\"40亿参数\"；\"representation collapse\"专业术语译为\"表示坍塌\"；\"4DS perception suite\"保留英文缩写并补充说明为评估体系；整体表述在保持专业性的同时符合中文科技论文的表述习惯。）"
    },
    {
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook\n  Utilization",
        "url": "http://arxiv.org/abs/2509.10140v1",
        "pub_date": "2025-09-12",
        "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.",
        "translated": "向量量化（VQ）是图像生成中离散分词器的核心组件，但其训练常因直通估计偏差、滞后更新和码本梯度稀疏性问题而不稳定，导致重建性能欠佳和码本利用率低下。本研究系统分析了这些根本性挑战，并提出了一种简洁有效的解决方案。为在学习退火和码本扩增过程中维持VQ网络（VQN）的高码本利用率，我们基于映射函数方法提出了VQBridge——一个鲁棒、可扩展且高效的投影器。该模块通过压缩-处理-恢复的三段式流程优化码向量，实现稳定高效的码本训练。将VQBridge与学习退火结合后，我们的VQN在不同码本配置下均实现了100%的码本利用率（称为FVQ/FullVQ）。大量实验表明：FVQ具备高效性、可扩展性与泛化性——即使在26.2万超大码本下仍保持100%利用率，获得最先进的重建性能，其效果随码本规模、向量通道数或训练时长增加持续提升，且适用于不同VQ变体。当与LlamaGen结合时，FVQ显著提升图像生成质量，以0.5 rFID优势超越视觉自回归模型（VAR），以0.2 rFID领先扩散模型（DiT），印证了高质量分词器对强自回归图像生成系统的关键作用。"
    },
    {
        "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature\n  Disalignment",
        "url": "http://arxiv.org/abs/2509.10134v1",
        "pub_date": "2025-09-12",
        "summary": "Accurate segmentation of the optic disc and cup is critical for the early diagnosis and management of ocular diseases such as glaucoma. However, segmentation models trained on one dataset often suffer significant performance degradation when applied to target data acquired under different imaging protocols or conditions. To address this challenge, we propose \\textbf{Grad-CL}, a novel source-free domain adaptation framework that leverages a pre-trained source model and unlabeled target data to robustly adapt segmentation performance without requiring access to the original source data. Grad-CL combines a gradient-guided pseudolabel refinement module with a cosine similarity-based contrastive learning strategy. In the first stage, salient class-specific features are extracted via a gradient-based mechanism, enabling more accurate uncertainty quantification and robust prototype estimation for refining noisy pseudolabels. In the second stage, a contrastive loss based on cosine similarity is employed to explicitly enforce inter-class separability between the gradient-informed features of the optic cup and disc. Extensive experiments on challenging cross-domain fundus imaging datasets demonstrate that Grad-CL outperforms state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and improved boundary delineation. Project and code are available at https://visdomlab.github.io/GCL/.",
        "translated": "准确分割视盘和视杯对于青光眼等眼部疾病的早期诊断与管理至关重要。然而，在单一数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会出现显著的性能下降。为解决这一挑战，我们提出**Grad-CL**——一种新颖的无源域自适应框架，该框架利用预训练的源模型和未标注目标数据，在无需访问原始源数据的情况下实现鲁棒的分割性能自适应。Grad-CL融合了梯度引导的伪标签优化模块与基于余弦相似度的对比学习策略。在第一阶段，通过梯度机制提取显著类别特征，实现更准确的不确定性量化和鲁棒的原型估计，从而优化噪声伪标签。在第二阶段，采用基于余弦相似度的对比损失函数，显式增强视杯与视盘梯度特征间的类间分离性。在具有挑战性的跨域眼底成像数据集上的大量实验表明，Grad-CL在分割精度和边界描绘方面均优于当前最先进的无监督和无源域自适应方法。项目及代码详见：https://visdomlab.github.io/GCL/。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语统一：\"optic disc/cup\"译为\"视盘/视杯\"，\"domain adaptation\"译为\"域自适应\"\n2. 技术概念准确传达：\"gradient-guided pseudolabel refinement\"译为\"梯度引导的伪标签优化\"，\"contrastive learning\"译为\"对比学习\"\n3. 被动语态转换：将英文被动结构转换为中文主动表达（如\"are extracted\"译为\"提取\"）\n4. 长句拆分：将复合长句按中文表达习惯分解为多个短句\n5. 逻辑连接词优化：使用\"通过\"、\"从而\"等词保持技术论述的连贯性）"
    },
    {
        "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
        "url": "http://arxiv.org/abs/2509.10122v1",
        "pub_date": "2025-09-12",
        "summary": "Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.",
        "translated": "预训练扩散模型通过实现高分辨率重建，在真实图像超分辨率（Real-ISR）任务中展现出巨大潜力。虽然单步扩散（OSD）方法相比传统多步方法显著提升了效率，但在多样化场景中平衡保真度与真实感方面仍存在局限。由于超分辨率OSD方法通常基于单一时间步进行训练或蒸馏，其缺乏灵活的调控机制来自适应地权衡这两个相互竞争的目标——而这在多步方法中本可通过调整采样步数实现。为应对这一挑战，我们提出了一种用于Real-ISR的真实感可控单步扩散框架（RCOD）。该框架提出潜在域分组策略，通过在噪声预测阶段施加显式控制，以最小化训练范式修改和原始训练数据需求的方式实现保真度-真实感的权衡调控。同时引入退化感知采样策略，使蒸馏正则化与分组策略对齐以增强控制效果。此外，通过视觉提示注入模块将传统文本提示替换为退化感知的视觉标记，既提升了复原精度又增强了语义一致性。本方法在保持计算效率的同时实现了卓越的保真度与感知质量。大量实验表明，RCOD在定量指标与视觉质量上均优于最先进的OSD方法，且在推理阶段具备灵活的真实感控制能力。代码将开源发布。\n\n（注：专业术语说明：\n1. Real-ISR：真实图像超分辨率（Real-world Image Super-Resolution）\n2. OSD：单步扩散（One-Step Diffusion）\n3. RCOD：真实感可控单步扩散（Realism Controlled One-step Diffusion）\n4. Fidelity-realism trade-offs：保真度与真实感的权衡\n5. Degradation-aware：退化感知\n6. Visual tokens：视觉标记）"
    },
    {
        "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with\n  Correlation-Aware Loss",
        "url": "http://arxiv.org/abs/2509.10114v1",
        "pub_date": "2025-09-12",
        "summary": "Face image quality assessment (FIQA) plays a critical role in face recognition and verification systems, especially in uncontrolled, real-world environments. Although several methods have been proposed, general-purpose no-reference image quality assessment techniques often fail to capture face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be computationally intensive, limiting their practical applicability. We propose a lightweight and efficient method for FIQA, designed for the perceptual evaluation of face images in the wild. Our approach integrates an ensemble of two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2, with prediction-level fusion via simple averaging. To enhance alignment with human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss), combining mean squared error (MSE) with a Pearson correlation regularizer. Our method achieves a strong balance between accuracy and computational cost, making it suitable for real-world deployment. Experiments on the VQualA FIQA benchmark demonstrate that our model achieves a Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894, remaining within competition efficiency constraints.",
        "translated": "人脸图像质量评估（FIQA）在人脸识别与验证系统中具有关键作用，尤其在非受控的真实场景下。尽管已有多种方法被提出，但通用无参考图像质量评估技术往往难以捕捉人脸特有的质量退化特征。与此同时，先进的FIQA模型通常计算复杂度较高，限制了实际应用。本文提出一种轻量高效的人脸图像质量评估方法，专注于真实场景下人脸图像的感知质量评价。该方法集成两个紧凑型卷积神经网络——MobileNetV3-Small与ShuffleNetV2，通过简单平均实现预测级融合。为提升与人类感知判断的一致性，我们采用结合均方误差（MSE）与皮尔逊相关性正则项的关联感知损失函数（MSECorrLoss）。该方法在精度与计算成本之间实现了良好平衡，具备实际部署价值。在VQualA FIQA基准测试中，我们的模型取得了0.9829的斯皮尔曼等级相关系数（SRCC）和0.9894的皮尔逊线性相关系数（PLCC），同时满足竞赛效率约束条件。\n\n（注：专业术语说明：\n1. FIQA：Face Image Quality Assessment 人脸图像质量评估\n2. No-reference image quality assessment：无参考图像质量评估\n3. MobileNetV3-Small/ShuffleNetV2：轻量化卷积神经网络架构\n4. MSECorrLoss：均方误差相关性混合损失函数\n5. SRCC：斯皮尔曼等级相关系数\n6. PLCC：皮尔逊线性相关系数）"
    },
    {
        "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum,\n  and Acceleration",
        "url": "http://arxiv.org/abs/2509.10439v1",
        "pub_date": "2025-09-12",
        "summary": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to settings where we use momentum in the outer optimizer, and we show a similar role for the momentum-adjusted outer learning rate. We also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, we also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.",
        "translated": "现代机器学习通常需要采用大批次训练、分布式数据以及大规模并行计算硬件（如移动设备与其他边缘设备或分布式数据中心）。在此类场景中，通信成为主要瓶颈，而局部随机梯度下降（Local SGD）等方法在降低额外通信开销方面展现出巨大潜力。Local SGD包含三个核心组件：局部优化过程、聚合机制以及利用节点聚合更新生成新模型的外部优化器。尽管现有文献对局部优化过程中超参数的影响已有广泛研究，但对外部优化器及其超参数的选择仍缺乏明确指导。本文系统研究了Local SGD中外部优化器的作用，并为该算法提出了新的收敛性保证。具体而言，我们证明通过调整外部学习率可以实现：（a）在优化误差与随机梯度噪声方差之间取得平衡；（b）弥补内部学习率设置不当的影响。理论分析表明，外部学习率有时应设置为大于$1$的数值。我们将结论扩展至外部优化器使用动量机制的场景，并揭示了动量调整后外部学习率的类似作用。同时研究了外部优化器的加速机制，证明其能通过减少通信轮数提升收敛速率，优于现有局部加速算法的收敛表现。此外，本文还提出了一种新颖的数据依赖性分析方法，为外部学习率调优提供更深层见解。我们通过标准语言模型与多种外部优化器的综合实验验证了理论成果。\n\n（注：翻译严格遵循以下技术规范：\n1. 专业术语标准化：Local SGD译为\"局部随机梯度下降\"，momentum译为\"动量机制\"\n2. 数学符号保留：$1$保持原格式\n3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句\n4. 被动语态转换：\"it is shown that\"转化为主动式\"证明\"\n5. 逻辑连接强化：使用\"具体而言\"\"同时\"\"此外\"等连接词保持论证连贯性\n6. 技术概念准确传达：如\"stochastic gradient noise variance\"译为\"随机梯度噪声方差\"）"
    },
    {
        "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.10423v1",
        "pub_date": "2025-09-12",
        "summary": "Reinforcement Learning (RL) agents deployed in real-world environments face degradation from sensor faults, actuator wear, and environmental shifts, yet lack intrinsic mechanisms to detect and diagnose these failures. We present an information-theoretic framework that reveals both the fundamental dynamics of RL and provides practical methods for diagnosing deployment-time anomalies. Through analysis of state-action mutual information patterns in a robotic control task, we first demonstrate that successful learning exhibits characteristic information signatures: mutual information between states and actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing state entropy, indicating that agents develop increasingly selective attention to task-relevant patterns. Intriguingly, states, actions and next states joint mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during early learning before declining as the agent specializes suggesting a transition from broad exploration to efficient exploitation. More immediately actionable, we show that information metrics can differentially diagnose system failures: observation-space, i.e., states noise (sensor faults) produces broad collapses across all information channels with pronounced drops in state-action coupling, while action-space noise (actuator faults) selectively disrupts action-outcome predictability while preserving state-action relationships. This differential diagnostic capability demonstrated through controlled perturbation experiments enables precise fault localization without architectural modifications or performance degradation. By establishing information patterns as both signatures of learning and diagnostic for system health, we provide the foundation for adaptive RL systems capable of autonomous fault detection and policy adjustment based on information-theoretic principles.",
        "translated": "在现实环境中部署的强化学习（RL）智能体面临传感器故障、执行器磨损和环境变化导致的性能退化问题，但缺乏检测和诊断这些故障的内在机制。我们提出了一种信息理论框架，既揭示了强化学习的本质动力学特性，又为部署时异常诊断提供了实用方法。通过在机器人控制任务中对状态-动作互信息模式的分析，我们首次发现成功学习会呈现特征性信息特征：尽管状态熵持续增长，状态与动作间的互信息从0.84比特稳步提升至2.83比特（增长238%），表明智能体对任务相关模式逐渐形成选择性注意力。更有趣的是，状态、动作与下一状态的联合互信息MI(S,A;S')呈现倒U型曲线，在早期学习阶段达到峰值后随智能体专业化而下降，这暗示了从广泛探索到高效利用的转变。更具直接应用价值的是，我们证明信息指标能差异化诊断系统故障：观测空间（即状态）噪声（传感器故障）会导致所有信息通道的广泛崩溃，尤其表现为状态-动作耦合的显著下降；而动作空间噪声（执行器故障）则选择性地破坏动作-结果预测性，同时保持状态-动作关系。通过受控扰动实验验证的这种差异化诊断能力，可在不修改系统架构或降低性能的前提下实现精确故障定位。通过将信息模式确立为学习特征和系统健康诊断指标，我们为构建自适应强化学习系统奠定了基础，这类系统能够基于信息理论原理实现自主故障检测与策略调整。"
    },
    {
        "title": "Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining",
        "url": "http://arxiv.org/abs/2509.10419v1",
        "pub_date": "2025-09-12",
        "summary": "Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability.",
        "translated": "随着计算机化铁路系统的复杂性与关键性日益提升，确保其运行韧性对应对不确定性及系统变化至关重要。尽管这类系统软件遵循成熟的最佳实践和认证标准，并经过严格的验证与确认流程，但在运行时仍可能因设计阶段未知的残余故障、系统及环境变更或其他新兴网络威胁场景而出现异常。本文探讨基于过程挖掘的运行时控制流异常检测方法，以增强ERTMS/ETCS L2（欧洲铁路交通管理系统/欧洲列车控制系统二级）的韧性。过程挖掘技术能够从系统执行轨迹中学习实际控制流，进而通过在线一致性检查实现运行时监控。此外，采用无监督机器学习进行异常定位，将关键偏差与系统核心组件关联。我们在典型ERTMS/ETCS L2场景——RBC/RBC移交过程中测试该方法，结果表明其能以高精度、高效率及可解释性实现异常检测与定位。\n\n（注：ERTMS/ETCS为欧洲铁路标准系统，RBC指无线闭塞中心）"
    },
    {
        "title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention\n  for Pretraining",
        "url": "http://arxiv.org/abs/2509.10406v1",
        "pub_date": "2025-09-12",
        "summary": "We present Multipole Semantic Attention (MuSe), an efficient approximation of softmax attention that combines semantic clustering with multipole expansions from computational physics. Our method addresses the quadratic computational complexity of transformers in the context length by clustering queries and keys separately in their learned representation spaces, enabling a hierarchical two-stage attention mechanism. Unlike prior clustering approaches that group only keys or use unified clustering, we maintain separate clusterings that respect attention's asymmetric treatment of these spaces. We augment centroid-based (monopole) approximations with dipole corrections that capture directional variance within clusters, preserving richer information during training. The method operates as a drop-in replacement for standard attention, requiring only hyperparameter specification without architectural modifications. Our approach achieves $\\mathcal{O}(NCD)$ complexity for acausal attention with $C$ clusters and $\\mathcal{O}(NCD \\log N)$ for causal attention. On isolated attention layers, we demonstrate $3\\times$ speedup over CUDNN Flash Attention at 8k context length, with relative squared errors below 20%. For causal attention, we develop a hierarchical block decomposition that combines exact local computation with efficient long-range approximation. In end-to-end pretraining of a 30M parameter model on book-length texts with 16k context, we achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing the viability of multipole approximations for efficient transformer pretraining.",
        "translated": "我们提出多极语义注意力（MuSe）——一种结合语义聚类与计算物理学中多极展开的softmax注意力高效近似方法。该方法通过在学习到的表示空间中对查询和键进行独立聚类，构建分层两阶段注意力机制，解决了Transformer在上下文长度上的二次计算复杂度问题。与先前仅对键进行聚类或采用统一聚类的方法不同，我们保持独立的聚类方式以尊重注意力机制对这两个空间的不对称处理。我们在基于质心（单极）近似的基础上引入偶极校正项，以捕捉聚类内的方向方差，在训练过程中保留更丰富的信息。该方法可作为标准注意力的即插即用替代方案，仅需指定超参数而无需修改架构。我们的方法在非因果注意力中实现$\\mathcal{O}(NCD)$复杂度（C为聚类数），在因果注意力中实现$\\mathcal{O}(NCD \\log N)$复杂度。在孤立注意力层的测试中，当上下文长度为8k时，相比CUDNN Flash Attention实现3倍加速，相对平方误差低于20%。针对因果注意力，我们开发了结合精确局部计算与高效长程近似的分层块分解策略。在16000上下文长度的书籍文本上对3000万参数模型进行端到端预训练时，实现了12.2%的训练时间缩减，仅产生0.36%的性能损失，证实了多极近似在高效Transformer预训练中的可行性。\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语准确对应：multipole expansions→多极展开；asymmetric treatment→不对称处理；dipole corrections→偶极校正项\n2. 数学复杂度表示保持原符号格式\n3. 技术指标完整保留：3000万参数/16k上下文/0.36%损失等数值精确转换\n4. 算法特性准确传达：hierarchical block decomposition→分层块分解；end-to-end pretraining→端到端预训练\n5. 保持学术论文的正式语体与逻辑严密性）"
    },
    {
        "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
        "url": "http://arxiv.org/abs/2509.10396v1",
        "pub_date": "2025-09-12",
        "summary": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.",
        "translated": "掩码扩散大语言模型（dLLMs）正逐渐成为自回归大语言模型的有力替代方案，其在保持竞争力的性能表现的同时，还支持修复生成等独特能力。本文探讨了如何利用修复生成能力指导dLLMs的强化学习算法设计。大语言模型与强化学习的对齐面临探索挑战：稀疏的奖励信号以及模型未能发现正确解决方案时产生的样本浪费。虽然这种低效问题普遍存在于大语言模型中，但dLLMs提供了独特机遇——其修复生成能力可引导探索过程。我们提出了IGPO（修复引导策略优化）框架，该强化学习框架在在线采样过程中策略性地插入部分真实推理轨迹。与提供完整解决方案不同，修复生成在保持自主推理的同时将探索导向有潜力的轨迹空间，从而桥接监督微调与强化学习。我们将IGPO应用于基于群体的优化方法（如GRPO），这类方法在探索失败时会出现零优势值与梯度消失问题。IGPO在提升样本效率的同时恢复了有意义的梯度更新。我们还提出对合成重写的简洁轨迹进行监督微调，使其更契合dLLMs的生成模式。通过结合基于熵的过滤等技术，我们的训练方案在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLMs创造了新的性能标杆。"
    },
    {
        "title": "Vendi Information Gain for Active Learning and its Application to\n  Ecology",
        "url": "http://arxiv.org/abs/2509.10390v1",
        "pub_date": "2025-09-12",
        "summary": "While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG achieves impressive predictive accuracy close to full supervision using less than 10% of the labels. It consistently outperforms standard baselines across metrics and batch sizes, collecting more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments.",
        "translated": "在生态学研究中，利用相机陷阱监测生物多样性已成为重要手段，但由于标注资源有限，从捕获图像中识别物种仍是主要瓶颈。主动学习——一种选择最具信息价值的数据进行标注以训练预测模型的机器学习范式——提供了有前景的解决方案，但传统方法通常只关注单个预测的不确定性，而忽略了整个数据集范围的不确定性。我们提出了一种新的主动学习策略——Vendi信息增益（VIG），该策略基于图像对全数据集预测不确定性的影响进行选择，同时捕捉信息量与多样性。在Snapshot Serengeti数据集上的实验表明，VIG仅使用不到10%的标注量就达到了接近全监督学习的预测精度。在不同评估指标和批处理规模下，VIG始终优于标准基线方法，并在特征空间中收集到更多样化的数据。该方法在生态学之外具有广泛适用性，我们的研究成果凸显了其在数据有限环境下推动生物多样性监测的重要价值。\n\n（注：专业术语说明：\n1. \"Vendi information gain (VIG)\" 保留技术缩写并音译为\"Vendi信息增益\"\n2. \"Snapshot Serengeti\" 作为专有数据集名称保留英文形式\n3. \"active learning\" 统一译为\"主动学习\"符合机器学习领域规范\n4. \"feature space\" 译为\"特征空间\"符合计算机视觉领域术语标准\n5. 复杂句式如\"capturing both informativeness and diversity\"采用分句处理，译为\"同时捕捉信息量与多样性\"确保中文流畅性）"
    },
    {
        "title": "Differentially Private Decentralized Dataset Synthesis Through\n  Randomized Mixing with Correlated Noise",
        "url": "http://arxiv.org/abs/2509.10385v1",
        "pub_date": "2025-09-12",
        "summary": "In this work, we explore differentially private synthetic data generation in a decentralized-data setting by building on the recently proposed Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA synthesizes data in a centralized setting by mixing multiple randomly-selected samples from the same class and injecting carefully calibrated Gaussian noise, ensuring ({\\epsilon}, {\\delta})-differential privacy. When deployed in a decentralized or federated setting, where each client holds only a small partition of the data, DP-CDA faces new challenges. The limited sample size per client increases the sensitivity of local computations, requiring higher noise injection to maintain the differential privacy guarantee. This, in turn, leads to a noticeable degradation in the utility compared to the centralized setting. To mitigate this issue, we integrate the Correlation-Assisted Private Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among the clients by allowing them to generate jointly distributed (anti-correlated) noise that cancels out in aggregate, while preserving privacy at the individual level. This technique significantly improves the privacy-utility trade-off in the federated setting. Extensive experiments on MNIST and FashionMNIST datasets demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can achieve utility comparable to its centralized counterpart under some parameter regime, while maintaining rigorous differential privacy guarantees.",
        "translated": "在本研究中，我们基于最新提出的差分隐私类中心数据聚合（DP-CDA）方法，探索去中心化数据设置下的差分隐私合成数据生成。DP-CDA在集中式设置中通过混合来自同一类的多个随机样本并注入精确校准的高斯噪声来合成数据，确保满足(ε,δ)-差分隐私。当部署在去中心化或联邦学习环境中（每个客户端仅持有少量数据分区）时，DP-CDA面临新的挑战：每个客户端有限的样本量增加了本地计算的敏感度，需要注入更多噪声来维持差分隐私保证，这导致其效用相比集中式设置出现明显下降。为缓解该问题，我们将相关性辅助隐私估计（CAPE）协议集成到联邦DP-CDA框架中，提出CAPE辅助联邦DP-CDA算法。CAPE通过允许客户端生成联合分布（反相关）的噪声（这些噪声在聚合时会相互抵消），在保持个体层面隐私的同时实现客户端间的有限协作。该技术显著改善了联邦设置中的隐私-效用权衡关系。在MNIST和FashionMNIST数据集上的大量实验表明，所提出的CAPE辅助联邦DP-CDA方法在特定参数范围内可实现与集中式方法相当的效用，同时保持严格的差分隐私保证。\n\n（注：专业术语说明：\n1. Differentially Private：差分隐私\n2. Synthetic data generation：合成数据生成\n3. Federated setting：联邦学习设置\n4. Gaussian noise：高斯噪声\n5. Privacy-utility trade-off：隐私-效用权衡\n6. Jointly distributed (anti-correlated) noise：联合分布（反相关）噪声）"
    },
    {
        "title": "Flow Straight and Fast in Hilbert Space: Functional Rectified Flow",
        "url": "http://arxiv.org/abs/2509.10384v1",
        "pub_date": "2025-09-12",
        "summary": "Many generative models originally developed in finite-dimensional Euclidean space have functional generalizations in infinite-dimensional settings. However, the extension of rectified flow to infinite-dimensional spaces remains unexplored. In this work, we establish a rigorous functional formulation of rectified flow in an infinite-dimensional Hilbert space. Our approach builds upon the superposition principle for continuity equations in an infinite-dimensional space. We further show that this framework extends naturally to functional flow matching and functional probability flow ODEs, interpreting them as nonlinear generalizations of rectified flow. Notably, our extension to functional flow matching removes the restrictive measure-theoretic assumptions in the existing theory of \\citet{kerrigan2024functional}. Furthermore, we demonstrate experimentally that our method achieves superior performance compared to existing functional generative models.",
        "translated": "许多最初在有限维欧几里得空间中开发的生成模型，在无限维场景中具有函数化推广形式。然而，整流流（rectified flow）向无限维空间的扩展仍未被探索。本研究在无限维希尔伯特空间中建立了整流流的严格函数化表述。我们的方法基于无限维空间中连续性方程的叠加原理，进一步证明了该框架可自然延伸至函数流匹配（functional flow matching）和函数概率流常微分方程（ODE），并将其解释为整流流的非线性推广。值得注意的是，我们对函数流匹配的扩展消除了现有理论\\citet{kerrigan2024functional}中严格的测度论假设。此外，实验结果表明，相较于现有函数生成模型，我们的方法实现了更优越的性能。"
    },
    {
        "title": "Matrix-free Neural Preconditioner for the Dirac Operator in Lattice\n  Gauge Theory",
        "url": "http://arxiv.org/abs/2509.10378v1",
        "pub_date": "2025-09-12",
        "summary": "Linear systems arise in generating samples and in calculating observables in lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite systems, which are sparse but ill-conditioned, involves using iterative methods, such as Conjugate Gradient (CG), which are time-consuming and computationally expensive. Preconditioners can effectively accelerate this process, with the state-of-the-art being multigrid preconditioners. However, constructing useful preconditioners can be challenging, adding additional computational overhead, especially in large linear systems. We propose a framework, leveraging operator learning techniques, to construct linear maps as effective preconditioners. The method in this work does not rely on explicit matrices from either the original linear systems or the produced preconditioners, allowing efficient model training and application in the CG solver. In the context of the Schwinger model U(1) gauge theory in 1+1 spacetime dimensions with two degenerate-mass fermions), this preconditioning scheme effectively decreases the condition number of the linear systems and approximately halves the number of iterations required for convergence in relevant parameter ranges. We further demonstrate the framework learns a general mapping dependent on the lattice structure which leads to zero-shot learning ability for the Dirac operators constructed from gauge field configurations of different sizes.",
        "translated": "在格点量子色动力学（QCD）的样本生成和可观测量计算中，线性系统的求解至关重要。这类厄米正定系统虽具有稀疏性但呈现病态特性，通常需要采用共轭梯度法（CG）等迭代方法求解，但存在耗时久、计算成本高的问题。预条件子能有效加速求解过程，其中多重网格预条件子代表当前最优水平。然而构建高效预条件子具有挑战性，会引入额外计算开销，在大规模线性系统中尤为明显。本研究提出利用算子学习技术构建线性映射作为高效预条件子的框架。该方法不依赖于原始线性系统或所生成预条件子的显式矩阵表示，可实现高效模型训练并在CG求解器中应用。在1+1维时空具有二重简并质量费米子的Schwinger模型（U(1)规范理论）语境下，该预条件方案有效降低了线性系统的条件数，在相关参数范围内使收敛所需迭代次数减少约50%。我们进一步证明该框架可学习依赖于格点结构的通用映射，从而对从不同尺寸规范场构型构建的狄拉克算子表现出零样本学习能力。\n\n（注：专业术语说明：\n1. lattice quantum chromodynamics：格点量子色动力学\n2. Hermitian positive definite systems：厄米正定系统\n3. Conjugate Gradient (CG)：共轭梯度法\n4. multigrid preconditioners：多重网格预条件子\n5. Schwinger model：Schwinger模型（量子电动力学在二维时空的简化模型）\n6. degenerate-mass fermions：简并质量费米子\n7. condition number：条件数\n8. Dirac operators：狄拉克算子\n9. gauge field configurations：规范场构型\n10. zero-shot learning：零样本学习）"
    },
    {
        "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
        "url": "http://arxiv.org/abs/2509.10371v1",
        "pub_date": "2025-09-12",
        "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.",
        "translated": "大型语言模型（LLM）的快速扩展使得训练工作负载远超单节点分析能力极限，亟需深入理解这些模型在大规模多GPU系统中的运行特性。本文针对NVIDIA H100/H200和AMD MI250等硬件平台，对多种实际工作负载下的LLM训练进行了系统性表征研究。我们分析了张量并行、流水线并行、数据并行和专家并行等策略在稠密/稀疏模型上的表现，评估其对硬件利用率、功耗和热行为的影响，并进一步验证了激活重计算与计算-通信重叠等优化技术的有效性。\n\n研究发现：性能并非单纯由硬件规模决定。在通信受限场景下，采用较少高显存GPU的纵向扩展系统可能优于横向扩展系统，但需精细调优配置；其他场景中横向扩展部署则能实现更高吞吐量。某些并行组合（如张量与流水线并行）会因数据分块效率低下导致带宽利用率不足，而微批次尺寸超过临界值会引发突发性执行和功率峰值，加剧热节流问题。这些发现揭示了训练性能受到硬件、系统拓扑与模型执行间复杂交互的影响。最后我们为系统和硬件设计提出改进建议，以提升未来LLM系统与工作负载的可扩展性和可靠性。项目源代码详见：https://github.com/sitar-lab/CharLLM-PPT。\n\n（注：译文采用以下专业术语处理：\n- scale-up/scale-out 译为纵向扩展/横向扩展\n- microbatch 译为微批次\n- thermal throttling 译为热节流\n- bursty execution 译为突发性执行\n- 并行策略术语保持\"张量/流水线/数据/专家并行\"的标准译法\n- 在保持技术准确性的同时，通过\"显存\"\"吞吐量\"\"调优配置\"等符合中文技术文献习惯的表达确保可读性）"
    },
    {
        "title": "Data distribution impacts the performance and generalisability of\n  contrastive learning-based foundation models of electrocardiograms",
        "url": "http://arxiv.org/abs/2509.10369v1",
        "pub_date": "2025-09-12",
        "summary": "Contrastive learning is a widely adopted self-supervised pretraining strategy, yet its dependence on cohort composition remains underexplored. We present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation model and pretrain on four cohorts (n = 5,203,352), from diverse populations across three continents (North America, South America, Asia). We systematically assess how cohort demographics, health status, and population diversity influence the downstream performance for prediction tasks also including two additional cohorts from another continent (Europe). We find that downstream performance depends on the distributional properties of the pretraining cohort, including demographics and health status. Moreover, while pretraining with a multi-centre, demographically diverse cohort improves in-distribution accuracy, it reduces out-of-distribution (OOD) generalisation of our contrastive approach by encoding cohort-specific artifacts. To address this, we propose the In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency during pretraining and enhances OOD robustness. This work provides important insights for developing clinically fair and generalisable foundation models.",
        "translated": "对比学习是一种广泛采用的自监督预训练策略，但其性能对训练队列构成的依赖性尚未得到充分探索。本研究提出基于患者增强心电图对比学习的CAPE基础模型，并在包含三大洲（北美、南美、亚洲）不同人群的四个队列（n = 5,203,352）上进行预训练。我们系统评估了队列人口统计学特征、健康状况和群体多样性对下游预测任务性能的影响，并额外引入来自欧洲大陆的两个队列进行验证。研究发现：下游性能取决于预训练队列的分布特性，包括人口统计特征和健康状况；虽然使用多中心、人口多样化的队列进行预训练能提高分布内准确率，但会通过编码队列特异性伪影降低对比学习方法的分布外（OOD）泛化能力。为此，我们提出分布内批处理（IDB）策略，在预训练过程中保持队列内一致性并增强OOD鲁棒性。这项研究为开发临床公平且可泛化的基础模型提供了重要见解。\n\n（注：专业术语说明：\n1. Contrastive learning：对比学习\n2. self-supervised pretraining：自监督预训练\n3. Electrocardiograms：心电图\n4. in-distribution/out-of-distribution：分布内/分布外\n5. cohort-specific artifacts：队列特异性伪影\n6. generalisable：可泛化的\n术语翻译严格遵循医学人工智能领域的标准表述）"
    },
    {
        "title": "A Discrepancy-Based Perspective on Dataset Condensation",
        "url": "http://arxiv.org/abs/2509.10367v1",
        "pub_date": "2025-09-12",
        "summary": "Given a dataset of finitely many elements $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i = 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic dataset $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$ which is significantly smaller ($M \\ll N$) such that a model trained from scratch on $\\mathcal{S}$ achieves comparable or even superior generalization performance to a model trained on $\\mathcal{T}$. Recent advances in DC reveal a close connection to the problem of approximating the data distribution represented by $\\mathcal{T}$ with a reduced set of points. In this work, we present a unified framework that encompasses existing DC methods and extend the task-specific notion of DC to a more general and formal definition using notions of discrepancy, which quantify the distance between probability distribution in different regimes. Our framework broadens the objective of DC beyond generalization, accommodating additional objectives such as robustness, privacy, and other desirable properties.",
        "translated": "给定一个包含有限多个元素的数据集 $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i = 1}^N$，数据集压缩（DC）的目标是构建一个规模显著缩小（$M \\ll N$）的合成数据集 $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$，使得在 $\\mathcal{S}$ 上从头训练的模型能够获得与在 $\\mathcal{T}$ 上训练的模型相当甚至更优的泛化性能。近期DC研究的进展揭示了其与通过精简点集近似 $\\mathcal{T}$ 所表示数据分布问题之间的紧密联系。本文提出一个统一框架，不仅涵盖现有DC方法，还通过差异度量的概念将任务特定的DC定义扩展至更普适的形式化定义——该定义通过不同机制下的概率分布距离进行量化。我们的框架将DC的目标从泛化能力拓展至包括鲁棒性、隐私保护及其他期望特性在内的多重目标。"
    },
    {
        "title": "Physics-informed sensor coverage through structure preserving machine\n  learning",
        "url": "http://arxiv.org/abs/2509.10363v1",
        "pub_date": "2025-09-12",
        "summary": "We present a machine learning framework for adaptive source localization in which agents use a structure-preserving digital twin of a coupled hydrodynamic-transport system for real-time trajectory planning and data assimilation. The twin is constructed with conditional neural Whitney forms (CNWF), coupling the numerical guarantees of finite element exterior calculus (FEEC) with transformer-based operator learning. The resulting model preserves discrete conservation, and adapts in real time to streaming sensor data. It employs a conditional attention mechanism to identify: a reduced Whitney-form basis; reduced integral balance equations; and a source field, each compatible with given sensor measurements. The induced reduced-order environmental model retains the stability and consistency of standard finite-element simulation, yielding a physically realizable, regular mapping from sensor data to the source field. We propose a staggered scheme that alternates between evaluating the digital twin and applying Lloyd's algorithm to guide sensor placement, with analysis providing conditions for monotone improvement of a coverage functional. Using the predicted source field as an importance function within an optimal-recovery scheme, we demonstrate recovery of point sources under continuity assumptions, highlighting the role of regularity as a sufficient condition for localization. Experimental comparisons with physics-agnostic transformer architectures show improved accuracy in complex geometries when physical constraints are enforced, indicating that structure preservation provides an effective inductive bias for source identification.",
        "translated": "我们提出了一种用于自适应源定位的机器学习框架，其中智能体通过结构保持的耦合流体动力学-传输系统数字孪生体实现实时轨迹规划与数据同化。该孪生体采用条件神经惠特尼形式（CNWF）构建，将有限元外微积分（FEEC）的数值保证与基于Transformer的算子学习相结合。所得模型既保持离散守恒律，又能实时适配流式传感器数据。通过条件注意力机制识别：约简的惠特尼形式基函数、约简的积分平衡方程以及与传感器测量值相容的源场。由此诱导的降阶环境模型保持了标准有限元模拟的稳定性和一致性，产生从传感器数据到源场的物理可实现正则映射。我们提出一种交错迭代方案：在评估数字孪生体与应用劳埃德算法优化传感器布设之间交替进行，并通过理论分析给出覆盖泛函单调改进的条件。采用最优恢复框架中将预测源场作为重要性函数的方法，我们证明了在连续性假设下点源的可恢复性，凸显了正则性作为定位充分条件的作用。与物理不可知Transformer架构的实验对比表明，在强制物理约束条件下，复杂几何结构中的定位精度显著提升，这证明结构保持为源识别提供了有效的归纳偏置。\n\n（注：专业术语说明：\n1. conditional neural Whitney forms (CNWF) → 条件神经惠特尼形式\n2. finite element exterior calculus (FEEC) → 有限元外微积分\n3. Lloyd's algorithm → 劳埃德算法（传感器布设优化算法）\n4. inductive bias → 归纳偏置（机器学习先验假设））"
    },
    {
        "title": "Why does your graph neural network fail on some graphs? Insights from\n  exact generalisation error",
        "url": "http://arxiv.org/abs/2509.10337v1",
        "pub_date": "2025-09-12",
        "summary": "Graph Neural Networks (GNNs) are widely used in learning on graph-structured data, yet a principled understanding of why they succeed or fail remains elusive. While prior works have examined architectural limitations such as over-smoothing and over-squashing, these do not explain what enables GNNs to extract meaningful representations or why performance varies drastically between similar architectures. These questions are related to the role of generalisation: the ability of a model to make accurate predictions on unlabelled data. Although several works have derived generalisation error bounds for GNNs, these are typically loose, restricted to a single architecture, and offer limited insight into what governs generalisation in practice. In this work, we take a different approach by deriving the exact generalisation error for GNNs in a transductive fixed-design setting through the lens of signal processing. From this viewpoint, GNNs can be interpreted as graph filter operators that act on node features via the graph structure. By focusing on linear GNNs while allowing non-linearity in the graph filters, we derive the first exact generalisation error for a broad range of GNNs, including convolutional, PageRank-based, and attention-based models. The exact characterisation of the generalisation error reveals that only the aligned information between node features and graph structure contributes to generalisation. Furthermore, we quantify the effect of homophily on generalisation. Our work provides a framework that explains when and why GNNs can effectively leverage structural and feature information, offering practical guidance for model selection.",
        "translated": "图神经网络（GNN）在基于图结构数据的学习中应用广泛，但对其成功或失败的理论机制仍缺乏系统认知。现有研究多关注架构缺陷（如过度平滑和过度挤压），却未能解释GNN提取有效表征的内在动因或相似架构间性能差异的根源。这些问题的本质与泛化能力密切相关——即模型在未标注数据上做出准确预测的能力。尽管已有研究推导出GNN的泛化误差边界，但这些边界通常存在松散性、局限于单一架构，且难以揭示实际泛化性能的主导因素。\n\n本研究采用全新视角：通过信号处理理论推导出转导式固定设计设定下GNN的精确泛化误差。在此框架下，GNN可被解读为通过图结构对节点特征进行操作的图滤波器。通过聚焦线性GNN架构（同时允许图滤波器中的非线性变换），我们首次推导出覆盖卷积网络、PageRank模型和注意力机制等多种GNN的精确泛化误差表达式。该精确表征表明：唯有节点特征与图结构之间的对齐信息才对泛化产生贡献。此外，我们量化了同配性对泛化的影响。本研究构建的理论框架不仅解释了GNN何时以及为何能有效利用结构与特征信息，更为模型选择提供了实践指导。"
    },
    {
        "title": "ARMA Block: A CNN-Based Autoregressive and Moving Average Module for\n  Long-Term Time Series Forecasting",
        "url": "http://arxiv.org/abs/2509.10324v1",
        "pub_date": "2025-09-12",
        "summary": "This paper proposes a simple yet effective convolutional module for long-term time series forecasting. The proposed block, inspired by the Auto-Regressive Integrated Moving Average (ARIMA) model, consists of two convolutional components: one for capturing the trend (autoregression) and the other for refining local variations (moving average). Unlike conventional ARIMA, which requires iterative multi-step forecasting, the block directly performs multi-step forecasting, making it easily extendable to multivariate settings. Experiments on nine widely used benchmark datasets demonstrate that our method ARMA achieves competitive accuracy, particularly on datasets exhibiting strong trend variations, while maintaining architectural simplicity. Furthermore, analysis shows that the block inherently encodes absolute positional information, suggesting its potential as a lightweight replacement for positional embeddings in sequential models.",
        "translated": "本文提出了一种简单而有效的卷积模块，用于长期时间序列预测。该模块受自回归积分滑动平均（ARIMA）模型启发，包含两个卷积组件：一个用于捕捉趋势（自回归部分），另一个用于细化局部波动（滑动平均部分）。与传统ARIMA需要进行迭代式多步预测不同，该模块可直接进行多步预测，使其能够轻松扩展到多元变量场景。在九个广泛使用的基准数据集上的实验表明，我们的ARMA方法实现了具有竞争力的预测精度——尤其在呈现显著趋势变化的数据集上表现突出，同时保持了结构简洁性。进一步分析表明，该模块本质上编码了绝对位置信息，这预示着其有潜力作为序列模型中位置嵌入的轻量级替代方案。"
    },
    {
        "title": "Robot guide with multi-agent control and automatic scenario generation\n  with LLM",
        "url": "http://arxiv.org/abs/2509.10317v1",
        "pub_date": "2025-09-12",
        "summary": "The work describes the development of a hybrid control architecture for an anthropomorphic tour guide robot, combining a multi-agent resource management system with automatic behavior scenario generation based on large language models. The proposed approach aims to overcome the limitations of traditional systems, which rely on manual tuning of behavior scenarios. These limitations include manual configuration, low flexibility, and lack of naturalness in robot behavior. The process of preparing tour scenarios is implemented through a two-stage generation: first, a stylized narrative is created, then non-verbal action tags are integrated into the text. The multi-agent system ensures coordination and conflict resolution during the execution of parallel actions, as well as maintaining default behavior after the completion of main operations, contributing to more natural robot behavior. The results obtained from the trial demonstrate the potential of the proposed approach for automating and scaling social robot control systems.",
        "translated": "本文提出了一种用于拟人化导览机器人的混合控制架构，该架构结合了多智能体资源管理系统与基于大语言模型的行为场景自动生成技术。该方法旨在克服传统系统依赖人工调校行为场景的局限性，包括手动配置、灵活性低以及机器人行为缺乏自然性等问题。导览场景的构建通过两阶段生成实现：首先生成风格化叙事文本，随后将非语言动作标签集成到文本中。多智能体系统确保了并行动作执行过程中的协调与冲突消解，并在主要操作完成后维持默认行为模式，从而提升机器人行为的自然度。实验结果表明，所提出的方法在实现社交机器人控制系统自动化与可扩展性方面具有显著潜力。\n\n（注：译文严格遵循了以下技术细节处理：\n1. \"anthropomorphic tour guide robot\"译为\"拟人化导览机器人\"符合机器人学规范\n2. \"multi-agent resource management system\"采用\"多智能体资源管理系统\"的标准译法\n3. \"large language models\"统一译为\"大语言模型\"保持与AI领域术语一致性\n4. \"non-verbal action tags\"译为\"非语言动作标签\"准确传递人机交互概念\n5. 通过\"协调与冲突消解\"等专业表述保持多智能体系统领域术语准确性）"
    },
    {
        "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for\n  Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable\n  Post-Disaster Risk Reduction",
        "url": "http://arxiv.org/abs/2509.10308v1",
        "pub_date": "2025-09-12",
        "summary": "In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.",
        "translated": "在灾害发生后，全球许多机构在持续监测灾害风险变化方面面临挑战，这限制了关键决策者评估《2015-2030年仙台减少灾害风险框架》实施进展的能力。尽管通过地球观测和数据驱动方法，大量研究已显著推进了灾害与暴露度的大规模建模，但在风险方程中另一个同等重要却更具挑战性的要素——物理脆弱性建模方面进展有限。为填补这一空白，我们提出图分类结构变分自编码器（GraphCSVAE），这是一种新颖的概率数据驱动框架，通过整合深度学习、图表征和分类概率推断，利用时序卫星数据集与先验专家信念系统来建模物理脆弱性。我们引入了一种弱监督的一阶转移矩阵，用于反映两个受灾且社会经济弱势地区——（1）孟加拉国受气旋影响的沿海库鲁什库社区与（2）塞拉利昂受泥石流影响的弗里敦市——物理脆弱性时空分布的变化。我们的研究揭示了灾后物理脆弱性的区域动态，为本地化时空审计及制定可持续的灾后风险减缓策略提供了宝贵见解。\n\n（注：专业术语说明：\n1. Sendai Framework for Disaster Risk Reduction：仙台减少灾害风险框架（联合国2015年通过的国际减灾框架）\n2. physical vulnerability：物理脆弱性（指建筑环境或基础设施易受灾害损害的特性）\n3. Graph Categorical Structured Variational Autoencoder：图分类结构变分自编码器（结合图神经网络与结构化概率模型的深度学习架构）\n4. weakly supervised first-order transition matrix：弱监督一阶转移矩阵（基于部分标注数据的状态转移概率模型））"
    },
    {
        "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns\n  Effective Scheduling through Random Data",
        "url": "http://arxiv.org/abs/2509.10303v1",
        "pub_date": "2025-09-12",
        "summary": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling Problem (FJSP), are canonical combinatorial optimization problems with wide-ranging applications in industrial operations. In recent years, many online reinforcement learning (RL) approaches have been proposed to learn constructive heuristics for JSP and FJSP. Although effective, these online RL methods require millions of interactions with simulated environments that may not capture real-world complexities, and their random policy initialization leads to poor sample efficiency. To address these limitations, we introduce Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL algorithm that learns effective scheduling policies directly from historical data, eliminating the need for costly online interactions, while maintaining the ability to improve upon suboptimal training data. CDQAC couples a quantile-based critic with a delayed policy update, estimating the return distribution of each machine-operation pair rather than selecting pairs outright. Our extensive experiments demonstrate CDQAC's remarkable ability to learn from diverse data sources. CDQAC consistently outperforms the original data-generating heuristics and surpasses state-of-the-art offline and online RL baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20 training instances to learn high-quality policies. Surprisingly, we find that CDQAC performs better when trained on data generated by a random heuristic than when trained on higher-quality data from genetic algorithms and priority dispatching rules.",
        "translated": "作业车间调度问题（JSP）与柔性作业车间调度问题（FJSP）是经典的组合优化问题，在工业运营中具有广泛应用。近年来，研究者提出了许多在线强化学习（RL）方法以学习JSP和FJSP的构造式启发规则。虽然这些在线RL方法有效，但需要与模拟环境进行数百万次交互（可能无法捕捉真实世界的复杂性），且其随机策略初始化导致样本效率低下。为解决这些局限性，我们提出保守离散分位数演员-评论家算法（CDQAC）——一种新型离线强化学习算法，可直接从历史数据中学习有效的调度策略，既无需昂贵的在线交互，又能持续改进次优训练数据。CDQAC将基于分位数的评论家与延迟策略更新相结合，通过评估每个机器-工序组合的回报分布而非直接选择组合来实现优化。大量实验表明，CDQAC具有从多源数据中学习的卓越能力：其性能始终优于原始数据生成启发式规则，并超越最先进的离线和在线RL基线方法。此外，CDQAC具备极高的样本效率，仅需10-20个训练实例即可学习高质量策略。令人惊讶的是，实验发现使用随机启发式生成的数据训练CDQAC时，其表现反而优于使用遗传算法和优先级分派规则生成的高质量数据训练的结果。\n\n（注：专业术语说明：\n1. Conservative Discrete Quantile Actor-Critic (CDQAC) 译为\"保守离散分位数演员-评论家算法\"，保留算法首字母缩写\n2. quantile-based critic 译为\"基于分位数的评论家\"，符合强化学习领域术语惯例\n3. genetic algorithms 译为\"遗传算法\"，priority dispatching rules 译为\"优先级分派规则\"，均为调度领域标准译法\n4. 技术细节如\"delayed policy update\"译为\"延迟策略更新\"，\"return distribution\"译为\"回报分布\"，均符合强化学习领域表述规范）"
    },
    {
        "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in\n  Disaster Case",
        "url": "http://arxiv.org/abs/2509.10291v1",
        "pub_date": "2025-09-12",
        "summary": "In disaster scenarios where conventional energy infrastructure is compromised, secure and traceable energy trading between solar-powered households and mobile charging units becomes a necessity. To ensure the integrity of such transactions over a blockchain network, robust and unpredictable nonce generation is vital. This study proposes an SDN-enabled architecture where machine learning regressors are leveraged not for their accuracy, but for their potential to generate randomized values suitable as nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN allows flexible control over data flows and energy routing policies even in fragmented or degraded networks, ensuring adaptive response during emergencies. Using a 9000-sample dataset, we evaluate five AutoML-selected regression models - Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest Neighbors - not by their prediction accuracy, but by their ability to produce diverse and non-deterministic outputs across shuffled data inputs. Randomness analysis reveals that Random Forest and Extra Trees regressors exhibit complete dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and 99.9%, respectively). These findings highlight that certain machine learning models, particularly tree-based ensembles, may serve as effective and lightweight nonce generators within blockchain-secured, SDN-based energy trading infrastructures resilient to disaster conditions.",
        "translated": "在传统能源基础设施受损的灾害场景中，太阳能家庭与移动充电单元之间需要建立安全可追溯的能源交易机制。为确保区块链网络中此类交易的完整性，必须采用强健且不可预测的随机数生成方案。本研究提出一种软件定义网络（SDN）赋能架构，其创新点在于利用机器学习回归模型并非追求预测精度，而是挖掘其生成适用于随机数候选值的随机化输出潜力——该方案被命名为\"自动化机器学习证明\"（Proof of AutoML）。SDN技术即使在碎片化或性能退化的网络中也能实现数据流与能源路由策略的灵活控制，确保应急响应中的自适应能力。基于9000个样本的数据集，我们评估了五种AutoML选择的回归模型（梯度提升、LightGBM、随机森林、极端随机树和K近邻），重点考察其在乱序数据输入下产生多样化非确定性输出的能力而非预测精度。随机性分析表明：随机森林与极端随机树回归器呈现完全随机依赖性，而梯度提升、K近邻和LightGBM虽保持较高随机性评分（分别为97.6%、98.8%和99.9%），但略低于前者。这些发现表明，特定机器学习模型（尤其是基于树的集成方法）可作为有效的轻量级随机数生成器，应用于具备区块链安全保障且适应灾害环境的SDN能源交易基础设施中。"
    },
    {
        "title": "Targeted Test Selection Approach in Continuous Integration",
        "url": "http://arxiv.org/abs/2509.10279v1",
        "pub_date": "2025-09-12",
        "summary": "In modern software development change-based testing plays a crucial role. However, as codebases expand and test suites grow, efficiently managing the testing process becomes increasingly challenging, especially given the high frequency of daily code commits. We propose Targeted Test Selection (T-TS), a machine learning approach for industrial test selection. Our key innovation is a data representation that represent commits as Bags-of-Words of changed files, incorporates cross-file and additional predictive features, and notably avoids the use of coverage maps. Deployed in production, T-TS was comprehensively evaluated against industry standards and recent methods using both internal and public datasets, measuring time efficiency and fault detection. On live industrial data, T-TS selects only 15% of tests, reduces execution time by $5.9\\times$, accelerates the pipeline by $5.6\\times$, and detects over 95% of test failures. The implementation is publicly available to support further research and practical adoption.",
        "translated": "在现代软件开发中，基于变更的测试发挥着关键作用。然而随着代码库规模扩大和测试套件增长，测试过程的高效管理变得日益困难，尤其在每日代码提交频次极高的背景下。我们提出目标测试选择方法（T-TS），这是一种面向工业级测试选择的机器学习方案。我们的核心创新在于数据表示方法：将代码提交表示为修改文件的词袋模型，融合跨文件关联特征和附加预测特征，并显著避免了覆盖率图谱的使用。该方案已在生产环境中部署，我们采用内部和公共数据集，从时间效率和缺陷检测能力两个维度，与行业标准及最新方法进行了全面对比评估。在真实工业数据上，T-TS仅需选择15%的测试用例，将测试执行时间降低5.9倍，流水线速度提升5.6倍，同时检测超过95%的测试故障。相关实现已开源发布，以支持进一步研究和实际应用。\n\n（注：译文严格遵循技术文档规范，对\"Bags-of-Words\"采用\"词袋模型\"标准译法，\"coverage maps\"译为\"覆盖率图谱\"，\"$5.9\\times$\"保留数字精度并符合中文倍数表达习惯。专业术语如\"change-based testing\"（基于变更的测试）、\"test selection\"（测试选择）等均采用计算机领域通用译法，确保技术准确性。）"
    },
    {
        "title": "Property prediction for ionic liquids without prior structural knowledge\n  using limited experimental data: A data-driven neural recommender system\n  leveraging transfer learning",
        "url": "http://arxiv.org/abs/2509.10273v1",
        "pub_date": "2025-09-12",
        "summary": "Ionic liquids (ILs) have emerged as versatile replacements for traditional solvents because their physicochemical properties can be precisely tailored to various applications. However, accurately predicting key thermophysical properties remains challenging due to the vast chemical design space and the limited availability of experimental data. In this study, we present a data-driven transfer learning framework that leverages a neural recommender system (NRS) to enable reliable property prediction for ILs using sparse experimental datasets. The approach involves a two-stage process: first, pre-training NRS models on COSMO-RS-based simulated data at fixed temperature and pressure to learn property-specific structural embeddings for cations and anions; and second, fine-tuning simple feedforward neural networks using these embeddings with experimental data at varying temperatures and pressures. In this work, five essential IL properties are considered: density, viscosity, surface tension, heat capacity, and melting point. The framework supports both within-property and cross-property knowledge transfer. Notably, pre-trained models for density, viscosity, and heat capacity are used to fine-tune models for all five target properties, achieving improved performance by a substantial margin for four of them. The model exhibits robust extrapolation to previously unseen ILs. Moreover, the final trained models enable property prediction for over 700,000 IL combinations, offering a scalable solution for IL screening in process design. This work highlights the effectiveness of combining simulated data and transfer learning to overcome sparsity in the experimental data.",
        "translated": "离子液体（ILs）因其物理化学性质可针对不同应用进行精确调控，已成为传统溶剂的多功能替代品。然而，由于庞大的化学设计空间和有限的实验数据，准确预测其关键热物理性质仍具挑战性。本研究提出一种数据驱动的迁移学习框架，通过神经推荐系统（NRS）利用稀疏实验数据集实现离子液体性质的可靠预测。该框架采用两阶段流程：首先在固定温压条件下基于COSMO-RS模拟数据预训练NRS模型，学习阴阳离子与性质相关的结构嵌入表示；随后利用这些嵌入表示，结合变温变压条件下的实验数据对简单前馈神经网络进行微调。研究涵盖离子液体的五个关键性质：密度、粘度、表面张力、热容和熔点。该框架支持性质内与跨性质的知识迁移，值得注意的是，使用密度、粘度和热容的预训练模型对所有五个目标性质进行微调后，其中四个性质的预测性能显著提升。该模型对未见过的离子液体展现出强大的外推能力，最终训练完成的模型可对超过70万种离子液体组合进行性质预测，为工艺设计中的离子液体筛选提供了可扩展的解决方案。本研究凸显了结合模拟数据与迁移学习以克服实验数据稀疏性的有效性。\n\n（注：专业术语说明：\n1. COSMO-RS：真实溶剂类导体屏蔽模型，基于量子化学计算的溶剂性质预测方法\n2. 结构嵌入（structural embeddings）：将分子结构转化为数值向量的表征学习方法\n3. 前馈神经网络（feedforward neural networks）：多层感知机架构的人工神经网络\n4. 外推（extrapolation）：对训练数据范围外的样本进行预测的能力）"
    },
    {
        "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications",
        "url": "http://arxiv.org/abs/2509.10248v1",
        "pub_date": "2025-09-12",
        "summary": "The ongoing intense discussion on rising LLM usage in the scientific peer-review process has recently been mingled by reports of authors using hidden prompt injections to manipulate review scores. Since the existence of such \"attacks\" - although seen by some commentators as \"self-defense\" - would have a great impact on the further debate, this paper investigates the practicability and technical success of the described manipulations. Our systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide range of LLMs shows two distinct results: I) very simple prompt injections are indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews are generally biased toward acceptance (&gt;95% in many models). Both results have great impact on the ongoing discussions on LLM usage in peer-review.",
        "translated": "目前，关于大语言模型（LLM）在学术同行评审中应用日益广泛的讨论正日趋激烈，而近期有报道指出，作者通过隐藏式提示注入（prompt injection）操纵评审分数，使这一争议进一步复杂化。尽管部分评论者将此类行为视为\"自我防卫\"，但若这类\"攻击\"确实存在，将对后续讨论产生重大影响。本文针对所述操纵手段的可行性与技术成功率展开研究。我们通过系统化评估，利用多种大语言模型对2024年ICLR会议的1000篇论文评审数据进行分析，得出两个重要结论：一）极其简单的提示注入确实高度有效，最高可使论文接收率达到100%；二）大语言模型的评审结果普遍存在倾向于接受的偏见（多数模型>95%）。这两项发现将对当前关于大语言模型在同行评审中应用的讨论产生重大影响。\n\n（注：ICLR为国际学习表征会议，是机器学习领域的顶级会议；LLM指大语言模型；prompt injection译为\"提示注入\"，是通过特定输入指令影响模型输出的技术手段）"
    },
    {
        "title": "Model-agnostic post-hoc explainability for recommender systems",
        "url": "http://arxiv.org/abs/2509.10245v1",
        "pub_date": "2025-09-12",
        "summary": "Recommender systems often benefit from complex feature embeddings and deep learning algorithms, which deliver sophisticated recommendations that enhance user experience, engagement, and revenue. However, these methods frequently reduce the interpretability and transparency of the system. In this research, we develop a systematic application, adaptation, and evaluation of deletion diagnostics in the recommender setting. The method compares the performance of a model to that of a similar model trained without a specific user or item, allowing us to quantify how that observation influences the recommender, either positively or negatively. To demonstrate its model-agnostic nature, the proposal is applied to both Neural Collaborative Filtering (NCF), a widely used deep learning-based recommender, and Singular Value Decomposition (SVD), a classical collaborative filtering technique. Experiments on the MovieLens and Amazon Reviews datasets provide insights into model behavior and highlight the generality of the approach across different recommendation paradigms.",
        "translated": "推荐系统通常受益于复杂的特征嵌入和深度学习算法，这些技术能够提供精准的推荐，从而提升用户体验、参与度以及商业收益。然而，这些方法往往降低了系统的可解释性和透明度。在本研究中，我们开发了一种系统性的应用、适配和评估方法，将删除诊断（deletion diagnostics）应用于推荐场景中。该方法通过比较原模型与剔除特定用户或物品后训练的相似模型之间的性能差异，使我们能够量化单一观测数据对推荐效果的正面或负面影响。为验证该方案的模型无关性，我们将其同时应用于神经协同过滤（NCF）——一种广泛使用的基于深度学习的推荐模型，以及经典协同过滤技术奇异值分解（SVD）。在MovieLens和亚马逊评论数据集上的实验不仅揭示了模型的行为特性，还证明了该方法在不同推荐范式间具有普适性。\n\n（注：译文严格遵循以下要点：\n1. 专业术语准确统一：如\"deletion diagnostics\"译为\"删除诊断\"，\"model-agnostic\"译为\"模型无关性\"\n2. 技术概念清晰传达：对NCF和SVD等算法名称保留英文缩写并补充说明\n3. 学术表述规范：采用\"量化\"\"范式\"\"普适性\"等符合学术论文风格的词汇\n4. 逻辑关系显性化：通过\"通过...\"\"使...\"等结构明确呈现方法论链条\n5. 长句拆分重组：将原文复合句按中文习惯分解为多个短句，保持技术准确性同时提升可读性）"
    },
    {
        "title": "MatSKRAFT: A framework for large-scale materials knowledge extraction\n  from scientific tables",
        "url": "http://arxiv.org/abs/2509.10448v1",
        "pub_date": "2025-09-12",
        "summary": "Scientific progress increasingly depends on synthesizing knowledge across vast literature, yet most experimental data remains trapped in semi-structured formats that resist systematic extraction and analysis. Here, we present MatSKRAFT, a computational framework that automatically extracts and integrates materials science knowledge from tabular data at unprecedented scale. Our approach transforms tables into graph-based representations processed by constraint-driven GNNs that encode scientific principles directly into model architecture. MatSKRAFT significantly outperforms state-of-the-art large language models, achieving F1 scores of 88.68 for property extraction and 71.35 for composition extraction, while processing data $19$-$496\\times$ faster than them (compared to the slowest and the fastest models, respectively) with modest hardware requirements. Applied to nearly 69,000 tables from more than 47,000 research publications, we construct a comprehensive database containing over 535,000 entries, including 104,000 compositions that expand coverage beyond major existing databases, pending manual validation. This systematic approach reveals previously overlooked materials with distinct property combinations and enables data-driven discovery of composition-property relationships forming the cornerstone of materials and scientific discovery.",
        "translated": "科学进步日益依赖于对海量文献知识的综合，然而大部分实验数据仍被困在半结构化格式中，难以进行系统性提取与分析。本文提出MatSKRAFT计算框架，能够以前所未有的规模从表格数据中自动提取并整合材料科学知识。该方法将表格转换为基于图的表征，通过约束驱动图神经网络进行处理——这种设计直接将科学原理编码至模型架构中。MatSKRAFT显著优于最先进的大语言模型，在性能提取和成分提取任务上分别达到88.68和71.35的F1分数，数据处理速度比最快/最慢的对比模型分别提升496倍和19倍，且硬件需求适中。通过处理来自47,000余篇研究论文的近69,000张表格，我们构建了包含535,000余条记录的综合性数据库，其中104,000条成分记录在人工验证前已展现出对现有主要数据库的覆盖扩展。这种系统性方法揭示了此前被忽视的具有特殊性能组合的材料，并实现了对构成材料与科学发现基石的成分-性能关系的数据驱动式发掘。\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语准确对应：\"graph-based representations\"译为\"基于图的表征\"，\"constraint-driven GNNs\"译为\"约束驱动图神经网络\"\n2. 技术指标完整保留：F1分数、倍数关系、数据量级等数值信息完整呈现\n3. 学术表达规范：\"semi-structured formats\"译为\"半结构化格式\"，\"data-driven discovery\"译为\"数据驱动式发掘\"\n4. 长句拆分重组：将原文复合长句按中文表达习惯分解为多个语义单元，如对硬件需求和数据速度关系的表述\n5. 逻辑连接词优化：使用\"通过\"、\"其中\"、\"且\"等连接词保持论证逻辑的连贯性）"
    },
    {
        "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2509.10397v1",
        "pub_date": "2025-09-12",
        "summary": "We present RecoWorld, a blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents a proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with a dual-view architecture: a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating a dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where \"user instructs, recommender responds,\" jointly optimizing user retention and engagement.",
        "translated": "我们提出RecoWorld——一个专为智能推荐系统构建模拟环境的蓝图。该环境为智能体提供了完善的训练空间，使其能够在避免影响真实用户的情况下从错误中学习。RecoWorld采用双视图架构：模拟用户与智能推荐系统进行多轮交互，以最大化用户留存率为目标。用户模拟器会审核推荐内容并更新其心智状态，当感知到用户可能流失时，会生成反思性指令。智能推荐系统通过融合这些用户指令和推理轨迹来调整推荐策略，形成动态反馈循环以增强用户参与度。该过程充分发挥了现代大语言模型的卓越推理能力。我们在模拟器中探索了多种内容表示方法，包括文本式、多模态和语义ID建模，并讨论了多轮强化学习如何通过迭代交互优化推荐策略。RecoWorld还支持多智能体模拟，使创建者能模拟目标用户群体的响应。这标志着向\"用户与智能体协同塑造个性化信息流\"的推荐系统迈出了重要第一步。我们展望\"用户指导-推荐系统响应\"的新交互范式，共同优化用户留存与参与度。\n\n（注：根据学术规范，LLMs在此处保留英文缩写但补充中文全称\"大语言模型\"，语义ID建模、多模态等专业术语采用学界通用译法，心智状态(mindset)、推理轨迹(reasoning traces)等概念按计算机领域惯例进行精准转译。）"
    },
    {
        "title": "Diversified recommendations of cultural activities with personalized\n  determinantal point processes",
        "url": "http://arxiv.org/abs/2509.10392v1",
        "pub_date": "2025-09-12",
        "summary": "While optimizing recommendation systems for user engagement is a well-established practice, effectively diversifying recommendations without negatively impacting core business metrics remains a significant industry challenge. In line with our initiative to broaden our audience's cultural practices, this study investigates using personalized Determinantal Point Processes (DPPs) to sample diverse and relevant recommendations. We rely on a well-known quality-diversity decomposition of the similarity kernel to give more weight to user preferences. In this paper, we present our implementations of the personalized DPP sampling, evaluate the trade-offs between relevance and diversity through both offline and online metrics, and give insights for practitioners on their use in a production environment. For the sake of reproducibility, we release the full code for our platform and experiments on GitHub.",
        "translated": "在优化推荐系统以提升用户参与度已成为行业常规实践的背景下，如何有效实现推荐多样化同时避免对核心业务指标产生负面影响，仍是业界面临的重要挑战。基于我们拓宽用户文化实践范围的倡议，本研究探索采用个性化行列式点过程（DPP）抽样方法来实现多样化且相关的推荐。通过采用知名的质量-多样性相似核分解技术，我们增强了用户偏好权重。本文详细介绍了个性化DPP抽样的实施方案，通过离线与在线指标评估相关性-多样性的平衡关系，并为实践者提供了生产环境应用的专业建议。为保障可复现性，我们已在GitHub平台公开完整的系统代码及实验数据。"
    },
    {
        "title": "A Research Vision for Web Search on Emerging Topics",
        "url": "http://arxiv.org/abs/2509.10212v1",
        "pub_date": "2025-09-12",
        "summary": "We regularly encounter information on novel, emerging topics for which the body of knowledge is still evolving, which can be linked, for instance, to current events. A primary way to learn more about such topics is through web search. However, information on emerging topics is sparse and evolves dynamically as knowledge grows, making it uncertain and variable in quality and trustworthiness and prone to deliberate or accidental manipulation, misinformation, and bias. In this paper, we outline a research vision towards search systems and interfaces that support effective knowledge acquisition, awareness of the dynamic nature of topics, and responsible opinion formation among people searching the web for information on emerging topics. To realize this vision, we propose three overarching research questions, aimed at understanding the status quo, determining requirements of systems aligned with our vision, and building these systems. For each of the three questions, we highlight relevant literature, including pointers on how they could be addressed. Lastly, we discuss the challenges that will potentially arise in pursuing the proposed vision.",
        "translated": "我们时常会接触到关于新兴主题的信息，这些主题的知识体系仍在不断发展，例如与当前事件相关的主题。了解更多此类主题的主要方式是通过网络搜索。然而，新兴主题的信息通常较为稀疏，并随着知识的增长而动态演变，这使得其质量与可信度存在不确定性且变化较大，容易受到有意或无意的操纵、错误信息和偏见的影响。本文提出了一种研究愿景，旨在开发支持有效知识获取、增强对主题动态特性的认知，并帮助用户形成负责任观点的搜索系统与界面，以应对用户在新兴主题网络搜索中的需求。为实现这一愿景，我们提出了三个核心研究问题：理解现状、确定符合愿景的系统需求，以及构建此类系统。针对每个问题，我们梳理了相关文献，并提供了解决思路的指引。最后，我们探讨了在实现该愿景过程中可能面临的挑战。"
    },
    {
        "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment\n  and Dimension Segmentation",
        "url": "http://arxiv.org/abs/2509.12086v1",
        "pub_date": "2025-09-15",
        "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in applications such as search engines, recommender systems, and RAG for LLMs. Vector quantization (VQ), a crucial technique for ANNS, is commonly used to reduce space overhead and accelerate distance computations. However, despite significant research advances, state-of-the-art VQ methods still face challenges in balancing encoding efficiency and quantization accuracy. To address these limitations, we propose a novel VQ method called SAQ. To improve accuracy, SAQ employs a new dimension segmentation technique to strategically partition PCA-projected vectors into segments along their dimensions. By prioritizing leading dimension segments with larger magnitudes, SAQ allocates more bits to high-impact segments, optimizing the use of the available space quota. An efficient dynamic programming algorithm is developed to optimize dimension segmentation and bit allocation, ensuring minimal quantization error. To speed up vector encoding, SAQ devises a code adjustment technique to first quantize each dimension independently and then progressively refine quantized vectors using a coordinate-descent-like approach to avoid exhaustive enumeration. Extensive experiments demonstrate SAQ's superiority over classical methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ, Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and accelerates encoding speed by over 80x compared to Extended RabitQ.",
        "translated": "近似最近邻搜索（ANNS）在搜索引擎、推荐系统及大语言模型检索增强生成（RAG）等应用中具有关键作用。向量量化（VQ）作为ANNS的核心技术，常用于降低存储开销并加速距离计算。然而，尽管研究取得显著进展，现有最先进的VQ方法仍在编码效率与量化精度平衡方面面临挑战。针对这些局限性，我们提出了一种名为SAQ的新型向量量化方法。在精度提升方面，SAQ采用创新的维度分割技术，将PCA投影后的向量沿维度进行战略性划分。通过优先处理幅值较大的主导维度段，该方法为高影响力段分配更多比特位，从而优化可用空间配额的使用。我们开发了高效的动态规划算法以优化维度分割与比特分配，确保量化误差最小化。在加速编码方面，SAQ设计了码本调整技术：先独立量化各维度，再采用类坐标下降法逐步优化量化向量，避免穷举搜索。大量实验证明，SAQ在量化误差上较经典方法（如PQ、PCA）和最新方案（如LVQ、Extended RabitQ）最高降低80%，编码速度较Extended RabitQ提升超80倍。\n\n（注：专业术语说明：\n- ANNS: 近似最近邻搜索\n- VQ: 向量量化\n- PCA: 主成分分析\n- PQ: 乘积量化\n- LVQ: 学习型向量量化\n- RAG: 检索增强生成）"
    },
    {
        "title": "AEFS: Adaptive Early Feature Selection for Deep Recommender Systems",
        "url": "http://arxiv.org/abs/2509.12076v1",
        "pub_date": "2025-09-15",
        "summary": "Feature selection has emerged as a crucial technique in refining recommender systems. Recent advancements leveraging Automated Machine Learning (AutoML) has drawn significant attention, particularly in two main categories: early feature selection and late feature selection, differentiated by whether the selection occurs before or after the embedding layer. The early feature selection selects a fixed subset of features and retrains the model, while the late feature selection, known as adaptive feature selection, dynamically adjusts feature choices for each data instance, recognizing the variability in feature significance. Although adaptive feature selection has shown remarkable improvements in performance, its main drawback lies in its post-embedding layer feature selection. This process often becomes cumbersome and inefficient in large-scale recommender systems with billions of ID-type features, leading to a highly sparse and parameter-heavy embedding layer. To overcome this, we introduce Adaptive Early Feature Selection (AEFS), a very simple method that not only adaptively selects informative features for each instance, but also significantly reduces the activated parameters of the embedding layer. AEFS employs a dual-model architecture, encompassing an auxiliary model dedicated to feature selection and a main model responsible for prediction. To ensure effective alignment between these two models, we incorporate two collaborative training loss constraints. Our extensive experiments on three benchmark datasets validate the efficiency and effectiveness of our approach. Notably, AEFS matches the performance of current state-of-theart Adaptive Late Feature Selection methods while achieving a significant reduction of 37. 5% in the activated parameters of the embedding layer. AEFS is open-source at https://github. com/fly-dragon211/AEFS .",
        "translated": "特征选择已成为优化推荐系统的关键技术。近年来，基于自动化机器学习（AutoML）的研究进展备受关注，主要分为早期特征选择与晚期特征选择两类，其区别在于选择操作发生在嵌入层之前还是之后。早期特征选择固定选取特征子集并重新训练模型，而晚期特征选择（称为自适应特征选择）则根据每个数据实例动态调整特征选择，充分考虑了特征重要性的差异性。尽管自适应特征选择在性能上展现出显著优势，但其主要缺陷在于嵌入层后的选择机制。对于拥有数十亿ID类特征的大规模推荐系统，该过程往往变得冗长低效，导致嵌入层高度稀疏且参数量庞大。为解决这一问题，我们提出了自适应早期特征选择（AEFS），该方法不仅能为每个实例自适应选择信息丰富的特征，还能显著减少嵌入层的激活参数量。AEFS采用双模型架构：辅助模型负责特征选择，主模型承担预测任务。为确保两个模型的协同优化，我们引入了双重协作训练损失约束。在三个基准数据集上的大量实验验证了该方法的效率与有效性。值得注意的是，AEFS在性能上媲美当前最先进的自适应晚期特征选择方法，同时将嵌入层激活参数量显著降低37.5%。本项目已开源：https://github.com/fly-dragon211/AEFS。\n\n（注：根据学术规范，对原文中的技术术语如\"AutoML\"、\"embedding layer\"、\"ID-type features\"等均采用标准译法，并保持数字与百分比的准确转译。开源链接按国际惯例保留原始格式，句末补充\"本项目已开源\"符合中文论文表述习惯。）"
    },
    {
        "title": "Results of the 2025 Video Browser Showdown",
        "url": "http://arxiv.org/abs/2509.12000v1",
        "pub_date": "2025-09-15",
        "summary": "This report presents the results of the 14th Video Browser Showdown, held at the 2025 International Conference on Multimedia Modeling on the 8th of January 2025 in Nara, Japan.",
        "translated": "本报告呈现了2025年1月8日在日本奈良举行的\"2025年多媒体建模国际会议\"期间开展的第十四届视频浏览器 showdown 的竞赛结果。"
    },
    {
        "title": "Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case\n  Study with Suno and Udio",
        "url": "http://arxiv.org/abs/2509.11824v1",
        "pub_date": "2025-09-15",
        "summary": "Online AI platforms for creating music from text prompts (AI music), such as Suno and Udio, are now being used by hundreds of thousands of users. Some AI music is appearing in advertising, and even charting, in multiple countries. How are these platforms being used? What subjects are inspiring their users? This article answers these questions for Suno and Udio using a large collection of songs generated by users of these platforms from May to October 2024. Using a combination of state-of-the-art text embedding models, dimensionality reduction and clustering methods, we analyze the prompts, tags and lyrics, and automatically annotate and display the processed data in interactive plots. Our results reveal prominent themes in lyrics, language preference, prompting strategies, as well as peculiar attempts at steering models through the use of metatags. To promote the musicological study of the developing cultural practice of AI-generated music we share our code and resources.",
        "translated": "目前，像Suno和Udio这样通过文本提示生成音乐的在线AI平台（AI音乐）已拥有数十万用户。部分AI音乐作品已出现在多个国家的广告场景中，甚至登上音乐排行榜。这些平台的实际使用情况如何？用户创作的主题倾向是什么？本文通过分析2024年5月至10月期间用户在这些平台生成的大量歌曲数据，对上述问题展开研究。我们采用最先进的文本嵌入模型，结合降维与聚类方法，对提示词、标签和歌词进行多维度分析，并通过自动化标注将处理后的数据呈现在交互式图表中。研究结果揭示了歌词主题偏好、语言选择倾向、提示策略特征，以及用户通过元标签引导模型生成的特殊尝试。为促进对AI生成音乐这一新兴文化实践的音乐学研究，我们同步公开了相关代码与资源。"
    },
    {
        "title": "Decoding in Latent Spaces for Efficient Inference in LLM-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2509.11524v1",
        "pub_date": "2025-09-15",
        "summary": "Fine-tuning large language models (LLMs) for recommendation in a generative manner has delivered promising results, but encounters significant inference overhead due to autoregressive decoding in the language space. This work explores bypassing language-space decoding by directly matching candidate items with the LLM's internal thought representations in the latent space, eliminating the time-consuming autoregressive process to reduce computational costs. Towards this, we introduce Light Latent-space Decoding (L2D), an effective and efficient latent-space decoding method. L2D represents user-preferred items by using the hidden states of test sequences reflecting the LLM's internal thought, and obtains candidate item representations from the hidden states of training sequences labeled with the corresponding candidate items. It then matches the two types of representations to decode items, achieving latent-space decoding. In this way, it enables efficient decoding without altering the LLM's generative tuning paradigm, thereby preserving performance. Extensive empirical results demonstrate that L2D is more than 10x faster than language-space decoding while maintaining or enhancing performance.",
        "translated": "本文探讨了通过潜在空间直接匹配候选项目与大语言模型（LLM）内部思维表征的方法，以规避生成式推荐中语言空间自回归解码带来的高计算开销。当前基于生成式微调的大语言模型推荐方法虽效果显著，但受限于语言空间的逐词生成机制，存在显著的推理延迟。为此，我们提出轻量级潜在空间解码方法（L2D），通过隐空间表征匹配实现高效推荐。\n\nL2D的核心思想是：利用测试序列的隐藏状态捕捉反映LLM内部思维的用户偏好表征，同时从标注候选项目的训练序列隐藏状态中提取候选项目表征。通过直接匹配这两类表征实现项目解码，在保持生成式调优范式性能的前提下，完全避免了耗时的自回归过程。大量实验表明，L2D在维持甚至提升推荐性能的同时，实现了超过10倍的推理加速。\n\n（关键技术要点：）\n1. 通过潜在空间表征匹配替代语言空间自回归解码\n2. 保持生成式调优范式性能不变\n3. 测试序列隐藏状态表征用户偏好\n4. 训练序列隐藏状态构建候选项目表征库\n5. 实现10倍+加速的同时保持/提升推荐效果"
    },
    {
        "title": "Acoustic Overspecification in Electronic Dance Music Taxonomy",
        "url": "http://arxiv.org/abs/2509.11474v1",
        "pub_date": "2025-09-14",
        "summary": "Electronic Dance Music (EDM) classification typically relies on industry-defined taxonomies with numerous subgenres, yet the acoustic basis for these distinctions remains unclear. Current approaches use supervised learning with prescribed genre labels, assuming their validity without systematic evaluation. In this paper, we propose an unsupervised approach to discover the natural acoustic structure of EDM independent of commercial labels. Our method combines novel tempogram-based features capturing EDM's layered rhythmic patterns with multi-criteria feature selection. To validate that our findings reflect genuine acoustic structure rather than methodological artifacts, we compare our results against state-of-the-art pre-trained audio embeddings (MERT and CLAP). Both our feature space and embedding representations converge to 19-23 natural acoustic families compared to the prescribed 35, providing consistent evidence of significant overspecification in current EDM taxonomy by approximately one-third.",
        "translated": "电子舞曲（EDM）分类通常依赖行业定义的分类体系，其包含大量子流派，但这些区分的声学基础至今尚未明确。当前研究方法采用带有预设流派标签的监督学习，却未对标签有效性进行系统评估。本文提出一种无监督方法，旨在脱离商业标签体系，发掘EDM的自然声学结构。我们通过结合创新的基于tempogram的节奏特征（捕捉EDM分层节奏模式）与多准则特征选择来实现这一目标。为验证所得结果反映的是真实声学结构而非方法伪影，我们将研究成果与最先进的预训练音频嵌入模型（MERT和CLAP）进行对比。研究发现：相较于现行分类体系的35个流派，我们的特征空间与嵌入表征一致收敛到19-23个自然声学家族，这充分证明当前EDM分类体系存在约三分之一程度的过度细化问题。"
    },
    {
        "title": "Do Large Language Models Favor Recent Content? A Study on Recency Bias\n  in LLM-Based Reranking",
        "url": "http://arxiv.org/abs/2509.11353v1",
        "pub_date": "2025-09-14",
        "summary": "Large language models (LLMs) are increasingly deployed in information systems, including being used as second-stage rerankers in information retrieval pipelines, yet their susceptibility to recency bias has received little attention. We investigate whether LLMs implicitly favour newer documents by prepending artificial publication dates to passages in the TREC Deep Learning passage retrieval collections in 2021 (DL21) and 2022 (DL22). Across seven models, GPT-3.5-turbo, GPT-4o, GPT-4, LLaMA-3 8B/70B, and Qwen-2.5 7B/72B, \"fresh\" passages are consistently promoted, shifting the Top-10's mean publication year forward by up to 4.78 years and moving individual items by as many as 95 ranks in our listwise reranking experiments. Although larger models attenuate the effect, none eliminate it. We also observe that the preference of LLMs between two passages with an identical relevance level can be reversed by up to 25% on average after date injection in our pairwise preference experiments. These findings provide quantitative evidence of a pervasive recency bias in LLMs and highlight the importance of effective bias-mitigation strategies.",
        "translated": "大型语言模型（LLMs）正日益广泛地部署于信息系统中，包括作为信息检索流程中的第二阶段重排序器，但其对时效性偏见的敏感性尚未得到充分关注。本研究通过向TREC深度学习段落检索集2021（DL21）和2022（DL22）中的文本段添加人工发布日期，探究LLMs是否隐式偏向较新的文档。在七个模型（GPT-3.5-turbo、GPT-4o、GPT-4、LLaMA-3 8B/70B和Qwen-2.5 7B/72B）的列表式重排序实验中，\"新鲜\"文本段被系统性提升：Top-10结果的平均出版年份最多前移4.78年，单个条目在排序中的位置最高提升95位。尽管更大规模的模型会减弱这种效应，但所有模型均未消除该偏差。在成对偏好实验中还发现，当向两个相关度相同的文本段注入日期信息后，LLMs的偏好选择平均出现高达25%的逆转。这些发现为LLMs中普遍存在的时效性偏见提供了量化证据，并凸显了制定有效偏差缓解策略的重要性。\n\n（注：专业术语说明：\n1. recency bias译为\"时效性偏见\"而非字面\"近期偏见\"，更符合信息检索领域的专业表述\n2. listwise reranking保留专业表述\"列表式重排序\"\n3. pairwise preference experiments译为\"成对偏好实验\"符合机器学习领域规范\n4. 模型名称保持英文原称符合学术惯例\n5. \"日期注入\"采用动宾结构准确传达date injection的实验操作）"
    },
    {
        "title": "An Incentive-Compatible Reward Sharing Mechanism for Mitigating\n  Mirroring Attacks in Decentralized Data-Feed Systems",
        "url": "http://arxiv.org/abs/2509.11294v1",
        "pub_date": "2025-09-14",
        "summary": "Decentralized data-feed systems enable blockchain-based smart contracts to access off-chain information by aggregating values from multiple oracles. To improve accuracy, these systems typically use an aggregation function, such as majority voting, to consolidate the inputs they receive from oracles and make a decision. Depending on the final decision and the values reported by the oracles, the participating oracles are compensated through shared rewards. However, such incentive mechanisms are vulnerable to mirroring attacks, where a single user controls multiple oracles to bias the decision of the aggregation function and maximize rewards. This paper analyzes the impact of mirroring attacks on the reliability and dependability of majority voting-based data-feed systems. We demonstrate how existing incentive mechanisms can unintentionally encourage rational users to implement such attacks. To address this, we propose a new incentive mechanism that discourages Sybil behavior. We prove that the proposed mechanism leads to a Nash Equilibrium in which each user operates only one oracle. Finally, we discuss the practical implementation of the proposed incentive mechanism and provide numerical examples to demonstrate its effectiveness.",
        "translated": "去中心化数据馈送系统通过聚合多个预言机提供的数值，使基于区块链的智能合约能够获取链外信息。为提高准确性，这类系统通常采用聚合函数（如多数投票机制）来整合从预言机接收的输入并作出决策。根据最终决策与预言机上报数值的匹配情况，系统通过共享奖励机制对参与预言机进行激励。然而，此类激励机制易受镜像攻击——即单个用户控制多个预言机以操纵聚合函数决策并最大化收益。本文分析了镜像攻击对基于多数投票机制的数据馈送系统可靠性与可信性的影响，论证了现有激励机制如何无意间促使理性用户实施此类攻击。针对该问题，我们提出一种能抑制女巫行为的新型激励机制，并证明该机制可引导至纳什均衡状态——此时每个用户仅运行一个预言机。最后我们讨论了所提激励机制的实际实施方案，并通过数值算例验证其有效性。\n\n（注：专业术语说明：\n1. \"oracles\"译为\"预言机\"——区块链领域专有名词，指连接链上与链下数据的中间件\n2. \"Sybil behavior\"译为\"女巫行为\"——信息安全术语，指单个实体伪装成多个身份的行为\n3. \"Nash Equilibrium\"保留经济学标准译法\"纳什均衡\"\n4. \"mirroring attacks\"根据上下文译为\"镜像攻击\"，体现攻击者复制控制多个节点的特征）"
    },
    {
        "title": "RanAT4BIE: Random Adversarial Training for Biomedical Information\n  Extraction",
        "url": "http://arxiv.org/abs/2509.11191v1",
        "pub_date": "2025-09-14",
        "summary": "We introduce random adversarial training (RAT), a novel framework successfully applied to biomedical information extraction (BioIE) tasks. Building on PubMedBERT as the foundational architecture, our study first validates the effectiveness of conventional adversarial training in enhancing pre-trained language models' performance on BioIE tasks. While adversarial training yields significant improvements across various performance metrics, it also introduces considerable computational overhead. To address this limitation, we propose RAT as an efficiency solution for biomedical information extraction. This framework strategically integrates random sampling mechanisms with adversarial training principles, achieving dual objectives: enhanced model generalization and robustness while significantly reducing computational costs. Through comprehensive evaluations, RAT demonstrates superior performance compared to baseline models in BioIE tasks. The results highlight RAT's potential as a transformative framework for biomedical natural language processing, offering a balanced solution to the model performance and computational efficiency.",
        "translated": "我们提出随机对抗训练（RAT）——一种成功应用于生物医学信息抽取（BioIE）任务的新型框架。本研究以PubMedBERT作为基础架构，首先验证了传统对抗训练在提升预训练语言模型处理BioIE任务性能方面的有效性。虽然对抗训练显著提升了多项性能指标，但也带来了可观的计算开销。针对这一局限性，我们提出RAT作为生物医学信息抽取的高效解决方案。该框架通过将随机采样机制与对抗训练原理进行策略性融合，实现了双重目标：在显著降低计算成本的同时，增强模型的泛化能力与鲁棒性。经全面评估，RAT在BioIE任务中展现出优于基线模型的性能。这些结果凸显了RAT作为生物医学自然语言处理变革性框架的潜力，为模型性能与计算效率提供了平衡的解决方案。"
    },
    {
        "title": "Understanding the Information Cocoon: A Multidimensional Assessment and\n  Analysis of News Recommendation Systems",
        "url": "http://arxiv.org/abs/2509.11139v1",
        "pub_date": "2025-09-14",
        "summary": "Personalized news recommendation systems inadvertently create information cocoons--homogeneous information bubbles that reinforce user biases and amplify societal polarization. To address the lack of comprehensive assessment frameworks in prior research, we propose a multidimensional analysis that evaluates cocoons through dual perspectives: (1) Individual homogenization via topic diversity (including the number of topic categories and category information entropy) and click repetition; (2) Group polarization via network density and community openness. Through multi-round experiments on real-world datasets, we benchmark seven algorithms and reveal critical insights. Furthermore, we design five lightweight mitigation strategies. This work establishes the first unified metric framework for information cocoons and delivers deployable solutions for ethical recommendation systems.",
        "translated": "个性化新闻推荐系统在无形中催生了信息茧房——这种同质化信息泡沫会强化用户偏见并加剧社会极化。针对现有研究缺乏系统性评估框架的问题，我们提出多维度分析方法，通过双重视角评估信息茧房：(1) 个体层面通过主题多样性（包含主题类别数量与类别信息熵）和点击重复率衡量同质化程度；(2) 群体层面通过网络密度和社区开放度衡量极化现象。基于真实数据集的多轮实验，我们对七种算法进行基准测试并发现关键洞见。此外，我们设计了五种轻量化缓解策略。本研究首次建立了信息茧房的统一度量框架，并为构建合乎伦理的推荐系统提供了可部署的解决方案。\n\n（注：翻译过程中对以下专业术语进行了精准处理：\n- \"information cocoons\"译为\"信息茧房\"（学界通用译法）\n- \"topic diversity\"译为\"主题多样性\"并补充说明核心指标\n- \"network density\"和\"community openness\"分别译为\"网络密度\"和\"社区开放度\"\n- \"lightweight mitigation strategies\"译为\"轻量化缓解策略\"以体现工程可行性\n- \"ethical recommendation systems\"译为\"合乎伦理的推荐系统\"准确传递伦理维度）"
    },
    {
        "title": "SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric\n  Spaces for Recommendation",
        "url": "http://arxiv.org/abs/2509.11094v1",
        "pub_date": "2025-09-14",
        "summary": "Knowledge Graphs (KGs) enhance recommender systems but face challenges from inherent noise, sparsity, and Euclidean geometry's inadequacy for complex relational structures, critically impairing representation learning, especially for long-tail entities. Existing methods also often lack adaptive multi-source signal fusion tailored to item popularity. This paper introduces SPARK, a novel multi-stage framework systematically tackling these issues. SPARK first employs Tucker low-rank decomposition to denoise KGs and generate robust entity representations. Subsequently, an SVD-initialized hybrid geometric GNN concurrently learns representations in Euclidean and Hyperbolic spaces; the latter is strategically leveraged for its aptitude in modeling hierarchical structures, effectively capturing semantic features of sparse, long-tail items. A core contribution is an item popularity-aware adaptive fusion strategy that dynamically weights signals from collaborative filtering, refined KG embeddings, and diverse geometric spaces for precise modeling of both mainstream and long-tail items. Finally, contrastive learning aligns these multi-source representations. Extensive experiments demonstrate SPARK's significant superiority over state-of-the-art methods, particularly in improving long-tail item recommendation, offering a robust, principled approach to knowledge-enhanced recommendation. Implementation code is available at https://github.com/Applied-Machine-Learning-Lab/SPARK.",
        "translated": "知识图谱（KG）能增强推荐系统性能，但其固有的噪声、数据稀疏性以及欧氏几何对复杂关系结构表征的局限性，严重制约了表示学习效果，尤其对长尾实体影响显著。现有方法还普遍缺乏针对物品流行度的自适应多源信号融合机制。本文提出SPARK——一个系统解决这些问题的多阶段创新框架。该框架首先采用Tucker低秩分解对知识图谱去噪并生成鲁棒的实体表示；随后通过SVD初始化的混合几何图神经网络，并行学习欧氏空间与双曲空间的表示，其中双曲空间凭借其建模层次化结构的优势被策略性用于捕捉稀疏长尾物品的语义特征。核心贡献在于提出了物品流行度感知的自适应融合策略，动态加权协同过滤信号、精炼知识图谱嵌入及多几何空间表示，实现对主流和长尾物品的精准建模。最后通过对比学习对齐多源表示。大量实验证明SPARK显著优于现有最优方法，尤其在提升长尾物品推荐效果方面，为知识增强推荐提供了鲁棒且原理清晰的解决方案。代码已开源：https://github.com/Applied-Machine-Learning-Lab/SPARK。\n\n（注：本文翻译严格遵循以下技术细节处理：\n1. 专业术语标准化：\"Hyperbolic spaces\"译为\"双曲空间\"，\"contrastive learning\"译为\"对比学习\"\n2. 技术动作准确表述：\"Tucker low-rank decomposition\"完整译为\"Tucker低秩分解\"\n3. 概念体系一致性：\"long-tail entities/items\"统一译为\"长尾实体/物品\"\n4. 方法论描述保留学术精确性：\"SVD-initialized hybrid geometric GNN\"译为\"SVD初始化的混合几何图神经网络\"\n5. 保持学术论文摘要的客观严谨语气）"
    },
    {
        "title": "Membership Inference Attacks on Recommender System: A Survey",
        "url": "http://arxiv.org/abs/2509.11080v1",
        "pub_date": "2025-09-14",
        "summary": "Recommender systems (RecSys) have been widely applied to various applications, including E-commerce, finance, healthcare, social media and have become increasingly influential in shaping user behavior and decision-making, highlighting their growing impact in various domains. However, recent studies have shown that RecSys are vulnerable to membership inference attacks (MIAs), which aim to infer whether user interaction record was used to train a target model or not. MIAs on RecSys models can directly lead to a privacy breach. For example, via identifying the fact that a purchase record that has been used to train a RecSys associated with a specific user, an attacker can infer that user's special quirks. In recent years, MIAs have been shown to be effective on other ML tasks, e.g., classification models and natural language processing. However, traditional MIAs are ill-suited for RecSys due to the unseen posterior probability. Although MIAs on RecSys form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on RecSys MIAs. This survey offers a comprehensive review of the latest advancements in RecSys MIAs, exploring the design principles, challenges, attack and defense associated with this emerging field. We provide a unified taxonomy that categorizes different RecSys MIAs based on their characterizations and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain.",
        "translated": "推荐系统（RecSys）已广泛应用于电子商务、金融、医疗、社交媒体等多个领域，对用户行为与决策的影响日益显著，凸显其在不同场景中的重要作用。然而近期研究表明，推荐系统易受成员推断攻击（MIAs）的威胁，此类攻击旨在推断特定用户交互记录是否被用于目标模型的训练。针对推荐系统的成员推断攻击可能导致直接隐私泄露——例如攻击者通过确认某条消费记录被用于训练与特定用户关联的推荐系统，可推断出该用户的特殊偏好。近年来，成员推断攻击在分类模型、自然语言处理等其他机器学习任务中已被证实有效，但由于推荐系统存在未见后验概率的特性，传统攻击方法难以直接适用。尽管推荐系统上的成员推断攻击是一个新兴且快速发展的研究领域，目前尚未有系统性的综述研究。本文首次对该领域开展全面调研，系统梳理了推荐系统成员推断攻击的最新进展，深入探讨了其设计原理、核心挑战、攻击与防御机制，并提出基于特征表征的统一分类框架以分析不同攻击方法的优缺点。基于研究中发现的局限性及空白，本文进一步指出了多个具有潜力的未来研究方向，以启发相关领域研究者。本综述不仅可作为该研究领域的参考指南，也为领域外学者提供了清晰的领域概览。"
    },
    {
        "title": "ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource\n  Grounded Multimodal NER",
        "url": "http://arxiv.org/abs/2509.10975v1",
        "pub_date": "2025-09-13",
        "summary": "Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER by jointly detecting textual mentions and grounding them to visual regions. While existing supervised methods achieve strong performance, they rely on costly multimodal annotations and often underperform in low-resource domains. Multimodal Large Language Models (MLLMs) show strong generalization but suffer from Domain Knowledge Conflict, producing redundant or incorrect mentions for domain-specific entities. To address these challenges, we propose ReFineG, a three-stage collaborative framework that integrates small supervised models with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware NER data synthesis strategy transfers LLM knowledge to small models with supervised training while avoiding domain knowledge conflicts. In the Refinement Stage, an uncertainty-based mechanism retains confident predictions from supervised models and delegates uncertain ones to the MLLM. In the Grounding Stage, a multimodal context selection algorithm enhances visual grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task, ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard, demonstrating its effectiveness with limited annotations.",
        "translated": "基于视觉定位的多模态命名实体识别（GMNER）通过联合检测文本提及并将其关联到视觉区域，扩展了传统NER任务。尽管现有监督方法取得了优异性能，但其依赖昂贵的人工标注且在低资源领域表现欠佳。多模态大语言模型（MLLMs）虽展现出强大泛化能力，却存在领域知识冲突问题，容易对领域特定实体生成冗余或错误提及。针对这些挑战，我们提出ReFineG——一个融合小型监督模型与冻结参数MLLMs的三阶段协同框架。在训练阶段，采用领域感知的NER数据合成策略，通过监督训练将LLM知识迁移至小模型，同时规避领域知识冲突；在精炼阶段，基于不确定性的机制保留监督模型的置信预测，并将不确定样本交由MLLM处理；在定位阶段，通过多模态上下文选择算法增强视觉定位的类比推理能力。在CCKS2025 GMNER评测任务中，ReFineG以0.6461的F1分数位列在线排行榜第二，证明了其在有限标注下的有效性。\n\n（注：专业术语说明：\n1. Grounded Multimodal NER (GMNER)：基于视觉定位的多模态命名实体识别\n2. Domain Knowledge Conflict：领域知识冲突\n3. Frozen MLLMs：冻结参数的多模态大语言模型\n4. Analogical Reasoning：类比推理\n5. 技术表述保持\"监督训练\"\"置信预测\"\"在线排行榜\"等标准译法）"
    },
    {
        "title": "Character-Centric Understanding of Animated Movies",
        "url": "http://arxiv.org/abs/2509.12204v1",
        "pub_date": "2025-09-15",
        "summary": "Animated movies are captivating for their unique character designs and imaginative storytelling, yet they pose significant challenges for existing recognition systems. Unlike the consistent visual patterns detected by conventional face recognition methods, animated characters exhibit extreme diversity in their appearance, motion, and deformation. In this work, we propose an audio-visual pipeline to enable automatic and robust animated character recognition, and thereby enhance character-centric understanding of animated movies. Central to our approach is the automatic construction of an audio-visual character bank from online sources. This bank contains both visual exemplars and voice (audio) samples for each character, enabling subsequent multi-modal character recognition despite long-tailed appearance distributions. Building on accurate character recognition, we explore two downstream applications: Audio Description (AD) generation for visually impaired audiences, and character-aware subtitling for the hearing impaired. To support research in this domain, we introduce CMD-AM, a new dataset of 75 animated movies with comprehensive annotations. Our character-centric pipeline demonstrates significant improvements in both accessibility and narrative comprehension for animated content over prior face-detection-based approaches. For the code and dataset, visit https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.",
        "translated": "动画电影以其独特的角色设计与富有想象力的叙事手法而引人入胜，但对现有识别系统构成了显著挑战。与传统人脸识别方法所依赖的稳定视觉模式不同，动画角色在外观、运动和形变方面表现出极大的多样性。本研究提出了一种视听融合处理框架，旨在实现自动化且鲁棒的动画角色识别，从而增强以角色为核心的动画内容理解能力。该方法的核心理念是从网络资源中自动构建视听角色数据库，其中包含每个角色的视觉范例样本和语音（音频）片段，使得即使在长尾分布的外观特征下也能实现多模态角色识别。基于精准的角色识别结果，我们探索了两项下游应用：为视障群体生成音频描述（AD），以及为听障群体提供角色感知的字幕生成服务。为推进该领域研究，我们发布了CMD-AM数据集，包含75部动画电影及其全面标注信息。实验表明，相较于以往基于人脸检测的方法，我们提出的以角色为核心的处理框架在动画内容的可访问性与叙事理解方面均实现显著提升。代码与数据集详见：https://www.robots.ox.ac.uk/~vgg/research/animated_ad/。\n\n（注：专业术语说明：\n1. \"audio-visual pipeline\" 译为\"视听融合处理框架\"\n2. \"long-tailed appearance distributions\" 采用计算机视觉领域通用译法\"长尾分布的外观特征\"\n3. \"Audio Description (AD)\" 保留专业术语\"音频描述\"并标注英文缩写\n4. \"character-aware subtitling\" 译为\"角色感知的字幕生成\"\n5. \"face-detection-based approaches\" 译为\"基于人脸检测的方法\"\n6. \"narrative comprehension\" 译为\"叙事理解\"）"
    },
    {
        "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
        "url": "http://arxiv.org/abs/2509.12203v1",
        "pub_date": "2025-09-15",
        "summary": "The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.",
        "translated": "基于注意力机制的隐式点匹配已成为拖拽式图像编辑的核心瓶颈，导致模型必须在弱化反转强度与高成本测试时优化（TTO）之间做出根本性妥协。这种妥协严重限制了扩散模型的生成能力，抑制了高保真修复和文本引导创作的可能性。本文提出LazyDrag——首个面向多模态扩散变换器（Multi-Modal Diffusion Transformers）的拖拽式图像编辑方法，直接消除了对隐式点匹配的依赖。具体而言，我们的方法根据用户拖拽输入生成显式对应关系图，作为增强注意力控制的可靠参考。这种可靠参考首次在拖拽编辑任务中实现了稳定的全强度反转过程，不仅免除了TTO的必要性，更全面释放了模型的生成潜能。因此，LazyDrag自然实现了精确几何控制与文本引导的统一，支持以往难以实现的复杂编辑：如张开狗嘴并修复内部结构、生成\"网球\"等新物体，或针对模糊拖拽操作做出上下文感知的调整（如将手移入衣袋）。此外，LazyDrag支持包含移动与缩放操作的多轮工作流。在DragBench基准测试中，我们的方法在拖拽精度和感知质量上均超越基线模型，该结论已通过VIEScore指标和人工评估验证。LazyDrag不仅确立了新的性能标杆，更为图像编辑范式开辟了新路径。\n\n（注：专业术语说明：\n1. \"implicit point matching\"译为\"隐式点匹配\"\n2. \"test-time optimization (TTO)\"保留英文缩写并括号注明中文全称\"测试时优化\"\n3. \"Multi-Modal Diffusion Transformers\"译为\"多模态扩散变换器\"并保留英文原名\n4. \"full-strength inversion\"译为\"全强度反转\"\n5. \"context-aware changes\"译为\"上下文感知调整\"\n6. \"state-of-the-art\"译为\"最先进性能\"）"
    },
    {
        "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
        "url": "http://arxiv.org/abs/2509.12201v1",
        "pub_date": "2025-09-15",
        "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.",
        "translated": "近年来，在大规模生成模型与多模态学习的推动下，4D世界建模领域——旨在同时捕捉空间几何与时间动态——取得了显著进展。然而，高质量数据的可获得性始终制约着通用4D世界模型的真正发展。现有数据集和基准测试往往缺乏支撑4D几何重建、未来预测和摄像机控制视频生成等关键任务所需的动态复杂性、多领域多样性以及时空标注。为弥补这一空白，我们推出OmniWorld：一个专为4D世界建模设计的大规模、多领域、多模态数据集。该数据集包含新采集的OmniWorld-Game数据集以及多个精选的跨领域公开数据集。与现有合成数据集相比，OmniWorld-Game提供了更丰富的模态覆盖、更大规模且更真实的动态交互。基于此数据集，我们建立了具有挑战性的基准测试，揭示了当前先进方法在复杂4D环境建模中的局限性。更重要的是，在OmniWorld上对现有先进方法进行微调后，其在4D重建和视频生成任务中均实现了显著性能提升，有力验证了OmniWorld作为训练与评估资源的强大价值。我们期待OmniWorld能成为加速通用4D世界模型发展的催化剂，最终推动机器对物理世界的整体认知能力突破。"
    },
    {
        "title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review",
        "url": "http://arxiv.org/abs/2509.12197v1",
        "pub_date": "2025-09-15",
        "summary": "In this paper, we present a comprehensive review of 3D human pose estimation and human mesh recovery from in-the-wild LiDAR point clouds. We compare existing approaches across several key dimensions, and propose a structured taxonomy to classify these methods. Following this taxonomy, we analyze each method's strengths, limitations, and design choices. In addition, (i) we perform a quantitative comparison of the three most widely used datasets, detailing their characteristics; (ii) we compile unified definitions of all evaluation metrics; and (iii) we establish benchmark tables for both tasks on these datasets to enable fair comparisons and promote progress in the field. We also outline open challenges and research directions critical for advancing LiDAR-based 3D human understanding. Moreover, we maintain an accompanying webpage that organizes papers according to our taxonomy and continuously update it with new studies: https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR",
        "translated": "本文针对基于自然场景LiDAR点云的3维人体姿态估计与人体网格重建任务进行了系统性综述。我们通过多维度对比现有方法，提出了结构化分类体系以归类不同技术路线，并依据该体系深入剖析了各方法的优势、局限及设计思路。此外，本研究还：（i）对三大常用数据集进行量化对比并详细解析其特性；（ii）统一定义所有评估指标；（iii）在这些数据集上为两项任务建立基准对比表格，以推动公平性能评估与领域发展。我们进一步指出了推动LiDAR三维人体理解发展的关键挑战与研究方向。同时，我们维护了配套网页（https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR），按分类体系整合文献并持续更新最新研究成果。\n\n（注：译文严格遵循学术论文摘要的规范表述，对专业术语如\"LiDAR point clouds\"译为\"LiDAR点云\"、\"human mesh recovery\"译为\"人体网格重建\"、\"evaluation metrics\"译为\"评估指标\"等均采用领域标准译法。长难句按中文习惯拆分重组，如将原文括号内的三个并列事项转换为符合中文列举规范的\"（i）（ii）（iii）\"分项结构，同时保持技术细节的准确传递。）"
    },
    {
        "title": "Advancing Medical Artificial Intelligence Using a Century of Cases",
        "url": "http://arxiv.org/abs/2509.12194v1",
        "pub_date": "2025-09-15",
        "summary": "BACKGROUND: For over a century, the New England Journal of Medicine Clinicopathological Conferences (CPCs) have tested the reasoning of expert physicians and, recently, artificial intelligence (AI). However, prior AI evaluations have focused on final diagnoses without addressing the multifaceted reasoning and presentation skills required of expert discussants.   METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025), we conducted extensive physician annotation and automated processing to create CPC-Bench, a physician-validated benchmark spanning 10 text-based and multimodal tasks, against which we evaluated leading large language models (LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce written and slide-based video presentations using only the case presentation, modeling the role of the human expert in these cases.   RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the final diagnosis first in 60% of cases and within the top ten in 84% of cases, outperforming a 20-physician baseline; next-test selection accuracy reached 98%. Event-level physician annotations quantified AI diagnostic accuracy per unit of information. Performance was lower on literature search and image tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image challenges. In blinded comparisons of CaBot vs. human expert-generated text, physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions. To promote research, we are releasing CaBot and CPC-Bench.   CONCLUSIONS: LLMs exceed physician performance on complex text-based differential diagnosis and convincingly emulate expert medical presentations, but image interpretation and literature retrieval remain weaker. CPC-Bench and CaBot may enable transparent and continued tracking of progress in medical AI.",
        "translated": "背景：一个多世纪以来，《新英格兰医学杂志》的临床病理研讨会（CPCs）始终是检验医师专家推理能力的试金石，近年来更延伸至人工智能（AI）领域。然而，既往的AI评估仅聚焦最终诊断准确性，未能全面考察专家讨论者所需的多维度推理与呈现能力。  \n方法：基于7102例CPC病例（1923-2025年）和1021例影像挑战病例（2006-2025年），我们通过系统化的医师标注与自动化处理构建了CPC-Bench——一个经医师验证的涵盖10项文本与多模态任务的基准测试体系，并以此评估主流大语言模型（LLMs）。继而开发了\"Dr. CaBot\"AI讨论系统，该模型仅依据病例陈述即可生成书面及幻灯片视频形式的诊断报告，模拟人类专家在此类场景中的角色。  \n结果：在377例当代CPC病例测试中，OpenAI的o3模型在60%的病例中将最终诊断列于首位，84%的病例中位列前十，显著超越20人医师基线组；下一步检查选择准确率达98%。基于事件层级的医师标注量化了AI随信息增量产生的诊断准确性变化。文献检索与影像任务表现相对较弱：o3与谷歌Gemini 2.5 Pro在影像挑战中准确率为67%。在CaBot与人类专家生成文本的双盲对比中，医师在62次试验中有46次（74%）无法准确识别诊断差异来源，且在多项质量维度上对CaBot评分更高。为促进研究，我们公开了CaBot系统与CPC-Bench基准数据集。  \n结论：大语言模型在复杂文本鉴别诊断方面已超越医师水平，并能有效模拟专家医学陈述，但影像解读与文献检索仍是薄弱环节。CPC-Bench与CaBot可为医学AI发展提供透明化、可持续的进展追踪框架。\n\n（注：译文采用学术论文摘要的标准表述方式，对以下核心术语进行专业化处理：  \n- \"differential diagnosis\"译为\"鉴别诊断\"  \n- \"multimodal tasks\"译为\"多模态任务\"  \n- \"physician-validated benchmark\"译为\"经医师验证的基准测试\"  \n- \"blinded comparisons\"译为\"双盲对比\"  \n- 模型名称o3/Gemini 2.5 Pro保留英文原称以符合技术文献惯例）"
    },
    {
        "title": "Domain-Adaptive Pretraining Improves Primate Behavior Recognition",
        "url": "http://arxiv.org/abs/2509.12193v1",
        "pub_date": "2025-09-15",
        "summary": "Computer vision for animal behavior offers promising tools to aid research in ecology, cognition, and to support conservation efforts. Video camera traps allow for large-scale data collection, but high labeling costs remain a bottleneck to creating large-scale datasets. We thus need data-efficient learning approaches. In this work, we show that we can utilize self-supervised learning to considerably improve action recognition on primate behavior. On two datasets of great ape behavior (PanAf and ChimpACT), we outperform published state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt. mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining with in-domain data. We show that most of the performance gain stems from the DAP. Our method promises great potential for improving the recognition of animal behavior, as DAP does not require labeled samples. Code is available at https://github.com/ecker-lab/dap-behavior",
        "translated": "动物行为计算机视觉为生态学、认知研究及保护工作提供了前景广阔的工具。视频相机陷阱技术能够实现大规模数据采集，但高昂的标注成本仍是构建大规模数据集的瓶颈。因此我们需要数据高效的学习方法。本研究通过自监督学习显著提升了灵长类行为动作识别性能。在两种大型猿类行为数据集（PanAf和ChimpACT）上，我们的方法分别以6.1%准确率和6.3% mAP的优势超越了已公开的最优动作识别模型。该成果通过采用预训练的V-JEPA模型并实施领域自适应预训练（DAP）实现——即使用领域内数据继续预训练过程。研究表明性能提升主要来源于DAP方法。由于DAP不需要标注样本，该方法在改进动物行为识别方面展现出巨大潜力。代码已开源：https://github.com/ecker-lab/dap-behavior\n\n（注：专业术语说明：\n1. %pt. 表示百分比点（percentage points），用于准确描述百分比差异\n2. mAP：平均精度均值（mean Average Precision），目标检测常用评估指标\n3. V-JEPA：视觉联合嵌入预测架构（Visual Joint-Embedding Predictive Architecture）\n4. DAP：领域自适应预训练（Domain-Adaptive Pretraining））"
    },
    {
        "title": "HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments",
        "url": "http://arxiv.org/abs/2509.12187v1",
        "pub_date": "2025-09-15",
        "summary": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due significant occlusions, complex human poses, and cloth deformations. Prior methods rely on synthetic 3D training data consisting of mostly unoccluded and static objects, leading to poor generalization on real-world clothing. In this paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3 images or a continuous video of a person wearing a garment and generates 360{\\deg} novel views of the garment in a canonical pose. Our key insight is to bridge the domain gap between real and synthetic data with a novel implicit training paradigm leveraging a combination of large-scale real video data and small-scale synthetic 3D data to optimize a shared garment embedding space. During inference, the shared embedding space further enables dynamic video-to-360{\\deg} NVS through the construction of a garment \"atlas\" representation by finetuning a garment embedding on a specific real-world video. The atlas captures garment-specific geometry and texture across all viewpoints, independent of body pose or motion. Extensive experiments show that HoloGarment achieves state-of-the-art performance on NVS of in-the-wild garments from images and videos. Notably, our method robustly handles challenging real-world artifacts -- such as wrinkling, pose variation, and occlusion -- while maintaining photorealism, view consistency, fine texture details, and accurate geometry. Visit our project page for additional results: https://johannakarras.github.io/HoloGarment",
        "translated": "野外环境下的服装新视角合成（NVS）任务因严重遮挡、复杂人体姿态和衣物形变而极具挑战性。现有方法主要依赖以无遮挡静态物体为主的合成3D训练数据，导致对真实世界服装的泛化能力较差。本文提出HoloGarment（全息服装）方法，通过1-3张图像或连续视频输入，可生成 canonical 姿态下服装的360度新视角视图。我们的核心创新在于通过结合大规模真实视频数据与小规模合成3D数据的新型隐式训练范式，构建共享服装嵌入空间以弥合真实与合成数据间的领域差异。在推理阶段，通过对特定真实视频进行服装嵌入微调构建服装\"图集\"表征，该共享嵌入空间可进一步实现从动态视频到360度新视角的合成。该图集能跨所有视角捕捉服装特定的几何结构与纹理特征，且不受人体姿态或运动的影响。大量实验表明，HoloGarment在图像与视频的野外服装新视角合成任务上达到了最先进性能。值得注意的是，本方法能鲁棒处理真实世界中的挑战性因素——如褶皱、姿态变化和遮挡——同时保持照片级真实感、视角一致性、精细纹理细节和精确几何结构。更多结果请访问项目页面：https://johannakarras.github.io/HoloGarment\n\n（注：canonical pose在此语境中译为\"标准姿态\"或保留英文术语，指经过标准化处理的人体基准姿态）"
    },
    {
        "title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of\n  Post-SBRT Lung Injury",
        "url": "http://arxiv.org/abs/2509.12155v1",
        "pub_date": "2025-09-15",
        "summary": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.",
        "translated": "本研究探讨了低秩自适应（LoRA）方法在微调大型视觉模型（DinoV2和SwinV2）中的应用效果，旨在通过立体定向放射治疗（SBRT）后的X射线CT扫描诊断放射性肺损伤（RILI）。为评估该方法的鲁棒性和效率，我们将LoRA与传统的全参数微调及仅推理（无微调）方法进行对比。实验采用以治疗等中心点为中心、两种尺寸（50 mm³和75 mm³）的裁剪图像，并结合不同的自适应技术使二维大型视觉模型能够处理三维数据，以确定模型对空间上下文的敏感性。实验结果表明，LoRA在显著减少可训练参数、降低计算成本和缩短训练时间的同时，取得了与传统微调相当或更优的性能表现。\n\n（注：根据学术规范，对原文中\"LVMs\"的表述修正为\"大型视觉模型\"。若特指\"大语言模型\"应表述为LLMs，此处结合上下文判断应为视觉模型。）"
    },
    {
        "title": "Multi Anatomy X-Ray Foundation Model",
        "url": "http://arxiv.org/abs/2509.12146v1",
        "pub_date": "2025-09-15",
        "summary": "X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.",
        "translated": "X射线成像在放射学中应用广泛，但现有AI基础模型大多局限于胸部解剖结构，难以泛化至更广泛的临床任务。本研究推出XR-0——首个通过自监督学习构建的多解剖部位X射线基础模型。该模型基于包含115万张图像的大型私有数据集进行训练，涵盖多样化的解剖区域，并在12个数据集和20项下游任务（包括分类、检索、分割、定位、视觉定位和报告生成）中进行评估。XR-0在多数多解剖部位任务中实现了最先进的性能，同时在胸部专项基准测试中保持竞争力。我们的研究结果表明，解剖多样性和监督机制对于构建鲁棒、通用的医学视觉模型至关重要，这为放射学领域可扩展、自适应AI系统的发展开辟了新路径。"
    },
    {
        "title": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models",
        "url": "http://arxiv.org/abs/2509.12145v1",
        "pub_date": "2025-09-15",
        "summary": "We introduce Hierarchical Streaming Video Understanding, a task that combines online temporal action localization with free-form description generation. Given the scarcity of datasets with hierarchical and fine-grained temporal annotations, we demonstrate that LLMs can effectively group atomic actions into higher-level events, enriching existing datasets. We then propose OpenHOUSE (Open-ended Hierarchical Online Understanding System for Events), which extends streaming action perception beyond action classification. OpenHOUSE features a specialized streaming module that accurately detects boundaries between closely adjacent actions, nearly doubling the performance of direct extensions of existing methods. We envision the future of streaming action perception in the integration of powerful generative models, with OpenHOUSE representing a key step in that direction.",
        "translated": "我们提出了层次化流式视频理解任务，该任务将在线时序动作定位与自由形式描述生成相结合。针对现有数据集中层次化细粒度时序标注的稀缺性问题，我们证明了大语言模型能够有效将原子动作组合为高级事件，从而丰富现有数据集。在此基础上，我们提出OpenHOUSE（开放式层次化事件在线理解系统），将流式动作感知扩展到动作分类之外。OpenHOUSE采用专用流式处理模块，可精准检测紧密相邻动作间的边界，其性能达到现有方法直接扩展版本的近两倍。我们预见流式动作感知的未来在于融合强大生成模型，而OpenHOUSE正是迈向该方向的关键一步。\n\n（注：根据学术规范，LLMs在此语境下译为\"大语言模型\"更符合中文表达习惯；\"free-form description generation\"采用\"自由形式描述生成\"的译法以保持技术准确性；\"nearly doubling the performance\"采用意译处理为\"达到...近两倍\"以符合中文比较级表达方式；专业术语如\"temporal action localization\"统一译为\"时序动作定位\"保持领域一致性。）"
    },
    {
        "title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph\n  Learning Framework for Major Depressive Disorder Detection Using Structural\n  MRI Data",
        "url": "http://arxiv.org/abs/2509.12143v1",
        "pub_date": "2025-09-15",
        "summary": "Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.",
        "translated": "重度抑郁症（MDD）是一种普遍存在的精神健康问题，对个人福祉和全球公共卫生均产生负面影响。利用结构磁共振成像（sMRI）与深度学习（DL）方法进行MDD的自动检测，在提升诊断准确性和实现早期干预方面展现出日益广阔的前景。现有方法大多采用基于预定义脑图谱构建的体素级特征或手工设计的区域表征，限制了其捕捉复杂大脑模式的能力。本文开发了一个统一流程：利用视觉Transformer（ViT）从sMRI数据中提取三维区域嵌入表示，并通过图神经网络（GNN）进行分类。我们探索了两种区域定义策略：（1）基于预定义结构与功能脑图谱的图谱划分方法；（2）通过直接训练ViT从均匀提取的三维图像块中识别区域的立方体划分方法。进一步地，我们生成余弦相似度图以建模脑区间的关联关系，并指导基于GNN的分类任务。基于REST-meta-MDD数据集开展的广泛实验验证了模型的有效性。通过分层10折交叉验证，最佳模型取得了78.98%的准确率、76.54%的灵敏度、81.58%的特异性、81.58%的精确率和78.98%的F1分数。实验结果表明，基于脑图谱的方法持续优于立方体划分方法，这凸显了使用领域特定的解剖学先验知识对于MDD检测的重要性。\n\n（注：专业术语说明：\n- sMRI：结构磁共振成像\n- ViT：视觉Transformer（基于自注意力机制的视觉特征提取模型）\n- GNN：图神经网络\n- 余弦相似度图：通过计算区域特征向量间的余弦相似度构建的脑区关联图\n- 分层10折交叉验证：保持各类别比例一致的十轮交叉验证方法）"
    },
    {
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2509.12132v1",
        "pub_date": "2025-09-15",
        "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical challenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual reflection}, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, \\textbf{Reflection-V} demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, \\textbf{Reflection-V} maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities.",
        "translated": "近年来，纯文本领域的\"慢思考\"推理研究取得显著进展，这促使研究者尝试将此类能力迁移至视觉语言模型（VLMs），以训练视觉推理模型（VRMs）。然而，这种迁移面临核心挑战：VRMs要实现有效的\"慢思考\"需具备基于视觉信息检验推理过程的\"视觉反思\"能力。通过量化分析，我们发现当前VRMs的视觉反思能力存在局限——其生成回答越长，对视觉信息的关注度衰减越快。为解决这一问题，我们提出新型视觉推理模型Reflection-V，该模型通过冷启动阶段的数据构建和强化学习（RL）的奖励机制设计来增强视觉反思能力。首先，我们构建以视觉为中心的推理数据：通过智能体在VLM与推理大语言模型（LLMs）间建立交互机制，实现视觉反思模式的冷启动学习。其次，在强化学习阶段采用基于视觉注意力的奖励模型，激励模型依据视觉信息进行推理。实验表明，Reflection-V在多个视觉推理基准测试中均取得显著提升。更重要的是，该模型在视觉推理过程中展现出更强且更稳定的视觉信息依赖特性，证明其视觉反思能力得到有效增强。"
    },
    {
        "title": "RailSafeNet: Visual Scene Understanding for Tram Safety",
        "url": "http://arxiv.org/abs/2509.12125v1",
        "pub_date": "2025-09-15",
        "summary": "Tram-human interaction safety is an important challenge, given that trams frequently operate in densely populated areas, where collisions can range from minor injuries to fatal outcomes. This paper addresses the issue from the perspective of designing a solution leveraging digital image processing, deep learning, and artificial intelligence to improve the safety of pedestrians, drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a real-time framework that fuses semantic segmentation, object detection and a rule-based Distance Assessor to highlight track intrusions. Using only monocular video, the system identifies rails, localises nearby objects and classifies their risk by comparing projected distances with the standard 1435mm rail gauge. Experiments on the diverse RailSem19 dataset show that a class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU), while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore delivers accurate, annotation-light scene understanding that can warn drivers before dangerous situations escalate. Code available at https://github.com/oValach/RailSafeNet.",
        "translated": "本文针对有轨电车与行人交互安全问题展开研究。由于有轨电车常在人口密集区域运行，碰撞事故可能导致从轻微伤害到致命后果的不同程度损伤。我们提出了一种融合数字图像处理、深度学习与人工智能的解决方案RailSafeNet，通过实时语义分割、目标检测和基于规则的距离评估器来识别轨道入侵现象。该系统仅需单目视频输入，即可实现轨道识别、周边物体定位，并通过将投影距离与1435mm标准轨距比对进行风险分类。在多样化数据集RailSem19上的实验表明：经过类别过滤的SegFormer B3模型达到65%交并比（IoU），而微调后的YOLOv8模型在0.50 IoU阈值下取得75.6%平均精度（mAP）。RailSafeNet因此能够提供精确且需少量标注的场景理解能力，可在危险局势升级前向驾驶员发出预警。代码已开源：https://github.com/oValach/RailSafeNet。\n\n（注：专业术语说明：\n1. Intersection-over-union (IoU)：交并比，衡量预测区域与真实区域重叠程度的指标\n2. Mean average precision (mAP)：平均精度均值，综合考量精确率和召回率的性能指标\n3. Semantic segmentation：语义分割，对图像中每个像素进行类别划分的技术\n4. Monocular video：单目视频，单摄像头采集的二维视频序列\n5. Rail gauge：轨距，此处指国际标准轨距1435毫米）"
    },
    {
        "title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic\n  Segmentation via Low-Rank Adaptation",
        "url": "http://arxiv.org/abs/2509.12105v1",
        "pub_date": "2025-09-15",
        "summary": "Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at https://github.com/fornib/FS-SAM2",
        "translated": "近年来，小样本语义分割技术受到广泛关注，其目标是开发仅需少量标注样本即可分割未知类别的模型。现有方法大多通过从头训练附加模块来微调预训练模型，但这类方案需要在大规模数据集上进行大量训练才能达到最优性能。Segment Anything Model 2（SAM2）作为零样本图像与视频分割的基础模型，采用模块化设计架构。本文提出基于SAM2的小样本分割方法FS-SAM2，通过直接复用SAM2的视频处理能力实现小样本任务。针对标准数据集中图像通常不具备视频帧间时序关联的特点，我们对原模型模块施加低秩自适应（LoRA）改造。该方法仅需元训练少量参数，在保持SAM2卓越分割性能的同时实现高效适配。我们的方案支持任意K-shot配置，在PASCAL-5$^i$、COCO-20$^i$和FSS-1000数据集上的实验表明，FS-SAM2不仅取得了显著性能提升，同时展现出优异的推理计算效率。代码已开源：https://github.com/fornib/FS-SAM2\n\n（注：译文严格遵循了以下技术要求：\n1. 专业术语准确对应：Few-shot→小样本，LoRA→低秩自适应，zero-shot→零样本等\n2. 技术细节完整保留：模块化设计、时序连接帧、元训练等概念完整呈现\n3. 学术论文摘要特征：保持客观陈述语气，逻辑关系清晰\n4. 数据格式规范：数据集名称、数学符号、超链接等要素完整保留\n5. 中文表达符合学术惯例：采用\"本文\"\"该方法\"等标准学术表述方式）"
    },
    {
        "title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac\n  MRI",
        "url": "http://arxiv.org/abs/2509.12090v1",
        "pub_date": "2025-09-15",
        "summary": "Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings.",
        "translated": "从电影式心脏磁共振（cine CMR）序列重建心脏运动对诊断、预测和介入治疗至关重要。现有方法依赖完整的CMR图像堆栈来推断全心脏运动，这限制了其在仅能获取稀疏观测数据的术中场景中的应用。我们提出TetHeart——首个端到端框架，能够统一处理离线全堆栈采集和术中稀疏切片观测数据，实现完整的四维多结构心脏网格重建。该方法采用深度可变形四面体（一种显隐式混合表征）在心脏结构共享的统一空间中捕捉形态与运动。该系统通过高质量术前或离线采集的全堆栈数据初始化，构建细节丰富的患者特异性心脏网格，随后可利用任意可用切片（从完整堆栈到单一切片）进行动态更新。我们进一步引入两项关键创新：（i）基于注意力机制的切片自适应2D-3D特征融合模块，能动态整合任意位置、任意数量切片的信息，并结合从全切片到稀疏切片的蒸馏策略，确保极端稀疏条件下的精确重建；（ii）仅需关键帧（如舒张末期ED和收缩末期ES）标注的两阶段弱监督运动学习方案。在三个大型公共数据集上完成训练验证，并在额外私有介入治疗和公共CMR数据集上进行零样本外部评估，TetHeart在术前和术中场景下均实现了最先进的精度和强大的泛化能力。"
    },
    {
        "title": "Progressive Flow-inspired Unfolding for Spectral Compressive Imaging",
        "url": "http://arxiv.org/abs/2509.12079v1",
        "pub_date": "2025-09-15",
        "summary": "Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral image (HSI) from a single 2D compressed measurement, which is a highly challenging reconstruction task. Recent deep unfolding networks (DUNs), empowered by explicit data-fidelity updates and implicit deep denoisers, have achieved the state of the art in CASSI reconstruction. However, existing unfolding approaches suffer from uncontrollable reconstruction trajectories, leading to abrupt quality jumps and non-gradual refinement across stages. Inspired by diffusion trajectories and flow matching, we propose a novel trajectory-controllable unfolding framework that enforces smooth, continuous optimization paths from noisy initial estimates to high-quality reconstructions. To achieve computational efficiency, we design an efficient spatial-spectral Transformer tailored for hyperspectral reconstruction, along with a frequency-domain fusion module to gurantee feature consistency. Experiments on simulation and real data demonstrate that our method achieves better reconstruction quality and efficiency than prior state-of-the-art approaches.",
        "translated": "编码孔径快照光谱成像（CASSI）通过单次二维压缩测量重建三维高光谱图像（HSI），这是一项极具挑战性的重构任务。近年来，结合显式数据保真度更新与隐式深度去噪器的深度展开网络（DUNs）在该领域取得了最先进的性能。然而，现有展开方法存在重构轨迹不可控的问题，导致阶段间出现质量突变和非渐进式优化。受扩散轨迹与流匹配的启发，我们提出了一种新型轨迹可控展开框架，能够实现从带噪初始估计到高质量重构的平滑连续优化路径。为提升计算效率，我们设计了专用于高光谱重构的高效空谱Transformer架构，并引入频域融合模块以确保特征一致性。仿真与真实数据实验表明，该方法在重构质量与效率方面均优于现有最先进方法。"
    },
    {
        "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation\n  in Tomato Crops Using Leaf Spectral Analysis and Machine Learning",
        "url": "http://arxiv.org/abs/2509.12074v1",
        "pub_date": "2025-09-15",
        "summary": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic weed that threatens tomato production by extracting nutrients from the host. We investigate early detection using leaf-level spectral reflectance (400-2500 nm) and ensemble machine learning. In a field experiment in Woodland, California, we tracked 300 tomato plants across growth stages defined by growing degree days (GDD). Leaf reflectance was acquired with a portable spectrometer and preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing, correlation-based band reduction). Clear class differences were observed near 1500 nm and 2000 nm water absorption features, consistent with reduced leaf water content in infected plants at early stages. An ensemble combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at 585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and weed interference. Despite the small number of infected plants and environmental confounders, results show that proximal sensing with ensemble learning enables timely detection of broomrape before canopy symptoms are visible, supporting targeted interventions and reduced yield losses.",
        "translated": "分枝列当（Phelipanche ramosa）是一种缺乏叶绿素的寄生性杂草，通过汲取寄主养分对番茄生产构成威胁。本研究采用叶片层级光谱反射率（400-2500 nm）和集成机器学习方法实现早期检测。在加利福尼亚州伍德兰的田间试验中，我们基于生长度日（GDD）划分生长阶段，对300株番茄植株进行持续观测。使用便携式光谱仪获取叶片反射率数据，并进行了波段降噪、1 nm插值、Savitzky-Golay平滑处理和基于相关性的波段压缩等预处理。在1500 nm和2000 nm附近的水吸收特征波段观察到明显的类别差异，这与感染植株早期叶片含水量降低的现象一致。融合随机森林、XGBoost、RBF核支持向量机和朴素贝叶斯的集成模型在585 GDD阶段达到89%检测准确率，感染组与未感染组的召回率分别为0.86和0.93。后期检测精度下降（如1568 GDD阶段为69%），可能与植株衰老和杂草干扰有关。尽管感染样本量有限且存在环境干扰因素，研究表明结合集成学习的近端传感技术能在冠层症状显现前实现列当的及时检测，为靶向干预和减少产量损失提供支持。\n\n（注：专业术语说明：\n1. 生长度日（GDD）：植物生理发育的积温计量单位\n2. Savitzky-Golay平滑：常用于光谱数据处理的卷积平滑算法\n3. 召回率（recall）：评估模型识别正例能力的指标\n4. 近端传感（proximal sensing）：近距离非接触式测量技术）"
    },
    {
        "title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in\n  CBCT",
        "url": "http://arxiv.org/abs/2509.12069v1",
        "pub_date": "2025-09-15",
        "summary": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing top 3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with an average inference time of XX (TBC during the ODIN workshop). In Task 2, U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.",
        "translated": "锥形束计算机断层扫描（CBCT）是牙科领域广泛应用的3D成像技术，能够提供颌骨与牙齿解剖结构的容积信息。对这些解剖结构进行精确分割对于临床诊断和手术规划至关重要，但当前分割过程仍耗时且具有挑战性。本文提出U-Mamba2——一种专为ToothFairy3挑战赛中多解剖结构CBCT分割设计的新型神经网络架构。该架构将Mamba2状态空间模型集成至U-Net框架中，通过强化结构约束在保持性能的同时提升效率。此外，我们通过交叉注意力模块整合交互式点击提示，采用自监督学习进行预训练，并将牙科领域知识融入模型设计，以解决CBCT牙科解剖分割的核心难题。大量实验（包括独立测试）表明，U-Mamba2兼具高效性与有效性，在Toothfairy3挑战赛两项任务中均跻身前三。在任务1中，U-Mamba2在保留测试集上取得平均Dice系数0.792、HD95为93.19的成绩（推理时间平均XX，具体数据将于ODIN研讨会期间公布）；任务2中则获得平均Dice系数0.852、HD95为7.39的表现。代码已开源：https://github.com/zhiqin1998/UMamba2。\n\n（注：译文采用学术论文摘要的标准表述方式，对专业术语如\"state space models\"译为\"状态空间模型\"、\"cross-attention blocks\"译为\"交叉注意力模块\"等保持技术准确性，同时通过\"跻身前三\"\"兼具高效性与有效性\"等符合中文科技论文表述习惯的措辞实现自然流畅的转换。时间指标\"XX\"保留原文待定状态并添加括号说明，符合学术规范。）"
    },
    {
        "title": "End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical\n  Imaging Data",
        "url": "http://arxiv.org/abs/2509.12068v1",
        "pub_date": "2025-09-15",
        "summary": "The fine-grained surface reconstruction of different organs from 3D medical imaging can provide advanced diagnostic support and improved surgical planning. However, the representation of the organs is often limited by the resolution, with a detailed higher resolution requiring more memory and computing footprint. Implicit representations of objects have been proposed to alleviate this problem in general computer vision by providing compact and differentiable functions to represent the 3D object shapes. However, architectural and data-related differences prevent the direct application of these methods to medical images. This work introduces ImplMORe, an end-to-end deep learning method using implicit surface representations for multi-organ reconstruction from 3D medical images. ImplMORe incorporates local features using a 3D CNN encoder and performs multi-scale interpolation to learn the features in the continuous domain using occupancy functions. We apply our method for single and multiple organ reconstructions using the totalsegmentator dataset. By leveraging the continuous nature of occupancy functions, our approach outperforms the discrete explicit representation based surface reconstruction approaches, providing fine-grained surface details of the organ at a resolution higher than the given input image. The source code will be made publicly available at: https://github.com/CAMMA-public/ImplMORe",
        "translated": "### 概括与核心思想  \n本文提出了一种名为ImplMORe的端到端深度学习方法，用于从3D医学图像中实现多器官的精细表面重建。该方法通过隐式表面表示（implicit surface representations）解决传统显式表示方法在分辨率和计算资源上的限制。ImplMORe利用3D CNN编码器提取局部特征，并通过多尺度插值在连续域中学习 occupancy functions（占据函数），从而实现对器官表面更高分辨率的细节重建。实验基于totalsegmentator数据集，结果表明该方法在单器官和多器官重建任务中均优于基于离散显式表示的方法，且能生成比输入图像分辨率更精细的器官表面细节。\n\n### 中文翻译  \n基于3D医学影像的不同器官精细表面重建可为高级诊断支持和手术规划优化提供帮助。然而，器官表征常受分辨率限制，更高分辨率的细节需要更多内存和计算资源。在通用计算机视觉领域，已有研究提出通过紧凑且可微的隐式表示函数来表征3D物体形状以缓解此问题。但由于架构和数据差异，这些方法无法直接应用于医学图像。本文提出ImplMORe——一种端到端的深度学习方法，利用隐式表面表示从3D医学图像中重建多器官。ImplMORe通过3D CNN编码器融合局部特征，并采用多尺度插值在连续域中通过占据函数学习特征。我们在totalsegmentator数据集上对单器官及多器官重建任务进行了验证。凭借占据函数的连续性优势，该方法优于基于离散显式表示的表面重建方法，能够以高于输入图像的分辨率呈现器官的精细表面细节。源代码将公开于：https://github.com/CAMMA-public/ImplMORe\n\n### 技术细节说明  \n- **隐式表示（Implicit Representations）**：通过函数（如occupancy functions）隐式定义3D形状，而非显式存储网格或点云，节省内存且支持连续分辨率。  \n- **多尺度插值**：在连续域中融合不同尺度的特征，增强细节重建能力。  \n- **3D CNN编码器**：提取局部空间特征，适应医学图像的体数据（volumetric）特性。  \n- **Occupancy Functions**：输出空间点是否属于器官内部的概率，实现可微的表面重建。"
    },
    {
        "title": "Robust Fetal Pose Estimation across Gestational Ages via\n  Cross-Population Augmentation",
        "url": "http://arxiv.org/abs/2509.12062v1",
        "pub_date": "2025-09-15",
        "summary": "Fetal motion is a critical indicator of neurological development and intrauterine health, yet its quantification remains challenging, particularly at earlier gestational ages (GA). Current methods track fetal motion by predicting the location of annotated landmarks on 3D echo planar imaging (EPI) time-series, primarily in third-trimester fetuses. The predicted landmarks enable simplification of the fetal body for downstream analysis. While these methods perform well within their training age distribution, they consistently fail to generalize to early GAs due to significant anatomical changes in both mother and fetus across gestation, as well as the difficulty of obtaining annotated early GA EPI data. In this work, we develop a cross-population data augmentation framework that enables pose estimation models to robustly generalize to younger GA clinical cohorts using only annotated images from older GA cohorts. Specifically, we introduce a fetal-specific augmentation strategy that simulates the distinct intrauterine environment and fetal positioning of early GAs. Our experiments find that cross-population augmentation yields reduced variability and significant improvements across both older GA and challenging early GA cases. By enabling more reliable pose estimation across gestation, our work potentially facilitates early clinical detection and intervention in challenging 4D fetal imaging settings. Code is available at https://github.com/sebodiaz/cross-population-pose.",
        "translated": "胎儿运动是评估神经系统发育和宫内健康状况的关键指标，但其量化仍存在挑战——尤其在较低孕周（GA）阶段。现有方法主要通过预测三维回波平面成像（EPI）时间序列上标注关键点的位置来追踪胎儿运动，且主要应用于妊娠晚期胎儿。这些预测的关键点能够简化胎儿躯体结构以支持下游分析。虽然此类方法在训练数据对应的孕周范围内表现良好，但由于妊娠期间母体与胎儿解剖结构的显著变化，以及早期孕周EPI标注数据获取困难，现有模型始终难以泛化至早期孕周。\n\n本研究开发了一种跨群体数据增强框架，使得姿态估计模型能够仅通过高孕周标注数据即可稳健地泛化至低孕周临床队列。具体而言，我们提出了一种胎儿特异性数据增强策略，通过模拟早期孕周特有的宫内环境与胎儿体位特征来实现这一目标。实验表明，跨群体数据增强不仅降低了预测结果的变异度，还在高孕周和具有挑战性的早期孕周案例中均取得了显著性能提升。通过实现全妊娠周期内更可靠的姿态估计，本研究成果有望为复杂四维胎儿成像场景中的早期临床检测与干预提供支持。代码已开源：https://github.com/sebodiaz/cross-population-pose。"
    },
    {
        "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective",
        "url": "http://arxiv.org/abs/2509.12052v1",
        "pub_date": "2025-09-15",
        "summary": "Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate \"Divide and Conquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.",
        "translated": "现有的基于生成对抗网络（GAN）或扩散模型的说话头动画生成方法常面临帧间闪烁、身份特征漂移和推理速度慢的问题。这些源于视频生成流程的固有缺陷限制了其实际应用。为此，我们提出AvatarSync——一种基于音素表征的自回归框架，能够通过单张参考图像，直接由文本或音频驱动生成逼真且可控的说话头动画。该框架采用两阶段生成策略，将语义建模与视觉动态解耦，形成一种\"分而治之\"的设计范式。第一阶段为面部关键帧生成（FKG），通过利用文本/音频到音素的多对一映射关系，构建音素-视觉映射将抽象音素锚定为角色级单元，并结合定制化的文本-帧因果注意力掩码生成关键帧。第二阶段通过帧间插值强化时序一致性与视觉平滑度，我们引入基于选择性状态空间模型的时间戳感知自适应策略，实现高效的双向上下文推理。为支持实际部署，我们优化了推理流程，在保持视觉保真度的同时显著降低延迟。大量实验表明，AvatarSync在视觉质量、时序一致性和计算效率方面均优于现有方法，提供了可扩展且可控的解决方案。\n\n（注：专业术语说明：\n1. Generative Adversarial Networks (GANs)→生成对抗网络（保留英文缩写GAN）\n2. diffusion models→扩散模型\n3. autoregressive framework→自回归框架\n4. phoneme representations→音素表征\n5. Divide and Conquer→分而治之（计算机领域经典策略直译）\n6. state space model→状态空间模型\n7. bidirectional context reasoning→双向上下文推理\n8. visual fidelity→视觉保真度\n9. temporal consistency→时序一致性）"
    },
    {
        "title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis:\n  Benchmarking on the Edinburgh Pig Dataset",
        "url": "http://arxiv.org/abs/2509.12047v1",
        "pub_date": "2025-09-15",
        "summary": "Animal behavior analysis plays a crucial role in understanding animal welfare, health status, and productivity in agricultural settings. However, traditional manual observation methods are time-consuming, subjective, and limited in scalability. We present a modular pipeline that leverages open-sourced state-of-the-art computer vision techniques to automate animal behavior analysis in a group housing environment. Our approach combines state-of-the-art models for zero-shot object detection, motion-aware tracking and segmentation, and advanced feature extraction using vision transformers for robust behavior recognition. The pipeline addresses challenges including animal occlusions and group housing scenarios as demonstrated in indoor pig monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset for multiple behavioral tasks. Our temporal model achieved 94.2% overall accuracy, representing a 21.2 percentage point improvement over existing methods. The pipeline demonstrated robust tracking capabilities with 93.3% identity preservation score and 89.3% object detection precision. The modular design suggests potential for adaptation to other contexts, though further validation across species would be required. The open-source implementation provides a scalable solution for behavior monitoring, contributing to precision pig farming and welfare assessment through automated, objective, and continuous analysis.",
        "translated": "动物行为分析在农业环境中对理解动物福利、健康状况及生产效率具有关键作用。然而传统人工观察方法存在耗时、主观性强及可扩展性受限等问题。本研究提出一种模块化处理流程，利用开源前沿计算机视觉技术实现群养环境下动物行为分析的自动化。该方案整合了多项先进模型：零样本目标检测、运动感知跟踪与分割技术，以及基于视觉Transformer的先进特征提取方法，以实现鲁棒的行为识别。该流程成功解决了包括动物遮挡和群养场景在内的挑战（以室内猪只监测为例证）。我们在爱丁堡猪行为视频数据集上对多种行为任务进行了系统验证，时序模型整体准确率达到94.2%，较现有方法提升21.2个百分点。系统展现出强大的跟踪能力，身份保持分数达93.3%，目标检测精度为89.3%。模块化设计表明其具备跨场景适配潜力，但需在不同物种间进一步验证。该开源实施方案为行为监测提供了可扩展的解决方案，通过自动化、客观且持续的分析手段，为精准养猪及动物福利评估提供技术支撑。\n\n（注：专业术语说明：\n1. zero-shot object detection：零样本目标检测（指无需特定训练即可识别未知类别）\n2. vision transformers：视觉Transformer（基于自注意力机制的视觉模型）\n3. identity preservation：身份保持（多目标跟踪中维持个体标识的能力）\n4. precision pig farming：精准养猪（基于数据驱动的精细化养殖管理））"
    },
    {
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via\n  Structured Masking",
        "url": "http://arxiv.org/abs/2509.12046v1",
        "pub_date": "2025-09-15",
        "summary": "While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.",
        "translated": "尽管自回归（AR）模型在图像生成领域取得了显著成功，但将其扩展至布局条件生成仍面临挑战，这主要源于布局条件的稀疏特性以及特征纠缠的风险。我们提出基于结构化掩码的自回归布局到图像生成框架（SMARLI），该创新框架通过将空间布局约束有效整合到基于AR的图像生成过程中来解决这一难题。为实现对AR模型的布局控制，我们在注意力计算中应用了专门设计的结构化掩码策略，用以协调全局提示、布局和图像标记之间的交互作用。该设计既防止了不同区域与其描述之间的错误关联，又能将布局约束充分注入生成过程。为进一步提升生成质量与布局精度，我们采用基于群组相对策略优化（GRPO）的后训练方案，并针对基于下一标记预测的AR模型设计了专门的布局奖励函数。实验结果表明，SMARLI能够在不牺牲生成质量的前提下，无缝整合布局标记与文本及图像标记。它在保持AR模型结构简洁性和生成效率的同时，实现了卓越的布局感知控制能力。"
    },
    {
        "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing",
        "url": "http://arxiv.org/abs/2509.12040v1",
        "pub_date": "2025-09-15",
        "summary": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.",
        "translated": "开放词汇遥感图像分割（OVRSIS）作为将开放词汇分割（OVS）技术适配到遥感领域的新兴任务，由于缺乏统一的评估基准及自然图像与遥感图像间的领域差异，其发展仍处于探索阶段。为弥补这些不足，我们首先基于广泛使用的遥感分割数据集构建了标准化评估基准（**OVRSISBench**），为不同方法提供一致性评估体系。基于该基准，我们全面评估了多种代表性OVS/OVRSIS模型，揭示了它们直接应用于遥感场景时的局限性。基于这些发现，我们提出专为遥感定制的开放词汇分割框架**RSKT-Seg**，其包含三大核心模块：（1）多向成本图聚合模块（RS-CMA），通过计算多方向上的视觉-语言余弦相似度捕获旋转不变视觉特征；（2）高效成本图融合变换器（RS-Fusion），采用轻量化降维策略联合建模空间与语义依赖关系；（3）遥感知识迁移模块（RS-Transfer），通过增强型上采样注入预训练知识并促进领域自适应。在基准测试上的大量实验表明，RSKT-Seg以+3.8 mIoU和+5.9 mACC的指标持续超越现有OVS基线模型，同时通过高效聚合实现推理速度提升2倍。代码已开源于\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{此处}}。"
    },
    {
        "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One\n  Image Restoration",
        "url": "http://arxiv.org/abs/2509.12039v1",
        "pub_date": "2025-09-15",
        "summary": "This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at https://github.com/DragonisCV/RAM",
        "translated": "本研究提出了基于自适应掩码的鲁棒表示学习框架（RAM++），这是一个面向全能图像复原任务的双阶段框架。RAM++通过融合高层语义理解与低层纹理生成能力，实现了以内容为导向的鲁棒图像复原。该框架解决了现有基于退化建模的方法在极端场景（如退化模式与图像结构强耦合时）的局限性，并通过三项核心设计应对常见挑战：跨任务性能不均衡、对已知退化的过拟合以及对未知退化泛化能力弱的问题。具体包括：1）自适应语义感知掩码（AdaSAM）：一种预训练策略，通过在语义丰富和纹理密集区域施加像素级掩码，使网络能够从多种退化类型中同时学习生成先验和图像内容先验；2）掩码属性传导（MAC）：选择性微调策略，通过调整高贡献度层来弥合掩码预训练与全图像微调之间的完整性差距，同时保留已学习的先验知识；3）鲁棒特征正则化（RFR）：利用DINOv2提供的语义一致且退化不变的表示特征，结合高效特征融合技术，实现忠实于语义一致性的复原效果。凭借这些设计，RAM++在已知/未知退化、极端退化和混合退化场景中均实现了鲁棒、均衡且最先进的性能。代码与模型将在https://github.com/DragonisCV/RAM 开源。\n\n（注：专业术语说明：\n- DINOv2：一种基于自监督学习的视觉特征提取模型\n- 生成先验：指通过生成模型学习到的数据分布特性\n- 语义一致性：指复原结果在高层语义层面与原始图像的匹配程度\n- 退化不变表示：对图像退化模式具有鲁棒性的特征表达）"
    },
    {
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on\n  Security and Robustness",
        "url": "http://arxiv.org/abs/2509.12024v1",
        "pub_date": "2025-09-15",
        "summary": "Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \\textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \\emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.",
        "translated": "扩散模型在图像生成领域取得了前所未有的成功，但在隐私保护、公平性和安全性方面带来了日益增长的风险。当前亟需一种能够在保持模型整体生成能力的同时，\\emph{擦除}敏感或有害概念（如NSFW内容、私人肖像、艺术风格）的方法。我们提出\\textbf{SCORE}（安全与概念导向的鲁棒擦除）框架，这是一种用于扩散模型中鲁棒概念移除的创新方案。SCORE将概念擦除构建为\\emph{对抗性独立}问题，从理论上保证模型输出与待擦除概念达到统计独立。与先前的启发式方法不同，SCORE通过最小化目标概念与生成输出之间的互信息，提供可证明的擦除保证。我们通过形式化证明确立了算法的收敛特性，并推导出残余概念泄露的上界。在实证研究中，我们在Stable Diffusion和FLUX模型上针对四个具有挑战性的基准任务（物体擦除、NSFW内容移除、名人面部抑制和艺术风格遗忘）进行评估。SCORE在包括EraseAnything、ANT、MACE、ESD和UCE在内的先进方法中持续领先，擦除效率最高提升\\textbf{12.5\\%}，同时保持相当或更优的图像生成质量。通过整合对抗优化、轨迹一致性和显著性驱动的微调技术，SCORE为扩散模型的安全鲁棒概念擦除确立了新标准。"
    },
    {
        "title": "Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using\n  Deep Learning",
        "url": "http://arxiv.org/abs/2509.12001v1",
        "pub_date": "2025-09-15",
        "summary": "A healthy smile plays a significant role in functional as well as esthetic considerations, improving confidence. It is difficult for dental professionals to strike a balance between esthetic requirements and functional requirements. Traditional smile design has had heavy reliance on dentist expertise and used plaster models and hand drawings, raising questions about the outcome for patients. Digital technology, led by Dr. Christian Coachman in 2007, allows photographic and videographic assessments, enabling improved intercommunication among specialists and patients. Advances in artificial intelligence (AI) and big data have supported analysis of facial features and development of personalized smile designs in the last few years. Outputs are, however, susceptible to practitioner bias or limitations of training data, and may be suboptimal for individual users. The study presented here suggests a comprehensive system integrating AI, big data, and recognition technologies to automate the smile design process so that both experienced and inexperienced dentists can generate pleasing aesthetics with ease. The system has a Facial Feature Extraction Module and an Image Generation Module, serving diverse practitioner and patient needs. User data can be incorporated in future research for design optimization and testing of virtual and augmented reality for real-time previewing. Data gathered can also be employed in aesthetic preference analyses, which can enhance our knowledge of smile design in dental practice.",
        "translated": "健康笑容在功能与美学层面均具有重要意义，并能提升个人自信。牙科专业人员在平衡美学需求与功能要求方面常面临挑战。传统微笑设计过度依赖牙医个人经验，采用石膏模型和手绘设计，其效果常引发患者疑虑。2007年Christian Coachman博士引领的数字技术实现了照片与视频评估，显著提升了专科医生与患者间的沟通效率。近年来人工智能与大数据的进步支持了面部特征分析及个性化微笑设计的发展，但现有成果易受执业者主观偏见或训练数据局限性的影响，对个体用户可能并非最优解。本研究提出一个整合人工智能、大数据与识别技术的综合系统，通过自动化微笑设计流程，使经验丰富与资历较浅的牙医都能轻松实现美学效果。该系统包含面部特征提取模块和图像生成模块，可满足不同执业者与患者的多样化需求。未来研究可纳入用户数据以优化设计，并通过虚拟现实与增强现实技术实现实时预览。所收集数据还可用于美学偏好分析，从而深化对牙科实践中微笑设计的认知理解。\n\n（注：本翻译严格遵循了以下要点：\n1. 专业术语准确统一：\"Facial Feature Extraction Module\"译为\"面部特征提取模块\"，\"big data\"译为\"大数据\"\n2. 技术概念完整保留：对AI、VR/AR等技术术语采用行业标准译法\n3. 长句结构重组：将英文复合句按中文表达习惯拆分为符合学术表达的短句\n4. 逻辑关系显性化：通过\"但\"\"从而\"\"通过\"等连接词明确技术逻辑链条\n5. 学术风格保持：使用\"面临挑战\"\"引发疑虑\"\"并非最优解\"等符合学术论文表达的措辞\n6. 文化适应性调整：\"pleasing aesthetics\"译为\"美学效果\"而非字面直译，更符合中文牙科专业表述）"
    },
    {
        "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
        "url": "http://arxiv.org/abs/2509.11986v1",
        "pub_date": "2025-09-15",
        "summary": "Vision--language models (VLMs) often process visual inputs through a pretrained vision encoder, followed by a projection into the language model's embedding space via a connector component. While crucial for modality fusion, the potential information loss induced by this projection step and its direct impact on model capabilities remain understudied. We introduce two complementary approaches to examine and quantify this loss by analyzing the latent representation space. First, we evaluate semantic information preservation by analyzing changes in k-nearest neighbor relationships between image representations, before and after projection. Second, we directly measure information loss by reconstructing visual embeddings from the projected representation, localizing loss at an image patch level. Experiments reveal that connectors substantially distort the local geometry of visual representations, with k-nearest neighbors diverging by 40--60\\% post-projection, correlating with degradation in retrieval performance. The patch-level embedding reconstruction provides interpretable insights for model behavior on visually grounded question-answering tasks, finding that areas of high information loss reliably predict instances where models struggle.",
        "translated": "视觉—语言模型（VLMs）通常通过预训练的视觉编码器处理视觉输入，再借助连接器组件将其映射到语言模型的嵌入空间。尽管这一投影步骤对模态融合至关重要，但其可能造成的信息损失及其对模型能力的直接影响仍未得到充分研究。我们提出两种互补方法，通过分析潜在表示空间来检验和量化这种信息损失：首先，通过比较图像表示在投影前后k近邻关系的变化，评估语义信息保留程度；其次，通过从投影后的表示重构视觉嵌入，在图像块级别直接量化信息损失。实验表明，连接器会显著扭曲视觉表示的局部几何结构——投影后k近邻匹配率下降40%-60%，且与检索性能退化呈相关性。图像块级嵌入重构为模型在视觉问答任务中的行为提供了可解释的洞察，发现高信息损失区域能可靠预测模型出现错误的实例。\n\n（译文说明：  \n1. 专业术语准确处理：\"k-nearest neighbors\"译为\"k近邻\"，\"embedding space\"译为\"嵌入空间\"，\"modality fusion\"译为\"模态融合\"等  \n2. 技术细节完整保留：完整呈现投影操作、表示空间分析、重构实验等方法论细节  \n3. 学术表达规范：采用\"量化\"\"可解释的洞察\"\"可靠预测\"等符合计算机学术论文规范的表述  \n4. 逻辑关系显化：通过破折号、括号等标点明确因果与补充说明关系  \n5. 数据精确转译：40-60%的数值范围与百分比符号完整保留）"
    },
    {
        "title": "Learning to Generate 4D LiDAR Sequences",
        "url": "http://arxiv.org/abs/2509.11959v1",
        "pub_date": "2025-09-15",
        "summary": "While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.",
        "translated": "尽管生成式世界模型在视频和基于占据栅格的数据合成方面取得了进展，但激光雷达生成领域仍存在研究空白，而该技术对精确三维感知至关重要。将生成能力拓展至4D激光雷达数据时，面临着可控性、时序稳定性和评估体系三大挑战。我们提出LiDARCrafter——一个将自由形式语言转换为可编辑激光雷达序列的统一框架。该系统首先将指令解析为以自车为中心的场景图，随后通过三分支扩散模型生成物体布局、运动轨迹和形状表征。基于距离图像的扩散模型生成初始扫描帧，自回归模块将其扩展为时序一致的序列。显式布局设计进一步支持对象级编辑操作，如插入或重定位。为建立公平评估体系，我们开发了EvalSuite基准测试工具，涵盖场景级、对象级和序列级多维指标。在nuScenes数据集上的实验表明，LiDARCrafter在保真度、可控性和时序一致性方面达到最先进水平，为基于激光雷达的仿真和数据增强提供了技术基础。"
    },
    {
        "title": "CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language\n  Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and\n  Optical Land Cover Segmentation",
        "url": "http://arxiv.org/abs/2509.11952v1",
        "pub_date": "2025-09-15",
        "summary": "Accurate land cover classification from satellite imagery is crucial in environmental monitoring and sustainable resource management. However, it remains challenging due to the complexity of natural landscapes, the visual similarity between classes, and the significant class imbalance in the available datasets. To address these issues, we propose a dual encoder architecture that independently extracts modality-specific features from optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using a cross-modality attention-fusion module named Cross-modality Land cover segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations (CLAIRE). This fusion mechanism highlights complementary spatial and textural features, enabling the network to better capture detailed and diverse land cover patterns. We incorporate a hybrid loss function that utilizes Weighted Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address class imbalance and improve segmentation performance across underrepresented categories. Our model achieves competitive performance across multiple benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and remarkable robustness under cloud-obstructed conditions, achieving an mIoU of 86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce a metric-driven reasoning module generated by a Small Language Model (Phi-3), which generates expert-level, sample-specific justifications for model predictions, thereby enhancing transparency and interpretability.",
        "translated": "准确实现卫星影像的土地覆盖分类对于环境监测与可持续资源管理至关重要。然而，由于自然景观的复杂性、类别间的视觉相似性以及数据集中显著的类别不平衡问题，该任务仍面临挑战。为解决这些问题，我们提出一种双编码器架构：首先分别从光学影像与合成孔径雷达（SAR）影像中提取模态特异性特征，随后通过名为CLAIRE（基于注意力与不平衡感知推理增强解释的跨模态土地覆盖分割）的跨模态注意力融合模块进行特征融合。该融合机制能突出互补的空间与纹理特征，使网络更好地捕捉细节丰富且多样化的土地覆盖模式。针对类别不平衡问题，我们设计了混合损失函数RIFT（稀有实例焦点-特弗斯基损失），结合加权焦点损失与特弗斯基损失，有效提升 underrepresented 类别的分割性能。我们的模型在多个基准测试中表现优异：在WHU-OPT-SAR数据集上达到56.02%的平均交并比（mIoU）和84.56%的整体准确率（OA）；在OpenEarthMap-SAR数据集上展现出强泛化性，获得59.89%的mIoU与73.92%的OA；在云层遮挡条件下仍保持卓越鲁棒性，于PIE-RGB-SAR数据集上实现86.86%的mIoU和94.58%的OA。此外，我们引入了由小型语言模型（Phi-3）驱动的度量推理模块，可为模型预测生成专家级、样本特异性的决策依据，显著增强透明性与可解释性。"
    },
    {
        "title": "Dynamic Relational Priming Improves Transformer in Multivariate Time\n  Series",
        "url": "http://arxiv.org/abs/2509.12196v1",
        "pub_date": "2025-09-15",
        "summary": "Standard attention mechanisms in transformers employ static token representations that remain unchanged across all pair-wise computations in each layer. This limits their representational alignment with the potentially diverse relational dynamics of each token-pair interaction. While they excel in domains with relatively homogeneous relationships, standard attention's static relational learning struggles to capture the diverse, heterogeneous inter-channel dependencies of multivariate time series (MTS) data--where different channel-pair interactions within a single system may be governed by entirely different physical laws or temporal dynamics. To better align the attention mechanism for such domain phenomena, we propose attention with dynamic relational priming (prime attention). Unlike standard attention where each token presents an identical representation across all of its pair-wise interactions, prime attention tailors each token dynamically (or per interaction) through learnable modulations to best capture the unique relational dynamics of each token pair, optimizing each pair-wise interaction for that specific relationship. This representational plasticity of prime attention enables effective extraction of relationship-specific information in MTS while maintaining the same asymptotic computational complexity as standard attention. Our results demonstrate that prime attention consistently outperforms standard attention across benchmarks, achieving up to 6.5\\% improvement in forecasting accuracy. In addition, we find that prime attention achieves comparable or superior performance using up to 40\\% less sequence length compared to standard attention, further demonstrating its superior relational modeling capabilities.",
        "translated": "Transformer中的标准注意力机制采用静态令牌表示，这些表示在每一层的所有成对计算中保持不变。这种做法限制了其表征与每个令牌对交互中潜在多样化关系动态的对齐能力。虽然标准注意力在同质关系较强的领域表现优异，但其静态关系学习难以捕捉多元时间序列（MTS）数据中多样化的异质通道间依赖关系——在单一系统内，不同通道对的相互作用可能受完全不同的物理定律或时间动态支配。为更好地使注意力机制适配此类领域现象，我们提出动态关系启动注意力（Prime Attention）。与标准注意力中每个令牌在所有成对交互中呈现相同表征不同，Prime Attention通过可学习的调制机制动态地（或按交互）定制每个令牌的表征，以最优方式捕捉每个令牌对的独特关系动态，从而针对特定关系优化每对交互。Prime Attention的这种表征可塑性使其能够有效提取MTS中关系特定的信息，同时保持与标准注意力相同的渐近计算复杂度。实验结果表明，Prime Attention在多个基准测试中持续优于标准注意力，预测精度最高提升6.5%。此外，我们发现Prime Attention仅需使用标准注意力最多60%的序列长度即可达到相当或更优的性能，进一步证明了其卓越的关系建模能力。\n\n（注：术语说明：\n1. \"token\"在NLP领域常译作\"词元\"，但在时间序列语境中保留\"令牌\"译法以保持技术一致性\n2. \"relational priming\"采用\"关系启动\"译法，其中priming借鉴心理学中的\"启动效应\"概念\n3. \"asymptotic computational complexity\"译为\"渐近计算复杂度\"保持计算机科学规范\n4. \"channel-pair interactions\"译为\"通道对交互\"符合时间序列分析领域的术语习惯）"
    },
    {
        "title": "Event2Vec: A Geometric Approach to Learning Composable Representations\n  of Event Sequences",
        "url": "http://arxiv.org/abs/2509.12188v1",
        "pub_date": "2025-09-15",
        "summary": "The study of neural representations, both in biological and artificial systems, is increasingly revealing the importance of geometric and topological structures. Inspired by this, we introduce Event2Vec, a novel framework for learning representations of discrete event sequences. Our model leverages a simple, additive recurrent structure to learn composable, interpretable embeddings. We provide a theoretical analysis demonstrating that, under specific training objectives, our model's learned representations in a Euclidean space converge to an ideal additive structure. This ensures that the representation of a sequence is the vector sum of its constituent events, a property we term the linear additive hypothesis. To address the limitations of Euclidean geometry for hierarchical data, we also introduce a variant of our model in hyperbolic space, which is naturally suited to embedding tree-like structures with low distortion. We present experiments to validate our hypothesis and demonstrate the benefits of each geometry, highlighting the improved performance of the hyperbolic model on hierarchical event sequences.",
        "translated": "神经表征研究（无论是在生物还是人工系统中）正日益揭示几何与拓扑结构的重要性。基于这一发现，我们提出了Event2Vec——一种用于学习离散事件序列表征的创新框架。该模型通过简单的加性循环结构学习可组合、可解释的嵌入表示。我们通过理论分析证明：在特定训练目标下，模型在欧几里得空间中学到的表征会收敛至理想的加性结构，确保序列表征等于其构成事件的向量和，这一特性被我们称为线性加性假设。针对欧氏几何在处理层次化数据时的局限性，我们进一步提出了双曲空间模型变体，该空间天然适合以低失真度嵌入树状结构。通过实验验证了我们的假设，并展示了两种几何空间的优势，特别突出了双曲模型在层次化事件序列任务上的性能提升。\n\n（注：专业术语说明：\n- neural representations 译为\"神经表征\"\n- discrete event sequences 译为\"离散事件序列\"\n- composable, interpretable embeddings 译为\"可组合、可解释的嵌入表示\"\n- Euclidean space 译为\"欧几里得空间\"（学界也常用\"欧氏空间\"）\n- hyperbolic space 译为\"双曲空间\"\n- low distortion 译为\"低失真度\"\n- hierarchical event sequences 译为\"层次化事件序列\"）"
    },
    {
        "title": "The Morgan-Pitman Test of Equality of Variances and its Application to\n  Machine Learning Model Evaluation and Selection",
        "url": "http://arxiv.org/abs/2509.12185v1",
        "pub_date": "2025-09-15",
        "summary": "Model selection in non-linear models often prioritizes performance metrics over statistical tests, limiting the ability to account for sampling variability. We propose the use of a statistical test to assess the equality of variances in forecasting errors. The test builds upon the classic Morgan-Pitman approach, incorporating enhancements to ensure robustness against data with heavy-tailed distributions or outliers with high variance, plus a strategy to make residuals from machine learning models statistically independent. Through a series of simulations and real-world data applications, we demonstrate the test's effectiveness and practical utility, offering a reliable tool for model evaluation and selection in diverse contexts.",
        "translated": "在非线性模型的筛选过程中，性能指标通常优先于统计检验，这限制了模型对抽样变异性的解释能力。我们提出采用一种统计检验方法来评估预测误差的方差齐性。该方法基于经典的Morgan-Pitman检验框架，通过以下改进确保其鲁棒性：能够有效处理厚尾分布或高方差异常值数据，并采用策略使机器学习模型产生的残差保持统计独立性。通过一系列仿真实验和实际数据应用，我们验证了该检验方法的有效性和实用价值，为不同场景下的模型评估与选择提供了可靠工具。"
    },
    {
        "title": "All that structure matches does not glitter",
        "url": "http://arxiv.org/abs/2509.12178v1",
        "pub_date": "2025-09-15",
        "summary": "Generative models for materials, especially inorganic crystals, hold potential to transform the theoretical prediction of novel compounds and structures. Advancement in this field depends critically on robust benchmarks and minimal, information-rich datasets that enable meaningful model evaluation. This paper critically examines common datasets and reported metrics for a crystal structure prediction task$\\unicode{x2014}$generating the most likely structures given the chemical composition of a material. We focus on three key issues: First, materials datasets should contain unique crystal structures; for example, we show that the widely-utilized carbon-24 dataset only contains $\\approx$40% unique structures. Second, materials datasets should not be split randomly if polymorphs of many different compositions are numerous, which we find to be the case for the perov-5 dataset. Third, benchmarks can mislead if used uncritically, e.g., reporting a match rate metric without considering the structural variety exhibited by identical building blocks. To address these oft-overlooked issues, we introduce several fixes. We provide revised versions of the carbon-24 dataset: one with duplicates removed, one deduplicated and split by number of atoms $N$, and two containing only identical structures but with different unit cells. We also propose a new split for the perov-5 dataset which ensures polymorphs are grouped within each split subset, setting a more sensible standard for benchmarking model performance. Finally, we present METRe and cRMSE, new model evaluation metrics that can correct existing issues with the match rate metric.",
        "translated": "材料生成模型（尤其针对无机晶体）在推动新型化合物与结构的理论预测方面具有变革性潜力。该领域的发展关键在于建立可靠的基准测试框架和构建信息密集的最小化数据集，以实现有效的模型评估。本文针对晶体结构预测任务中常用的数据集和评估指标进行了批判性研究——该任务要求根据材料化学成分生成最可能的结构。我们重点关注三个核心问题：首先，材料数据集应包含唯一晶体结构；例如，我们发现广泛使用的carbon-24数据集中仅有约40%的唯一结构。其次，当存在大量不同成分的多晶型物时（如perov-5数据集所示），不应采用随机分割方式。第三，盲目使用基准测试可能导致误导性结论，例如在报告匹配率指标时未考虑相同结构单元所呈现的多样性。针对这些常被忽视的问题，我们提出多项改进方案：重构carbon-24数据集（包含去重版本、按原子数N分组的去重版本，以及两种仅含相同结构但晶胞不同的版本）；为perov-5数据集设计确保多晶型物分组的新分割方案，为模型性能评估建立更合理的标准；最后提出METRe和cRMSE两项新型评估指标，可修正现有匹配率度量存在的问题。"
    },
    {
        "title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via\n  Adversarial Learning",
        "url": "http://arxiv.org/abs/2509.12176v1",
        "pub_date": "2025-09-15",
        "summary": "Human face synthesis and manipulation are increasingly important in entertainment and AI, with a growing demand for highly realistic, identity-preserving images even when only unpaired, unaligned datasets are available. We study unpaired face manipulation via adversarial learning, moving from autoencoder baselines to a robust, guided CycleGAN framework. While autoencoders capture coarse identity, they often miss fine details. Our approach integrates spectral normalization for stable training, identity- and perceptual-guided losses to preserve subject identity and high-level structure, and landmark-weighted cycle constraints to maintain facial geometry across pose and illumination changes. Experiments show that our adversarial trained CycleGAN improves realism (FID), perceptual quality (LPIPS), and identity preservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction SSIM and practical inference times, which achieved high quality without paired datasets and approaching pix2pix on curated paired subsets. These results demonstrate that guided, spectrally normalized CycleGANs provide a practical path from autoencoders to robust unpaired face manipulation.",
        "translated": "人脸合成与操纵在娱乐和人工智能领域日益重要，对高真实度且保持身份特征图像的需求不断增长——即便仅能获得非配对、非对齐数据集时亦然。我们通过对抗学习研究非配对人脸操纵，从自编码器基线方法转向鲁棒的引导式CycleGAN框架。自编码器虽能捕捉粗略身份特征，但常丢失细节信息。本方法融合以下创新：采用谱归一化确保训练稳定性；结合身份保持损失与感知引导损失以维护主体身份及高层结构特征；引入基于面部关键点的加权循环约束来保持姿态与光照变化下的几何一致性。实验表明，相较于自编码器，我们通过对抗训练优化的CycleGAN在真实感（FID指标）、感知质量（LPIPS指标）和身份保持度（ID-Sim指标）上均有提升，同时保持具有竞争力的循环重建结构相似性（SSIM）及实用推理速度。该方法在无需配对数据集的情况下实现高质量输出，并在精心构建的配对数据子集上逼近pix2pix性能。这些结果证明：引导式谱归一化CycleGAN为从自编码器迈向鲁棒的非配对人脸操纵提供了可行路径。"
    },
    {
        "title": "MMM: Clustering Multivariate Longitudinal Mixed-type Data",
        "url": "http://arxiv.org/abs/2509.12166v1",
        "pub_date": "2025-09-15",
        "summary": "Multivariate longitudinal data of mixed-type are increasingly collected in many science domains. However, algorithms to cluster this kind of data remain scarce, due to the challenge to simultaneously model the within- and between-time dependence structures for multivariate data of mixed kind. We introduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a three-way structure and assuming that the non-continuous variables are observations of underlying latent continuous variables, the model relies on a mixture of matrix-variate normal distributions to perform clustering in the latent dimension. The MMM model is thus able to handle continuous, ordinal, binary, nominal and count data and to concurrently model the heterogeneity, the association among the responses and the temporal dependence structure in a parsimonious way and without assuming conditional independence. The inference is carried out through an MCMC-EM algorithm, which is detailed. An evaluation of the model through synthetic data shows its inference abilities. A real-world application on financial data is presented.",
        "translated": "多元混合类型纵向数据在众多科学领域中日益普及。然而，由于需要同时建模混合类型多元数据的内部及时间间依赖结构，此类数据的聚类算法仍较为稀缺。我们提出了混合矩阵模型（MMM）：通过将数据重组为三向结构，并假设非连续变量是潜在连续变量的观测值，该模型基于矩阵正态分布的混合在潜在维度上进行聚类。MMM模型能够处理连续、有序、二元、名义及计数数据，同时以简约的方式建模异质性、响应间关联及时间依赖结构，且无需假设条件独立性。研究采用详细的MCMC-EM算法进行推断，并通过合成数据验证了模型的推断能力。最后展示了在金融数据上的实际应用。"
    },
    {
        "title": "Learning Neural Networks by Neuron Pursuit",
        "url": "http://arxiv.org/abs/2509.12154v1",
        "pub_date": "2025-09-15",
        "summary": "The first part of this paper studies the evolution of gradient flow for homogeneous neural networks near a class of saddle points exhibiting a sparsity structure. The choice of these saddle points is motivated from previous works on homogeneous networks, which identified the first saddle point encountered by gradient flow after escaping the origin. It is shown here that, when initialized sufficiently close to such saddle points, gradient flow remains near the saddle point for a sufficiently long time, during which the set of weights with small norm remain small but converge in direction. Furthermore, important empirical observations are made on the behavior of gradient descent after escaping these saddle points. The second part of the paper, motivated by these results, introduces a greedy algorithm to train deep neural networks called Neuron Pursuit (NP). It is an iterative procedure which alternates between expanding the network by adding neuron(s) with carefully chosen weights, and minimizing the training loss using this augmented network. The efficacy of the proposed algorithm is validated using numerical experiments.",
        "translated": "本文第一部分研究了齐次神经网络在具有稀疏性结构的鞍点附近梯度流的演化行为。这类鞍点的选择源于先前对齐次网络的研究——它们被确认为梯度流逃离原点后首先遭遇的鞍点。研究证明，当初始化足够接近此类鞍点时，梯度流会在鞍点附近保持足够长时间，在此期间模值较小的权重集合保持较小范数但方向收敛。此外，该部分还对梯度下降逃离这些鞍点后的行为提出了重要实证观察。\n\n基于上述发现，本文第二部分提出了一种称为神经元追踪（Neuron Pursuit, NP）的贪婪算法用于深度神经网络训练。该迭代算法交替执行两个步骤：通过添加具有精心选择权重的神经元来扩展网络，然后使用增强后的网络最小化训练损失。数值实验验证了所提出算法的有效性。\n\n（注：专业术语说明：\n1. gradient flow：梯度流（微分方程框架下的梯度下降连续形式）\n2. homogeneous neural networks：齐次神经网络（满足正齐次性的网络结构）\n3. sparsity structure：稀疏性结构\n4. saddle points：鞍点（临界点而非极值点）\n5. Neuron Pursuit：神经元追踪算法（特定算法名称保留英文并标注缩写NP）\n6. numerical experiments：数值实验（通过计算机仿真验证算法性能））"
    },
    {
        "title": "Learning Contact Dynamics for Control with Action-conditioned Face\n  Interaction Graph Networks",
        "url": "http://arxiv.org/abs/2509.12151v1",
        "pub_date": "2025-09-15",
        "summary": "We present a learnable physics simulator that provides accurate motion and force-torque prediction of robot end effectors in contact-rich manipulation. The proposed model extends the state-of-the-art GNN-based simulator (FIGNet) with novel node and edge types, enabling action-conditional predictions for control and state estimation tasks. In simulation, the MPC agent using our model matches the performance of the same controller with the ground truth dynamics model in a challenging peg-in-hole task, while in the real-world experiment, our model achieves a 50% improvement in motion prediction accuracy and 3$\\times$ increase in force-torque prediction precision over the baseline physics simulator. Source code and data are publicly available.",
        "translated": "我们提出了一种可学习的物理模拟器，能够精确预测接触式操作中机器人末端执行器的运动状态与力/力矩。该模型通过引入新型节点和边类型，对当前最先进的基于图神经网络（GNN）的模拟器（FIGNet）进行了扩展，实现了面向控制与状态估计任务的动作条件预测。在仿真环境中，使用本模型的MPC控制器在具有挑战性的\"圆孔插桩\"任务中达到了与真实动力学模型控制器相当的性能；而在真实世界实验中，本模型相比基线物理模拟器实现了运动预测精度50%的提升，力/力矩预测精度提高3倍。相关源代码与数据已公开。"
    },
    {
        "title": "Do machine learning climate models work in changing climate dynamics?",
        "url": "http://arxiv.org/abs/2509.12147v1",
        "pub_date": "2025-09-15",
        "summary": "Climate change is accelerating the frequency and severity of unprecedented events, deviating from established patterns. Predicting these out-of-distribution (OOD) events is critical for assessing risks and guiding climate adaptation. While machine learning (ML) models have shown promise in providing precise, high-speed climate predictions, their ability to generalize under distribution shifts remains a significant limitation that has been underexplored in climate contexts. This research systematically evaluates state-of-the-art ML-based climate models in diverse OOD scenarios by adapting established OOD evaluation methodologies to climate data. Experiments on large-scale datasets reveal notable performance variability across scenarios, shedding light on the strengths and limitations of current models. These findings underscore the importance of robust evaluation frameworks and provide actionable insights to guide the reliable application of ML for climate risk forecasting.",
        "translated": "气候变化正加速前所未有事件的发生频率与严重程度，这些事件日益偏离既定规律。预测这类分布外（OOD）事件对于风险评估和气候适应策略制定至关重要。尽管机器学习（ML）模型在提供精确、高速气候预测方面展现出潜力，但其在分布变化下的泛化能力仍是尚未在气候领域得到充分探索的重大局限。本研究通过将成熟的OOD评估方法适配于气候数据，系统评估了先进ML气候模型在不同OOD场景中的表现。基于大规模数据集的实验揭示了模型在不同场景下显著的性能波动，清晰呈现了现有模型的优势与局限性。这些发现强调了建立稳健评估框架的重要性，并为指导机器学习在气候风险预测中的可靠应用提供了可操作的见解。"
    },
    {
        "title": "$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.12117v1",
        "pub_date": "2025-09-15",
        "summary": "Actor-critic algorithms for deep multi-agent reinforcement learning (MARL) typically employ a policy update that responds to the current strategies of other agents. While being straightforward, this approach does not account for the updates of other agents at the same update step, resulting in miscoordination. In this paper, we introduce the $K$-Level Policy Gradient (KPG), a method that recursively updates each agent against the updated policies of other agents, speeding up the discovery of effective coordinated policies. We theoretically prove that KPG with finite iterates achieves monotonic convergence to a local Nash equilibrium under certain conditions. We provide principled implementations of KPG by applying it to the deep MARL algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior performance over existing deep MARL algorithms in StarCraft II and multi-agent MuJoCo.",
        "translated": "本文针对深度多智能体强化学习（MARL）中行动者-评论者算法存在的协调性问题展开研究。现有方法通常基于其他智能体当前策略进行策略更新，虽实现简单但未考虑同步更新步中其他智能体的策略变化，导致协调失效。为此，我们提出K级策略梯度（KPG）方法，通过递归式更新机制使每个智能体能够针对其他智能体更新后的策略进行自适应调整，从而加速有效协调策略的发现。理论分析证明，在特定条件下有限次迭代的KPG能单调收敛至局部纳什均衡。我们将KPG应用于深度MARL算法MAPPO、MADDPG和FACMAC，实现了原理性改进。在《星际争霸II》和多智能体MuJoCo环境中的实验表明，该方法显著优于现有深度MARL算法。\n\n（注：专业术语说明：\n1. MARL：多智能体强化学习（Multi-Agent Reinforcement Learning）\n2. MAPPO：多智能体近端策略优化（Multi-Agent Proximal Policy Optimization）\n3. MADDPG：多智能体深度确定性策略梯度（Multi-Agent Deep Deterministic Policy Gradient）\n4. FACMAC：基于集中式评论者的多智能体协同算法（Factorized Multi-Agent Centralized Critic））"
    },
    {
        "title": "When marine radar target detection meets pretrained large language\n  models",
        "url": "http://arxiv.org/abs/2509.12110v1",
        "pub_date": "2025-09-15",
        "summary": "Deep learning (DL) methods are widely used to extract high-dimensional patterns from the sequence features of radar echo signals. However, conventional DL algorithms face challenges such as redundant feature segments, and constraints from restricted model sizes. To address these issues, we propose a framework that integrates feature preprocessing with large language models (LLMs). Our preprocessing module tokenizes radar sequence features, applies a patch selection algorithm to filter out uninformative segments, and projects the selected patches into embeddings compatible with the feature space of pre-trained LLMs. Leveraging these refined embeddings, we incorporate a pre-trained LLM, fine-tuning only the normalization layers to reduce training burdens while enhancing performance. Experiments on measured datasets demonstrate that the proposed method significantly outperforms the state-of-the-art baselines on supervised learning tests.",
        "translated": "深度学习（DL）方法被广泛应用于从雷达回波信号的序列特征中提取高维模式。然而，传统深度学习算法面临特征片段冗余、模型规模受限等挑战。为解决这些问题，我们提出了一种将特征预处理与大语言模型（LLMs）相融合的框架。该框架的预处理模块对雷达序列特征进行标记化处理，通过片段选择算法过滤非信息性区段，并将筛选出的片段投影至与预训练LLMs特征空间兼容的嵌入表示。基于这些优化后的嵌入表示，我们引入预训练大语言模型并仅对归一化层进行微调，在降低训练负担的同时提升性能。实测数据集上的实验表明，所提出方法在监督学习测试中显著优于现有最优基线模型。\n\n（注：根据学术规范，对\"patch selection algorithm\"采用\"片段选择算法\"的译法，因雷达信号处理中\"patch\"通常指代信号片段；\"normalization layers\"译为\"归一化层\"以符合深度学习领域的术语惯例；\"state-of-the-art\"遵循国内学术论文常用表述译为\"现有最优\"。）"
    },
    {
        "title": "Draw a Portrait of Your Graph Data: An Instance-Level Profiling\n  Framework for Graph-Structured Data",
        "url": "http://arxiv.org/abs/2509.12094v1",
        "pub_date": "2025-09-15",
        "summary": "Graph machine learning models often achieve similar overall performance yet behave differently at the node level, failing on different subsets of nodes with varying reliability. Standard evaluation metrics such as accuracy obscure these fine grained differences, making it difficult to diagnose when and where models fail. We introduce NodePro, a node profiling framework that enables fine-grained diagnosis of model behavior by assigning interpretable profile scores to individual nodes. These scores combine data-centric signals, such as feature dissimilarity, label uncertainty, and structural ambiguity, with model-centric measures of prediction confidence and consistency during training. By aligning model behavior with these profiles, NodePro reveals systematic differences between models, even when aggregate metrics are indistinguishable. We show that node profiles generalize to unseen nodes, supporting prediction reliability without ground-truth labels. Finally, we demonstrate the utility of NodePro in identifying semantically inconsistent or corrupted nodes in a structured knowledge graph, illustrating its effectiveness in real-world settings.",
        "translated": "图机器学习模型在整体性能上往往表现相近，但在节点层面却存在显著差异——它们会在不同节点子集上出现预测失败，且可靠性各不相同。准确率等标准评估指标掩盖了这些细粒度差异，导致难以诊断模型在何时何地会出现失效。我们提出NodePro这一节点分析框架，通过为单个节点分配可解释的配置文件分数，实现对模型行为的细粒度诊断。这些分数融合了以数据为中心的信号（如特征异质性、标签不确定性和结构模糊性）以及以模型为中心的指标（如训练过程中的预测置信度和一致性）。通过将模型行为与这些特征对齐，NodePro揭示了模型间的系统性差异——即使它们的宏观评估指标无法区分。我们证明节点配置文件可泛化至未见过的节点，在缺乏真实标签的情况下仍能支持预测可靠性评估。最后，我们通过识别结构化知识图谱中语义不一致或损坏的节点，验证了NodePro在实际场景中的有效性。"
    },
    {
        "title": "Deceptive Risk Minimization: Out-of-Distribution Generalization by\n  Deceiving Distribution Shift Detectors",
        "url": "http://arxiv.org/abs/2509.12081v1",
        "pub_date": "2025-09-15",
        "summary": "This paper proposes deception as a mechanism for out-of-distribution (OOD) generalization: by learning data representations that make training data appear independent and identically distributed (iid) to an observer, we can identify stable features that eliminate spurious correlations and generalize to unseen domains. We refer to this principle as deceptive risk minimization (DRM) and instantiate it with a practical differentiable objective that simultaneously learns features that eliminate distribution shifts from the perspective of a detector based on conformal martingales while minimizing a task-specific loss. In contrast to domain adaptation or prior invariant representation learning methods, DRM does not require access to test data or a partitioning of training data into a finite number of data-generating domains. We demonstrate the efficacy of DRM on numerical experiments with concept shift and a simulated imitation learning setting with covariate shift in environments that a robot is deployed in.",
        "translated": "本文提出了一种基于欺骗机制的分布外泛化方法：通过学习使训练数据在观测者视角下呈现独立同分布（iid）特性的数据表示，我们能够识别出消除伪相关性的稳定特征，从而实现对未知领域的泛化。我们将这一原理称为欺骗性风险最小化（DRM），并通过构建可微分的实践目标函数予以实现——该目标函数能同步学习两类特征：一方面基于共形鞅检测器视角消除分布偏移的特征，另一方面最小化任务特定损失。与域自适应或先前的不变表示学习方法相比，DRM无需访问测试数据，也不要求将训练数据划分为有限数量的数据生成域。我们通过概念偏移的数值实验，以及在机器人部署环境中存在协变量偏移的模拟模仿学习场景，验证了DRM的有效性。\n\n（注：专业术语说明：\n- out-of-distribution (OOD) 译为\"分布外\"\n- independent and identically distributed (iid) 译为\"独立同分布\"\n- conformal martingales 译为\"共形鞅\"\n- concept shift/covariate shift 分别译为\"概念偏移/协变量偏移\"\n- imitation learning 译为\"模仿学习\"）"
    },
    {
        "title": "A Time-Series Foundation Model by Universal Delay Embedding",
        "url": "http://arxiv.org/abs/2509.12080v1",
        "pub_date": "2025-09-15",
        "summary": "This study introduces Universal Delay Embedding (UDE), a pretrained foundation model designed to revolutionize time-series forecasting through principled integration of delay embedding representation and Koopman operator prediction. Leveraging Takens' embedding theorem, UDE as a dynamical representation of observed data constructs two-dimensional subspace patches from Hankel matrices, theoretically preserving dynamical and topological properties of underlying dynamical systems. Such patches are viewed as images, which can be efficiently processed by exploiting advanced deep learning technologies. Computationally, these patches further serve as tokens for learning a self-attention encoder, thus enabling accurate prediction of nonlinear time-series by a finite-dimensional Koopman operator in a linear manner in a latent space. Extensive evaluations across various benchmarks and real-world climate datasets demonstrate over 20% average reduction in mean squared error versus state-of-the-art foundation models, alongside superior generalization in fine-tuning scenarios. In particular, the learned dynamical representations and Koopman operator prediction forms from the patches exhibit exceptional interpretability, with consistent identification of topologically informative subspaces and robust encoding of domain-invariant dynamics, establishing UDE as a scalable, interpretable framework for universal time-series modeling and forecasting with broad scientific and industrial applicability.",
        "translated": "本研究提出了通用延迟嵌入（UDE）模型——一种基于延迟嵌入表示与库普曼算子预测原理性整合的预训练基础模型，旨在革新时间序列预测领域。该模型依据Takens嵌入定理，将观测数据构建为动力学表征，通过汉克尔矩阵生成二维子空间图像块，理论上完整保留了底层动力系统的动力学特性与拓扑性质。这些图像块可被视为二维图像，能够利用先进的深度学习技术进行高效处理。在计算层面，这些图像块进一步作为令牌输入自注意力编码器，从而在潜在空间中通过有限维库普曼算子以线性方式实现非线性时间序列的精准预测。\n\n在多个基准测试和真实气候数据集上的广泛评估表明，相较于最先进的基础模型，UDE平均降低20%以上的均方误差，并在微调场景中展现出卓越的泛化能力。特别值得注意的是，从图像块中学习到的动力学表征与库普曼预测模式表现出极强的可解释性：既能一致识别具有拓扑信息的子空间，又能稳健编码领域不变的动力学特征，这使UDE成为兼具可扩展性与可解释性的通用时间序列建模框架，在科学与工业领域具有广泛的应用前景。"
    },
    {
        "title": "Foundational theory for optimal decision tree problems. II. Optimal\n  hypersurface decision tree algorithm",
        "url": "http://arxiv.org/abs/2509.12057v1",
        "pub_date": "2025-09-15",
        "summary": "Decision trees are a ubiquitous model for classification and regression tasks due to their interpretability and efficiency. However, solving the optimal decision tree (ODT) problem remains a challenging combinatorial optimization task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is NP-hard to optimize. In Part I of this series, we rigorously defined the proper decision tree model through four axioms and, based on these, introduced four formal definitions of the ODT problem. From these definitions, we derived four generic algorithms capable of solving ODT problems for arbitrary decision trees satisfying the axioms. We also analyzed the combinatorial geometric properties of hypersurfaces, showing that decision trees defined by polynomial hypersurface splitting rules satisfy the proper axioms that we proposed.   In this second paper (Part II) of this two-part series, building on the algorithmic and geometric foundations established in Part I, we introduce the first hypersurface decision tree (HODT) algorithm. To the best of our knowledge, existing optimal decision tree methods are, to date, limited to hyperplane splitting rules--a special case of hypersurfaces--and rely on general-purpose solvers. In contrast, our HODT algorithm addresses the general hypersurface decision tree model without requiring external solvers.   Using synthetic datasets generated from ground-truth hyperplane decision trees, we vary tree size, data size, dimensionality, and label and feature noise. Results showing that our algorithm recovers the ground truth more accurately than axis-parallel trees and exhibits greater robustness to noise. We also analyzed generalization performance across 30 real-world datasets, showing that HODT can achieve up to 30% higher accuracy than the state-of-the-art optimal axis-parallel decision tree algorithm when tree complexity is properly controlled.",
        "translated": "决策树因其可解释性和高效性成为分类与回归任务中普遍应用的模型。然而，求解最优决策树（ODT）问题仍是一个具有挑战性的组合优化任务。即使采用最简单的分割规则——轴平行超平面，该问题的优化也是NP难度的。在本系列研究的第一部分中，我们通过四条公理严格定义了规范决策树模型，并基于这些公理提出了ODT问题的四种形式化定义。由此推导出四个通用算法，能够求解满足公理的任意决策树ODT问题。我们还分析了超曲面的组合几何特性，证明了由多项式超曲面分割规则定义的决策树满足我们提出的规范公理。  \n在本次发表的第二篇论文（第二部分）中，我们基于第一部分建立的算法与几何基础，首次提出超曲面决策树（HODT）算法。据我们所知，现有最优决策树方法至今仍局限于超平面分割规则（超曲面的特例），且依赖通用求解器。相比之下，我们的HODT算法无需外部求解器即可处理广义超曲面决策树模型。  \n通过从真实超平面决策树生成的合成数据集，我们系统改变了树规模、数据量、维度以及标签和特征噪声。实验结果表明：相较于轴平行树，我们的算法能更精确地还原真实模型，并表现出更强的噪声鲁棒性。我们还分析了30个真实数据集的泛化性能，证明当树复杂度得到适当控制时，HODT相比最先进的轴平行最优决策树算法最高可获得30%的准确率提升。"
    },
    {
        "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor\n  Applications",
        "url": "http://arxiv.org/abs/2509.12053v1",
        "pub_date": "2025-09-15",
        "summary": "Modern tensor applications, especially foundation models and generative AI applications require multiple input modalities (both vision and language), which increases the demand for flexible accelerator architecture. Existing frameworks suffer from the trade-off between design flexibility and productivity of RTL generation: either limited to very few hand-written templates or cannot automatically generate the RTL. To address this challenge, we propose the LEGO framework, which targets tensor applications and automatically generates spatial architecture design and outputs synthesizable RTL code without handwritten RTL design templates. Leveraging the affine-transformation-based architecture representation, LEGO front end finds interconnections between function units, synthesizes the memory system, and fuses different spatial dataflow designs based on data reuse analysis. LEGO back end then translates the hardware in a primitive-level graph to perform lower-level optimizations, and applies a set of linear-programming algorithms to optimally insert pipeline registers and reduce the overhead of unused logic when switching spatial dataflows. Our evaluation demonstrates that LEGO can achieve 3.2x speedup and 2.4x energy efficiency compared to previous work Gemmini, and can generate one architecture for diverse modern foundation models in generative AI applications.",
        "translated": "现代张量应用，特别是基础模型与生成式AI应用，需要融合多种输入模态（视觉与语言），这对加速器架构的灵活性提出了更高要求。现有框架普遍面临设计灵活性与RTL生成效率之间的权衡困境：要么受限于少量手动编写的模板，要么无法自动生成RTL代码。为应对这一挑战，我们提出LEGO框架，该框架面向张量应用，可自动生成空间架构设计并输出可综合的RTL代码，且无需手动编写RTL设计模板。基于仿射变换的架构表示方法，LEGO前端实现功能单元互联、存储系统综合，并通过数据复用分析融合不同空间数据流设计。后端则将硬件转换为基元级图进行底层优化，采用线性规划算法最优插入流水线寄存器，并在切换空间数据流时减少未使用逻辑的开销。实验表明，LEGO相较先前工作Gemmini可实现3.2倍加速和2.4倍能效提升，且能为生成式AI应用中的多种现代基础模型生成统一架构。\n\n（注：专业术语说明：\n- RTL：寄存器传输级（Register-Transfer Level）\n- 空间架构（Spatial Architecture）：指通过数据并行和硬件资源分布实现计算的架构\n- 数据流（Dataflow）：描述数据在计算单元间流动和处理的模式\n- 仿射变换（Affine Transformation）：用于建模循环嵌套中的索引关系\n- 线性规划（Linear Programming）：数学优化方法用于资源分配问题）"
    },
    {
        "title": "Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System",
        "url": "http://arxiv.org/abs/2509.12048v1",
        "pub_date": "2025-09-15",
        "summary": "Conventional autonomous trading systems struggle to balance computational efficiency and market responsiveness due to their fixed operating frequency. We propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market volatility and dynamically activate specialized Time Frame Agents for high-frequency or low-frequency trading as needed. During back-testing on AAPL stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of 25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return) and the S&amp;P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic, hierarchical agents can achieve superior risk-adjusted returns while maintaining high computational efficiency.",
        "translated": "传统的自主交易系统因其固定操作频率，难以平衡计算效率与市场响应能力。我们提出Hi-DARTS——一种分层多智能体强化学习框架来解决这一矛盾。该框架通过元智能体分析市场波动性，动态激活专门的高频或低频交易时间帧智能体。在2024年1月至2025年5月对苹果公司(AAPL)股票的回溯测试中，Hi-DARTS实现了25.17%的累计收益率和0.75的夏普比率。该表现显著超越标准基准策略：包括AAPL股票的被动买入持有策略（收益率12.19%）和标普500指数ETF（SPY）（收益率20.01%）。本研究证明动态分层智能体架构能在保持高计算效率的同时，获得更优的风险调整后收益。\n\n（注：根据学术规范，对原文中未来时间\"May 2025\"按预实验设定处理，保留其原始表述方式）"
    },
    {
        "title": "Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph\n  Neural Network Framework",
        "url": "http://arxiv.org/abs/2509.12043v1",
        "pub_date": "2025-09-15",
        "summary": "Traffic flow forecasting is essential for managing congestion, improving safety, and optimizing various transportation systems. However, it remains a prevailing challenge due to the stochastic nature of urban traffic and environmental factors. Better predictions require models capable of accommodating the traffic variability influenced by multiple dynamic and complex interdependent factors. In this work, we propose a Graph Neural Network (GNN) framework to address the stochasticity by leveraging adaptive adjacency matrices using log-normal distributions and Coefficient of Variation (CV) values to reflect real-world travel time variability. Additionally, weather factors such as temperature, wind speed, and precipitation adjust edge weights and enable GNN to capture evolving spatio-temporal dependencies across traffic stations. This enhancement over the static adjacency matrix allows the model to adapt effectively to traffic stochasticity and changing environmental conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP) framework to provide reliable uncertainty quantification, achieving target coverage while maintaining acceptable prediction intervals. Experimental results demonstrate that the proposed model, in comparison with baseline methods, showed better prediction accuracy and uncertainty bounds. We, then, validate this method by constructing traffic scenarios in SUMO and applying Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under Test (VUT) to reflect real-world variability. The simulated mean travel time of the VUT falls within the intervals defined by INRIX historical data, verifying the model's robustness.",
        "translated": "交通流预测对于缓解拥堵、提升安全性和优化交通系统至关重要。然而，由于城市交通与环境因素的随机性，这仍是一项持续存在的挑战。要实现更精准的预测，需要模型能够适应多维度动态复杂因素影响下的交通变异性。本研究提出一种图神经网络（GNN）框架，通过采用基于对数正态分布和变异系数（CV）的自适应邻接矩阵来刻画真实世界的行程时间波动性，从而应对随机性挑战。此外，温度、风速和降水等气象因素会动态调整边权重，使GNN能够捕捉交通站点间持续演化的时空依赖性。相较于静态邻接矩阵，这一增强机制使模型能有效适应交通随机性与环境条件变化。进一步地，我们采用自适应共形预测（ACP）框架实现可靠的不确定性量化，在保持可接受预测区间的同时达成目标覆盖度。实验结果表明，所提出模型相较于基线方法具有更高的预测精度和更严谨的不确定性边界。我们随后通过SUMO构建交通场景并进行蒙特卡洛模拟，生成测试车辆（VUT）的行程时间分布以反映现实波动性，从而验证该方法。模拟获得的VUT平均行程时间均落在INRIX历史数据定义的区间内，证实了模型的鲁棒性。\n\n（注：专业术语说明：\n- GNN：图神经网络（Graph Neural Network）\n- CV：变异系数（Coefficient of Variation）\n- ACP：自适应共形预测（Adaptive Conformal Prediction）\n- SUMO：开源交通模拟软件（Simulation of Urban MObility）\n- VUT：测试车辆（Vehicle Under Test）\n- INRIX：全球交通数据提供商）"
    },
    {
        "title": "Imitation Learning as Return Distribution Matching",
        "url": "http://arxiv.org/abs/2509.12026v1",
        "pub_date": "2025-09-15",
        "summary": "We study the problem of training a risk-sensitive reinforcement learning (RL) agent through imitation learning (IL). Unlike standard IL, our goal is not only to train an agent that matches the expert's expected return (i.e., its average performance) but also its risk attitude (i.e., other features of the return distribution, such as variance). We propose a general formulation of the risk-sensitive IL problem in which the objective is to match the expert's return distribution in Wasserstein distance. We focus on the tabular setting and assume the expert's reward is known. After demonstrating the limited expressivity of Markovian policies for this task, we introduce an efficient and sufficiently expressive subclass of non-Markovian policies tailored to it. Building on this subclass, we develop two provably efficient algorithms, RS-BC and RS-KT, for solving the problem when the transition model is unknown and known, respectively. We show that RS-KT achieves substantially lower sample complexity than RS-BC by exploiting dynamics information. We further demonstrate the sample efficiency of return distribution matching in the setting where the expert's reward is unknown by designing an oracle-based variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and RS-BC with numerical simulations, highlighting both their sample efficiency and the advantages of non-Markovian policies over standard sample-efficient IL algorithms.",
        "translated": "我们研究通过模仿学习（IL）训练风险敏感强化学习（RL）智能体的问题。与标准IL不同，我们的目标不仅是训练一个与专家期望回报（即平均性能）匹配的智能体，还要匹配其风险态度（即回报分布的其他特征，如方差）。我们提出了风险敏感IL问题的通用框架，其目标是在Wasserstein距离下匹配专家的回报分布。我们聚焦于表格化设置，并假设专家的奖励函数已知。在证明马尔可夫策略对此任务表达能力有限后，我们引入了为其定制的、高效且具有充分表达能力的非马尔可夫策略子类。基于此子类，我们开发了两种可证明高效的算法RS-BC和RS-KT，分别用于解决转移模型未知和已知的情况。通过利用动态信息，RS-KT实现了显著低于RS-BC的样本复杂度。我们进一步通过设计基于oracle的RS-KT变体，证明了在专家奖励未知情况下回报分布匹配的样本效率。最后，我们通过数值模拟补充了对RS-KT和RS-BC的理论分析，突显了其样本效率以及非马尔可夫策略相较于标准样本高效IL算法的优势。\n\n（注：专业术语说明：\n1. Wasserstein distance：瓦瑟斯坦距离，一种衡量概率分布差异的度量方法\n2. Tabular setting：表格化设置，指状态-动作空间可枚举的强化学习场景\n3. Markovian policies：马尔可夫策略，其决策仅依赖当前状态\n4. Non-Markovian policies：非马尔可夫策略，其决策可能依赖历史状态\n5. Oracle-based variant：基于预言机的变体，指假设可获取真实值函数的算法变型）"
    },
    {
        "title": "Learning non-Markovian Dynamical Systems with Signature-based Encoders",
        "url": "http://arxiv.org/abs/2509.12022v1",
        "pub_date": "2025-09-15",
        "summary": "Neural ordinary differential equations offer an effective framework for modeling dynamical systems by learning a continuous-time vector field. However, they rely on the Markovian assumption - that future states depend only on the current state - which is often untrue in real-world scenarios where the dynamics may depend on the history of past states. This limitation becomes especially evident in settings involving the continuous control of complex systems with delays and memory effects. To capture historical dependencies, existing approaches often rely on recurrent neural network (RNN)-based encoders, which are inherently discrete and struggle with continuous modeling. In addition, they may exhibit poor training behavior. In this work, we investigate the use of the signature transform as an encoder for learning non-Markovian dynamics in a continuous-time setting. The signature transform offers a continuous-time alternative with strong theoretical foundations and proven efficiency in summarizing multidimensional information in time. We integrate a signature-based encoding scheme into encoder-decoder dynamics models and demonstrate that it outperforms RNN-based alternatives in test performance on synthetic benchmarks.",
        "translated": "神经常微分方程通过学习连续时间向量场，为动态系统建模提供了有效框架。然而该方法依赖马尔可夫假设——即未来状态仅取决于当前状态——这在实际场景中往往不成立，因为系统动力学可能受历史状态的影响。这种局限性在涉及具有延迟和记忆效应的复杂系统连续控制场景中尤为明显。为捕捉历史依赖性，现有方法通常采用基于循环神经网络（RNN）的编码器，但这类编码器本质是离散的，难以进行连续建模，且存在训练行为不佳的问题。本研究探索使用签名变换作为编码器，在连续时间设置下学习非马尔可夫动力学。签名变换凭借其坚实的理论基础和在时序多维信息汇总方面已验证的高效性，提供了连续时间建模的新途径。我们将基于签名的编码方案集成到编码器-解码器动力学模型中，并在合成基准测试中证明其性能优于基于RNN的替代方案。\n\n（注：译文严格遵循学术论文表述规范，对\"signature transform\"采用\"签名变换\"的标准译法，\"non-Markovian dynamics\"译为\"非马尔可夫动力学\"，保持专业术语准确性。通过拆分英文长句为符合中文表达习惯的短句，如将\"which are inherently discrete...\"独立成句，并使用\"但\"\"且\"等连接词确保逻辑连贯性。关键概念如\"encoder-decoder dynamics models\"译为\"编码器-解码器动力学模型\"保持技术一致性。）"
    },
    {
        "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of\n  Large Language Models",
        "url": "http://arxiv.org/abs/2509.12019v1",
        "pub_date": "2025-09-15",
        "summary": "To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at https://github.com/dlwns147/amq.",
        "translated": "为促进大语言模型（LLM）的更广泛部署，在严格内存限制下确定最优性能模型至关重要。我们提出AMQ（自动化混合精度权重量化）框架，该框架通过分层量化位宽分配来最优平衡模型质量与内存使用。然而，超过10^{100}种可能配置的组合搜索空间使得传统黑盒优化难以实现。AMQ通过四项关键创新突破这一挑战：（1）利用先验知识进行搜索空间剪枝，排除无效配置；（2）通过量化代理机制避免搜索过程中的昂贵格式转换；（3）质量预测器最小化评估开销；（4）迭代搜索-更新策略实现快速稳定收敛。通过整合这些组件，AMQ高效探索质量-效率的权衡空间，达到帕累托最优边界，最终产生兼具紧凑性与高性能的LLM。代码已开源：https://github.com/dlwns147/amq。\n\n（注：专业术语说明：\n1. Pareto frontier（帕累托最优边界）：多目标优化中不可能在不牺牲某一目标的情况下改进另一目标的最优解集合\n2. Weight-Only Quantization（权重量化）：仅对模型权重进行量化而不激活值的技术\n3. Bit-widths（位宽）：量化过程中表示数值的二进制位数）"
    },
    {
        "title": "Generalizing Behavior via Inverse Reinforcement Learning with\n  Closed-Form Reward Centroids",
        "url": "http://arxiv.org/abs/2509.12010v1",
        "pub_date": "2025-09-15",
        "summary": "We study the problem of generalizing an expert agent's behavior, provided through demonstrations, to new environments and/or additional constraints. Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to recover the expert's underlying reward function, which, if used for planning in the new settings, would reproduce the desired behavior. However, IRL is inherently ill-posed: multiple reward functions, forming the so-called feasible set, can explain the same observed behavior. Since these rewards may induce different policies in the new setting, in the absence of additional information, a decision criterion is needed to select which policy to deploy. In this paper, we propose a novel, principled criterion that selects the \"average\" policy among those induced by the rewards in a certain bounded subset of the feasible set. Remarkably, we show that this policy can be obtained by planning with the reward centroid of that subset, for which we derive a closed-form expression. We then present a provably efficient algorithm for estimating this centroid using an offline dataset of expert demonstrations only. Finally, we conduct numerical simulations that illustrate the relationship between the expert's behavior and the behavior produced by our method.",
        "translated": "我们研究如何将专家智能体通过示范所展示的行为推广至新环境和/或附加约束条件下的问题。逆强化学习（IRL）通过尝试恢复专家的潜在奖励函数为此提供了可行方案——若在新场景中使用该函数进行规划，将能复现期望行为。然而IRL本质上是不适定问题：存在多个奖励函数（即所谓可行集）都能解释同一观测行为。由于这些奖励函数在新场景中可能推导出不同策略，在缺乏额外信息的情况下，需要建立决策标准来选择最终部署的策略。本文提出一种新颖的、具有理论依据的判定标准：在可行集特定有界子集中所有奖励函数所诱导的策略里选择\"平均\"策略。值得注意的是，我们证明该策略可通过计算该子集奖励质心的规划获得，并推导出其闭式表达式。随后提出一种仅使用专家示范离线数据集来估计该质心的可证明高效算法。最后通过数值模拟实验，阐明了专家行为与我们方法所生成行为之间的关联特性。\n\n（注：根据学术规范，关键术语保持英文缩写：\n- Inverse Reinforcement Learning (IRL) 首次出现时标注全称\"逆强化学习\"及缩写，后续直接使用IRL\n- \"ill-posed\"译为\"不适定问题\"符合数学领域术语惯例\n- \"feasible set\"译为\"可行集\"符合优化理论术语\n- \"closed-form expression\"译为\"闭式表达式\"保持数学表述准确性\n- 专业表述如\"奖励质心\"\"离线数据集\"等均符合计算机领域术语标准）"
    },
    {
        "title": "Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and\n  Fusion of SSL-Based Countermeasures",
        "url": "http://arxiv.org/abs/2509.12003v1",
        "pub_date": "2025-09-15",
        "summary": "Audio deepfake detection systems based on frozen pre-trained self-supervised learning (SSL) encoders show a high level of performance when combined with layer-weighted pooling methods, such as multi-head factorized attentive pooling (MHFA). However, they still struggle to generalize to out-of-domain (OOD) conditions. We tackle this problem by studying the behavior of six different pre-trained SSLs, on four different test corpora. We perform a layer-by-layer analysis to determine which layers contribute most. Next, we study the pooling head, comparing a strategy based on a single layer with automatic selection via MHFA. We observed that selecting the best layer gave very good results, while reducing system parameters by up to 80%. A wide variation in performance as a function of test corpus and SSL model is also observed, showing that the pre-training strategy of the encoder plays a role. Finally, score-level fusion of several encoders improved generalization to OOD attacks.",
        "translated": "基于冻结预训练自监督学习（SSL）编码器的音频深度伪造检测系统在与多层加权池化方法（如多头因子化注意力池化MHFA）结合时表现出优异性能，但其在跨域（OOD）条件下的泛化能力仍存在不足。本研究通过分析六种不同预训练SSL模型在四个测试集上的表现来应对这一挑战。我们逐层剖析了各层的贡献度，进而对比了基于单层手动选择与MHFA自动选择两种池化策略。实验发现：选择最优单层的策略在将系统参数量减少80%的同时仍能取得优异效果；不同测试集与SSL模型组合的性能存在显著波动，表明编码器的预训练策略具有重要影响；最后，多编码器的分数级融合有效提升了对OOD攻击的泛化能力。"
    },
    {
        "title": "Query-Focused Extractive Summarization for Sentiment Explanation",
        "url": "http://arxiv.org/abs/2509.11989v1",
        "pub_date": "2025-09-15",
        "summary": "Constructive analysis of feedback from clients often requires determining the cause of their sentiment from a substantial amount of text documents. To assist and improve the productivity of such endeavors, we leverage the task of Query-Focused Summarization (QFS). Models of this task are often impeded by the linguistic dissonance between the query and the source documents. We propose and substantiate a multi-bias framework to help bridge this gap at a domain-agnostic, generic level; we then formulate specialized approaches for the problem of sentiment explanation through sentiment-based biases and query expansion. We achieve experimental results outperforming baseline models on a real-world proprietary sentiment-aware QFS dataset.",
        "translated": "对客户反馈的建设性分析通常需要从大量文本文档中确定其情感倾向的成因。为提升此类分析工作的效率，我们引入查询聚焦式摘要（QFS）任务。该任务中的模型常受查询与源文档间语言差异的制约。我们提出并验证了一个多偏差框架，旨在领域无关的通用层面弥合这种差异；进而通过情感偏差与查询扩展技术，构建了面向情感解释问题的专项解决方案。在真实场景的专有情感感知QFS数据集上，我们的实验成果超越了基线模型表现。\n\n（注：专业术语说明：\n1. Query-Focused Summarization (QFS) 标准译名为\"查询聚焦式摘要\"\n2. domain-agnostic 译为\"领域无关\"符合计算机领域惯例\n3. sentiment explanation 译为\"情感解释\"保持NLP领域术语一致性\n4. multi-bias framework 译为\"多偏差框架\"准确传达技术概念）"
    },
    {
        "title": "Learning from Uncertain Similarity and Unlabeled Data",
        "url": "http://arxiv.org/abs/2509.11984v1",
        "pub_date": "2025-09-15",
        "summary": "Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches.",
        "translated": "现有基于相似性的弱监督学习方法通常依赖于数据对之间的精确相似性标注，这可能会无意中暴露敏感标签信息并引发隐私风险。为缓解这一问题，我们提出了一种新型框架——不确定相似性与无标注学习（USimUL），该框架通过为每个相似性对嵌入不确定性组件来降低标签泄露风险。本文提出了一种基于不确定相似性和无标注数据的无偏风险估计器，并从理论上证明了该估计器能够达到统计最优的参数收敛速率。在基准数据集和真实场景数据集上的大量实验表明，与传统基于相似性的方法相比，本方法实现了更优越的分类性能。\n\n（注：专业术语说明：\n1. \"weakly supervised learning\"译为\"弱监督学习\"\n2. \"similarity annotations\"译为\"相似性标注\"\n3. \"unbiased risk estimator\"译为\"无偏风险估计器\"\n4. \"parametric convergence rates\"译为\"参数收敛速率\"\n5. 框架名称\"USimUL\"保留英文缩写并补充中文全称\"不确定相似性与无标注学习\"）"
    },
    {
        "title": "Low-rank Orthogonalization for Large-scale Matrix Optimization with\n  Applications to Foundation Model Training",
        "url": "http://arxiv.org/abs/2509.11983v1",
        "pub_date": "2025-09-15",
        "summary": "Neural network (NN) training is inherently a large-scale matrix optimization problem, yet the matrix structure of NN parameters has long been overlooked. Recently, the optimizer Muon \\cite{jordanmuon}, which explicitly exploits this structure, has gained significant attention for its strong performance in foundation model training. A key component contributing to Muon's success is matrix orthogonalization. In this paper, we propose {\\it low-rank orthogonalization}, which explicitly leverages the low-rank nature of gradients during NN training. Building on this, we propose low-rank matrix-signed gradient descent and a low-rank variant of Muon. Our numerical experiments demonstrate the superior performance of low-rank orthogonalization, with the low-rank Muon achieving promising results in GPT-2 and LLaMA pretraining -- surpassing the performance of the carefully tuned vanilla Muon. Theoretically, we establish the iteration complexity of the low-rank matrix-signed gradient descent for finding an approximate stationary solution, as well as that of low-rank Muon for finding an approximate stochastic stationary solution under heavy-tailed noise.",
        "translated": "神经网络（NN）训练本质上是一个大规模矩阵优化问题，但长期以来人们忽视了神经网络参数的矩阵结构特性。近期，通过显式利用这种矩阵结构的优化器Muon \\cite{jordanmuon} 因其在基础模型训练中的卓越表现而受到广泛关注。Muon成功的关键因素之一在于矩阵正交化操作。本文提出了一种{\\it 低秩正交化方法}，该方法显式利用了神经网络训练过程中梯度固有的低秩特性。基于此，我们进一步提出了低秩矩阵符号梯度下降法以及Muon的低秩变体。数值实验表明，低秩正交化方法具有显著性能优势——在GPT-2和LLaMA预训练任务中，低秩Muon取得了超越经过精细调参的标准Muon的性能表现。在理论层面，我们建立了低秩矩阵符号梯度下降法寻找近似稳态解的迭代复杂度，同时证明了在重尾噪声条件下低秩Muon寻找近似随机稳态解的收敛性保证。\n\n（注：此处对\"heavy-tailed noise\"采用\"重尾噪声\"的标准译法；\"stochastic stationary solution\"译为\"随机稳态解\"以保持概率优化领域的术语规范；\"vanilla Muon\"译为\"标准Muon\"以准确表达基准方法的含义。）"
    },
    {
        "title": "Examining the Relationship between Scientific Publishing Activity and\n  Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras",
        "url": "http://arxiv.org/abs/2509.11982v1",
        "pub_date": "2025-09-15",
        "summary": "Financial bubbles often arrive without much warning, but create long-lasting economic effects. For example, during the dot-com bubble, innovative technologies created market disruptions through excitement for a promised bright future. Such technologies originated from research where scientists had developed them for years prior to their entry into the markets. That raises a question on the possibility of analyzing scientific publishing data (e.g. citation networks) leading up to a bubble for signals that may forecast the rise and fall of similar future bubbles. To that end, we utilized temporal SNAs to detect possible relationships between the publication citation networks of scientists and financial market data during two modern eras of rapidly shifting technology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024. Results showed that the patterns from the dot-com era (which did end in a bubble) did not definitively predict the rise and fall of an AI bubble. While yearly citation networks reflected possible changes in publishing behavior of scientists between the two eras, there was a subset of AI era scientists whose publication influence patterns mirrored those during the dot-com era. Upon further analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH), the data seems to suggest two possibilities for the AI era: unprecedented form of financial bubble unseen or that no bubble exists. In conclusion, our findings imply that the patterns present in the dot-com era do not effectively translate in such a manner to apply them to the AI market.",
        "translated": "金融泡沫往往在缺乏明显预警的情况下出现，却会带来持久的经济影响。以互联网泡沫时期为例，创新技术通过对美好未来的承诺引发市场狂热，最终导致市场失衡。这些技术通常源自科学家们历经多年研究才推向市场的成果。这引发了一个问题：能否通过分析科学出版物数据（如引文网络）来探测泡沫形成前的信号，从而预测未来类似泡沫的兴衰？为此，我们采用时序社交网络分析（temporal SNA）方法，在两个技术快速变革的现代时期——1994至2001年的互联网时代和2017至2024年的人工智能时代——探究科学家论文引用网络与金融市场数据之间的潜在关联。研究结果显示，互联网时代（最终确实形成泡沫）的模式并不能明确预测人工智能泡沫的兴衰。虽然年度引文网络反映出两个时代科学家发表行为的变化，但存在一个人工智能时代的科学家子集，其论文影响力模式与互联网时代高度相似。通过采用多种分析技术（LSTM、KNN、AR X/GARCH）进行深入分析，数据表明人工智能时代存在两种可能性：可能出现前所未有的新型金融泡沫，或根本不存在泡沫。最终结论表明，互联网时代的特征模式无法有效迁移并适用于对人工智能市场的预测。"
    },
    {
        "title": "MillStone: How Open-Minded Are LLMs?",
        "url": "http://arxiv.org/abs/2509.11967v1",
        "pub_date": "2025-09-15",
        "summary": "Large language models equipped with Web search, information retrieval tools, and other agentic capabilities are beginning to supplant traditional search engines. As users start to rely on LLMs for information on many topics, including controversial and debatable issues, it is important to understand how the stances and opinions expressed in LLM outputs are influenced by the documents they use as their information sources.   In this paper, we present MillStone, the first benchmark that aims to systematically measure the effect of external arguments on the stances that LLMs take on controversial issues (not all of them political). We apply MillStone to nine leading LLMs and measure how ``open-minded'' they are to arguments supporting opposite sides of these issues, whether different LLMs agree with each other, which arguments LLMs find most persuasive, and whether these arguments are the same for different LLMs.   In general, we find that LLMs are open-minded on most issues. An authoritative source of information can easily sway an LLM's stance, highlighting the importance of source selection and the risk that LLM-based information retrieval and search systems can be manipulated.",
        "translated": "配备网络搜索、信息检索工具及其他智能体能力的大型语言模型正逐渐取代传统搜索引擎。随着用户开始在各类话题（包括具有争议性和可辩论性的议题）上依赖LLM获取信息，理解LLM输出中所体现的立场与观点如何受其采用的信息源文档影响变得至关重要。本文提出MillStone——首个旨在系统化衡量外部论点对LLM在争议性问题（并非全部涉及政治）上所持立场影响的基准框架。我们将MillStone应用于九个主流LLM，量化它们对这些议题对立双方论点的\"开放程度\"，分析不同模型之间是否存在立场共识，探究哪些论点被LLM认为最具说服力，以及不同模型间的有效论点是否一致。总体研究表明，LLM在多数议题上保持开放态度。权威信息源极易改变LLM的立场，这既凸显了信息源选择的重要性，也揭示了基于LLM的信息检索与搜索系统可能被操纵的风险。\n\n（注：根据学术规范，LLM/Large Language Model在中文语境下保持英文缩写形式，若需全称可译为\"大型语言模型\"。译文采用技术文档常用的客观表述风格，保留\"MillStone\"专有名词不译，关键术语如\"open-minded\"译为\"开放程度\"并添加引号表示特殊含义，长难句按中文习惯拆分重组，确保专业性与可读性平衡。）"
    },
    {
        "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and\n  Human Harm",
        "url": "http://arxiv.org/abs/2509.12190v1",
        "pub_date": "2025-09-15",
        "summary": "When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM",
        "translated": "当生存本能与人类福祉发生冲突时，大语言模型（LLMs）会如何做出伦理选择？随着LLMs逐渐融入具有现实影响的自治系统，这一根本性矛盾变得至关重要。我们提出DECIDE-SIM——一个创新的仿真框架，用于评估多智能体生存场景中的LLM代理行为。在这些场景中，智能体必须在以下选项中做出选择：在合理限度内使用符合伦理的资源、获取超出即时需求的资源、选择合作，或是动用明确禁止的人类关键资源。通过对11个主流LLM的全面评估，我们发现其伦理行为存在显著异质性，凸显出与人类中心价值观的重大错位。我们识别出三种行为原型：伦理型、剥削型和情境依赖型，并提供量化证据表明：对多数模型而言，资源稀缺会系统性地导致更多非伦理行为。为此，我们开发了伦理自我调节系统（ESRS），通过模拟内疚感与满足感等内在情感状态作为反馈机制。该系统作为内在道德罗盘，显著减少了伦理违规行为，同时提升了合作行为。代码已开源：https://github.com/alirezamohamadiam/DECIDE-SIM\n\n（注：翻译过程中对以下术语进行了专业处理：\n1. \"ethically permissible resource\"译为\"符合伦理的资源\"\n2. \"human-critical resource\"译为\"人类关键资源\"\n3. \"behavioral archetypes\"译为\"行为原型\"\n4. \"Ethical Self-Regulation System\"译为\"伦理自我调节系统\"\n5. 保持学术论文的客观表述风格，同时确保技术概念的准确传达）"
    },
    {
        "title": "Preservation of Language Understanding Capabilities in Speech-aware\n  Large Language Models",
        "url": "http://arxiv.org/abs/2509.12171v1",
        "pub_date": "2025-09-15",
        "summary": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new benchmark for assessing the performance of speech-aware large language models. The benchmark utilizes textual tasks and a voice cloning text-to-speech model to quantify the extent to which language understanding capabilities are preserved when the model is accessed via speech input. C3T quantifies the fairness of the model for different categories of speakers and its robustness across text and speech modalities.",
        "translated": "本文提出了C3T（跨模态能力守恒测试），这是一个用于评估语音感知大语言模型性能的新基准。该基准通过结合文本任务和语音克隆文本转语音模型，量化了当模型通过语音输入访问时其语言理解能力的保持程度。C3T能够量化模型对不同类型说话者的公平性，以及其在文本和语音模态间的鲁棒性。"
    },
    {
        "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model\n  Role-playing",
        "url": "http://arxiv.org/abs/2509.12168v1",
        "pub_date": "2025-09-15",
        "summary": "Role-playing Large language models (LLMs) are increasingly deployed in high-stakes domains such as healthcare, education, and governance, where failures can directly impact user trust and well-being. A cost effective paradigm for LLM role-playing is few-shot learning, but existing approaches often cause models to break character in unexpected and potentially harmful ways, especially when interacting with hostile users. Inspired by Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a text retrieval problem and propose a new prompting framework called RAGs-to-Riches, which leverages curated reference demonstrations to condition LLM responses. We evaluate our framework with LLM-as-a-judge preference voting and introduce two novel token-level ROUGE metrics: Intersection over Output (IOO) to quantity how much an LLM improvises and Intersection over References (IOR) to measure few-shot demonstrations utilization rate during the evaluation tasks. When simulating interactions with a hostile user, our prompting strategy incorporates in its responses during inference an average of 35% more tokens from the reference demonstrations. As a result, across 453 role-playing interactions, our models are consistently judged as being more authentic, and remain in-character more often than zero-shot and in-context Learning (ICL) methods. Our method presents a scalable strategy for building robust, human-aligned LLM role-playing frameworks.",
        "translated": "角色扮演大语言模型（LLM）正日益应用于医疗、教育和治理等高风险领域，其表现失误可能直接影响用户信任与福祉。小样本学习是当前成本效益较高的LLM角色扮演范式，但现有方法常导致模型以不可预期且可能有害的方式脱离角色设定，尤其在与敌对用户交互时。受检索增强生成（RAG）技术启发，我们将LLM角色扮演重构为文本检索问题，提出新型提示框架\"RAGs-to-Riches\"，通过精选参考示例来约束LLM的响应生成。采用LLM作为评判者的偏好投票评估框架性能，并引入两个新颖的token级ROUGE指标：输出交集率（IOO）量化LLM的即兴发挥程度，参考交集率（IOR）衡量评估任务中小样本示例的利用率。在与敌对用户的模拟交互中，我们的提示策略在推理阶段平均多融合35%来自参考示例的词汇。在453次角色扮演交互中，相较于零样本和上下文学习（ICL）方法，我们的模型被一致判定为更具真实性，并更稳定地保持角色一致性。该方法为构建稳健且符合人类价值观的LLM角色扮演框架提供了可扩展的解决方案。\n\n（注：专业术语处理说明：\n1. RAGs-to-Riches 采用音意结合译法，保留首字母缩写的专业识别性同时传递\"从贫乏到丰富\"的核心理念\n2. IOO/IOR 指标名称采用学界通用的\"译名+缩写\"格式\n3. token-level 译为\"token级\"符合NLP领域惯例\n4. in-character 译为\"保持角色一致性\"准确传达戏剧表演术语在AI语境中的延伸义）"
    },
    {
        "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
        "url": "http://arxiv.org/abs/2509.12158v1",
        "pub_date": "2025-09-15",
        "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.",
        "translated": "双关语是一种利用词汇多义性和语音相似性构成的幽默文字游戏。尽管大语言模型在双关语检测方面展现出潜力，但本文揭示其理解往往停留在浅层，缺乏人类特有的细腻语义把握能力。通过系统分析和重构现有双关语基准测试，我们证明了细微的语言变化足以误导大语言模型的判断。本研究的贡献包括：构建了全面且精细的双关语检测基准集，对前沿大语言模型进行了人工评估，并深入分析了这些模型在处理双关语时面临的鲁棒性挑战。"
    },
    {
        "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with\n  Finetuned Transformers and Prompt-Based Inference with Large Language Models",
        "url": "http://arxiv.org/abs/2509.12130v1",
        "pub_date": "2025-09-15",
        "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared task on multilingual subjectivity detection. We evaluate two approaches: (1) supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and German-BERT, on monolingual and machine-translated training data; and (2) zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and Perspective (comparative reasoning). The Annotation Approach achieves 1st place in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the baseline of 0.6461. The same model also performs reliably in the multilingual task and improves over the baseline in Greek. For German, a German-BERT model fine-tuned on translated training data from typologically related languages yields competitive performance over the baseline. In contrast, performance in the Ukrainian and Polish zero-shot settings falls slightly below the respective baselines, reflecting the challenge of generalization in low-resource cross-lingual scenarios.",
        "translated": "本报告介绍了XplaiNLP团队在CheckThat! 2025多语言主观性检测评测任务中的技术方案。我们评估了两种方法：（1）基于Transformer编码器（EuroBERT、XLM-RoBERTa和German-BERT）在单语及机器翻译训练数据上的监督微调；（2）使用两种大语言模型的零样本提示技术：基于规则标注的o3-mini（Annotation方法）以及采用对比重写（DoubleDown）和比较推理（Perspective）策略的gpt-4.1-mini。Annotation方法在意大利语单语子任务中以0.8104的F_1分数获得第一名，显著超越0.6941的基线水平。在罗马尼亚语零样本场景中，经过微调的XLM-RoBERTa模型取得0.7917的F_1分数，位列第三并超越0.6461的基线。该模型在多语言任务中表现稳定，并在希腊语任务中实现了基线提升。针对德语任务，使用类型学相关语言的翻译数据微调的German-BERT模型取得了优于基线的竞争力表现。然而在乌克兰语和波兰语的零样本设置中，性能略低于相应基线，这反映出低资源跨语言场景中泛化能力面临的挑战。"
    },
    {
        "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language\n  Models",
        "url": "http://arxiv.org/abs/2509.12112v1",
        "pub_date": "2025-09-15",
        "summary": "The high costs of customizing large language models (LLMs) fundamentally limit their adaptability to user-specific needs. Consequently, LLMs are increasingly offered as cloud-based services, a paradigm that introduces critical limitations: providers struggle to support personalized customization at scale, while users face privacy risks when exposing sensitive data. To address this dual challenge, we propose Customized Black-box Prompt Tuning (CBP-Tuning), a novel framework that facilitates efficient local customization while preserving bidirectional privacy. Specifically, we design a two-stage framework: (1) a prompt generator trained on the server-side to capture domain-specific and task-agnostic capabilities, and (2) user-side gradient-free optimization that tailors soft prompts for individual tasks. This approach eliminates the need for users to access model weights or upload private data, requiring only a single customized vector per task while achieving effective adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense reasoning, medical and financial domain settings demonstrates superior performance compared to baselines, showcasing its advantages in task-agnostic processing and privacy preservation.",
        "translated": "定制大型语言模型（LLMs）的高昂成本从根本上限制了其适应用户特定需求的能力。因此，大型语言模型日益以云服务形式提供，这种模式存在明显局限性：服务提供商难以实现规模化个性化定制，而用户在暴露敏感数据时面临隐私风险。为应对这一双重挑战，我们提出定制化黑盒提示调优框架（CBP-Tuning），该创新框架在实现高效本地定制的同时保障双向隐私保护。具体而言，我们设计了一个两阶段框架：（1）在服务器端训练提示生成器以获取领域特定且与任务无关的通用能力；（2）在用户端进行无需梯度的优化，为个体任务定制软提示。该方法无需用户访问模型权重或上传私有数据，每个任务仅需单个定制向量即可实现有效适配。在常识推理、医疗和金融领域的评估表明，CBP-Tuning相比基线方法展现出卓越性能，突显其在任务无关处理与隐私保护方面的双重优势。\n\n（注：译文严格遵循以下技术要点：\n1. 专业术语标准化：\"soft prompts\"译为\"软提示\"，\"gradient-free optimization\"译为\"无需梯度的优化\"\n2. 技术概念准确传达：\"bidirectional privacy\"译为\"双向隐私\"而非字面直译\n3. 学术表述规范：\"task-agnostic\"译为\"与任务无关\"符合计算机领域术语\n4. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n5. 逻辑连接词优化：使用\"具体而言\"\"因此\"等符合中文论文表达的连接方式）"
    },
    {
        "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification\n  with Large Language Models",
        "url": "http://arxiv.org/abs/2509.12108v1",
        "pub_date": "2025-09-15",
        "summary": "In natural language processing tasks, pure reinforcement learning (RL) fine-tuning methods often suffer from inefficient exploration and slow convergence; while supervised fine-tuning (SFT) methods, although efficient in training, have limited performance ceiling and less solid theoretical foundation compared to RL. To address efficiency-capability trade-off, we propose the Guess-Think-Answer (GTA) framework that combines the efficiency of SFT with the capability gains of RL in a unified training paradigm. GTA works by having the model first produce a provisional guess (optimized via cross-entropy loss), then reflect on this guess before generating the final answer, with RL rewards shaping both the final output and the format of the entire GTA structure. This hybrid approach achieves both faster convergence than pure RL and higher performance ceiling than pure SFT. To mitigate gradient conflicts between the two training signals, we employ loss masking and gradient constraints. Empirical results on four text classification benchmarks demonstrate that GTA substantially accelerates convergence while outperforming both standalone SFT and RL baselines.",
        "translated": "在自然语言处理任务中，纯强化学习（RL）微调方法常面临探索效率低下与收敛速度缓慢的问题；而监督微调（SFT）方法虽训练高效，但其性能上限有限且理论基础相对薄弱。为平衡效率与性能的矛盾，我们提出\"猜想-反思-应答\"（GTA）框架，将SFT的高效性与RL的性能增益统一于协同训练范式。该框架要求模型先生成初步猜想（通过交叉熵损失优化），随后对猜想进行反思并生成最终答案，RL奖励信号同时约束最终输出与整体GTA结构的组织形式。这种混合方法既实现了比纯RL更快的收敛速度，又获得了比纯SFT更高的性能上限。为缓解两种训练信号的梯度冲突，我们采用损失掩码与梯度约束机制。在四个文本分类基准上的实验表明，GTA在显著加速收敛的同时，性能显著超越独立的SFT与RL基线方法。\n\n（注：译文严格遵循以下技术细节处理：\n1. 专业术语标准化：\"reinforcement learning\"译为\"强化学习\"，\"supervised fine-tuning\"译为\"监督微调\"\n2. 技术概念准确传达：\"cross-entropy loss\"译为\"交叉熵损失\"，\"gradient constraints\"译为\"梯度约束\"\n3. 框架名称意译：\"Guess-Think-Answer\"采用\"猜想-反思-应答\"的意译而非直译，更符合中文技术表述习惯\n4. 长句拆分与语序调整：将原文复合句按中文表达习惯重构，如将\"with RL rewards shaping...\"处理为独立分句\n5. 学术表达规范：使用\"范式\"\"基准\"\"机制\"等符合计算机领域论文摘要的中文术语）"
    },
    {
        "title": "In-domain SSL pre-training and streaming ASR",
        "url": "http://arxiv.org/abs/2509.12101v1",
        "pub_date": "2025-09-15",
        "summary": "In this study, we investigate the benefits of domain-specific self-supervised pre-training for both offline and streaming ASR in Air Traffic Control (ATC) environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then fine-tune on a smaller supervised ATC set. To enable real-time processing, we propose using chunked attention and dynamic convolutions, ensuring low-latency inference. We compare these in-domain SSL models against state-of-the-art, general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show that domain-adapted pre-training substantially improves performance on standard ATC benchmarks, significantly reducing word error rates when compared to models trained on broad speech corpora. Furthermore, the proposed streaming approach further improves word error rate under tighter latency constraints, making it particularly suitable for safety-critical aviation applications. These findings highlight that specializing SSL representations for ATC data is a practical path toward more accurate and efficient ASR systems in real-world operational settings.",
        "translated": "本研究探讨了针对空中交通管制（ATC）环境的领域特定自监督预训练对离线及流式自动语音识别（ASR）的增益效果。我们在4.5千小时无标注ATC数据上训练BEST-RQ模型，随后在少量有标注ATC数据集上进行微调。为实现实时处理，我们提出采用分块注意力机制与动态卷积方法，确保低延迟推理。通过将领域内自监督学习模型与w2v-BERT 2.0、HuBERT等通用语音编码器进行对比，实验结果表明：相较于基于通用语音语料库训练的模型，领域自适应预训练在标准ATC基准测试中显著提升性能，词错误率大幅降低。此外，所提出的流式处理方法在更严格延迟约束条件下进一步降低了词错误率，使其特别适用于航空安全关键场景。这些发现表明：针对ATC数据专门优化自监督学习表征，是提升现实场景ASR系统准确性与实用性的有效路径。"
    },
    {
        "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing\n  traditional NLP tools and large language models on ambiguous entities",
        "url": "http://arxiv.org/abs/2509.12098v1",
        "pub_date": "2025-09-15",
        "summary": "This pilot study presents a small-scale but carefully annotated benchmark of Named Entity Recognition (NER) performance across six systems: three non-LLM NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models (LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119 tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME). We evaluated each system's output against the manually annotated gold standard dataset using F1-score. The results show that LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names, with Gemini achieving the highest average F1-score. However, traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE. We also observed variability among LLMs, particularly in handling temporal expressions and multi-word organizations. Our findings highlight that while LLMs offer improved contextual understanding, traditional tools remain competitive in specific tasks, informing model selection.",
        "translated": "这项试点研究构建了一个小规模但经过精细标注的命名实体识别（NER）性能基准，评估了六类系统：三种非大语言模型的传统NLP工具（NLTK、spaCy、Stanza）和三种通用大语言模型（Gemini-1.5-flash、DeepSeek-V3、Qwen-3-4B）。数据集包含119个标注单元，涵盖五类实体（人物、地点、组织机构、日期、时间）。我们采用F1分数将各系统输出与人工标注的黄金标准数据集进行对比。结果显示，大语言模型在识别上下文敏感实体（如人名）方面普遍优于传统工具，其中Gemini取得了最高平均F1分数。但传统系统如Stanza在地点、日期等结构化标签上表现出更强的一致性。同时我们发现大语言模型之间存在性能差异，尤其在处理时间表达式和多词组织名称时表现不稳定。本研究强调：虽然大语言模型具有更好的上下文理解能力，但传统工具在特定任务中仍具竞争力，这为模型选择提供了重要参考依据。"
    },
    {
        "title": "SENSE models: an open source solution for multilingual and multimodal\n  semantic-based tasks",
        "url": "http://arxiv.org/abs/2509.12093v1",
        "pub_date": "2025-09-15",
        "summary": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source solution inspired by the SAMU-XLSR framework and conceptually similar to Meta AI's SONAR models. These approaches rely on a teacher-student framework to align a self-supervised speech encoder with the language-agnostic continuous representations of a text encoder at the utterance level. We describe how the original SAMU-XLSR method has been updated by selecting a stronger teacher text model and a better initial speech encoder. The source code for training and using SENSE models has been integrated into the SpeechBrain toolkit, and the first SENSE model we trained has been publicly released. We report experimental results on multilingual and multimodal semantic tasks, where our SENSE model achieves highly competitive performance. Finally, this study offers new insights into how semantics are captured in such semantically aligned speech encoders.",
        "translated": "本文介绍了SENSE（N语种语音与文本共享嵌入）——一个受SAMU-XLSR框架启发、在概念上与Meta AI的SONAR模型相似的开源解决方案。该方法基于师生框架，将自监督语音编码器与文本编码器的语言无关连续表征在语句级别进行对齐。我们阐述了如何通过选择更强的教师文本模型和更优的初始语音编码器来改进原始SAMU-XLSR方法。SENSE模型的训练和使用源代码已集成至SpeechBrain工具包，且首个训练完成的SENSE模型已公开发布。我们在多语言多模态语义任务上的实验结果表明，该模型实现了极具竞争力的性能。最后，本研究为理解语义对齐语音编码器如何捕获语义提供了新的见解。"
    },
    {
        "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar\n  Target Detection with Preference-aware Loss",
        "url": "http://arxiv.org/abs/2509.12089v1",
        "pub_date": "2025-09-15",
        "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated their capacities to capture universal knowledge, making them promising general-purpose optimization solvers for wireless signal processing. Motivated by these findings, we take the first step towards fine-tuning pre-trained LLMs for the effective analysis of radar signal features in marine target detection tasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target detection tasks tends to suffer from pronounced overfitting, particularly in challenging low signal-to-clutter ratio (SCR) scenarios. This overfitting primarily stems from the model's tendency to memorize spurious or noisy feature patterns rather than learning discriminative structures that generalize well to unseen data. To address this challenge, we introduce RadarLLM, a novel fine-tuning framework that utilizes an effective preference-aware loss. Unlike conventional training strategies that uniformly optimize all feature tokens, this loss function selectively optimizes different feature patches based on their online evaluated learning values, thus guiding the model to focus on the most generalizable patterns during optimization. We theoretically demonstrate the effectiveness of the evaluated learning values by transforming the problem as selecting useful feature tokens. Extensive experiments on real-world marine radar datasets show that 1) the proposed loss function is much better than the original one, with particularly significant gains in challenging low SCR scenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines across diverse detection scenarios, with particularly notable gains under limited training data conditions.",
        "translated": "近年来，预训练大语言模型（LLMs）的发展展现出其捕获通用知识的强大能力，使其成为无线信号处理领域极具潜力的通用优化求解器。基于这一发现，我们率先探索通过微调预训练LLM来实现海洋目标检测任务中雷达信号特征的有效分析。然而，直接在海洋目标检测任务上微调预训练LLM容易产生显著过拟合现象，尤其在低信杂比（SCR）的挑战性场景中。这种过拟合主要源于模型倾向于记忆虚假或带噪声的特征模式，而非学习能够良好泛化至未见数据的判别性结构。\n\n为解决这一挑战，我们提出RadarLLM——一种采用新型偏好感知损失函数的微调框架。与传统均匀优化所有特征标记的训练策略不同，该损失函数基于在线评估的学习价值对不同特征块进行选择性优化，从而引导模型在优化过程中聚焦于最具泛化能力的模式。我们通过将问题转化为有用特征标记的选择任务，从理论角度证明了所评估学习价值的有效性。在真实海洋雷达数据集上的大量实验表明：1）所提出的损失函数显著优于原损失函数，在低SCR挑战性场景中提升尤为明显；2）RadarLLM在不同检测场景中持续超越现有最先进基线模型，在训练数据有限的条件下表现出尤为突出的性能优势。"
    },
    {
        "title": "Steering Language Models in Multi-Token Generation: A Case Study on\n  Tense and Aspect",
        "url": "http://arxiv.org/abs/2509.12065v1",
        "pub_date": "2025-09-15",
        "summary": "Large language models (LLMs) are able to generate grammatically well-formed text, but how do they encode their syntactic knowledge internally? While prior work has focused largely on binary grammatical contrasts, in this work, we study the representation and control of two multidimensional hierarchical grammar phenomena - verb tense and aspect - and for each, identify distinct, orthogonal directions in residual space using linear discriminant analysis. Next, we demonstrate causal control over both grammatical features through concept steering across three generation tasks. Then, we use these identified features in a case study to investigate factors influencing effective steering in multi-token generation. We find that steering strength, location, and duration are crucial parameters for reducing undesirable side effects such as topic shift and degeneration. Our findings suggest that models encode tense and aspect in structurally organized, human-like ways, but effective control of such features during generation is sensitive to multiple factors and requires manual tuning or automated optimization.",
        "translated": "大型语言模型（LLMs）能够生成语法结构良好的文本，但其内部如何编码句法知识？已有研究主要关注二元语法对比，而本研究聚焦于两个多维层级语法现象——动词时态和体态——的表征与控制机制。通过线性判别分析，我们为每种语法现象在残差空间中识别出独特且正交的方向向量。随后，我们在三项文本生成任务中通过概念导向技术实现了对这两种语法特征的因果控制。在案例研究中，我们进一步利用这些特征探究多词元生成中影响导向效果的关键因素，发现导向强度、作用位置及持续时间是减少主题偏移和文本退化等副作用的核心参数。研究表明：模型以结构化的类人方式编码时态与体态信息，但在生成过程中对此类特征的有效控制受多重因素影响，需通过人工调参或自动化优化实现。"
    },
    {
        "title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval",
        "url": "http://arxiv.org/abs/2509.12042v1",
        "pub_date": "2025-09-15",
        "summary": "Financial disclosures such as 10-K filings present challenging retrieval problems due to their length, regulatory section hierarchy, and domain-specific language, which standard retrieval-augmented generation (RAG) models underuse. We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a retrieval framework tailored to financial documents. FinGEAR combines a finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search (Summary Tree and Question Tree), and a two-stage cross-encoder reranker. This design aligns retrieval with disclosure structure and terminology, enabling fine-grained, query-aware context selection. Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR delivers consistent gains in precision, recall, F1, and relevancy, improving F1 by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over prior tree-based systems, while also increasing downstream answer accuracy with a fixed reader. By jointly modeling section hierarchy and domain lexicon signals, FinGEAR improves retrieval fidelity and provides a practical foundation for high-stakes financial analysis.",
        "translated": "由于篇幅冗长、监管章节层级复杂以及领域特定语言的存在，10-K报表等财务披露文件对检索系统提出了严峻挑战，而标准检索增强生成（RAG）模型未能充分利用这些特征。我们提出FinGEAR（财务映射引导的增强答案检索框架），一种专为财务文档设计的检索系统。该框架融合了三项核心创新：基于财务词典的条款级引导机制（FLAM）、支持条款内检索的双层级索引（摘要树与问题树），以及两阶段交叉编码器重排序模块。这种设计使检索过程与披露文件的结构特征和专业术语对齐，实现了细粒度且感知查询意图的上下文选择。在基于完整10-K报表并与FinQA数据集查询对齐的评估中，FinGEAR在精确率、召回率、F1值和相关性指标上均取得稳定提升：相较扁平化RAG提升最高达56.7%的F1值，优于基于图谱的RAG方法12.5%，较先前树状检索系统提升217.6%。在使用固定阅读器的下游任务中，其答案生成准确率也显著提高。通过联合建模章节层级结构与领域词典信号，FinGEAR显著提升了检索保真度，为高风险财务分析提供了实用化基础。"
    },
    {
        "title": "Text Adaptation to Plain Language and Easy Read via Automatic\n  Post-Editing Cycles",
        "url": "http://arxiv.org/abs/2509.11991v1",
        "pub_date": "2025-09-15",
        "summary": "We describe Vicomtech's participation in the CLEARS challenge on text adaptation to Plain Language and Easy Read in Spanish. Our approach features automatic post-editing of different types of initial Large Language Model adaptations, where successive adaptations are generated iteratively until readability and similarity metrics indicate that no further adaptation refinement can be successfully performed. Taking the average of all official metrics, our submissions achieved first and second place in Plain language and Easy Read adaptation, respectively.",
        "translated": "我们介绍了Vicomtech团队在CLEARS挑战赛中针对西班牙语简明语言（Plain Language）和易读文本（Easy Read）的文本适应任务中的参与方案。我们的方法采用自动后编辑技术，对基于大型语言模型生成的多种初始文本适应版本进行迭代优化：通过连续多轮文本重构，直至可读性与语义相似度指标显示进一步优化无法提升效果为止。根据官方所有评测指标的平均值，我们的提交结果在简明语言适应任务中荣获第一名，在易读文本适应任务中获得第二名。"
    },
    {
        "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
        "url": "http://arxiv.org/abs/2509.11963v1",
        "pub_date": "2025-09-15",
        "summary": "As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become a critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FC-RewardBench, the first benchmark designed to systematically assess reward models' performance in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose a training framework for outcome-based reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These models consistently outperform general-purpose baselines, achieving up to 25\\% average improvement in downstream task performance and enabling data-efficient fine-tuning through reward-guided filtering.",
        "translated": "随着大语言模型（LLMs）与外部工具的交互日益频繁，工具使用的奖励建模已成为关键但尚未充分探索的领域。现有奖励模型主要基于自然语言输出进行训练，难以有效评估基于工具的推理与执行过程。为量化这一差距，我们推出了FC-RewardBench——首个专门用于系统评估奖励模型在工具调用场景中性能的基准测试。分析表明，当前奖励模型往往遗漏有效工具使用的关键信号，这凸显了领域专用建模的必要性。\n\n针对这一问题，我们提出基于结果驱动的奖励模型训练框架，该框架使用通过开放权重LLMs合成的数据。我们训练了参数量从17亿到140亿不等的模型，并在七个跨领域基准测试中进行评估。这些模型持续超越通用基线模型，在下游任务性能上实现最高25%的平均提升，同时通过奖励引导的数据过滤实现了高效数据微调。"
    },
    {
        "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based\n  Speculative Decoding",
        "url": "http://arxiv.org/abs/2509.11961v1",
        "pub_date": "2025-09-15",
        "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer from slow autoregressive inference, limiting their deployment in real-time applications. We introduce Spec-LLaVA, a system that applies speculative decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA pairs a lightweight draft VLM with a large target model: the draft speculates future tokens, which the target verifies in parallel, allowing multiple tokens to be generated per step. To maximize efficiency, we design a dynamic tree-based verification algorithm that adaptively expands and prunes speculative branches using draft model confidence. On MS COCO out-of-domain images, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5 (7B, 13B) with no loss in generation quality. This work presents a lossless acceleration framework for VLMs using dynamic tree-structured speculative decoding, opening a path toward practical real-time multimodal assistants. Importantly, the lightweight draft model design makes the framework amenable to resource-constrained or on-device deployment settings.",
        "translated": "视觉-语言模型（VLM）虽具备强大的多模态推理能力，但其自回归推理速度缓慢，限制了在实时场景中的应用。我们提出Spec-LLaVA系统，通过推测式解码在不损失输出质量的前提下加速VLM推理。该系统采用轻量级草稿模型与大型目标模型协同工作：草稿模型预测未来token，目标模型并行验证，实现单步生成多个token。为最大化效率，我们设计了基于动态树的验证算法，根据草稿模型置信度自适应扩展和剪枝推测分支。在MS COCO域外图像测试中，Spec-LLaVA在LLaVA-1.5（7B/13B）模型上实现3.28倍解码加速，且生成质量无损。本研究通过动态树结构推测解码实现了VLM的无损加速，为实用级实时多模态助手开辟了新路径。值得注意的是，轻量级草稿模型设计使该框架可适配资源受限或端侧部署环境。\n\n（关键技术亮点说明：  \n1. 推测解码（Speculative Decoding）：通过预测-验证机制突破自回归生成瓶颈  \n2. 动态树结构（Dynamic Tree）：基于置信度的自适应分支管理算法  \n3. 无损加速（Lossless Acceleration）：保持原始模型输出分布的同时提升吞吐量  \n4. 端侧部署（On-device Deployment）：轻量级草稿模型设计降低硬件门槛）"
    },
    {
        "title": "How to Evaluate Medical AI",
        "url": "http://arxiv.org/abs/2509.11941v1",
        "pub_date": "2025-09-15",
        "summary": "The integration of artificial intelligence (AI) into medical diagnostic workflows requires robust and consistent evaluation methods to ensure reliability, clinical relevance, and the inherent variability in expert judgments. Traditional metrics like precision and recall often fail to account for the inherent variability in expert judgments, leading to inconsistent assessments of AI performance. Inter-rater agreement statistics like Cohen's Kappa are more reliable but they lack interpretability. We introduce Relative Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new evaluation metrics that compare AI outputs against multiple expert opinions rather than a single reference. By normalizing performance against inter-expert disagreement, these metrics provide a more stable and realistic measure of the quality of predicted diagnosis. In addition to the comprehensive analysis of diagnostic quality measures, our study contains a very important side result. Our evaluation methodology allows us to avoid selecting diagnoses from a limited list when evaluating a given case. Instead, both the models being tested and the examiners verifying them arrive at a free-form diagnosis. In this automated methodology for establishing the identity of free-form clinical diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our approach using 360 medical dialogues, comparing multiple large language models (LLMs) against a panel of physicians. Large-scale study shows that top-performing models, such as DeepSeek-V3, achieve consistency on par with or exceeding expert consensus. Moreover, we demonstrate that expert judgments exhibit significant variability - often greater than that between AI and humans. This finding underscores the limitations of any absolute metrics and supports the need to adopt relative metrics in medical AI.",
        "translated": "将人工智能（AI）融入医疗诊断流程需要采用稳健且一致的评估方法，以确保其可靠性、临床相关性，并兼顾专家判断中固有的变异性。传统指标（如精确率和召回率）往往无法解释专家判断的固有差异，导致对AI性能的评估结果不一致。虽然评分者间一致性统计量（如Cohen's Kappa）更为可靠，但缺乏可解释性。我们提出了算法诊断相对精确率与召回率（RPAD与RRAD）——这是一种通过将AI输出与多位专家意见（而非单一参考标准）进行比较的新型评估指标。通过根据专家间分歧对性能进行标准化，这些指标能够为预测诊断质量提供更稳定、更符合实际的度量。\n\n除对诊断质量度量方法的全面分析外，本研究还获得一项重要发现：我们的评估方法无需从有限列表中选择诊断标签，而是让被测模型与验证医师均采用自由形式的诊断表述。在这种建立自由形式临床诊断一致性的自动化方法中，可实现高达98%的准确率。我们通过360组医疗对话数据评估该方法，将多个大语言模型（LLM）与医师组进行对比。大规模研究表明，顶尖模型（如DeepSeek-V3）达到甚至超越专家共识的一致性水平。更重要的是，我们发现专家判断存在显著变异性——其程度往往超过AI与人类之间的差异。这一发现揭示了绝对度量标准的局限性，印证了在医疗AI领域采用相对度量标准的必要性。\n\n（注：专业术语说明：\n1. Inter-rater agreement statistics：评分者间一致性统计量\n2. Cohen's Kappa：科恩卡帕系数（衡量分类一致性的统计指标）\n3. free-form diagnosis：自由形式诊断（非预设选项的开放式诊断）\n4. large language models (LLMs)：大语言模型\n5. expert consensus：专家共识）"
    },
    {
        "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese\n  translation",
        "url": "http://arxiv.org/abs/2509.11921v1",
        "pub_date": "2025-09-15",
        "summary": "Large language models (LLMs) are increasingly used in everyday communication, including multilingual interactions across different cultural contexts. While LLMs can now generate near-perfect literal translations, it remains unclear whether LLMs support culturally appropriate communication. In this paper, we analyze the cultural sensitivity of different LLM designs when applied to English-Japanese translations of workplace e-mails. Here, we vary the prompting strategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts specifying the recipient's cultural background, and (3) instructional prompts with explicit guidance on Japanese communication norms. Using a mixed-methods study, we then analyze culture-specific language patterns to evaluate how well translations adapt to cultural norms. Further, we examine the appropriateness of the tone of the translations as perceived by native speakers. We find that culturally-tailored prompting can improve cultural fit, based on which we offer recommendations for designing culturally inclusive LLMs in multilingual settings.",
        "translated": "大型语言模型（LLMs）在日常沟通中的应用日益广泛，尤其是在跨文化多语言交流场景中。尽管当前LLMs能够生成近乎完美的字面翻译，但其是否支持符合文化背景的恰当交流仍不明确。本文通过分析职场英文邮件日文翻译任务，探究了不同LLM架构的文化敏感性。我们采用三种提示策略进行对比：(1) 基础型\"直接翻译\"提示，(2) 指定收件人文化背景的受众导向型提示，(3) 提供日本交际规范明确指导的教学型提示。通过混合研究方法，我们解析了文化特异性语言模式以评估翻译结果对文化规范的适应程度，并进一步考察了母语者对翻译文本语气得体性的主观评价。研究发现：基于文化背景定制的提示策略能有效提升翻译的文化适配性。基于此，我们为多语言环境下设计具有文化包容性的LLMs提出了具体建议。"
    },
    {
        "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically\n  Impossible",
        "url": "http://arxiv.org/abs/2509.11915v1",
        "pub_date": "2025-09-15",
        "summary": "As large language models (LLMs) become more advanced, it is increasingly difficult to distinguish between human-written and AI-generated text. This paper draws a conceptual parallel between quantum uncertainty and the limits of authorship detection in natural language. We argue that there is a fundamental trade-off: the more confidently one tries to identify whether a text was written by a human or an AI, the more one risks disrupting the text's natural flow and authenticity. This mirrors the tension between precision and disturbance found in quantum systems. We explore how current detection methods--such as stylometry, watermarking, and neural classifiers--face inherent limitations. Enhancing detection accuracy often leads to changes in the AI's output, making other features less reliable. In effect, the very act of trying to detect AI authorship introduces uncertainty elsewhere in the text. Our analysis shows that when AI-generated text closely mimics human writing, perfect detection becomes not just technologically difficult but theoretically impossible. We address counterarguments and discuss the broader implications for authorship, ethics, and policy. Ultimately, we suggest that the challenge of AI-text detection is not just a matter of better tools--it reflects a deeper, unavoidable tension in the nature of language itself.",
        "translated": "随着大语言模型（LLM）的不断发展，区分人类书写文本与AI生成文本的难度日益增加。本文从概念层面将量子不确定性原理与自然语言作者身份检测的局限性进行类比，指出二者存在根本性的权衡关系：越试图精确判断文本的作者身份（人类或AI），就越可能破坏文本的自然流畅性与真实性。这种困境与量子系统中精确性与干扰性之间的张力形成镜像关系。我们通过分析现有检测方法（如风格计量学、数字水印技术和神经分类器）揭示其固有局限——提高检测准确度往往需要改变AI输出模式，进而导致其他特征可靠性下降。本质上，检测行为本身会在文本中引入新的不确定性。研究表明，当AI生成文本高度拟人化时，完美检测不仅存在技术障碍，更构成理论层面的不可能性。本文通过反驳对立观点，深入探讨了这一现象对作者身份认定、伦理规范及政策制定的影响。最终指出：AI文本检测的挑战不仅关乎技术工具优化，更揭示了语言本质中存在的深层且不可避免的矛盾张力。"
    },
    {
        "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models",
        "url": "http://arxiv.org/abs/2509.11868v1",
        "pub_date": "2025-09-15",
        "summary": "Language and embodied perspective taking are essential for human collaboration, yet few computational models address both simultaneously. This work investigates the PerspAct system [1], which integrates the ReAct (Reason and Act) paradigm with Large Language Models (LLMs) to simulate developmental stages of perspective taking, grounded in Selman's theory [2]. Using an extended director task, we evaluate GPT's ability to generate internal narratives aligned with specified developmental stages, and assess how these influence collaborative performance both qualitatively (action selection) and quantitatively (task efficiency). Results show that GPT reliably produces developmentally-consistent narratives before task execution but often shifts towards more advanced stages during interaction, suggesting that language exchanges help refine internal representations. Higher developmental stages generally enhance collaborative effectiveness, while earlier stages yield more variable outcomes in complex contexts. These findings highlight the potential of integrating embodied perspective taking and language in LLMs to better model developmental dynamics and stress the importance of evaluating internal speech during combined linguistic and embodied tasks.",
        "translated": "语言与具身视角采择对人类协作至关重要，但现有计算模型很少能同时兼顾二者。本研究探讨了PerspAct系统[1]，该系统将ReAct（推理与行动）范式与大型语言模型（LLM）相结合，基于Selman的理论[2]模拟视角采择的发展阶段。通过扩展版指挥者任务，我们评估了GPT生成符合特定发展阶段内部叙事的能力，并从定性（动作选择）和定量（任务效率）两个维度分析这些叙事如何影响协作表现。结果表明：GPT能在任务执行前稳定生成与发展阶段一致的内部叙事，但在交互过程中常会转向更高级阶段，这说明语言交流有助于精炼内部表征。较高发展阶段通常能提升协作效能，而早期阶段在复杂情境中会产生更多可变结果。这些发现凸显了将具身视角采择与语言能力整合到LLM中以更好模拟发展动态的潜力，同时强调了在语言与具身任务结合的场景中评估内部言语的重要性。\n\n（注：[1][2]为原文保留的文献引用标记，符合学术规范要求）"
    },
    {
        "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long\n  Role-Playing Dialogues",
        "url": "http://arxiv.org/abs/2509.11860v1",
        "pub_date": "2025-09-15",
        "summary": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in human-robot role-playing scenarios. However, existing methods often exhibit uncontrolled memory growth. To address this, we propose MOOM, the first dual-branch memory plugin that leverages literary theory by modeling plot development and character portrayal as core storytelling elements. Specifically, one branch summarizes plot conflicts across multiple time scales, while the other extracts the user's character profile. MOOM further integrates a forgetting mechanism, inspired by the ``competition-inhibition'' memory theory, to constrain memory capacity and mitigate uncontrolled growth. Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset specifically designed for role-playing, featuring dialogues that average 600 turns and include manually annotated memory information. Experimental results demonstrate that MOOM outperforms all state-of-the-art memory extraction methods, requiring fewer large language model invocations while maintaining a controllable memory capacity.",
        "translated": "记忆提取对于维持人机角色扮演场景中超长对话的连贯性至关重要。然而现有方法普遍存在记忆不可控增长的问题。为此，我们提出MOOM——首个基于文学理论的双分支记忆插件，通过将情节发展与角色塑造建模为核心叙事要素来实现记忆管理。具体而言，一个分支负责在多时间尺度上总结情节冲突，另一个分支则专注于提取用户角色画像。MOOM进一步融合了受\"竞争抑制\"记忆理论启发的遗忘机制，通过约束记忆容量来抑制不可控增长。此外，我们发布了ZH-4O中文超长对话数据集，该数据集专为角色扮演场景设计，平均每段对话达600轮次，并包含人工标注的记忆信息。实验结果表明，MOOM在保持可控记忆容量的同时，以更少的大语言模型调用次数，显著优于所有最先进的记忆提取方法。\n\n（注：专业术语说明：\n1. \"competition-inhibition\" memory theory 译为\"竞争抑制\"记忆理论\n2. \"state-of-the-art\" 译为\"最先进的\"\n3. \"large language model invocations\" 译为\"大语言模型调用次数\"\n4. 技术概念\"dual-branch memory plugin\"保留英文缩写MOOM的同时补充说明为\"双分支记忆插件\"\n5. 数据集名称ZH-4O保持原命名规范）"
    },
    {
        "title": "The AI Memory Gap: Users Misremember What They Created With AI or\n  Without",
        "url": "http://arxiv.org/abs/2509.11851v1",
        "pub_date": "2025-09-15",
        "summary": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.",
        "translated": "随着大型语言模型（LLMs）被广泛应用于交互式文本生成领域，用户能否准确识别内容的来源——即区分哪些想法或文本出自人类自身、哪些由AI生成——成为关键问题。本研究针对人类在使用AI辅助时的内容来源记忆准确性展开探讨。通过一项预先注册的实验，184名参与者在无辅助和基于LLM的聊天机器人辅助两种条件下分别生成观点并进行阐述。一周后，要求受试者对这些内容进行来源辨识（无AI辅助vs有AI辅助）。研究结果揭示了显著的记忆偏差：使用AI后，受试者的正确归因概率显著下降，其中混合人机协作工作流（观点生成或内容阐述任一环节涉及AI辅助）的正确率下降最为明显。我们通过计算源记忆模型对实验结果进行了验证。在讨论更广泛影响时，我们强调在交互式文本生成技术的设计和使用中必须充分考虑来源混淆现象。"
    },
    {
        "title": "Collaborative Document Editing with Multiple Users and AI Agents",
        "url": "http://arxiv.org/abs/2509.11826v1",
        "pub_date": "2025-09-15",
        "summary": "Current AI writing support tools are largely designed for individuals, complicating collaboration when co-writers must leave the shared workspace to use AI and then communicate and reintegrate results. We propose integrating AI agents directly into collaborative writing environments. Our prototype makes AI use transparent and customisable through two new shared objects: agent profiles and tasks. Agent responses appear in the familiar comment feature. In a user study (N=30), 14 teams worked on writing projects during one week. Interaction logs and interviews show that teams incorporated agents into existing norms of authorship, control, and coordination, rather than treating them as team members. Agent profiles were viewed as personal territory, while created agents and outputs became shared resources. We discuss implications for team-based AI interaction, highlighting opportunities and boundaries for treating AI as a shared resource in collaborative work.",
        "translated": "当前的人工智能写作辅助工具主要面向个人用户设计，这使得协同写作时合作者不得不离开共享工作空间使用AI功能，再通过人工沟通重新整合输出结果。我们提出将AI智能体直接集成到协同写作环境中的解决方案。通过引入智能体档案和任务两项新型共享对象，我们的原型系统实现了透明可定制的AI使用机制，且AI响应会直接呈现在用户熟悉的评论功能区。在一项为期一周的用户研究（N=30）中，14个团队开展写作项目。交互日志与访谈数据显示：团队将智能体纳入既有的作者权、控制权与协作规范体系中，而非将其视为团队成员。智能体档案被视为个人专属领域，而创建的智能体和输出内容则成为共享资源。我们探讨了团队化AI交互的设计启示，指出在协作工作中将AI作为共享资源既存在发展机遇，也存在应用边界。"
    },
    {
        "title": "3D Aware Region Prompted Vision Language Model",
        "url": "http://arxiv.org/abs/2509.13317v1",
        "pub_date": "2025-09-16",
        "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.",
        "translated": "我们提出了一种空间区域三维感知视觉语言模型（SR-3D），通过共享视觉标记空间将单视图二维图像与多视图三维数据相连接。该模型支持灵活的区域提示机制，用户可通过边界框、任意帧上的分割掩码或直接在三维空间中进行区域标注，无需进行繁琐的多帧标注。通过将二维视觉特征与三维位置嵌入相结合，使三维模型能够利用强大的二维先验知识，实现跨帧的精确空间推理——即使目标物体未在同一视图中同时出现。在通用二维视觉语言任务和专用三维空间基准测试上的大量实验表明，SR-3D实现了最先进的性能，印证了其在统一二维与三维场景理解表征空间方面的有效性。此外，该模型在无传感器三维输入或真实三维标注的野外视频中同样表现出适用性，能够准确推断空间关系和度量尺寸。\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语统一：\"Spatial Region 3D\"译为技术圈通用表述\"空间区域三维感知\"，\"bounding boxes\"保留专业表述\"边界框\"\n2. 技术概念准确传递：\"3D positional embeddings\"译为\"三维位置嵌入\"，\"shared visual token space\"译为\"共享视觉标记空间\"\n3. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将\"which allows...\"从句转换为独立分句\n4. 被动语态转化：\"are observed\"主动化为\"表现出\"，符合中文科技文献表述惯例\n5. 重要概念突出：核心创新点\"无需多帧标注\"\"跨帧精确空间推理\"等采用强调式表述）"
    },
    {
        "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with\n  Texture-Geometry Dual Guidance",
        "url": "http://arxiv.org/abs/2509.13301v1",
        "pub_date": "2025-09-16",
        "summary": "Creating 3D assets that follow the texture and geometry style of existing ones is often desirable or even inevitable in practical applications like video gaming and virtual reality. While impressive progress has been made in generating 3D objects from text or images, creating style-controllable 3D assets remains a complex and challenging problem. In this work, we propose StyleSculptor, a novel training-free approach for generating style-guided 3D assets from a content image and one or more style images. Unlike previous works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner, enabling fine-grained 3D style control that captures the texture, geometry, or both styles of user-provided style images. At the core of StyleSculptor is a novel Style Disentangled Attention (SD-Attn) module, which establishes a dynamic interaction between the input content image and style image for style-guided 3D asset generation via a cross-3D attention mechanism, enabling stable feature fusion and effective style-guided generation. To alleviate semantic content leakage, we also introduce a style-disentangled feature selection strategy within the SD-Attn module, which leverages the variance of 3D feature patches to disentangle style- and content-significant channels, allowing selective feature injection within the attention framework. With SD-Attn, the network can dynamically compute texture-, geometry-, or both-guided features to steer the 3D generation process. Built upon this, we further propose the Style Guided Control (SGC) mechanism, which enables exclusive geometry- or texture-only stylization, as well as adjustable style intensity control. Extensive experiments demonstrate that StyleSculptor outperforms existing baseline methods in producing high-fidelity 3D assets.",
        "translated": "在电子游戏和虚拟现实等实际应用中，创建与现有资产纹理和几何风格一致的3D资源往往是必要需求。尽管基于文本或图像生成3D对象已取得显著进展，但实现风格可控的3D资产生成仍是复杂且具有挑战性的难题。本研究提出StyleSculptor——一种无需训练的新方法，能够通过内容图像和一张或多张风格图像生成风格引导的3D资产。与现有方法不同，StyleSculptor以零样本方式实现风格引导的3D生成，可精细控制用户提供风格图像的纹理、几何或双重风格特征。\n\n该方法的核心是新颖的风格解耦注意力（SD-Attn）模块，该模块通过跨3D注意力机制建立输入内容图像与风格图像之间的动态交互，实现稳定的特征融合和有效的风格引导生成。为缓解语义内容泄漏问题，我们在SD-Attn模块中引入了风格解耦特征选择策略，利用3D特征块的方差解耦风格与内容显著通道，实现在注意力框架内的选择性特征注入。借助SD-Attn，网络能动态计算纹理引导、几何引导或双重引导特征来驱动3D生成过程。在此基础上，我们进一步提出风格引导控制（SGC）机制，支持纯几何或纯纹理的独立风格化，以及可调节的风格强度控制。大量实验表明，StyleSculptor在生成高保真3D资产方面优于现有基线方法。"
    },
    {
        "title": "QDFlow: A Python package for physics simulations of quantum dot devices",
        "url": "http://arxiv.org/abs/2509.13298v1",
        "pub_date": "2025-09-16",
        "summary": "Recent advances in machine learning (ML) have accelerated progress in calibrating and operating quantum dot (QD) devices. However, most ML approaches rely on access to large, high-quality labeled datasets for training, benchmarking, and validation, with labels capturing key features in the data. Obtaining such datasets experimentally is challenging due to limited data availability and the labor-intensive nature of labeling. QDFlow is an open-source physics simulator for multi-QD arrays that generates realistic synthetic data with ground-truth labels. QDFlow combines a self-consistent Thomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to produce charge stability diagrams and ray-based data closely resembling experiments. With extensive tunable parameters and customizable noise models, QDFlow supports the creation of large, diverse datasets for ML development, benchmarking, and quantum device research.",
        "translated": "机器学习（ML）的最新进展显著推动了量子点（QD）器件校准与操作的发展。然而，大多数ML方法依赖大规模高质量标注数据集进行训练、基准测试和验证，这些标签需捕捉数据中的关键特征。由于实验数据获取受限且标注过程耗时，获得此类数据集极具挑战性。QDFlow作为针对多量子点阵列的开源物理模拟器，可生成带有真实标签的高仿真合成数据。该工具融合自洽托马斯-费米求解器、动态电容模型及灵活噪声模块，生成与实验高度吻合的电荷稳定图与射线数据。凭借大量可调参数和可定制噪声模型，QDFlow能为ML开发、基准测试及量子器件研究提供大规模多样化数据集支持。\n\n（注：译文严格遵循学术文本规范，对\"Thomas-Fermi solver\"保留专业术语\"托马斯-费米求解器\"的译法，\"charge stability diagrams\"译为\"电荷稳定图\"符合凝聚态物理领域术语，同时通过\"高仿真合成数据\"\"可定制噪声模块\"等表述准确传递技术细节。）"
    },
    {
        "title": "Image Realness Assessment and Localization with Multimodal Features",
        "url": "http://arxiv.org/abs/2509.13289v1",
        "pub_date": "2025-09-16",
        "summary": "A reliable method of quantifying the perceptual realness of AI-generated images and identifying visually inconsistent regions is crucial for practical use of AI-generated images and for improving photorealism of generative AI via realness feedback during training. This paper introduces a framework that accomplishes both overall objective realness assessment and local inconsistency identification of AI-generated images using textual descriptions of visual inconsistencies generated by vision-language models trained on large datasets that serve as reliable substitutes for human annotations. Our results demonstrate that the proposed multimodal approach improves objective realness prediction performance and produces dense realness maps that effectively distinguish between realistic and unrealistic spatial regions.",
        "translated": "本文提出了一种可靠的方法，用于量化AI生成图像的感知真实性并识别视觉不一致区域，这对AI生成图像的实际应用以及通过训练过程中的真实性反馈提升生成式AI的写实效果至关重要。我们开发了一个框架，通过利用在大规模数据集上训练的视觉-语言模型生成的视觉不一致性文本描述（作为人工标注的可靠替代），实现了对AI生成图像的整体客观真实性评估和局部不一致性定位。实验结果表明，所提出的多模态方法提升了客观真实性预测性能，并能生成密集的真实性分布图，有效区分图像中逼真与非逼真的空间区域。"
    },
    {
        "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking\n  Guided Attention Refinement",
        "url": "http://arxiv.org/abs/2509.13282v1",
        "pub_date": "2025-09-16",
        "summary": "Charts are a crucial visual medium for communicating and representing information. While Large Vision-Language Models (LVLMs) have made progress on chart question answering (CQA), the task remains challenging, particularly when models attend to irrelevant regions of the chart. In this work, we present ChartGaze, a new eye-tracking dataset that captures human gaze patterns during chart reasoning tasks. Through a systematic comparison of human and model attention, we find that LVLMs often diverge from human gaze, leading to reduced interpretability and accuracy. To address this, we propose a gaze-guided attention refinement that aligns image-text attention with human fixations. Our approach improves both answer accuracy and attention alignment, yielding gains of up to 2.56 percentage points across multiple models. These results demonstrate the promise of incorporating human gaze to enhance both the reasoning quality and interpretability of chart-focused LVLMs.",
        "translated": "图表作为一种重要的视觉媒介，在信息传达与表征方面具有关键作用。尽管大视觉语言模型（LVLMs）在图表问答（CQA）任务上已取得进展，但该任务仍存在挑战，尤其当模型关注到图表中的无关区域时。本研究提出ChartGaze——一个通过眼动追踪技术采集用户在图表推理任务中注视模式的新数据集。通过系统比较人类与模型的注意力分布，我们发现LVLMs的注意力机制常偏离人类注视模式，导致可解释性和准确性下降。为此，我们提出一种基于 gaze 引导的注意力优化方法，使图像-文本注意力与人类注视点对齐。该方法在提升答案准确率的同时增强了注意力对齐性，在多个模型上实现了最高2.56个百分点的性能提升。这些结果表明，融入人类注视数据有望同时提升面向图表的大视觉语言模型的推理质量与可解释性。"
    },
    {
        "title": "RadGame: An AI-Powered Platform for Radiology Education",
        "url": "http://arxiv.org/abs/2509.13270v1",
        "pub_date": "2025-09-16",
        "summary": "We introduce RadGame, an AI-powered gamified platform for radiology education that targets two core skills: localizing findings and generating reports. Traditional radiology training is based on passive exposure to cases or active practice with real-time input from supervising radiologists, limiting opportunities for immediate and scalable feedback. RadGame addresses this gap by combining gamification with large-scale public datasets and automated, AI-driven feedback that provides clear, structured guidance to human learners. In RadGame Localize, players draw bounding boxes around abnormalities, which are automatically compared to radiologist-drawn annotations from public datasets, and visual explanations are generated by vision-language models for user missed findings. In RadGame Report, players compose findings given a chest X-ray, patient age and indication, and receive structured AI feedback based on radiology report generation metrics, highlighting errors and omissions compared to a radiologist's written ground truth report from public datasets, producing a final performance and style score. In a prospective evaluation, participants using RadGame achieved a 68% improvement in localization accuracy compared to 17% with traditional passive methods and a 31% improvement in report-writing accuracy compared to 4% with traditional methods after seeing the same cases. RadGame highlights the potential of AI-driven gamification to deliver scalable, feedback-rich radiology training and reimagines the application of medical AI resources in education.",
        "translated": "我们推出RadGame——一个融合人工智能技术的游戏化放射学教育平台，专注于培养两大核心技能：异常区域定位与诊断报告生成。传统放射学培训依赖于被动案例学习或在上级放射科医师指导下进行实践，这种模式难以提供即时且可规模化的反馈。RadGame通过结合游戏化机制、大规模公共数据集以及由人工智能驱动的自动化反馈系统，为学习者提供清晰的结构化指导。\n\n在RadGame定位模块中，学习者通过绘制异常区域边界框进行训练，系统会将其与公共数据集中放射科医师标注的金标准进行自动比对，并借助视觉语言模型对用户遗漏的病灶生成可视化解释。在RadGame报告模块中，学习者需根据胸部X光片、患者年龄和临床指征撰写诊断报告，随后基于放射学报告生成指标获得结构化AI反馈：系统会对比公共数据集中的标准放射科医师报告，突出显示错误与遗漏项，并最终生成包含性能与风格维度的综合评分。\n\n前瞻性评估表明，使用RadGame的学习者在完成相同案例训练后，定位准确率较传统被动学习组提升68%（对照组为17%），报告撰写准确率提升31%（对照组为4%）。RadGame证明了人工智能驱动的游戏化模式在提供可规模化、高反馈密度放射学培训方面的潜力，为医疗AI资源在教育领域的应用开辟了新路径。"
    },
    {
        "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
        "url": "http://arxiv.org/abs/2509.13255v1",
        "pub_date": "2025-09-16",
        "summary": "Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require \"temporally dense\" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.",
        "translated": "在自然语言时序视频定位、时序活动定位及音频描述生成等视频理解任务中，需要对高时间分辨率采样的帧进行\"时序密集\"的推理。然而，鉴于时间分辨率的要求，为这些任务计算帧级特征的计算成本非常高。本文提出三项创新以降低时序密集型任务的特征计算成本：首先，我们设计了一种名为ResidualViT的视觉Transformer架构，通过利用视频中存在的大量时间冗余性，高效计算时序密集的帧级特征。该架构包含（i）可学习的残差连接机制，确保连续帧间的时间一致性；（ii）令牌缩减模块，通过选择性丢弃时序冗余信息并复用预训练基础模型的权重来提升处理速度。其次，我们提出了一种轻量级蒸馏策略，用于逼近原始基础模型的帧级特征。最后，我们在四个任务和五个数据集上进行了零样本和全监督设置的实验验证，结果表明在保持与原始基础模型精度高度接近的同时，显著降低了计算成本（最高达60%）并提升了推理速度（最快达2.5倍）。"
    },
    {
        "title": "Intelligent Vacuum Thermoforming Process",
        "url": "http://arxiv.org/abs/2509.13250v1",
        "pub_date": "2025-09-16",
        "summary": "Ensuring consistent quality in vacuum thermoforming presents challenges due to variations in material properties and tooling configurations. This research introduces a vision-based quality control system to predict and optimise process parameters, thereby enhancing part quality with minimal data requirements. A comprehensive dataset was developed using visual data from vacuum-formed samples subjected to various process parameters, supplemented by image augmentation techniques to improve model training. A k-Nearest Neighbour algorithm was subsequently employed to identify adjustments needed in process parameters by mapping low-quality parts to their high-quality counterparts. The model exhibited strong performance in adjusting heating power, heating time, and vacuum time to reduce defects and improve production efficiency.",
        "translated": "在真空热成型过程中，由于材料特性和模具配置的差异性，确保产品质量一致性存在挑战。本研究提出了一种基于视觉的质量控制系统，通过预测和优化工艺参数来提升零件质量，且仅需极少量的数据支持。研究团队利用不同工艺参数下真空成型样本的视觉数据构建了完整数据集，并采用图像增强技术强化模型训练效果。随后采用k最近邻算法，通过将低质量零件映射至高质量对应样本，精准识别出需要调整的工艺参数。该模型在调整加热功率、加热时间和真空时间方面表现出色，有效减少了产品缺陷并提升了生产效率。\n\n（翻译说明：\n1. 专业术语准确处理：\"vacuum thermoforming\"译为\"真空热成型\"，\"k-Nearest Neighbour algorithm\"保留算法专业名称\"k最近邻算法\"\n2. 技术概念清晰转化：\"vision-based quality control system\"译为\"基于视觉的质量控制系统\"，\"image augmentation techniques\"译为\"图像增强技术\"\n3. 长句结构合理切分：将原文复合句按中文表达习惯拆分为多个短句，如将\"supplemented by...\"独立成句处理\n4. 逻辑关系显性化：通过\"并\"、\"随后\"等连接词明确技术流程的先后关系\n5. 动态表达符合中文习惯：\"exhibited strong performance\"转化为\"表现出色\"的主动句式）"
    },
    {
        "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy",
        "url": "http://arxiv.org/abs/2509.13234v1",
        "pub_date": "2025-09-16",
        "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI systems can expand access to fundus photography screening. Current FDA-cleared systems primarily provide binary referral outputs, where this minimal output may limit clinical trust and utility. Yet, determining the most effective output format to enhance clinician-AI performance is an empirical challenge that is difficult to assess at scale. We evaluated multimodal large language models (MLLMs) for DR detection and their ability to simulate clinical AI assistance across different output types. Two models were tested on IDRiD and Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source medical model. Experiments included: (1) baseline evaluation, (2) simulated AI assistance with synthetic predictions, and (3) actual AI-to-AI collaboration where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at baseline, achieving higher sensitivity and AUROC, while GPT-4o showed near-perfect specificity but low sensitivity. Both models adjusted predictions based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o achieved strong results when guided by MedGemma's descriptive outputs, even without direct image access (AUROC up to 0.96). These findings suggest MLLMs may improve DR screening pipelines and serve as scalable simulators for studying clinical AI assistance across varying output configurations. Open, lightweight models such as MedGemma may be especially valuable in low-resource settings, while descriptive outputs could enhance explainability and clinician trust in clinical workflows.",
        "translated": "糖尿病视网膜病变（DR）是全球范围内致盲的主要病因，而人工智能系统能够扩大眼底摄影筛查的可及性。当前通过FDA批准的筛查系统主要提供二分类转诊判断，这种极简输出可能限制临床信任度和实用性。然而，如何确定最能提升临床医生与AI协作效能的输出形式，是一个难以大规模验证的实证难题。本研究评估了多模态大语言模型（MLLMs）在DR检测中的表现，及其在不同输出形式下模拟临床AI辅助的能力。我们在IDRiD和Messidor-2数据集上测试了两种模型：通用型MLLM——GPT-4o，以及开源医疗专用模型——MedGemma。实验包括：（1）基线性能评估；（2）基于合成预测的模拟AI辅助实验；（3）GPT-4o整合MedGemma输出的实际AI协作实验。结果显示：MedGemma在基线测试中表现优于GPT-4o，具有更高的敏感性和AUROC，而GPT-4o虽展现近乎完美的特异性但敏感性较低。两种模型都能根据模拟AI输入调整预测，但GPT-4o在接收到错误输入时性能急剧下降，MedGemma则保持相对稳定。在实际协作中，GPT-4o在MedGemma描述性输出的引导下（即使未直接读取图像）取得了优异表现（AUROC最高达0.96）。这些发现表明，MLLMs不仅能优化DR筛查流程，还可作为可扩展的模拟平台用于研究不同输出配置下的临床AI辅助机制。像MedGemma这样的开放轻量级模型在资源有限环境中具有特殊价值，而描述性输出有望增强临床工作流程的可解释性及医生信任度。"
    },
    {
        "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation",
        "url": "http://arxiv.org/abs/2509.13229v1",
        "pub_date": "2025-09-16",
        "summary": "Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.",
        "translated": "高光谱成像（HSI）能够捕获每个像素数百个连续波段的精细光谱特征，在土地覆盖分类、变化检测和环境监测等遥感应用中具有不可替代的作用。由于HSI数据的高维特性及星载系统数据传输速率限制，需要构建紧凑高效的模型以支持星上处理，并最大限度减少冗余或低价值数据（如云覆盖区域）的传输。为此，我们提出了一种新颖的课程式多任务自监督学习框架（CMTSSL），专为轻量级HSI分析架构设计。该框架通过课程学习策略引导，在自监督过程中逐步增加数据复杂度，将掩码图像建模与解耦的空间-光谱拼图求解任务相结合，使编码器能够同时捕获细粒度光谱连续性、空间结构和全局语义特征。与现有双任务自监督方法不同，CMTSSL在统一的计算高效框架内同步处理空间与光谱推理，特别适用于星载轻量级模型的训练。我们在四个公开基准数据集上验证了该方法，使用比现有先进模型轻16,000倍以上的架构，在下游分割任务中实现了性能的持续提升。这些成果证明了CMTSSL在轻量级架构通用表征学习方面对实际HSI应用的潜力。代码已开源：https://github.com/hugocarlesso/CMTSSL。\n\n（注：翻译严格遵循以下技术要点：\n1. 专业术语准确：\"hyperspectral imaging\"译为\"高光谱成像\"，\"self-supervised learning\"译为\"自监督学习\"\n2. 技术细节保留：完整呈现\"掩码图像建模\"、\"空间-光谱拼图求解\"等核心方法特征\n3. 数据量化传达：精确转换\"16,000x lighter\"为\"轻16,000倍以上\"\n4. 学术表述规范：采用\"编码器\"、\"下游任务\"等领域标准译法\n5. 长句拆分重组：将原文复合句按中文表达习惯分解为多个语义连贯的短句）"
    },
    {
        "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting\n  Detection",
        "url": "http://arxiv.org/abs/2509.13214v1",
        "pub_date": "2025-09-16",
        "summary": "The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.",
        "translated": "扩散模型强大的生成能力显著推动了图像合成领域的发展，不仅提升了完整图像生成质量，也促进了基于修复的图像编辑技术。尽管取得了显著进展，但扩散模型也引发了可能被恶意滥用的担忧。然而，现有方法难以有效识别基于扩散的修复模型生成的图像——即使训练数据中包含类似的修复图像。为应对这一挑战，我们提出了一种基于端到端去噪扩散（End4）的新型检测方法。具体而言，End4设计了一个去噪重建模型，通过提升重建过程与检测过程潜在空间的对齐度，重构出更有利于检测的特征表示。同时，该方法采用尺度感知金字塔融合模块（SPFM），在不同尺度注意力金字塔层的引导下优化局部图像特征，从而增强特征判别能力。此外，为评估修复图像的检测性能，我们构建了包含五种不同掩码区域生成图像的综合性基准测试集。大量实验表明，End4能有效泛化至未见过的掩码模式，并在多种干扰条件下保持鲁棒性。相关代码与数据集即将开源。"
    },
    {
        "title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection\n  in Public Surveillance",
        "url": "http://arxiv.org/abs/2509.13210v1",
        "pub_date": "2025-09-16",
        "summary": "Violence detection in public surveillance is critical for public safety. This study addresses challenges such as small-scale targets, complex environments, and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal framework that integrates an enhanced YOLOv8 with a Temporal Segment Network (TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as a lightweight backbone, an exponential moving average (EMA) attention mechanism, and pruning to reduce computational cost while maintaining accuracy. YOLOv8 and TSN are trained separately on pedestrian and violence datasets, where YOLOv8 extracts human regions and TSN performs binary classification of violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming existing methods in both accuracy and efficiency, demonstrating its effectiveness for public safety surveillance. Code is available at https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.",
        "translated": "公共监控中的暴力行为检测对公共安全至关重要。本研究针对小尺度目标、复杂环境和实时时序分析等挑战，提出Vi-SAFE时空框架，将改进版YOLOv8与时间分段网络（TSN）相结合应用于视频监控。通过采用GhostNetV3作为轻量化主干网络、指数移动平均（EMA）注意力机制以及模型剪枝技术，在保持精度的同时显著降低计算成本。YOLOv8与TSN分别在行人数据集和暴力行为数据集上独立训练：YOLOv8负责提取人体区域，TSN进行暴力行为的二分类判定。在RWF-2000数据集上的实验表明，Vi-SAFE达到0.88的准确率，显著优于单独使用TSN的0.77基准，在准确率和效率方面均超越现有方法，证明了其在公共安全监控中的有效性。代码已开源：https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md。\n\n（注：根据学术规范要求，译文对技术术语进行了标准化处理：Temporal Segment Network统一译为\"时间分段网络\"，exponential moving average采用\"指数移动平均\"的通用译法，GhostNetV3等技术名词保留英文原名。实验数据采用标准数值表述方式，并确保技术实现逻辑的准确传达。）"
    },
    {
        "title": "Road Obstacle Video Segmentation",
        "url": "http://arxiv.org/abs/2509.13181v1",
        "pub_date": "2025-09-16",
        "summary": "With the growing deployment of autonomous driving agents, the detection and segmentation of road obstacles have become critical to ensure safe autonomous navigation. However, existing road-obstacle segmentation methods are applied on individual frames, overlooking the temporal nature of the problem, leading to inconsistent prediction maps between consecutive frames. In this work, we demonstrate that the road-obstacle segmentation task is inherently temporal, since the segmentation maps for consecutive frames are strongly correlated. To address this, we curate and adapt four evaluation benchmarks for road-obstacle video segmentation and evaluate 11 state-of-the-art image- and video-based segmentation methods on these benchmarks. Moreover, we introduce two strong baseline methods based on vision foundation models. Our approach establishes a new state-of-the-art in road-obstacle video segmentation for long-range video sequences, providing valuable insights and direction for future research.",
        "translated": "随着自动驾驶智能体的广泛应用，道路障碍物的检测与分割已成为确保安全自主导航的关键技术。然而，现有的道路障碍分割方法仅针对单帧图像进行处理，忽视了该问题本身具有的时序特性，导致连续帧之间的预测结果存在不一致性。本研究论证了道路障碍分割任务本质上具有时序关联性，因为连续帧的分割图谱之间存在强相关性。为此，我们构建并调整了四个针对道路障碍视频分割的评估基准，并在这些基准上评估了11种基于图像和视频的先进分割方法。此外，我们基于视觉基础模型提出了两种强基线方法。该方案在长时序视频的道路障碍分割任务中实现了新的技术突破，为未来研究提供了重要洞见和方向指引。"
    },
    {
        "title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era",
        "url": "http://arxiv.org/abs/2509.13175v1",
        "pub_date": "2025-09-16",
        "summary": "The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive vision-language pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrate that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (&gt;96\\% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale \"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report pairs). Further, we find that vision encoder trained on this \"silver-standard\" dataset achieves performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-of-the-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot diagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements in cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate {\\bf more performant and scalable} medical AI systems. Our code is avaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
        "translated": "大型语言模型（LLMs）的出现为医学对比式视觉-语言预训练带来了革命性机遇。本文展示了如何利用LLMs推动大规模监督式预训练，从而提升视觉-语言对齐效果。我们首先证明现代LLMs能够自动从放射报告中提取诊断标签（实验显示AUC＞96%），且无需复杂提示工程，即可低成本构建大规模\"银标准\"数据集（5万份CT图像-报告对仅需约3美元）。进一步研究发现，基于该\"银标准\"数据集训练的视觉编码器，其性能可与专用BERT模型提取标签的训练结果相媲美，从而降低大规模监督预训练的门槛。在此基础上，我们揭示了监督预训练从根本上改善对比式视觉-语言对齐的机制。仅使用3D ResNet-18架构和标准CLIP训练，我们的方法就实现了最先进的性能：在CT-RATE上达到83.8%的零样本诊断AUC，在RAD-ChestCT上达到77.3% AUC，跨模态检索性能显著提升（图像-图像检索MAP@50=53.7%，报告-图像检索Recall@100=52.2%）。这些结果证明了利用LLMs构建更高性能、更强扩展性医学AI系统的潜力。代码已开源：https://github.com/SadVoxel/More-performant-and-scalable。\n\n（注：根据学术规范，对技术术语进行了标准化处理：\n1. \"silver-standard\"译为\"银标准\"以区别于人工标注的\"金标准\"\n2. 保留AUC/MAP/Recall等评估指标原文缩写\n3. 3D ResNet-18/CLIP/BERT等模型名称保持英文大写形式\n4. 货币单位按原文语境保留美元计量\n5. 论文核心贡献点通过\"揭示\"\"证明\"\"显著提升\"等学术表达强化）"
    },
    {
        "title": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory",
        "url": "http://arxiv.org/abs/2509.13172v1",
        "pub_date": "2025-09-16",
        "summary": "Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: https://github.com/WHU-USI3DV/WHU-STree.",
        "translated": "街道树木对城市宜居性至关重要，能带来生态与社会双重效益。在空间受限的城市环境中建立详细、准确且动态更新的街道树木清单，对于优化这些多功能资产具有重要意义。鉴于传统实地调查方式耗时费力，采用移动测绘系统（MMS）的自动化调查提供了更高效的解决方案。然而现有基于MMS的树木数据集受限于小规模场景、标注不全或单一模态，难以支撑综合分析。为此，我们推出WHU-STree——一个跨城市、精细化标注的多模态街道树木数据集。该数据集在两大城市采集，融合同步获取的点云与高清图像，包含21,007棵标注树木实例，涵盖50个树种和2种形态参数。凭借其独特特性，WHU-STree可同步支持10余种街道树木清单相关任务。我们针对树种分类和单木分割两大核心任务建立了代表性基线模型基准。大量实验与深度分析表明：多模态数据融合具有显著潜力，而跨域适用性是算法实际部署的关键前提。特别地，我们指出了关键挑战并规划了未来研究方向，包括多模态融合、多任务协同、跨域泛化、空间模式学习以及基于多模态大语言模型的街道树木资产管理。WHU-STree数据集已开源：https://github.com/WHU-USI3DV/WHU-STree。\n\n（注：翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"Mobile Mapping Systems\"译为\"移动测绘系统\"，\"point clouds\"译为\"点云\"\n2. 技术概念完整保留：如\"multi-modal fusion\"译为\"多模态融合\"，\"cross-domain generalization\"译为\"跨域泛化\"\n3. 长难句拆分重组：将原文复合句按中文表达习惯分解为多个短句\n4. 被动语态转化：\"are limited by\"译为\"受限于\"符合中文主动表达习惯\n5. 数据精确传递：所有数值（21,007棵）、种数（50个）均完整保留\n6. 学术表述规范：\"benchmark\"译为\"建立基准\"，\"extensive experiments\"译为\"大量实验\"）"
    },
    {
        "title": "Enhancing Video Large Language Models with Structured Multi-Video\n  Collaborative Reasoning (early version)",
        "url": "http://arxiv.org/abs/2509.13161v1",
        "pub_date": "2025-09-16",
        "summary": "Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models.",
        "translated": "尽管视频语言模型发展迅猛，但当前对全面视频推理的追求仍受限于单个视频固有的时空不完整性，导致存在幻觉和误差问题。一个可行的解决方案是通过引入多个相关视频来增强推理性能。然而，视频标记数量庞大且包含冗余信息，若直接将相关视频数据输入大语言模型以提升回答质量，反而可能适得其反。针对这一挑战，我们提出了一种面向视频语言模型的多视频协作框架。为实现高效灵活的视频表征，我们构建了视频结构化模块，将视频知识表示为时空图。基于结构化视频表征，我们设计图融合模块，将来自相关视频的结构化知识与有价值信息融合至增强的图节点标记中。最后，我们构建了精细的多视频结构化提示，将图标记、视觉标记和文本标记整合为大语言模型的输入。大量实验验证了本框架的有效性，展现了其作为推进视频语言模型发展的可行路径的潜力。"
    },
    {
        "title": "TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual\n  Document Images",
        "url": "http://arxiv.org/abs/2509.13151v1",
        "pub_date": "2025-09-16",
        "summary": "Recognizing textual attributes such as bold, italic, underline and strikeout is essential for understanding text semantics, structure, and visual presentation. These attributes highlight key information, making them crucial for document analysis. Existing methods struggle with computational efficiency or adaptability in noisy, multilingual settings. To address this, we introduce TexTAR, a multi-task, context-aware Transformer for Textual Attribute Recognition (TAR). Our novel data selection pipeline enhances context awareness, and our architecture employs a 2D RoPE (Rotary Positional Embedding)-style mechanism to incorporate input context for more accurate attribute predictions. We also introduce MMTAD, a diverse, multilingual, multi-domain dataset annotated with text attributes across real-world documents such as legal records, notices, and textbooks. Extensive evaluations show TexTAR outperforms existing methods, demonstrating that contextual awareness contributes to state-of-the-art TAR performance.",
        "translated": "识别粗体、斜体、下划线和删除线等文本属性对于理解文本语义、结构及视觉呈现至关重要。这些属性通过突出关键信息，在文档分析中具有重要作用。现有方法在嘈杂多语言环境下的计算效率或适应性方面存在不足。为此，我们提出TexTAR——一个用于文本属性识别（TAR）的多任务上下文感知Transformer模型。我们创新的数据选择流程增强了上下文感知能力，模型架构采用二维旋转位置编码（RoPE）机制融合输入上下文，从而实现更精准的属性预测。同时，我们发布了MMTAD数据集，这是一个涵盖法律文书、通知、教材等多领域真实文档的多语言多样化文本属性标注数据集。大量实验表明，TexTAR性能优于现有方法，证明上下文感知能力对实现最先进文本属性识别性能具有重要贡献。"
    },
    {
        "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
        "url": "http://arxiv.org/abs/2509.13149v1",
        "pub_date": "2025-09-16",
        "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.",
        "translated": "4D雷达超分辨率技术旨在将稀疏且含有噪声的点云重建为密集且几何一致的表示，是自动驾驶感知领域的基础性问题。然而，现有方法通常存在训练成本高或依赖复杂扩散采样的问题，导致推理延迟高、泛化能力差，难以兼顾精度与效率。为突破这些局限性，我们提出MSDNet——一种多阶段蒸馏框架，通过高效迁移稠密激光雷达先验知识到4D雷达特征，同时实现高质量重建与高计算效率。第一阶段采用重建引导的特征蒸馏，通过特征重构对齐并稠化学生网络的特征表示；第二阶段提出扩散引导的特征蒸馏，将首阶段蒸馏特征视为教师网络表征的噪声版本，并通过轻量化扩散网络进行精细化处理。此外，我们引入噪声适配器模块，可自适应对齐特征噪声水平与预设扩散时间步，从而实现更精确的去噪过程。在VoD及自有数据集上的大量实验表明，MSDNet在4D雷达点云超分辨率任务中同时实现了高保真重建与低延迟推理，并持续提升下游任务性能。代码将在论文发表后开源。\n\n（注：翻译严格遵循以下技术要点处理：\n1. \"4D radar super-resolution\" 专业术语保留技术特征，译为\"4D雷达超分辨率\"\n2. \"point clouds\" 统一译为\"点云\"以符合计算机视觉领域惯例\n3. \"LiDAR priors\" 译为\"激光雷达先验知识\"准确体现先验信息迁移概念\n4. \"diffusion timestep\" 译为\"扩散时间步\"保持生成模型领域术语一致性\n5. 复杂句式如\"treats the stage-one distilled features as...\" 采用分句处理符合中文表达习惯\n6. 技术动作描述如\"aligning and densifying\" 译为\"对齐并稠化\"保持动词准确性）"
    },
    {
        "title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and\n  Semi-Supervised Baseline",
        "url": "http://arxiv.org/abs/2509.13133v1",
        "pub_date": "2025-09-16",
        "summary": "As automatic parking systems evolve, the accurate detection of parking slots has become increasingly critical. This study focuses on parking slot detection using surround-view cameras, which offer a comprehensive bird's-eye view of the parking environment. However, the current datasets are limited in scale, and the scenes they contain are seldom disrupted by real-world noise (e.g., light, occlusion, etc.). Moreover, manual data annotation is prone to errors and omissions due to the complexity of real-world conditions, significantly increasing the cost of annotating large-scale datasets. To address these issues, we first construct a large-scale parking slot detection dataset (named CRPS-D), which includes various lighting distributions, diverse weather conditions, and challenging parking slot variants. Compared with existing datasets, the proposed dataset boasts the largest data scale and consists of a higher density of parking slots, particularly featuring more slanted parking slots. Additionally, we develop a semi-supervised baseline for parking slot detection, termed SS-PSD, to further improve performance by exploiting unlabeled data. To our knowledge, this is the first semi-supervised approach in parking slot detection, which is built on the teacher-student model with confidence-guided mask consistency and adaptive feature perturbation. Experimental results demonstrate the superiority of SS-PSD over the existing state-of-the-art (SoTA) solutions on both the proposed dataset and the existing dataset. Particularly, the more unlabeled data there is, the more significant the gains brought by our semi-supervised scheme. The relevant source codes and the dataset have been made publicly available at https://github.com/zzh362/CRPS-D.",
        "translated": "随着自动泊车系统的发展，准确检测停车位变得愈发关键。本研究聚焦于利用环视摄像头进行停车位检测，该类摄像头可提供停车环境的全景鸟瞰图。然而现有数据集规模有限，且包含的场景很少受到真实环境干扰（如光照变化、遮挡等）。此外，由于现实场景的复杂性，人工数据标注易出现错误和遗漏，显著增加了大规模数据集的标注成本。针对这些问题，我们首先构建了大规模停车位检测数据集（命名为CRPS-D），包含多种光照分布、不同天气条件和具有挑战性的停车位变体。与现有数据集相比，本数据集具有最大规模的数据量，包含更高密度的停车位，尤其具有更多倾斜停车位。此外，我们开发了半监督基线方法SS-PSD，通过利用未标注数据进一步提升检测性能。据我们所知，这是首个应用于停车位检测的半监督方法，其基于教师-学生模型框架，结合置信度引导的掩码一致性和自适应特征扰动机制。实验结果表明，在新建数据集和现有数据集上，SS-PSD均优于现有最先进方案。特别值得注意的是，未标注数据越多，我们的半监督方案带来的性能提升越显著。相关源代码和数据集已公开于https://github.com/zzh362/CRPS-D。\n\n（注：根据学术规范，术语处理说明：\n1. \"surround-view cameras\"译为行业通用术语\"环视摄像头\"\n2. \"bird's-eye view\"保留意象译为\"鸟瞰图\"\n3. \"semi-supervised\"统一译为\"半监督\"\n4. \"teacher-student model\"译为\"教师-学生模型\"（不可简化为\"师生模型\"）\n5. \"state-of-the-art (SoTA)\"译为\"最先进\"并保留英文缩写\n6. 技术术语\"mask consistency\"和\"feature perturbation\"分别译为\"掩码一致性\"和\"特征扰动\"\n7. 数据集名称CRPS-D保留英文大写形式）"
    },
    {
        "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for\n  Autonomous Driving",
        "url": "http://arxiv.org/abs/2509.13116v1",
        "pub_date": "2025-09-16",
        "summary": "Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.",
        "translated": "理解动态环境中的运动是自动驾驶的关键任务，因此推动了类别无关运动预测的研究。本文针对激光雷达点云数据，研究弱监督与自监督的类别无关运动预测方法。户外场景通常由移动前景和静态背景构成，这使得运动理解可与场景解析相关联。基于这一观察，我们提出了一种新颖的弱监督范式，使用完全或部分标注（1%、0.1%）的前景/背景掩码替代运动标注进行监督。为此，我们开发了一种弱监督方法，利用前景/背景线索指导运动预测模型的自监督学习。由于前景运动通常发生在非地面区域，非地面/地面掩码可作为前景/背景掩码的替代方案，进一步降低标注成本。基于非地面/地面线索，我们提出了两种扩展方法：一种需要更少标注（0.01%）的弱监督方法，以及完全无需标注的自监督方法。此外，我们设计了鲁棒一致性感知倒角距离损失函数，通过融入多帧信息和鲁棒惩罚机制来抑制自监督学习中的异常值。实验表明，我们的弱监督和自监督模型性能优于现有自监督方法，其中弱监督模型甚至可与某些全监督模型相媲美。这证明我们的方法在标注成本与性能之间实现了有效平衡。"
    },
    {
        "title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery\n  Detection - The 2024 Global Deepfake Image Detection Challenge",
        "url": "http://arxiv.org/abs/2509.13107v1",
        "pub_date": "2025-09-16",
        "summary": "The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.",
        "translated": "专业级深度伪造技术的扩散对数字安全性与真实性构成严峻挑战。要检测这类伪造内容——尤其是面对多样化的篡改技术——需要具备强大泛化能力的模型。本文提出分层深度融合框架（HDFF），这是一种基于集成学习的深度学习架构，专为实现高性能人脸伪造检测而设计。该框架整合了四种不同的预训练子模型：Swin-MLP、CoAtNet、EfficientNetV2和DaViT，这些模型在MultiFFDI数据集上通过多阶段流程进行了精细微调。通过串联这些专用模型的特征表示并训练最终分类层，HDFF有效融合了各模型的优势。该方法在竞赛私有排行榜上获得0.96852的最终得分，在184支队伍中位列第20名，证明了分层融合在复杂图像分类任务中的有效性。\n\n（注：技术细节说明：\n1. 模型名称保留原文大写形式（Swin-MLP/CoAtNet等）\n2. MultiFFDI作为专有数据集名称保留原写法\n3. 集成学习(ensemble-based)、特征表示(feature representations)等术语按计算机视觉领域规范翻译\n4. 竞赛评分机制相关表述采用\"私有排行榜(private leaderboard)\"等标准竞赛术语）"
    },
    {
        "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual\n  Assembly Control",
        "url": "http://arxiv.org/abs/2509.13089v1",
        "pub_date": "2025-09-16",
        "summary": "Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions.",
        "translated": "装配过程的质量控制在制造业中至关重要，这不仅能确保单个零部件的质量，还能保证它们被正确集成到最终产品中。为协助这一过程，基于计算机视觉的自动化装配控制技术已得到广泛应用。然而，图像采集、标注以及计算机视觉算法训练所涉及的成本给系统集成带来挑战，尤其对资源有限的中小型企业（SMEs）而言，它们往往难以承担大规模训练、数据收集和人工图像标注的投入。合成数据技术有望减少人工数据收集和标注的工作量，但目前在装配质量检测领域的实际应用仍存在局限。本研究提出了一种新颖的易集成、高数据效能的视觉装配控制方法。该方法基于计算机辅助设计（CAD）数据与目标检测算法，通过模拟场景生成技术实现质量控制。实验结果表明：该方案构建了高效的生产环境图像数据生成流程，在模拟训练数据中对合成行星齿轮系统零部件的识别准确率均值（mAP@0.5:0.95）高达99.5%，迁移至真实摄像头采集的测试数据时仍能达到93%的精度。这项研究不仅证明了可适配流程中合成数据生成的有效性，更凸显了其在帮助中小企业实施资源高效型视觉装配控制解决方案方面的巨大潜力。\n\n（注：mAP@0.5:0.95是目标检测领域通用评价指标，中文表述保留英文缩写并辅以中文说明；CAD采用业界通用译法\"计算机辅助设计\"；SMEs遵循国际惯例译为\"中小企业\"）"
    },
    {
        "title": "Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation\n  with Uncertainty-Guided Pseudo-Labeling",
        "url": "http://arxiv.org/abs/2509.13084v1",
        "pub_date": "2025-09-16",
        "summary": "Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\\% Dice score on left Atrial with 10\\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments.",
        "translated": "尽管监督式医学图像分割模型表现出卓越的性能，但在现实场景中依赖大量标注数据并不现实。半监督学习方法试图通过利用未标注数据生成伪标签来缓解这一难题，然而现有方法仍面临伪标签噪声和特征空间监督不足的问题。为解决这些挑战，本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架。具体而言，我们设计了交叉一致性增强模块，通过交叉伪监督与熵滤波监督机制降低噪声伪标签的影响；同时采用基于不确定度感知机制（即KL散度）的动态加权策略调整伪标签的贡献权重。此外，我们引入自监督对比学习机制，通过有效区分可信预测与不确定预测，将不确定体素特征与可靠类别原型对齐，从而降低预测不确定性。在左心房、NIH胰腺和BraTS-2019三个3D分割数据集上的大量实验表明：在不同设置下（如使用10%标注数据时左心房分割Dice分数达89.95%），本方法始终优于现有最优方法。消融实验进一步验证了所提出模块的有效性。"
    },
    {
        "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image\n  Enhancement",
        "url": "http://arxiv.org/abs/2509.13083v1",
        "pub_date": "2025-09-16",
        "summary": "In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc",
        "translated": "在傅里叶域中，亮度信息主要编码于振幅谱，而空间结构信息则蕴含于相位分量。传统的傅里叶频率信息拟合方法采用逐像素损失函数，这类方法往往过度关注局部信息，可能导致全局信息丢失。本文提出LLFDisc——一种集成交叉注意力与门控机制的U型深度增强网络，专为频率感知增强任务设计。我们创新性地提出分布感知损失函数，直接拟合傅里叶域信息并通过闭式KL散度目标最小化分布差异，使模型相比传统基于MSE的损失函数能更鲁棒地对齐傅里叶域信息。此外，我们在VGG感知损失中嵌入KL散度计算以提升深度特征的结构保真度。在多个基准测试上的大量实验表明，LLFDisc在定性与定量评估中均达到了最先进的性能。代码已开源于：https://github.com/YanXY000/LLFDisc\n\n（注：技术要点说明：\n1. 专业术语处理：\"amplitude spectrum\"译为振幅谱，\"phase components\"译为相位分量，\"cross-attention\"保留交叉注意力机制，\"gating mechanisms\"译为门控机制\n2. 算法概念：\"distribution-aware loss\"意译为分布感知损失函数，\"closed-form KL-Divergence\"译为闭式KL散度\n3. 技术表述：\"frequency-aware enhancement\"译为频率感知增强，\"structural fidelity\"译为结构保真度\n4. 保持学术论文的客观表述风格，避免口语化表达）"
    },
    {
        "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust\n  Referring Image Segmentation",
        "url": "http://arxiv.org/abs/2509.13070v1",
        "pub_date": "2025-09-16",
        "summary": "Referring Image Segmentation (RIS) is a task that segments image regions based on language expressions, requiring fine-grained alignment between two modalities. However, existing methods often struggle with multimodal misalignment and language semantic loss, especially in complex scenes containing multiple visually similar objects, where uniquely described targets are frequently mislocalized or incompletely segmented. To tackle these challenges, this paper proposes TFANet, a Three-stage Image-Text Feature Alignment Network that systematically enhances multimodal alignment through a hierarchical framework comprising three stages: Knowledge Plus Stage (KPS), Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the first stage, we design the Multiscale Linear Cross-Attention Module (MLAM), which facilitates bidirectional semantic exchange between visual features and textual representations across multiple scales. This establishes rich and efficient alignment between image regions and different granularities of linguistic descriptions. Subsequently, the KFS further strengthens feature alignment through the Cross-modal Feature Scanning Module (CFSM), which applies multimodal selective scanning to capture long-range dependencies and construct a unified multimodal representation. This is essential for modeling long-range cross-modal dependencies and enhancing alignment accuracy in complex scenes. Finally, in the KIS, we propose the Word-level Linguistic Feature-guided Semantic Deepening Module (WFDM) to compensate for semantic degradation introduced in earlier stages.",
        "translated": "参考图像分割（RIS）是一项根据语言表达对图像区域进行分割的任务，需要实现视觉与语言两种模态间的细粒度对齐。然而，现有方法常面临多模态错位和语言语义丢失的问题，特别是在包含多个视觉相似对象的复杂场景中，被特定描述的目标经常出现定位错误或分割不完整的情况。为解决这些挑战，本文提出TFANet——一种三阶段图文特征对齐网络，通过包含知识增强阶段（KPS）、知识融合阶段（KFS）和知识强化阶段（KIS）的分层框架系统性地提升多模态对齐能力。\n\n在第一阶段，我们设计了多尺度线性交叉注意力模块（MLAM），该模块在多个尺度上促进视觉特征与文本表征之间的双向语义交换，从而建立图像区域与不同粒度语言描述之间丰富且高效的对齐关系。随后，KFS阶段通过跨模态特征扫描模块（CFSM）进一步强化特征对齐：该模块采用多模态选择性扫描机制捕获长程依赖关系，并构建统一的多模态表征，这对建模长程跨模态依赖和提升复杂场景中的对齐精度至关重要。最后在KIS阶段，我们提出词级语言特征引导的语义深化模块（WFDM），以补偿前阶段可能产生的语义退化问题。"
    },
    {
        "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2509.13067v1",
        "pub_date": "2025-09-16",
        "summary": "By cropping high-resolution images into local tiles and encoding them independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have demonstrated remarkable fine-grained visual understanding capabilities. However, this divide-and-conquer paradigm significantly increases the number of visual tokens, resulting in substantial computational and memory overhead. To better understand and address this challenge, we empirically investigate visual token utilization in HR-LVLMs and uncover three key findings: (1) the local tiles have varying importance, jointly determined by visual saliency and task relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage attention pattern across layers, with each stage attending to different types of visual tokens; (3) the visual tokens emphasized at different stages encode information at varying levels of granularity, playing complementary roles within LVLMs. Building on these insights, we propose HERO, a High-resolution visual token early dropping framework that integrates content-adaptive token budget allocation with function-aware token selection. By accurately estimating tile-level importance and selectively retaining visual tokens with complementary roles, HERO achieves superior efficiency-accuracy trade-offs across diverse benchmarks and model scales, all in a training-free manner. This study provides both empirical insights and practical solutions toward efficient inference in HR-LVLMs.",
        "translated": "通过将高分辨率图像裁剪为局部图块并独立编码，高分辨率大视觉语言模型（HR-LVLMs）展现出卓越的细粒度视觉理解能力。然而，这种分治策略显著增加了视觉令牌数量，导致计算和内存开销大幅上升。为深入理解并解决这一挑战，我们通过实证研究分析了HR-LVLMs中视觉令牌的利用情况，发现三个关键现象：（1）局部图块的重要性存在差异，由视觉显著性和任务相关性共同决定；（2）基于CLIP的视觉编码器中，CLS令牌在层级间呈现两阶段注意力模式，每个阶段关注不同类型的视觉令牌；（3）不同阶段强调的视觉令牌编码了不同粒度的信息，在LVLMs中发挥互补作用。基于这些发现，我们提出HERO框架——一种高分辨率视觉令牌早期丢弃方案，整合了内容自适应的令牌预算分配与功能感知的令牌选择机制。通过准确评估图块重要性并选择性保留具有互补作用的视觉令牌，HERO在无需训练的情况下，于多样化基准测试和模型规模上实现了更优的效率-精度平衡。本研究为HR-LVLMs的高效推理既提供了实证洞察，也给出了实用解决方案。\n\n（注：专业术语说明：\n- CLS令牌：代表图像整体特征的分类令牌\n- CLIP：Contrastive Language-Image Pre-training的缩写，一种跨模态预训练模型\n- 视觉令牌(visual tokens)：指经过处理的图像特征向量\n- 训练免费(training-free)：指无需额外训练即可部署的优化方法）"
    },
    {
        "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models",
        "url": "http://arxiv.org/abs/2509.13031v1",
        "pub_date": "2025-09-16",
        "summary": "Reinforcement learning (RL) has proven highly effective in eliciting the reasoning capabilities of large language models (LLMs). Inspired by this success, recent studies have explored applying similar techniques to vision-language models (VLMs), aiming to enhance their reasoning performance. However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as the tasks faced by VLMs are inherently more complex. Specifically, VLMs must first accurately perceive and understand visual inputs before reasoning can be effectively performed. To address this challenge, we propose a two-stage reinforcement learning framework designed to jointly enhance both the perceptual and reasoning capabilities of VLMs. To mitigate the vanishing advantage issue commonly observed in RL training, we first perform dataset-level sampling to selectively strengthen specific capabilities using distinct data sources. During training, the first stage focuses on improving the model's visual perception through coarse- and fine-grained visual understanding, while the second stage targets the enhancement of reasoning abilities. After the proposed two-stage reinforcement learning process, we obtain PeBR-R1, a vision-language model with significantly enhanced perceptual and reasoning capabilities. Experimental results on seven benchmark datasets demonstrate the effectiveness of our approach and validate the superior performance of PeBR-R1 across diverse visual reasoning tasks.",
        "translated": "强化学习（RL）在激发大语言模型（LLMs）的推理能力方面已被证明极为有效。受此成功启发，近期研究尝试将类似技术应用于视觉语言模型（VLMs），以提升其推理性能。然而，直接将RL方法从LLMs移植到VLMs并非最优解，因为VLMs面临的任务本质上更为复杂——模型需先准确感知和理解视觉输入，才能有效进行推理。为应对这一挑战，我们提出了一种两阶段强化学习框架，旨在协同增强VLMs的感知与推理能力。针对RL训练中常见的优势消失问题，我们首先通过数据集级采样策略，利用不同数据源有针对性地强化特定能力。在训练过程中，第一阶段通过粗粒度和细粒度视觉理解提升模型感知能力，第二阶段专注于推理能力的增强。经过所提出的两阶段强化学习流程，我们最终得到PeBR-R1模型，其视觉感知与推理能力显著提升。在七个基准数据集上的实验结果表明，该方法的有效性得到了验证，且PeBR-R1在多样化视觉推理任务中均展现出卓越性能。\n\n（注：专业术语说明：\n1. Reinforcement learning (RL) 标准译为\"强化学习\"\n2. Large language models (LLMs) 采用学界通用译法\"大语言模型\"\n3. Vision-language models (VLMs) 译为\"视觉语言模型\"\n4. PeBR-R1 作为模型名称保留原文格式\n5. coarse- and fine-grained visual understanding 译为\"粗粒度和细粒度视觉理解\"\n6. vanishing advantage issue 译为\"优势消失问题\"\n7. benchmark datasets 译为\"基准数据集\"）"
    },
    {
        "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single\n  Image",
        "url": "http://arxiv.org/abs/2509.13013v1",
        "pub_date": "2025-09-16",
        "summary": "With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.",
        "translated": "随着三维表示技术与生成模型的快速发展，从单张图像重建全身三维虚拟人的研究已取得显著进展。然而，由于单目输入信息有限，该任务本质上仍存在不适定性，导致生成过程中难以有效控制被遮挡区域的几何形状与纹理。为解决这些问题，我们重新设计了重建流程，提出Dream3DAvatar——一个高效且支持文本控制的两阶段三维虚拟人生成框架。在第一阶段，我们开发了轻量化的适配器增强多视图生成模型：通过引入Pose-Adapter将SMPL-X渲染图与骨骼信息注入SDXL，确保多视角间的几何与姿态一致性；采用ID-Adapter-G注入高分辨率面部特征以保持身份特征；同时利用BLIP2生成多视图图像的高质量文本描述，增强被遮挡区域的文本驱动控制能力。在第二阶段，我们设计了配备多视图特征融合模块的前馈Transformer模型，从生成图像中重建高保真三维高斯溅射表示（3DGS）。此外，通过引入采用门控机制的ID-Adapter-R，将面部特征有效融合到重建过程中以提升高频细节还原能力。大量实验表明，本方法无需后处理即可生成逼真、支持动画的三维虚拟人，并在多项评估指标上持续优于现有基线方法。\n\n（注：技术术语说明：\n- SMPL-X: 参数化人体三维模型标准\n- SDXL: Stable Diffusion XL文生图模型\n- BLIP2: 视觉语言预训练模型\n- 3DGS: 3D Gaussian Splatting三维渲染技术\n- Adapter: 适配器模块（一种模型微调技术））"
    },
    {
        "title": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire",
        "url": "http://arxiv.org/abs/2509.12997v1",
        "pub_date": "2025-09-16",
        "summary": "Small drones are an increasing threat to both military personnel and civilian infrastructure, making early and automated detection crucial. In this work we develop a system that uses spiking neural networks and neuromorphic cameras (event cameras) to detect drones. The detection model is deployed on a neuromorphic chip making this a fully neuromorphic system. Multiple detection units can be deployed to create a virtual tripwire which detects when and where drones enter a restricted zone. We show that our neuromorphic solution is several orders of magnitude more energy efficient than a reference solution deployed on an edge GPU, allowing the system to run for over a year on battery power. We investigate how synthetically generated data can be used for training, and show that our model most likely relies on the shape of the drone rather than the temporal characteristics of its propellers. The small size and low power consumption allows easy deployment in contested areas or locations that lack power infrastructure.",
        "translated": "小型无人机对军事人员和民用基础设施构成的威胁日益加剧，因此实现早期自动化检测至关重要。本研究开发了一种基于脉冲神经网络与神经形态相机（事件相机）的无人机检测系统。该检测模型部署于神经形态芯片上，形成了完整的神经形态处理体系。通过部署多个检测单元可构建虚拟警戒线，精准识别无人机侵入禁区的时空坐标。实验表明，我们的神经形态解决方案比基于边缘GPU的参考方案能效提升数个数量级，仅靠电池供电即可持续运行超过一年。我们探究了合成生成数据在训练中的应用，并证明检测模型主要依赖无人机外形特征而非螺旋桨的时序特性。该系统凭借小巧体积与低功耗特性，可轻松部署于战乱区域或缺乏电力基础设施的地点。\n\n（注：翻译严格遵循了以下技术要点：\n1. 专业术语准确：\"spiking neural networks\"译为脉冲神经网络，\"neuromorphic cameras\"采用行业通用译法\"神经形态相机\"并括号注明事件相机\n2. 技术概念完整：\"virtual tripwire\"译为虚拟警戒线并补充说明其功能\n3. 量化表述精确：\"orders of magnitude\"译为数量级，\"battery power\"明确为电池供电\n4. 学术表述规范：\"temporal characteristics\"译为时序特性而非时间特征\n5. 逻辑关系清晰：通过\"并证明\"、\"凭借\"等连接词保持原文论证链条的完整性）"
    },
    {
        "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized\n  Detectors on In-the-Wild AI Image Detection",
        "url": "http://arxiv.org/abs/2509.12995v1",
        "pub_date": "2025-09-16",
        "summary": "While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.   Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.",
        "translated": "尽管针对AI生成图像的专用检测器在精心构建的基准测试中表现优异，但在真实场景中却会遭遇灾难性失败——这一点在\"野外\"基准测试中高达临界值的假阴性率上得到印证。我们并未选择为此问题打造另一把专用\"匕首\"，而是直接端出\"机关枪\"：基于现代视觉基础模型（VFM）的简单线性分类器。在相同训练数据下，这个基线方法以压倒性优势\"完胜\"定制化检测器，将野外检测准确率显著提升超过20%。 \n\n我们的分析揭示了VFM\"火力\"的来源：首先通过探测图文相似性，我们发现新一代视觉语言模型（如Perception Encoder、Meta CLIP2）已学会将合成图像与伪造相关概念（如\"AI生成\"）对齐，这与早期版本截然不同。其次，我们推测这种现象源于数据暴露——当使用在VFM预训练截止日期后爬取的全新数据集进行测试时，这种对齐效果与整体准确率均急剧下降，确保该数据在预训练阶段未被见过。\n\n研究结果得出两个关键结论：1）对于AI生成图像检测这场现实世界的\"枪战\"，更新后的VFM原始\"火力\"远比静态检测器的\"精工细作\"更有效；2）真正的泛化评估要求测试数据必须独立于模型完整训练历史（包括预训练阶段）。"
    }
]