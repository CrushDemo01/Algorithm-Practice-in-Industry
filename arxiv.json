[
    {
        "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
        "url": "http://arxiv.org/abs/2509.06902v1",
        "pub_date": "2025-09-08",
        "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \\emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \\emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \\emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.",
        "translated": "大型语言模型（LLMs）作为随机性系统可能生成偏离可用数据的数值，这种错误被称为\"数值幻觉\"。现有防护机制——检索增强生成、引用和不确定性估计——虽能提升透明度，但无法保证数值保真度：虚构或误引的数值仍可能以正确形式呈现。我们提出**可验证数值载体（PCN）**，这是一种通过机械验证确保数值保真度的呈现层协议。在PCN框架下，数值片段以与结构化声明绑定的\"声明约束令牌\"形式输出，验证器根据声明策略（如精确等价、舍入规则、别名系统或带限定条件的容差范围）检查每个令牌。关键创新在于PCN将验证环节置于渲染器而非模型中：只有通过声明验证的数值会被标记为已验证，其余数值默认处于未验证状态。这种分离设计有效防止欺骗行为，并确保故障封闭特性。我们形式化定义了PCN协议，并证明其具备可靠性、诚实令牌下的完备性、故障封闭特性以及策略优化下的单调性。PCN具有轻量化和模型无关特性，可无缝集成至现有应用系统，并能通过密码学承诺进行功能扩展。通过将验证作为数值显示前的强制步骤，PCN为数值敏感场景建立了简明契约：唯有通过验证才能获得信任，而缺乏验证标识则传递不确定性信息。"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们推出了一款名为\"智能快速投放\"(Smart Fast Finish, SFF)的预算调控功能。该功能基于行业标准的快速投放(FF)技术进行优化——传统FF系统会在固定投放周期临近结束时，以最快速度耗尽剩余广告预算。SFF通过分析历史广告活动数据，动态调整系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分割实验数据和离线模拟结果表明，SFF在预算调控过程中能有效缓解超量投放问题，是一种稳健的解决方案。\n\n（注：根据技术文档翻译规范，对以下术语作了标准化处理：\n1. \"budget pacing\"译为\"预算调控\"而非字面意义的\"预算步调\"\n2. \"overdelivery mitigation\"译为\"缓解超量投放\"符合广告技术领域表述\n3. \"throttle rate\"译为\"调控速率\"准确体现系统参数特性\n4. 保留DoorDash等专有名词原文，符合技术文献惯例）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关系，从而阻碍战略决策。为突破这些局限，我们提出KLIPA框架——一种结合知识图谱与大语言模型（LLM）的创新专利分析系统。该框架集成三大核心组件：用于构建专利间显性关系图谱的结构化知识库，揭示上下文关联的检索增强生成（RAG）系统，以及能动态确定最优查询策略的智能代理。我们在真实世界专利数据库上进行验证，结果表明KLIPA在知识提取、新颖关联发现和整体运营效率方面实现显著提升。该技术组合不仅提高了检索精度，降低了对领域专家的依赖，更为科技企业、律所等知识产权管理机构提供了可扩展的自动化解决方案，助力其更好地驾驭战略创新与竞争情报的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v1",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "科学文献的结构化信息提取对于捕捉专业领域的核心概念与新兴趋势至关重要。尽管现有数据集有助于模型开发，但由于领域复杂性和科学文本标注的高成本，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专门针对自然语言处理（NLP）领域全文实体与关系抽取的基准数据集。该数据集包含60篇人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行了对比实验，并评估了前沿监督模型在该数据集上的表现。实验结果表明，现有模型对不同长度学术文本的提取能力存在差异。与现有数据集的交叉对比显示，SciNLP在某些基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点度，表明其蕴含丰富的语义拓扑信息，可有效增强下游应用。数据集已公开于https://github.com/AKADDC/SciNLP。"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）系统需应对简短模糊的用户查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配问题的核心机制，其设计范式因预训练语言模型（PLM）和大语言模型（LLM）的出现发生了显著变革。本综述从三个维度系统梳理该领域：（i）建立查询扩展的四维框架——从扩展点注入方式（显式与隐式QE），经知识 grounding 与交互机制（知识库、模型内部能力、多轮检索）和学习对齐方法，延伸至基于知识图谱的论证；（ii）提出以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释各类模型在QE中的特色能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE技术在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与实施方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域问答/RAG、会话式检索、代码搜索及跨语言场景的应用图谱。研究提炼出知识 grounding 与交互、对齐/蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束三大核心策略，作为解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系以及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下选择和组合QE技术提供了系统化蓝图。"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文本文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂的结构特征要求研究者开发超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大发展时期的方法体系、核心挑战与应用场景。我们系统化地追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层级编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，本文还审视了特定领域应用场景、专项评估资源，并指明了效率权衡、多模态对齐和结果真实性等关键开放挑战。本综述旨在为基座模型时代的长文档检索研究既提供 consolidated 参考框架，又提出前瞻性的发展议程。"
    },
    {
        "title": "CAViAR: Critic-Augmented Video Agentic Reasoning",
        "url": "http://arxiv.org/abs/2509.07680v1",
        "pub_date": "2025-09-09",
        "summary": "Video understanding has seen significant progress in recent years, with models' performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets.",
        "translated": "近年来，视频理解领域取得显著进展，模型在短片段感知任务上的性能持续提升。然而在LVBench、Neptune和ActivityNet-RTL等最新基准测试中，随着查询指令复杂度增加和视频时长增长，需要复杂推理的视频任务性能出现明显下滑。本研究旨在探索：能否利用现有感知能力成功执行更复杂的视频推理？我们开发了一个配备视频处理模块作为子代理或工具的大型语言模型代理系统。与Visual Programming、ViperGPT和MoReVQA等前人工作中固定流程的查询处理方式不同，该代理通过分析每个模块调用的结果动态决定后续执行步骤。受文本推理领域研究启发，我们引入评估器机制来区分代理执行序列的成功与失败案例。实验表明，代理系统与评估器的组合在所述数据集上实现了强劲性能。"
    },
    {
        "title": "Visual Representation Alignment for Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2509.07979v1",
        "pub_date": "2025-09-09",
        "summary": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.",
        "translated": "通过视觉指令调优训练的多模态大语言模型（MLLMs）已在多样任务中展现出强大性能，但在以视觉为中心的任务（如目标计数或空间推理）中仍存在局限。我们将此不足归因于当前主流的纯文本监督范式——该范式仅对视觉通路提供间接指导，往往导致MLLMs在训练过程中丢失细粒度视觉细节。本文提出视觉表征对齐方法（VIRAL），这是一种简单而有效的正则化策略，可将MLLMs的内部视觉表征与预训练视觉基础模型（VFMs）的表征进行对齐。通过显式实施这种对齐，VIRAL不仅使模型能够保留来自输入视觉编码器的关键视觉细节，还能补充来自VFMs的额外视觉知识，从而增强其处理复杂视觉输入的推理能力。实验结果表明，该方法在广泛采用的多模态基准测试的所有任务中均取得持续改进。此外，我们通过全面的消融研究验证了框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。"
    },
    {
        "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation",
        "url": "http://arxiv.org/abs/2509.07978v1",
        "pub_date": "2025-09-09",
        "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: https://gzwsama.github.io/OnePoseviaGen.github.io/",
        "translated": "从单一参考图像准确估计未知物体的六维姿态，对于在现实长尾场景中运行的机器人技术至关重要。然而该设定存在显著挑战：三维模型通常难以获取，单视角重建缺乏公制尺度，生成模型与真实图像间的领域差异会削弱系统鲁棒性。我们提出OnePoseViaGen解决方案，通过两大核心组件应对这些挑战：首先，粗到精对齐模块通过结合多视角特征匹配与渲染比较优化，联合优化尺度与姿态估计；其次，文本引导生成式领域随机化策略通过多样化纹理，实现合成数据对姿态估计器的有效微调。这些技术使高保真单视角三维生成能够支撑可靠的单次六维姿态估计。在YCBInEOAT、Toyota-Light和LM-O等挑战性基准测试中，OnePoseViaGen以显著优势超越现有方法，达到最先进性能。我们进一步通过真实机器人手完成灵巧抓取实验，验证了该方法在现实操作中的实用性。项目页面：https://gzwsama.github.io/OnePoseviaGen.github.io/"
    },
    {
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
        "url": "http://arxiv.org/abs/2509.07969v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
        "translated": "近年来，大型多模态模型通过将基于图像的工具与强化学习相结合，在解决视觉问题方面取得显著进展。然而，现有开源方法通常存在推理模式单一、交互轮次有限的问题，难以胜任需要试错探索的复杂任务。本研究通过扩展基于工具的交互规模解决了这一局限性，提出Mini-o3系统——该系统能够执行深度多轮推理（可达数十个步骤），并在具有挑战性的视觉搜索任务中实现最先进的性能。\n\n我们重现OpenAI o3风格行为的方案包含三个核心组成部分：首先，构建了包含数千个挑战性视觉搜索问题的视觉探测数据集，专门用于探索性推理研究；其次，开发了迭代式数据收集流水线，获取呈现多样化推理模式（包括深度优先搜索、试错法和目标维持）的冷启动轨迹；第三，提出超轮次掩码策略，在强化学习过程中避免对达到最大交互轮次的响应进行惩罚，从而平衡训练效率与测试时的扩展性。尽管训练时仅设定六轮交互的上限，我们的模型在推理阶段能自然生成数十轮交互轨迹，且准确率随轮次增加持续提升。大量实验表明，Mini-o3能产生丰富的推理模式和深度思考路径，有效解决复杂视觉搜索问题。"
    },
    {
        "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
        "url": "http://arxiv.org/abs/2509.07966v1",
        "pub_date": "2025-09-09",
        "summary": "Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA.",
        "translated": "对表格等结构化数据的视觉推理是现代视觉语言模型（VLM）的核心能力，然而当前基准测试在规模、多样性和推理深度方面仍存在局限——尤其在处理渲染表格图像时更为明显。为填补这一空白，我们推出Visual-TableQA：一个专为评估和提升复杂表格数据视觉推理能力而构建的大规模开放域多模态数据集。我们的生成流程采用模块化、可扩展的全自动架构，通过多推理大语言模型协同完成生成、验证与启发三类角色。该数据集包含2,500张结构丰富的LaTeX渲染表格和6,000个推理密集型问答对，总成本控制在100美元以内。为提升多样性与创造性，我们通过跨模型提示（\"启发机制\"）和LLM陪审团过滤实现多模型协同数据生成——强模型负责生成布局与主题框架，弱模型进行细节扩展，共同将多样化的推理模式和视觉结构蒸馏到数据集中。实验表明，基于Visual-TableQA微调的模型能稳健泛化至外部基准测试，尽管数据为合成生成，其表现仍超越多个商用模型。完整流程与资源已开源：https://github.com/AI-4-Everyone/Visual-TableQA。"
    },
    {
        "title": "Feature Space Analysis by Guided Diffusion Model",
        "url": "http://arxiv.org/abs/2509.07936v1",
        "pub_date": "2025-09-09",
        "summary": "One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a decoder that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a guided diffusion model that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.",
        "translated": "深度神经网络（DNN）的核心问题之一在于其内部特征提取过程的黑箱特性。针对视觉相关领域，本文通过提出一种新型解码器，重点分析DNN的特征空间，该解码器能够生成保证其特征与用户指定特征高度匹配的图像。相较于以往研究中缺失的这种保证机制，我们的解码器通过生成特征与目标特征邻近的图像，可实证揭示图像中哪些属性被DNN编码到特定特征中。该解码器采用引导式扩散模型实现，通过引导预训练扩散模型的反向图像生成过程，逐步最小化每步生成的清晰图像特征与用户指定特征之间的欧氏距离。我们的解码器具有一项实用优势：无需额外训练即可分析不同DNN的特征空间，且仅需在单块商用GPU上运行。针对CLIP图像编码器、ResNet-50和视觉变换器的实验结果表明，解码器生成的图像特征与用户指定特征具有显著相似性，为理解这些DNN的特征空间提供了有价值的见解。"
    },
    {
        "title": "Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space\n  Object",
        "url": "http://arxiv.org/abs/2509.07932v1",
        "pub_date": "2025-09-09",
        "summary": "Characterization of uncooperative Resident Space Objects (RSO) play a crucial role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to assess the geometry and motion properties. To address the challenges of reconstructing tumbling uncooperative targets, this study evaluates the performance of existing state-of-the-art 3D reconstruction algorithms for dynamic scenes, focusing on their ability to generate geometrically accurate models with high-fidelity. To support our evaluation, we developed a simulation environment using Isaac Sim to generate physics-accurate 2D image sequences of tumbling satellite under realistic orbital lighting conditions. Our preliminary results on static scenes using Neuralangelo demonstrate promising reconstruction quality. The generated 3D meshes closely match the original CAD models with minimal errors and artifacts when compared using Cloud Compare (CC). The reconstructed models were able to capture critical fine details for mission planning. This provides a baseline for our ongoing evaluation of dynamic scene reconstruction.",
        "translated": "非合作空间 Resident Space Objects (RSO) 的几何与运动特性表征在在轨服务(On-Orbit Servicing, OOS)和主动碎片清除(Active Debris Removal, ADR)任务中具有关键作用。为解决翻滚类非合作目标三维重建的挑战，本研究系统评估了动态场景下现有最先进三维重建算法的性能，重点关注其生成高精度几何模型的能力。为支撑评估工作，我们基于Isaac Sim开发了仿真环境，在真实轨道光照条件下生成具有物理精确性的翻滚卫星二维图像序列。通过Neuralangelo对静态场景的初步测试显示出卓越的重建质量：生成的三维网格模型与原始CAD模型高度吻合，经Cloud Compare (CC)比对显示误差及伪影极小，能够有效捕捉任务规划所需的关键精细特征。该结果为后续动态场景重建研究建立了性能基准。"
    },
    {
        "title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s",
        "url": "http://arxiv.org/abs/2509.07928v1",
        "pub_date": "2025-09-09",
        "summary": "As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.",
        "translated": "随着本地人工智能日益普及，目标检测器的基准性能与其在消费级硬件上的实际可用性之间存在着关键差距。虽然YOLOv10s等模型承诺实现实时速度，但这些指标通常是在高性能桌面级GPU上达成的。本文揭示在资源受限系统（如配备RTX 4060 GPU的笔记本电脑）上，性能瓶颈并非来自计算能力，而是由系统级瓶颈主导——这一点通过简单的瓶颈测试得到验证。为突破硬件层级的限制，我们提出了一种双通道自适应推理算法，该模型无关方法无需改变网络架构。本研究主要聚焦自适应推理策略，对架构早期退出与分辨率自适应路由进行了对比分析，在统一评估框架中凸显了各自的技术权衡。该系统采用快速低分辨率通道进行初步检测，仅当检测置信度较低时才启动高分辨率模型通道。在5000张图像的COCO数据集测试中，本方法相比PyTorch早期退出基线实现1.85倍加速，仅损失5.51% mAP精度。这项工作通过将焦点从纯模型优化转向硬件感知的推理策略，为在消费级设备上部署高性能实时AI提供了实用且可复现的解决方案，从而实现吞吐量最大化。"
    },
    {
        "title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth\n  Segmentation",
        "url": "http://arxiv.org/abs/2509.07923v1",
        "pub_date": "2025-09-09",
        "summary": "Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\\% for CBCT segmentation and 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.",
        "translated": "数字化牙科代表了现代牙科实践的革命性转变。这一转型的基础步骤是获取患者牙列的精确数字化表征，该表征通过分割锥形束计算机断层扫描（CBCT）和口内扫描（IOS）数据获得。尽管数字牙科技术日益受到关注，但现有分割方法普遍缺乏严格验证，且表现出有限的性能和临床适用性。据我们所知，本研究首次提出面向牙齿分割的多模态预训练框架。我们开发了ToothMCL（牙齿多模态对比学习预训练模型），该框架创新性地整合了体积数据（CBCT）和表面数据（IOS）两种模态。通过多模态对比学习捕获模态不变表征，我们的方法能有效建模细粒度解剖特征，实现精确的多类别分割并准确识别国际牙科联合会（FDI）牙齿编号系统。\n\n配合该框架，我们构建了迄今最大的配对CBCT-IOS数据集CBCT-IOS3.8K，包含3,867例患者数据。随后我们在涵盖最全面、最多样化的独立数据集集合上进行了评估，这是迄今为止规模最大且最具多样性的验证。我们的方法在内部和外部测试中均达到最先进性能：CBCT分割的Dice相似系数（DSC）提升12%，IOS分割提升8%。此外，ToothMCL在不同牙组分类中持续超越现有方法，并在不同成像条件和临床场景下展现出强大的泛化能力。\n\n（注：译文严格遵循了以下技术规范：\n1. 专业术语标准化：\"Fédération Dentaire Internationale\"采用国内通用译名\"国际牙科联合会\"并保留FDI缩写\n2. 计量单位规范：完整保留\"Dice Similarity Coefficient (DSC)\"专业术语及百分比数据\n3. 技术概念准确传达：\"modality-invariant representations\"译为\"模态不变表征\"符合机器学习领域术语\n4. 数据呈现方式：3,867患者保持数字间隔规范，百分比数据完整保留\n5. 长句拆分重组：将原文复合句分解为符合中文表达习惯的短句，同时保持技术准确性）"
    },
    {
        "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion",
        "url": "http://arxiv.org/abs/2509.07920v1",
        "pub_date": "2025-09-09",
        "summary": "Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.",
        "translated": "人-物交互联合重建标志着人类理解自身与周边环境复杂互动的重大进展。然而，由于缺乏对人物交互关系的先验认知，传统优化方法往往难以实现物理可信的重建结果。本文提出ScoreHOI——一种基于扩散模型的有效优化器，通过引入扩散先验知识来实现精确的人-物交互重建。该模型利用分数引导采样的可控性，能够根据图像观测和物体特征重建出人体姿态与物体姿态的条件概率分布。在推理过程中，ScoreHOI通过特定物理约束引导去噪过程，有效提升重建质量。此外，我们提出接触驱动的迭代优化方法，以增强接触合理性并提高重建精度。在标准基准测试上的大量实验表明，ScoreHOI性能优于现有最先进方法，突显了其在人-物交互联合重建中实现精准鲁棒提升的卓越能力。"
    }
]