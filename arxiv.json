[
    {
        "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers\n  from LLMs via Claim Verification",
        "url": "http://arxiv.org/abs/2509.06902v1",
        "pub_date": "2025-09-08",
        "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \\emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \\emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \\emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.",
        "translated": "大型语言模型（LLMs）作为随机性系统可能生成偏离可用数据的数值，这种错误被称为\"数值幻觉\"。现有防护机制——检索增强生成、引用和不确定性估计——虽能提升透明度，但无法保证数值保真度：虚构或误引的数值仍可能以正确形式呈现。我们提出**可验证数值载体（PCN）**，这是一种通过机械验证确保数值保真度的呈现层协议。在PCN框架下，数值片段以与结构化声明绑定的\"声明约束令牌\"形式输出，验证器根据声明策略（如精确等价、舍入规则、别名系统或带限定条件的容差范围）检查每个令牌。关键创新在于PCN将验证环节置于渲染器而非模型中：只有通过声明验证的数值会被标记为已验证，其余数值默认处于未验证状态。这种分离设计有效防止欺骗行为，并确保故障封闭特性。我们形式化定义了PCN协议，并证明其具备可靠性、诚实令牌下的完备性、故障封闭特性以及策略优化下的单调性。PCN具有轻量化和模型无关特性，可无缝集成至现有应用系统，并能通过密码学承诺进行功能扩展。通过将验证作为数值显示前的强制步骤，PCN为数值敏感场景建立了简明契约：唯有通过验证才能获得信任，而缺乏验证标识则传递不确定性信息。"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们推出了一款名为\"智能快速投放\"(Smart Fast Finish, SFF)的预算调控功能。该功能基于行业标准的快速投放(FF)技术进行优化——传统FF系统会在固定投放周期临近结束时，以最快速度耗尽剩余广告预算。SFF通过分析历史广告活动数据，动态调整系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分割实验数据和离线模拟结果表明，SFF在预算调控过程中能有效缓解超量投放问题，是一种稳健的解决方案。\n\n（注：根据技术文档翻译规范，对以下术语作了标准化处理：\n1. \"budget pacing\"译为\"预算调控\"而非字面意义的\"预算步调\"\n2. \"overdelivery mitigation\"译为\"缓解超量投放\"符合广告技术领域表述\n3. \"throttle rate\"译为\"调控速率\"准确体现系统参数特性\n4. 保留DoorDash等专有名词原文，符合技术文献惯例）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关系，从而阻碍战略决策。为突破这些局限，我们提出KLIPA框架——一种结合知识图谱与大语言模型（LLM）的创新专利分析系统。该框架集成三大核心组件：用于构建专利间显性关系图谱的结构化知识库，揭示上下文关联的检索增强生成（RAG）系统，以及能动态确定最优查询策略的智能代理。我们在真实世界专利数据库上进行验证，结果表明KLIPA在知识提取、新颖关联发现和整体运营效率方面实现显著提升。该技术组合不仅提高了检索精度，降低了对领域专家的依赖，更为科技企业、律所等知识产权管理机构提供了可扩展的自动化解决方案，助力其更好地驾驭战略创新与竞争情报的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v1",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "科学文献的结构化信息提取对于捕捉专业领域的核心概念与新兴趋势至关重要。尽管现有数据集有助于模型开发，但由于领域复杂性和科学文本标注的高成本，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专门针对自然语言处理（NLP）领域全文实体与关系抽取的基准数据集。该数据集包含60篇人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行了对比实验，并评估了前沿监督模型在该数据集上的表现。实验结果表明，现有模型对不同长度学术文本的提取能力存在差异。与现有数据集的交叉对比显示，SciNLP在某些基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点度，表明其蕴含丰富的语义拓扑信息，可有效增强下游应用。数据集已公开于https://github.com/AKADDC/SciNLP。"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）系统需应对简短模糊的用户查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配问题的核心机制，其设计范式因预训练语言模型（PLM）和大语言模型（LLM）的出现发生了显著变革。本综述从三个维度系统梳理该领域：（i）建立查询扩展的四维框架——从扩展点注入方式（显式与隐式QE），经知识 grounding 与交互机制（知识库、模型内部能力、多轮检索）和学习对齐方法，延伸至基于知识图谱的论证；（ii）提出以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释各类模型在QE中的特色能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE技术在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与实施方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域问答/RAG、会话式检索、代码搜索及跨语言场景的应用图谱。研究提炼出知识 grounding 与交互、对齐/蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束三大核心策略，作为解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系以及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下选择和组合QE技术提供了系统化蓝图。"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文本文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂的结构特征要求研究者开发超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大发展时期的方法体系、核心挑战与应用场景。我们系统化地追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层级编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，本文还审视了特定领域应用场景、专项评估资源，并指明了效率权衡、多模态对齐和结果真实性等关键开放挑战。本综述旨在为基座模型时代的长文档检索研究既提供 consolidated 参考框架，又提出前瞻性的发展议程。"
    }
]