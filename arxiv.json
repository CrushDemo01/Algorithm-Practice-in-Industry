[
    {
        "title": "Soundtracks of Our Lives: How Age Influences Musical Preferences",
        "url": "http://arxiv.org/abs/2509.08337v1",
        "pub_date": "2025-09-10",
        "summary": "The majority of research in recommender systems, be it algorithmic improvements, context-awareness, explainability, or other areas, evaluates these systems on datasets that capture user interaction over a relatively limited time span. However, recommender systems can very well be used continuously for extended time. Similarly so, user behavior may evolve over that extended time. Although media studies and psychology offer a wealth of research on the evolution of user preferences and behavior as individuals age, there has been scant research in this regard within the realm of user modeling and recommender systems. In this study, we investigate the evolution of user preferences and behavior using the LFM-2b dataset, which, to our knowledge, is the only dataset that encompasses a sufficiently extensive time frame to permit real longitudinal studies and includes age information about its users. We identify specific usage and taste preferences directly related to the age of the user, i.e., while younger users tend to listen broadly to contemporary popular music, older users have more elaborate and personalized listening habits. The findings yield important insights that open new directions for research in recommender systems, providing guidance for future efforts.",
        "translated": "当前推荐系统领域的大多数研究——无论是算法改进、上下文感知、可解释性还是其他方向——均基于有限时间跨度的用户交互数据集进行评估。然而，推荐系统实际往往需要长期持续运行，用户行为也可能随时间推移产生演变。尽管媒体研究和心理学领域对用户偏好随年龄演变的规律已有丰富成果，但在用户建模与推荐系统领域中，针对这一问题的研究仍十分匮乏。本研究采用LFM-2b数据集（据我们所知，这是唯一兼具足够长时间跨度支持真实纵向研究、且包含用户年龄信息的数据集）深入探究用户偏好与行为的演变规律。我们发现：年轻用户倾向于广泛收听当代流行音乐，而年长用户则展现出更精细化、个性化的收听习惯——这种使用偏好与品味特征与用户年龄存在直接关联。这些发现为推荐系统研究开辟了新方向，为后续研究提供了重要指导。"
    },
    {
        "title": "Vector embedding of multi-modal texts: a tool for discovery?",
        "url": "http://arxiv.org/abs/2509.08216v1",
        "pub_date": "2025-09-10",
        "summary": "Computer science texts are particularly rich in both narrative content and illustrative charts, algorithms, images, annotated diagrams, etc. This study explores the extent to which vector-based multimodal retrieval, powered by vision-language models (VLMs), can improve discovery across multi-modal (text and images) content. Using over 3,600 digitized textbook pages largely from computer science textbooks and a Vision Language Model (VLM), we generate multi-vector representations capturing both textual and visual semantics. These embeddings are stored in a vector database. We issue a benchmark of 75 natural language queries and compare retrieval performance to ground truth and across four similarity (distance) measures. The study is intended to expose both the strengths and weakenesses of such an approach. We find that cosine similarity most effectively retrieves semantically and visually relevant pages. We further discuss the practicality of using a vector database and multi-modal embedding for operational information retrieval. Our paper is intended to offer design insights for discovery over digital libraries.   Keywords: Vector embedding, multi-modal document retrieval, vector database benchmark, digital library discovery",
        "translated": "计算机科学文献通常兼具丰富的叙述性内容与说明性图表、算法、图像及带标注的图示等。本研究探讨基于视觉语言模型（VLM）的向量多模态检索在提升跨文本与图像的多模态内容发现能力方面的效果。通过使用超过3,600页主要来自计算机科学教材的数字化页面及视觉语言模型，我们生成了能同时捕获文本与视觉语义的多向量表征。这些嵌入向量被存储于向量数据库中。我们发布了包含75个自然语言查询的基准测试集，并将检索效果与人工标注真值进行对比，同时评估了四种相似度（距离）度量方法的性能。本研究旨在揭示此类方法的优势与局限性。研究发现，余弦相似度能够最有效地检索出语义和视觉相关性较高的页面。我们进一步讨论了使用向量数据库与多模态嵌入在实际信息检索操作中的可行性。本文旨在为数字图书馆的文献发现机制提供设计参考。  \n关键词：向量嵌入，多模态文档检索，向量数据库基准测试，数字图书馆发现"
    },
    {
        "title": "Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at\n  DoorDash",
        "url": "http://arxiv.org/abs/2509.07929v1",
        "pub_date": "2025-09-09",
        "summary": "We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget.",
        "translated": "我们提出了一种名为\"智能快速完成\"（Smart Fast Finish, SFF）的预算调控功能。该功能基于行业标准的快速完成（Fast Finish, FF）技术进行优化——在固定时间段末期将剩余广告预算以最快速度消耗完毕。SFF通过分析历史广告活动数据，动态更新系统参数（包括启动时间和调控速率）。目前该功能已应用于美国最大配送平台之一DoorDash的预算调控系统。在线预算分流实验数据和离线仿真结果表明，SFF是一种能有效缓解预算调控过程中超量投放问题的稳健解决方案。\n\n（注：根据学术论文摘要的规范要求，译文采用以下处理：\n1. 专业术语统一：\"budget pacing\"译为\"预算调控\"，\"overdelivery mitigation\"译为\"缓解超量投放\"\n2. 技术概念准确传达：\"dynamic updates\"译为\"动态更新\"，\"throttle rate\"译为\"调控速率\"\n3. 企业名称保留原文：\"DoorDash\"不译\n4. 研究论证表述规范：\"online/offline\"译为\"在线/离线\"，\"robust solution\"译为\"稳健解决方案\"\n5. 保持学术文本的客观性，避免口语化表达）"
    },
    {
        "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
        "url": "http://arxiv.org/abs/2509.07860v1",
        "pub_date": "2025-09-09",
        "summary": "Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.",
        "translated": "有效管理知识产权是一项重大挑战。传统的专利分析方法依赖于劳动密集型的人工检索和僵化的关键词匹配。这些方法往往效率低下，难以揭示海量专利数据中隐藏的复杂关联，从而阻碍战略决策。为突破这些局限，我们提出KLIPA这一创新框架，该框架通过结合知识图谱和大语言模型（LLM）显著推进专利分析能力。我们的方法整合了三个核心组件：用于构建专利间显性关联的结构化知识图谱、用于挖掘上下文联系的检索增强生成（RAG）系统，以及能动态确定最优解策策略的智能代理。我们在真实世界的全量专利数据库上验证了KLIPA，其在知识提取、新颖关联发现和整体操作效率方面均展现出显著提升。这种技术融合增强了检索精度，降低了对领域专家的依赖，为包括科技企业和律师事务所在内的知识产权管理机构提供了可扩展的自动化解决方案，使其能更好地应对战略创新与竞争情报领域的复杂性。"
    },
    {
        "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and\n  Relation Extraction in NLP",
        "url": "http://arxiv.org/abs/2509.07801v2",
        "pub_date": "2025-09-09",
        "summary": "Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.",
        "translated": "从科学文献中抽取结构化信息对于捕捉专业领域的核心概念与新兴趋势至关重要。虽然现有数据集有助于模型开发，但由于领域复杂性和科学文本标注成本高昂，多数数据集仅聚焦特定章节。为突破这一局限，我们推出SciNLP——专为自然语言处理（NLP）领域设计的全文实体与关系抽取基准数据集。该数据集包含60篇经人工标注的NLP领域全文文献，涵盖7,072个实体和1,826组关系。与现有研究相比，SciNLP是首个提供NLP领域全文级实体及其关系标注的数据集。为验证SciNLP的有效性，我们与同类数据集进行对比实验，并评估了前沿监督模型在该数据集上的表现。结果显示现有模型对不同长度学术文本的抽取能力存在显著差异。与现有数据集的交叉对比表明，SciNLP在部分基线模型上实现了显著性能提升。基于SciNLP训练的模型，我们实现了NLP领域细粒度知识图谱的自动构建。该知识图谱平均每个实体拥有3.2个节点连接度，展现出丰富的语义拓扑信息，可有效增强下游应用性能。数据集已公开于https://github.com/AKADDC/SciNLP。\n\n（注：译文严格遵循学术论文表述规范，对\"entity and relation extraction\"采用\"实体与关系抽取\"标准译法，\"knowledge graph\"译为\"知识图谱\"，\"node degree\"译为\"节点连接度\"等专业术语均符合计算机领域中文表达惯例。长难句如\"Results reveal varying...\"通过拆分重组转化为符合中文表达习惯的短句，同时保持逻辑严谨性。）"
    },
    {
        "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
        "url": "http://arxiv.org/abs/2509.07794v1",
        "pub_date": "2025-09-09",
        "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key mechanism for mitigating vocabulary mismatch, but the design space has shifted markedly with pre-trained language models (PLMs) and large language models (LLMs). This survey synthesizes the field from three angles: (i) a four-dimensional framework of query expansion - from the point of injection (explicit vs. implicit QE), through grounding and interaction (knowledge bases, model-internal capabilities, multi-turn retrieval) and learning alignment, to knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning encoder-only, encoder-decoder, decoder-only, instruction-tuned, and domain/multilingual variants, highlighting their characteristic affordances for QE (contextual disambiguation, controllable generation, zero-/few-shot reasoning); and (iii) practice-oriented guidance on where and how neural QE helps in first-stage retrieval, multi-query fusion, re-ranking, and retrieval-augmented generation (RAG). We compare traditional query expansion with PLM/LLM-based methods across seven key aspects, and we map applications across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational and code search, and cross-lingual settings. The review distills design grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG constraints - as robust remedies to topic drift and hallucination. We conclude with an agenda on quality control, cost-aware invocation, domain/temporal adaptation, evaluation beyond end-task metrics, and fairness/privacy. Collectively, these insights provide a principled blueprint for selecting and combining QE techniques under real-world constraints.",
        "translated": "现代信息检索（IR）需要弥合简短模糊的查询与日益多样化、快速演变的语料库之间的鸿沟。查询扩展（QE）作为缓解词汇失配的关键机制，其设计范式已因预训练语言模型（PLM）和大语言模型（LLM）发生显著转变。本综述从三个维度系统梳理该领域：（i）提出查询扩展的四维框架——从注入方式（显式/隐式QE）出发，贯穿 grounding 与交互机制（知识库、模型内部能力、多轮检索）、学习对齐策略，直至基于知识图谱的论证；（ii）建立以模型为核心的分类体系，涵盖仅编码器、编码器-解码器、仅解码器、指令微调及领域/多语言变体，重点阐释其特有的QE能力（上下文消歧、可控生成、零样本/少样本推理）；（iii）提供实践导向的指南，说明神经QE在首阶段检索、多查询融合、重排序及检索增强生成（RAG）中的适用场景与方法。通过七个关键维度对比传统QE与基于PLM/LLM的方法，并绘制其在网络搜索、生物医学、电子商务、开放域QA/RAG、会话式搜索、代码检索及跨语言场景的应用图谱。研究提炼出三大核心设计原则：基于 grounding 的交互机制、对齐与蒸馏技术（SFT/PEFT/DPO）以及知识图谱约束——这些被证明是解决主题漂移和幻觉问题的有效方案。最后提出质量控制、成本感知调用、领域/时序适应性、超越终端任务指标的评估体系及公平性/隐私保护等未来研究方向。这些见解共同为实际约束条件下QE技术的选择与组合提供了系统化蓝图。\n\n（注：术语处理说明：\n- grounding 保留英文，因中文尚无统一译法且该术语在AI领域常直接使用\n- SFT/PEFT/DPO 为技术缩写（全称：Supervised Fine-Tuning/Parameter-Efficient Fine-Tuning/Direct Preference Optimization）\n- QA/RAG 等缩写已在领域内广泛采用\n- 保持\"零样本/少样本\"等标准译法以符合技术文献惯例）"
    },
    {
        "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era",
        "url": "http://arxiv.org/abs/2509.07759v1",
        "pub_date": "2025-09-09",
        "summary": "The proliferation of long-form documents presents a fundamental challenge to information retrieval (IR), as their length, dispersed evidence, and complex structures demand specialized methods beyond standard passage-level techniques. This survey provides the first comprehensive treatment of long-document retrieval (LDR), consolidating methods, challenges, and applications across three major eras. We systematize the evolution from classical lexical and early neural models to modern pre-trained (PLM) and large language models (LLMs), covering key paradigms like passage aggregation, hierarchical encoding, efficient attention, and the latest LLM-driven re-ranking and retrieval techniques. Beyond the models, we review domain-specific applications, specialized evaluation resources, and outline critical open challenges such as efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims to provide both a consolidated reference and a forward-looking agenda for advancing long-document retrieval in the era of foundation models.",
        "translated": "长文档的激增对信息检索（IR）领域提出了根本性挑战——其篇幅长度、分散的证据分布以及复杂结构要求采用超越标准段落级技术的专门方法。本综述首次对长文档检索（LDR）领域进行系统性梳理，整合了三大技术演进阶段的方法体系、核心挑战与应用实践。我们系统化追溯了从经典词法模型、早期神经模型到现代预训练模型（PLM）及大语言模型（LLMs）的技术演进，涵盖段落聚合、层次化编码、高效注意力机制等关键范式，以及最新LLM驱动的重排序与检索技术。除模型架构外，我们还审视了特定领域应用场景、专项评估资源，并指出效率权衡、多模态对齐和结果可信度等关键开放挑战。本综述旨在为基础模型时代的长文档检索研究提供系统化参考框架与前瞻性发展路线图。"
    },
    {
        "title": "Towards End-to-End Model-Agnostic Explanations for RAG Systems",
        "url": "http://arxiv.org/abs/2509.07620v1",
        "pub_date": "2025-09-09",
        "summary": "Retrieval Augmented Generation (RAG) systems, despite their growing popularity for enhancing model response reliability, often struggle with trustworthiness and explainability. In this work, we present a novel, holistic, model-agnostic, post-hoc explanation framework leveraging perturbation-based techniques to explain the retrieval and generation processes in a RAG system. We propose different strategies to evaluate these explanations and discuss the sufficiency of model-agnostic explanations in RAG systems. With this work, we further aim to catalyze a collaborative effort to build reliable and explainable RAG systems.",
        "translated": "尽管检索增强生成（RAG）系统在提升模型响应可靠性方面日益普及，但其可信度与可解释性仍面临挑战。本研究提出了一种新颖的、整体性的、模型无关的事后解释框架，该框架基于扰动技术来解释RAG系统中的检索与生成过程。我们提出了多种策略来评估这些解释的有效性，并探讨了模型无关解释在RAG系统中的充分性。通过此项研究，我们旨在推动学界与业界共同努力，构建更可靠、更可解释的RAG系统。\n\n（注：译文严格遵循了以下技术细节处理：\n1. \"post-hoc explanation\"译为\"事后解释\"（而非\"事后诸葛亮式解释\"），符合机器学习可解释性领域的术语规范\n2. \"perturbation-based techniques\"译为\"扰动技术\"，准确反映通过对输入进行微小扰动来评估模型敏感度的技术本质\n3. \"model-agnostic\"统一译为\"模型无关\"，保持与机器学习领域术语的一致性\n4. 使用\"可解释性\"而非\"解释性\"，符合人工智能透明度研究领域的标准译法\n5. 保留RAG、LLM等专业术语的英文缩写形式，确保学术严谨性）"
    },
    {
        "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2509.07594v1",
        "pub_date": "2025-09-09",
        "summary": "Click-through rate (CTR) prediction plays an important role in online advertising systems. On the one hand, traditional CTR prediction models capture the collaborative signals in tabular data via feature interaction modeling, but they lose semantics in text. On the other hand, Large Language Models (LLMs) excel in understanding the context and meaning behind text, but they face challenges in capturing collaborative signals and they have long inference latency. In this paper, we aim to leverage the benefits of both types of models and pursue collaboration, semantics and efficiency. We present ELEC, which is an Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for the CTR prediction task. In order to leverage the ability of the LLM but simultaneously keep efficiency, we utilize the pseudo-siamese network which contains a gain network and a vanilla network. We inject the high-level representation vector generated by the LLM into a collaborative CTR model to form the gain network such that it can take advantage of both tabular modeling and textual modeling. However, its reliance on the LLM limits its efficiency. We then distill the knowledge from the gain network to the vanilla network on both the score level and the representation level, such that the vanilla network takes only tabular data as input, but can still generate comparable performance as the gain network. Our approach is model-agnostic. It allows for the integration with various existing LLMs and collaborative CTR models. Experiments on real-world datasets demonstrate the effectiveness and efficiency of ELEC for CTR prediction.",
        "translated": "点击率（CTT）预测在在线广告系统中具有重要作用。一方面，传统CTR预测模型通过特征交互建模捕捉表格数据中的协同信号，但会丢失文本语义信息；另一方面，大语言模型（LLM）擅长理解文本背后的上下文和语义，但在捕捉协同信号方面存在局限且推理延迟较高。本文旨在融合两类模型的优势，实现协同性、语义理解与效率的平衡。我们提出ELEC框架——一种高效的大语言模型赋能CTR预测方案。首先针对CTR预测任务对大语言模型进行适配，为兼顾模型能力与效率，采用包含增益网络和基准网络的双伪孪生网络结构。通过将LLM生成的高层表征向量注入协同CTR模型形成增益网络，使其能同时利用表格建模和文本建模的优势。但该网络对LLM的依赖会影响效率，因此我们通过分数级和表征级蒸馏将增益网络的知识迁移至仅需输入表格数据的基准网络，使其在保持高效的同时达到与增益网络相当的性能。本方法具备模型无关性，可与多种现有LLM及协同CTR模型集成。真实场景数据集上的实验验证了ELEC在CTR预测中的有效性与高效性。"
    },
    {
        "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of\n  Scientific Documents",
        "url": "http://arxiv.org/abs/2509.07531v1",
        "pub_date": "2025-09-09",
        "summary": "Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.",
        "translated": "科学文献表示学习为各类任务提供了强大的嵌入向量，但现有方法面临三大挑战：1）基于引文结构信号的对比训练未能充分利用引文信息，且仍生成单一向量表示；2）细粒度表示学习虽能生成句子或方面级的多向量表示，但需要昂贵的人工整合且缺乏领域泛化能力；3）任务感知学习依赖人工预定义的任务分类，忽略了细微的任务差异，且需为特定任务模块提供额外训练数据。针对这些问题，我们提出统一三种范式的新方法FLeW以获取更优表示。具体而言，我们设计了一种新型三元组采样方法，通过引文意图（背景、方法、结果）和引用频次增强引文结构信号训练——引文意图与科学写作通用结构相契合，可为细粒度表示学习提供领域泛化的方面划分。随后采用简易权重搜索机制，无需任务感知微调即可自适应整合三个方面级嵌入形成任务适配的文档表示。实验表明，相较于现有模型，FLeW在多个科学任务和学科领域均展现出卓越的适用性与鲁棒性。"
    },
    {
        "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition\n  using Demonstration Retrieval",
        "url": "http://arxiv.org/abs/2509.07512v1",
        "pub_date": "2025-09-09",
        "summary": "Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\\%-10\\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.",
        "translated": "在化学与材料科学等自然科学的当代数据驱动研究中，大规模高性能的实体识别已成为科学数据集处理的关键需求。大型语言模型（LLMs）正被日益广泛地应用于实体识别任务，这一趋势也体现在全谱系自然语言处理任务中。当前主流的实体识别大模型依赖于微调技术，但微调过程往往伴随着高昂成本。为实现性能与成本的最优平衡，我们提出ALLabel——一个三阶段框架，通过三种主动学习策略的序列化应用，从标注数据中筛选信息量最大且最具代表性的样本，用于构建大模型上下文学习所需的真实标注检索库。在三个专业领域数据集上，ALLabel在相同标注预算下持续超越所有基线模型。实验结果表明：使用ALLabel仅需标注5%-10%的数据量即可达到全量标注方法的性能水平。进一步的解析与消融研究验证了该方案的有效性和泛化能力。\n\n（注：专业术语说明：\n1. entity recognition：实体识别\n2. large language models (LLMs)：大型语言模型\n3. fine-tuned technology：微调技术\n4. active learning strategies：主动学习策略\n5. in-context learning：上下文学习\n6. ablation studies：消融研究\n7. annotation budget：标注预算\n8. ground-truth retrieval corpus：真实标注检索库）"
    },
    {
        "title": "Multi-view-guided Passage Reranking with Large Language Models",
        "url": "http://arxiv.org/abs/2509.07485v1",
        "pub_date": "2025-09-09",
        "summary": "Recent advances in large language models (LLMs) have shown impressive performance in passage reranking tasks. Despite their success, LLM-based methods still face challenges in efficiency and sensitivity to external biases. (1) Existing models rely mostly on autoregressive generation and sliding window strategies to rank passages, which incur heavy computational overhead as the number of passages increases. (2) External biases, such as position or selection bias, hinder the model's ability to accurately represent passages and increase input-order sensitivity. To address these limitations, we introduce a novel passage reranking model, called Multi-View-guided Passage Reranking (MVP). MVP is a non-generative LLM-based reranking method that encodes query-passage information into diverse view embeddings without being influenced by external biases. For each view, it combines query-aware passage embeddings to produce a distinct anchor vector, which is then used to directly compute relevance scores in a single decoding step. In addition, it employs an orthogonal loss to make the views more distinctive. Extensive experiments demonstrate that MVP, with just 220M parameters, matches the performance of much larger 7B-scale fine-tuned models while achieving a 100x reduction in inference latency. Notably, the 3B-parameter variant of MVP achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks. The source code is available at: https://github.com/bulbna/MVP",
        "translated": "近年来，大语言模型（LLM）在段落重排序任务中展现出卓越性能。尽管成效显著，基于LLM的方法仍面临效率问题与外部偏差敏感性两大挑战：（1）现有模型主要依赖自回归生成和滑动窗口策略进行段落排序，随着段落数量增加会产生巨大计算开销；（2）位置偏差和选择偏差等外部因素会干扰模型对段落的准确表征，并增强对输入顺序的敏感性。为解决这些局限性，我们提出了一种新型段落重排序模型——多视角引导段落重排序（MVP）。该非生成式LLM重排序方法通过将查询-段落信息编码为多视角嵌入向量，有效规避外部偏差影响。针对每个视角，模型融合查询感知的段落嵌入生成独特锚点向量，进而通过单步解码直接计算相关性得分。此外，该方法采用正交损失函数以增强视角区分度。大量实验表明，仅需2.2亿参数的MVP模型在实现推理延迟降低100倍的同时，性能可媲美70亿参数规模的精调模型。特别值得注意的是，30亿参数版本的MVP在领域内和领域外基准测试中均达到了最先进的性能水平。源代码已开源：https://github.com/bulbna/MVP\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语统一处理：\"autoregressive generation\"译为\"自回归生成\"，\"orthogonal loss\"译为\"正交损失函数\"\n2. 数量单位规范：\"220M/7B\"转换为\"2.2亿/70亿\"符合中文计量习惯\n3. 技术概念准确传达：\"view embeddings\"意译为\"视角嵌入向量\"而非字面直译\n4. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n5. 被动语态转化：\"are used to\"转换为主动语态\"通过...实现\"\n6. 学术表达规范：\"state-of-the-art\"译为\"最先进的\"符合学术中文惯例）"
    },
    {
        "title": "MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for\n  Neural Recommendation Models",
        "url": "http://arxiv.org/abs/2509.07319v1",
        "pub_date": "2025-09-09",
        "summary": "Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance.",
        "translated": "神经协同过滤模型在推荐系统中应用广泛，但通常基于静态设定进行训练，假设数据分布固定不变。这限制了其在用户偏好动态演变环境中的适用性。增量学习虽提供了可行方案，但来自计算机视觉或自然语言处理领域的传统方法因数据稀疏性和任务范式差异，在推荐任务中面临挑战。现有神经推荐器的增量学习方法仍存在局限，且普遍缺乏泛化能力。为此，我们提出MEGG（基于极端GG分数的回放样本）——一种基于经验回放的增量学习框架。MEGG创新性地引入GGscore指标，通过量化样本影响力来实现高效选择性样本回放，从而有效缓解灾难性遗忘问题。该框架具备模型无关特性，可无缝集成至不同架构与框架。在三种神经模型和四个基准数据集上的实验表明，其性能显著优于现有最优基线方法，并展现出强大的可扩展性、高效性和鲁棒性。代码实现将在论文录用后开源发布。\n\n（注：GGscore保留英文大写形式，符合技术术语惯例；\"catastrophic forgetting\"译为专业术语\"灾难性遗忘\"；\"model-agnostic\"采用通用译法\"模型无关\"；\"state-of-the-art\"译为\"现有最优\"符合学术语境）"
    },
    {
        "title": "Datasets for Navigating Sensitive Topics in Recommendation Systems",
        "url": "http://arxiv.org/abs/2509.07269v1",
        "pub_date": "2025-09-08",
        "summary": "Personalized AI systems, from recommendation systems to chatbots, are a prevalent method for distributing content to users based on their learned preferences. However, there is growing concern about the adverse effects of these systems, including their potential tendency to expose users to sensitive or harmful material, negatively impacting overall well-being. To address this concern quantitatively, it is necessary to create datasets with relevant sensitivity labels for content, enabling researchers to evaluate personalized systems beyond mere engagement metrics. To this end, we introduce two novel datasets that include a taxonomy of sensitivity labels alongside user-content ratings: one that integrates MovieLens rating data with content warnings from the Does the Dog Die? community ratings website, and another that combines fan-fiction interaction data and user-generated warnings from Archive of Our Own.",
        "translated": "个性化人工智能系统（从推荐系统到聊天机器人）已成为根据用户学习偏好分发内容的普遍方式。然而，人们日益关注这些系统的负面影响，特别是其可能使用户接触敏感或有害内容，从而对整体福祉产生负面影响的倾向。为量化评估这一问题，需要构建带有内容敏感度标签的数据集，使研究者能够超越简单的参与度指标来评估个性化系统。为此，我们引入了两个新型数据集：一个将MovieLens评分数据与Does the Dog Die?社区评级网站的内容警示标签体系相结合，另一个整合了Archive of Our Own平台的同人小说互动数据与用户生成的警示标签。这些数据集不仅包含用户-内容评分，还提供了系统化的敏感度分类标注。"
    },
    {
        "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
        "url": "http://arxiv.org/abs/2509.07253v1",
        "pub_date": "2025-09-08",
        "summary": "Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.",
        "translated": "大型语言模型（LLM）是处理文本任务的卓越多功能工具，其催生了无数前所未有的应用场景。相比之下，检索模型领域尚未出现具备同等通用能力的模型。要实现这一目标，检索模型必须能够处理复杂检索任务——即查询语句包含多组成部分、约束条件或自然语言需求的场景。这类任务代表着对现有主流评估集中普遍采用的简单单维度查询的自然演进。随着用户期望搜索系统能处理更具体且更具挑战性的信息请求（正如基于LLM的信息系统的使用方式所展现的），复杂查询需求应运而生。\n\n尽管业界对检索模型拓展复杂检索能力的呼声日益高涨，但目前缺乏能够全面评估检索模型在多样化复杂任务上表现的标准资源。现有少数评估资源不仅覆盖范围有限，且往往缺乏真实场景设置，导致难以准确衡量检索模型在现实复杂检索任务中的真实能力。\n\n为弥补这一缺陷并推动下一代检索模型的发展，我们构建了具有多样性和真实性的复杂检索任务集，并对代表性前沿检索模型进行基准测试。此外，我们还探究了基于LLM的查询扩展与重写技术对检索质量的影响。实验结果表明：即使最优模型在复杂检索任务中也表现挣扎，所有任务的平均nDCG@10最高仅达0.346，R@100最高仅为0.587。虽然LLM增强技术能提升较弱模型的性能，但所有重写技术都会导致最强模型的全指标性能下降。"
    },
    {
        "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning\n  Intensive Retrieval",
        "url": "http://arxiv.org/abs/2509.07163v1",
        "pub_date": "2025-09-08",
        "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations: they are constrained by the initial retrieval quality of the top-k documents, and the growing computational demands of LLM-based rerankers restrict the number of documents that can be effectively processed. We introduce Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations by directly retrieving documents according to reranker preferences rather than following the traditional sequential reranking method. Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity. Experimental results demonstrate substantial performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9 on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100 documents. Our analysis suggests that, given a fixed pair of embedding and reranker models, strategically selecting documents to rerank can significantly improve retrieval accuracy under limited reranker budget.",
        "translated": "当前广泛使用的“检索-重排序”流程面临两个关键局限：其性能受限于前k篇文档的初始检索质量，且基于大语言模型的重排序器计算需求日益增长，限制了可有效处理的文档数量。我们提出了一种创新方法——重排序器引导搜索（Reranker-Guided-Search, RGS），通过直接根据重排序器的偏好检索文档（而非遵循传统的顺序重排序流程）来突破这些限制。该方法在近似最近邻算法生成的邻近图上进行贪婪搜索，基于文档相似性策略性地优先选择有潜力的文档进行重排序。实验结果表明，在多个基准测试中均取得显著性能提升：BRIGHT数据集提升3.5个点，FollowIR提升2.9个点，M-BEIR提升5.1个点——且所有这些改进均在100篇文档的重排序计算预算内实现。我们的分析表明，在固定嵌入模型和重排序器组合的前提下，通过策略性选择待重排序文档，能够在有限计算资源下显著提升检索精度。\n\n（注：专业术语说明：\n1. retrieve-and-rerank pipeline：译为\"检索-重排序流程\"\n2. LLM-based rerankers：译为\"基于大语言模型的重排序器\"\n3. proximity graphs：译为\"邻近图\"\n4. approximate nearest neighbor：译为\"近似最近邻\"\n5. constrained reranker budget：译为\"受限的重排序计算预算\"\n6. embedding model：译为\"嵌入模型\"\n所有技术概念均采用计算机信息检索领域标准译法，确保学术准确性。）"
    },
    {
        "title": "Avoiding Over-Personalization with Rule-Guided Knowledge Graph\n  Adaptation for LLM Recommendations",
        "url": "http://arxiv.org/abs/2509.07133v1",
        "pub_date": "2025-09-08",
        "summary": "We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods.",
        "translated": "我们提出了一种轻量级神经符号框架，通过推理时自适应调整用户侧知识图谱（KG）来缓解基于大语言模型的推荐系统中的过度个性化问题。与传统重训练模型或依赖不透明启发式方法不同，我们的方法通过重构用户个性化知识图谱（PKG），抑制那些强化个性化信息环境（PIE）的特征共现模式——即算法导致的限制内容多样性的信息茧房。调整后的PKG用于构建结构化提示，引导语言模型在保持主题相关性的同时生成更多样化的\"非PIE\"推荐。我们提出了一系列符号化适配策略，包括软重加权、硬反转和针对性移除偏见三元组，以及客户端学习算法以优化每用户的策略应用。在食谱推荐基准测试中，个性化PKG适配在保持推荐质量的同时显著提升内容新颖度，其效果优于全局适配和基于朴素提示的方法。\n\n（注：专业术语说明：\n1. Personalized Information Environments (PIEs) 译为\"个性化信息环境\"，特指算法导致的信息茧房效应\n2. Out-PIE recommendations 译为\"非PIE推荐\"，指突破信息茧房的推荐内容\n3. soft reweighting/hard inversion 分别译为\"软重加权/硬反转\"，保持机器学习领域的术语惯例\n4. client-side learning algorithm 译为\"客户端学习算法\"，强调分布式计算场景下的本地化特性）"
    },
    {
        "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
        "url": "http://arxiv.org/abs/2509.06888v1",
        "pub_date": "2025-09-08",
        "summary": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.",
        "translated": "编码器专用语言模型（Encoder-only language models）常被用于各类标准机器学习任务，包括分类与检索。然而当前针对编码器模型的研究，特别是多语言模型领域的研究仍显不足。我们提出了mmBERT——一种基于1800多种语言、3万亿多语种文本训练的纯编码器语言模型。在构建mmBERT过程中，我们引入了多项创新要素，包括逆向掩码比率调度机制和逆向温度采样策略。我们创新性地仅在训练衰减阶段加入1700余种低资源语言数据，实验表明这一方法显著提升模型性能，并最大限度利用了有限训练数据带来的增益。尽管这些低资源语言仅出现在短暂的衰减阶段，我们的模型在分类任务上达到了与OpenAI o3、Google Gemini 2.5 Pro等模型相当的性能。总体而言，mmBERT在高资源与低资源语言的分类和检索任务上均显著超越前代模型。"
    },
    {
        "title": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture",
        "url": "http://arxiv.org/abs/2509.06887v2",
        "pub_date": "2025-09-08",
        "summary": "Modern search systems play a crucial role in facilitating information acquisition. Traditional search engines typically rely on a cascaded architecture, where results are retrieved through recall, pre-ranking, and ranking stages. The complexity of designing and maintaining multiple modules makes it difficult to achieve holistic performance gains. Recent advances in generative recommendation have motivated the exploration of unified generative search as an alternative. However, existing approaches are not genuinely end-to-end: they typically train an item encoder to tokenize candidates first and then optimize a generator separately, leading to objective inconsistency and limited generalization. To address these limitations, we propose UniSearch, a unified generative search framework for Kuaishou Search. UniSearch replaces the cascaded pipeline with an end-to-end architecture that integrates a Search Generator and a Video Encoder. The Generator produces semantic identifiers of relevant items given a user query, while the Video Encoder learns latent item embeddings and provides their tokenized representations. A unified training framework jointly optimizes both components, enabling mutual enhancement and improving representation quality and generation accuracy. Furthermore, we introduce Search Preference Optimization (SPO), which leverages a reward model and real user feedback to better align generation with user preferences. Extensive experiments on industrial-scale datasets, together with online A/B testing in both short-video and live search scenarios, demonstrate the strong effectiveness and deployment potential of UniSearch. Notably, its deployment in live search yields the largest single-experiment improvement in recent years of our product's history, highlighting its practical value for real-world applications.",
        "translated": "现代搜索系统在促进信息获取方面发挥着关键作用。传统搜索引擎通常采用级联架构，通过召回、粗排和精排三个阶段获取结果。由于需要设计和维护多个模块，这种架构难以实现整体性能提升。生成式推荐的最新进展推动了统一生成式搜索的探索，但现有方法并非真正的端到端系统：它们通常先训练物品编码器对候选项目进行标记化，再单独优化生成器，导致目标不一致和泛化能力有限。\n\n为解决这些局限性，我们提出UniSearch——面向快手搜索的统一生成式搜索框架。该框架采用端到端架构替代级联流水线，集成搜索生成器与视频编码器。生成器根据用户查询生成相关项目的语义标识，视频编码器则学习潜在物品嵌入并提供其标记化表示。通过统一训练框架联合优化两个组件，实现相互增强并提升表示质量与生成准确性。此外，我们引入搜索偏好优化（SPO）技术，利用奖励模型和真实用户反馈使生成结果更贴合用户偏好。\n\n基于工业级数据集的大量实验，以及在短视频和直播搜索场景中的在线A/B测试，证明了UniSearch的强大有效性和部署潜力。值得注意的是，该框架在直播搜索场景的部署实现了我们产品近年来最大幅度的单次实验效果提升，凸显了其在实际应用中的重要价值。"
    },
    {
        "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction",
        "url": "http://arxiv.org/abs/2509.06883v1",
        "pub_date": "2025-09-08",
        "summary": "We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.",
        "translated": "我们参与了CheckThat!任务2的英文赛道，重点探索了多种提示方法与上下文学习策略，包括小样本提示（few-shot prompting）以及基于不同大语言模型（LLM）家族的微调方法，旨在从社交媒体文本中提取值得核查的声明。实验表明，通过对FLAN-T5模型进行微调，我们取得了最佳的METEOR评分。然而，我们注意到在某些情况下，即使其他方法的METEOR分数较低，仍能提取出质量更高的声明内容。"
    },
    {
        "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable\n  Physical Simulations from Monocular Video",
        "url": "http://arxiv.org/abs/2509.08828v1",
        "pub_date": "2025-09-10",
        "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
        "translated": "三维动态场景重建是计算机视觉领域中一个基础但具有挑战性的任务。本文提出了一种创新方法，将三维几何重建与基于物理渲染的外观估计相结合，开发出一个仅需单目RGB视频序列即可实现织物三维重建与外观估计的双任务系统。为获得逼真的高质量形变与渲染效果，我们采用布料物理仿真与可微分渲染技术。针对单目视频中的深度模糊问题，本文引入了两个新颖的正则化项以提升三维重建的合理性。与领域内最新方法相比，我们的方法将三维重建误差降低了2.64倍，且单场景平均仅需30分钟运行时间。此外，优化后的运动序列质量足以支持变形物体的外观估计，从单目RGB视频中成功恢复了清晰的细节特征。\n\n（注：专业术语说明：\n- differentiable rendering: 可微分渲染\n- monocular RGB video: 单目RGB视频\n- depth ambiguity: 深度模糊\n- regularization terms: 正则化项\n- physically based rendering: 基于物理的渲染）"
    },
    {
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "url": "http://arxiv.org/abs/2509.08826v1",
        "pub_date": "2025-09-10",
        "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "translated": "奖励模型（Reward Models, RMs）对于通过强化学习（RL）改进生成模型至关重要，但视觉生成领域的奖励模型规模化范式仍未被充分探索。这主要源于现有方法的根本性局限：基于CLIP的奖励模型受限于架构与输入模态约束，而主流的Bradley-Terry损失函数与视觉语言模型（VLMs）的下一词元预测机制存在本质错位，阻碍了有效扩展。更关键的是，RLHF优化过程长期受\"奖励破解\"（Reward Hacking）问题困扰——模型会利用奖励信号的缺陷而非真正提升生成质量。\n\n为解决这些挑战，我们提出RewardDance——一个可扩展的奖励建模框架。该框架通过创新的生成式奖励范式突破上述限制：将奖励分数重新定义为模型预测\"是\"词元的概率（即生成图像在特定标准下优于参考图像），使奖励目标与VLM架构本质对齐。这种对齐实现了两个维度的扩展：（1）模型规模：系统化将奖励模型参数量扩展至260亿；（2）上下文扩展：整合任务指令、参考示例和思维链（CoT）推理。大量实验表明，RewardDance在文本到图像、文本到视频及图像到视频生成任务上显著超越现有最优方法。\n\n尤为关键的是，我们解决了长期存在的\"奖励破解\"难题：大规模奖励模型在RL微调过程中始终展现并保持高奖励方差，证明其抗破解能力与生成多样化高质量输出的特性，极大缓解了困扰小模型的模式崩溃（mode collapse）问题。"
    },
    {
        "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts",
        "url": "http://arxiv.org/abs/2509.08818v1",
        "pub_date": "2025-09-10",
        "summary": "Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.",
        "translated": "近年来，概率生成模型的发展已从静态图像合成扩展到文本驱动视频生成。然而，其生成过程固有的随机性可能导致不可预测的伪影，例如违背物理规律的画面和时序不一致问题。解决这些挑战需要系统性基准测试，但由于视频独特的时空复杂性，现有数据集主要专注于生成式图像。为填补这一空白，我们推出了GeneVA——一个基于自然文本提示生成视频中时空伪影的大规模人工标注数据集。我们期待GeneVA能够支撑并助力关键应用，如模型性能基准测试与生成视频质量优化。"
    },
    {
        "title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching",
        "url": "http://arxiv.org/abs/2509.08805v1",
        "pub_date": "2025-09-10",
        "summary": "Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.",
        "translated": "密集图像匹配旨在为源图像中的每个像素在部分重叠的目标图像中找到对应点。现有先进方法通常采用由粗到精的匹配机制，即在每个尺度上为每个源位置生成单一对应假设。在具有挑战性的场景中——例如深度不连续区域或目标图像是源图像的强烈放大版本时——相邻源位置的对应点往往分布广泛，此时在每个尺度上为每个源位置预测单一对应假设可能导致错误匹配。本文研究了一种创新方案：在每个尺度上为每个源位置预测多个对应假设。我们采用波束搜索策略在每级尺度传播多重假设，并提出将这些多重假设集成到交叉注意力层中，由此构建出名为BEAMER的新型密集匹配架构。BEAMER能够学习跨尺度保存和传播多重假设，使其在深度不连续区域或目标图像强烈放大场景下的鲁棒性显著优于现有最优方法。"
    },
    {
        "title": "PianoVAM: A Multimodal Piano Performance Dataset",
        "url": "http://arxiv.org/abs/2509.08800v1",
        "pub_date": "2025-09-10",
        "summary": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",
        "translated": "音乐表演的多模态特性促使音乐信息检索（MIR）领域对音频之外的数据日益关注。本文推出PianoVAM——一个包含视频、音频、MIDI、手部关键点、指法标注及丰富元数据的综合性钢琴演奏数据集。该数据集通过Disklavier钢琴录制，采集了业余钢琴演奏者日常练习时的音频与MIDI数据，并在真实多样的表演环境下同步录制了俯视角视频。我们使用预训练的手部姿态估计模型和半自动化指法标注算法提取了手部关键点与指法标签。文中探讨了数据收集过程中面临的挑战以及多模态对齐的技术难点，详细介绍了基于视频手部关键点的指法标注方法。最后，我们使用PianoVAM数据集进行了纯音频与视听结合的钢琴转录基准测试，并讨论了该数据集的其他潜在应用场景。\n\n（注：根据学术规范，术语保持原文大写形式如PianoVAM/Disklavier，技术术语如MIDI/MIR等保留英文缩写，专业表述如\"手部关键点(hand landmarks)\"、\"半自动化指法标注(semi-automated fingering annotation)\"等符合计算机领域中文表达习惯。）"
    },
    {
        "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation",
        "url": "http://arxiv.org/abs/2509.08794v1",
        "pub_date": "2025-09-10",
        "summary": "Event-based cameras (EBCs) are a promising new technology for star tracking-based attitude determination, but prior studies have struggled to determine accurate ground truth for real data. We analyze the accuracy of an EBC star tracking system utilizing the Earth's motion as the ground truth for comparison. The Earth rotates in a regular way with very small irregularities which are measured to the level of milli-arcseconds. By keeping an event camera static and pointing it through a ground-based telescope at the night sky, we create a system where the only camera motion in the celestial reference frame is that induced by the Earth's rotation. The resulting event stream is processed to generate estimates of orientation which we compare to the International Earth Rotation and Reference System (IERS) measured orientation of the Earth. The event camera system is able to achieve a root mean squared across error of 18.47 arcseconds and an about error of 78.84 arcseconds. Combined with the other benefits of event cameras over framing sensors (reduced computation due to sparser data streams, higher dynamic range, lower energy consumption, faster update rates), this level of accuracy suggests the utility of event cameras for low-cost and low-latency star tracking. We provide all code and data used to generate our results: https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.",
        "translated": "基于事件相机（EBC）的星体跟踪姿态测定技术是一种新兴技术，但以往研究难以获取真实数据的精确地面真值。本研究创新性地利用地球自转作为基准真值，对EBC星体跟踪系统的精度进行量化分析。地球以高度规律的周期自转，其微小扰动可达毫角秒级测量精度。通过将事件相机固定于地面望远镜并对准夜空，我们在天球参考系中构建了仅受地球自转影响的观测系统。处理生成的事件流数据后，我们将估算的姿态方向与国际地球自转参考系（IERS）测量的地球定向进行对比。实验表明：该事件相机系统的定向估计均方根误差为18.47角秒，最大误差约78.84角秒。结合事件相机相较于帧传感器固有的优势（稀疏数据流降低计算量、更高动态范围、更低能耗、更快更新速率），这一精度水平证实了事件相机在低成本低延迟星体跟踪中的应用价值。我们已公开全部代码与数据：https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification。\n\n（注：译文严格遵循学术规范，对专业术语如\"ground truth\"译为\"地面真值\"、\"celestial reference frame\"译为\"天球参考系\"等保持准确；通过拆分长难句、调整语序（如将英文被动语态转换为中文主动表述）确保技术细节的清晰传达；保留原始数值精度及专业机构缩写IERS；完整呈现原文的技术逻辑链条与创新点。）"
    },
    {
        "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images",
        "url": "http://arxiv.org/abs/2509.08780v1",
        "pub_date": "2025-09-10",
        "summary": "Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.   Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.   Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.   Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.",
        "translated": "背景：砷中毒是南亚和东南亚地区严重的公共卫生问题，主要由长期饮用受砷污染的水导致。其早期皮肤表现具有重要临床意义但常被漏诊，尤其在缺乏皮肤科医生的农村地区。基于图像的自动化诊断方案可支持早期发现和及时干预。  \n方法：本研究提出端到端框架，通过手机拍摄的皮肤图像实现砷中毒诊断。构建包含20个类别、11,000余张砷性皮肤病及其他皮肤病症图像的数据集。对包括卷积神经网络（CNN）和基于Transformer的模型在内的多种深度学习架构进行砷中毒检测性能评估。通过LIME和Grad-CAM实现模型可解释性，并基于Web的诊断工具验证部署可行性。  \n结果：基于Transformer的模型显著优于CNN，其中Swin Transformer以86%的准确率取得最佳性能。LIME和Grad-CAM可视化证实模型聚焦于病变相关区域，增强临床透明度并辅助错误分析。该框架在外部验证样本中同样表现优异，证实其泛化能力。  \n结论：本研究框架证明了深度学习技术通过移动设备图像实现非侵入性、可普及且可解释的砷中毒诊断的潜力。通过提供可靠的图像筛查方案，该技术可在缺乏皮肤科医生的资源有限地区作为实用诊断辅助工具，支持早期发现与及时干预。\n\n（注：专业术语处理说明：  \n1. LIME (Local Interpretable Model-agnostic Explanations) 保留英文缩写  \n2. Grad-CAM (Gradient-weighted Class Activation Mapping) 保留英文缩写  \n3. Transformer/Swin Transformer 作为特定模型名称保留英文  \n4. 临床术语如\"cutaneous manifestations\"译为\"皮肤表现\"，\"lesion-relevant regions\"译为\"病变相关区域\"符合医学文献表述规范）"
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pub_date": "2025-09-10",
        "summary": "Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "translated": "多模态大语言模型（MLLMs）正日益被用于评估文本到图像（TTI）生成系统，其能够基于视觉与文本上下文提供自动化评判。然而，这些“裁判”模型常存在偏见、过度自信以及在多样化图像领域中表现不一致的问题。尽管提示集成（prompt ensembling）在单模态纯文本场景中已展现出缓解这些问题的潜力，但我们的实验表明，标准集成方法无法有效推广至TTI任务。针对这些局限性，我们提出了一种新型多模态感知方法——多模态贝叶斯提示集成混合（MMB）。该方法通过贝叶斯提示集成框架结合图像聚类技术，使裁判模型能够根据样本的视觉特征动态分配提示权重。我们证明，MMB在 pairwise 偏好判断中提升了准确性，并显著增强了校准能力，使其更易于评估模型真实的不确定性。在HPSv2和MJBench两个TTI基准测试中，MMB在人类标注对齐度和跨图像内容的校准性能上均优于现有基线方法。我们的研究结果凸显了多模态特异性策略对裁判校准的重要性，并为实现可靠的大规模TTI评估指明了一条可行路径。"
    },
    {
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "url": "http://arxiv.org/abs/2509.08764v1",
        "pub_date": "2025-09-10",
        "summary": "Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "translated": "可靠整合先验信息对于实现自验证与自更新的高精地图至关重要。然而，现有公开数据集均未包含\"先验地图-当前地图-传感器数据\"的三元组。这导致现有方法只能依赖合成先验数据，从而产生数据不一致性并引发严重的模拟到现实差异。为此，我们推出ArgoTweak——首个提供真实地图先验数据的三元组数据集。该数据集核心采用双射映射框架，将大规模地图修改分解为地图元素层级的细粒度原子级变更，确保修改过程的可解释性。这种范式转变能够在保持未变化元素高保真度的同时，实现精确的变化检测与整合。实验表明，使用ArgoTweak训练的模型相较于采用合成先验数据的方法，显著缩小了模拟到现实的性能差距。大量消融实验进一步验证了结构化先验数据与精细化变更标注的重要性。通过建立可解释的先验辅助高精地图绘制基准，ArgoTweak推动了可扩展自优化地图解决方案的发展。数据集、基线模型、地图修改工具箱及相关资源已开源：https://kth-rpl.github.io/ArgoTweak/。\n\n（注：专业术语说明：\n1. self-verifying/self-updating：自验证/自更新\n2. HD maps：高精地图（High-Definition maps）\n3. sim2real gap：模拟到现实差异（simulation-to-reality gap）\n4. bijective mapping：双射映射（数学中的一一对应关系）\n5. atomic changes：原子级变更（不可再分的最小修改单元）\n6. change detection：变化检测\n7. ablations：消融实验（ablation studies））"
    },
    {
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
        "url": "http://arxiv.org/abs/2509.08757v1",
        "pub_date": "2025-09-10",
        "summary": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .",
        "translated": "在动态化、以人为中心的环境中，机器人导航需要基于对场景的深度理解做出符合社会规范的行为决策。当前，视觉-语言模型（VLMs）展现出与社交机器人导航精细化需求高度契合的多种能力，包括目标识别、常识推理和上下文理解等。然而，这类模型是否能准确理解复杂的社交导航场景（例如推断智能体间的时空关系及人类意图）——这一实现安全合规导航的关键前提——仍存在疑问。尽管已有研究尝试将VLMs应用于社交机器人导航，但尚未有系统性工作评估其满足这些必要条件的实际能力。本文提出社交导航场景理解基准（SocialNav-SUB），这是一个基于视觉问答（VQA）任务的数据集与评测体系，专为评估VLMs在真实社交机器人导航场景中的理解能力而设计。该基准通过需要空间推理、时空推理及社会推理的VQA任务，构建了统一框架以对比VLMs与人类基线、规则基线的表现。通过对前沿VLMs的实验发现：虽然性能最优的VLM模型与人类答案的一致性概率达到鼓舞人心的水平，但其表现仍逊于简单的规则基线和人类共识基线，这表明现有VLMs在社交场景理解方面存在显著不足。本基准为社交机器人导航基础模型的后续研究奠定了基础，通过提供标准化框架推动探索如何定制VLMs以满足真实世界的社交导航需求。论文概述、代码及数据详见：https://larg.github.io/socialnav-sub\n\n（注：译文严格遵循学术论文摘要的规范表述，关键技术术语如\"Vision-Language Models (VLMs)\"译为\"视觉-语言模型\"，\"socially-compliant\"译为\"符合社会规范的\"，\"spatial-temporal relations\"译为\"时空关系\"等均采用领域内标准译法。长难句按中文习惯拆分重组，如将原文复合从句\"capabilities that align with...\"处理为独立分句\"展现出与...高度契合的多种能力\"，确保专业性与可读性平衡。）"
    },
    {
        "title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes",
        "url": "http://arxiv.org/abs/2509.08738v1",
        "pub_date": "2025-09-10",
        "summary": "This paper introduces a novel method for end-to-end crowd detection that leverages object density information to enhance existing transformer-based detectors. We present CrowdQuery (CQ), whose core component is our CQ module that predicts and subsequently embeds an object density map. The embedded density information is then systematically integrated into the decoder. Existing density map definitions typically depend on head positions or object-based spatial statistics. Our method extends these definitions to include individual bounding box dimensions. By incorporating density information into object queries, our method utilizes density-guided queries to improve detection in crowded scenes. CQ is universally applicable to both 2D and 3D detection without requiring additional data. Consequently, we are the first to design a method that effectively bridges 2D and 3D detection in crowded environments. We demonstrate the integration of CQ into both a general 2D and 3D transformer-based object detector, introducing the architectures CQ2D and CQ3D. CQ is not limited to the specific transformer models we selected. Experiments on the STCrowd dataset for both 2D and 3D domains show significant performance improvements compared to the base models, outperforming most state-of-the-art methods. When integrated into a state-of-the-art crowd detector, CQ can further improve performance on the challenging CrowdHuman dataset, demonstrating its generalizability. The code is released at https://github.com/mdaehl/CrowdQuery.",
        "translated": "本文提出了一种新颖的端到端人群检测方法，通过利用目标密度信息来增强现有基于Transformer的检测器。我们提出的CrowdQuery（CQ）方法核心是CQ模块，该模块可预测并嵌入目标密度图，随后将嵌入的密度信息系统化整合到解码器中。现有密度图定义通常依赖于头部位置或基于目标的空间统计量，而我们的方法扩展了这一定义，将个体边界框尺寸纳入考量。通过将密度信息融入目标查询机制，本方法采用密度引导查询来提升拥挤场景下的检测性能。CQ方法无需额外数据即可同时适用于2D和3D检测任务，由此成为首个有效贯通拥挤环境下2D与3D检测的解决方案。我们演示了将CQ集成至通用2D和3D基于Transformer的目标检测器中的架构CQ2D和CQ3D，且CQ的适用性不限于我们选择的特定Transformer模型。在STCrowd数据集上进行的2D与3D领域实验表明，相较于基线模型，该方法实现了显著性能提升，并优于多数现有先进方法。当集成至最先进的人群检测器时，CQ在具有挑战性的CrowdHuman数据集上可进一步提升性能，证明了其泛化能力。相关代码已发布于https://github.com/mdaehl/CrowdQuery。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"object density map\"译为\"目标密度图\"，\"transformer-based detectors\"译为\"基于Transformer的检测器\"\n2. 技术概念完整保留：\"density-guided queries\"译为\"密度引导查询\"，\"bounding box dimensions\"译为\"边界框尺寸\"\n3. 学术表述规范：\"end-to-end\"译为\"端到端\"，\"state-of-the-art\"译为\"最先进的\"\n4. 长句结构符合中文表达习惯，同时保持技术细节的精确性）"
    },
    {
        "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
        "url": "http://arxiv.org/abs/2509.08715v1",
        "pub_date": "2025-09-10",
        "summary": "As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at https://github.com/thico0224/BcQLM.",
        "translated": "随着多模态大语言模型（MLLMs）的发展，其大规模架构在资源受限环境中的部署面临挑战。在大模型时代，能源效率、计算可扩展性和环境可持续性至关重要，开发轻量级高性能模型对实际应用具有关键意义。为此，我们提出了一种面向端到端视觉问答的轻量级MLLM框架。该方案的核心是BreezeCLIP——一个紧凑而强大的视觉语言编码器，专为高效多模态理解优化。模型总参数量仅12亿，在显著降低计算成本的同时实现了与标准规模MLLM相当的性能。在多数据集上的实验进一步验证了其在精度与效率平衡方面的有效性。模块化可扩展的设计使其能泛化至更广泛的多模态任务。该轻量级视觉语言框架被命名为BcQLM（BreezeCLIP增强型Q门控多模态语言模型），为在实际硬件限制下部署MLLM提供了可行路径。源代码已开源：https://github.com/thico0224/BcQLM。\n\n（注：译文严格遵循学术论文表述规范，关键技术术语如\"multimodal large language models\"译为\"多模态大语言模型\"，\"vision-language encoder\"译为\"视觉语言编码器\"，\"Q-Gated\"保留技术特征译为\"Q门控\"。数字单位遵循中文计量规范，\"1.2 billion\"转换为\"12亿\"。长难句按中文习惯拆分重组，如将英文复合从句\"where...\"处理为独立判断句\"至关重要...具有关键意义\"，同时保持逻辑严密性。开源链接等要素完整保留。）"
    },
    {
        "title": "Computational Imaging for Enhanced Computer Vision",
        "url": "http://arxiv.org/abs/2509.08712v1",
        "pub_date": "2025-09-10",
        "summary": "This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.",
        "translated": "本文系统综述了计算成像（CI）技术及其对计算机视觉（CV）应用的变革性影响。传统成像方法在低光照、运动模糊或高动态范围场景等挑战性条件下往往难以提供高保真视觉数据，这限制了前沿计算机视觉系统的性能。计算成像技术通过增强图像采集与重建过程，有效解决了这些局限性，具体包括光场成像、高动态范围（HDR）成像、去模糊、高速成像和眩光抑制等技术。本综述系统探讨了CI技术与核心CV任务（含目标检测、深度估计、光流分析、人脸识别和关键点检测）之间的协同效应。通过分析CI方法与其对CV应用的实际贡献之间的关联，本研究揭示了新兴机遇、现存挑战及未来研究方向。我们重点探讨了面向特定任务的自适应成像流程的潜力，这些流程能在自动驾驶、监控、增强现实和机器人等现实场景中提升系统的鲁棒性、精度与效率。"
    },
    {
        "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
        "url": "http://arxiv.org/abs/2509.08699v1",
        "pub_date": "2025-09-10",
        "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.",
        "translated": "在机器人视觉导航领域，传统方法通常依赖全局一致的3D地图或学习型控制器，这些方法存在计算成本高且难以跨环境泛化的局限性。本研究提出了一种创新的纯RGB对象级拓扑导航框架，无需3D地图或预训练控制器即可实现零样本的长程机器人导航。该方法通过融合全局拓扑路径规划与局部度量轨迹控制，使机器人能够在避开障碍物的同时导航至对象级子目标。\n\n我们通过以下核心创新解决了现有方法的缺陷：利用单目深度估计和可通行性预测实现连续局部轨迹规划，并引入自动切换机制在必要时回退至基线控制器。该系统基于基础模型构建，无需领域特异性微调即可实现开放场景的适用性。通过仿真环境与真实场景测试，我们验证了该方法在鲁棒性和部署便利性方面的优势。实验表明，本方法在开放环境视觉导航任务中优于现有最优方案，提供了更具适应性的解决方案。相关源代码已开源：https://github.com/podgorki/TANGO。\n\n（注：根据学术规范，对技术术语进行了标准化处理：\n1. \"topometric navigation\"译为\"拓扑导航\"以符合机器人学规范\n2. \"zero-shot\"保留零样本特性但采用\"零样本\"标准译法\n3. \"foundational models\"译为\"基础模型\"符合AI领域共识\n4. \"open-set applicability\"译为\"开放场景适用性\"以准确传达原文语义\n5. 保持中英文术语对应关系，如\"traversability estimation\"统一译为\"可通行性预测\"）"
    },
    {
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework",
        "url": "http://arxiv.org/abs/2509.08694v1",
        "pub_date": "2025-09-10",
        "summary": "Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "translated": "基于卫星影像的海岸水域分割任务面临光谱特征复杂与边界形态不规则等独特挑战。传统RGB方法在多样化海洋环境中常出现训练不稳定和泛化能力不足的问题。本文提出一种系统性鲁棒增强框架——Robust U-Net，通过引入HSV色彩空间监督与多模态约束机制提升海岸水域分割性能。该框架集成五大协同组件：HSV色彩引导监督、梯度式海岸线优化、形态学后处理、海域净化和连通性控制。综合消融实验表明，HSV监督模块贡献度最高（影响系数0.85），完整框架可实现显著训练稳定性提升（方差降低84%）并增强分割质量。本方法在多项评估指标中均保持稳定改进，同时维持计算效率。为促进可复现性，训练配置与代码已开源：https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet。"
    },
    {
        "title": "FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical\n  Flow Estimation with Total Variation Regularization",
        "url": "http://arxiv.org/abs/2509.08670v1",
        "pub_date": "2025-09-10",
        "summary": "We present FractalPINN-Flow, an unsupervised deep learning framework for dense optical flow estimation that learns directly from consecutive grayscale frames without requiring ground truth. The architecture centers on the Fractal Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal geometry and self-similarity. Unlike traditional CNNs with sequential downsampling, FDN uses repeated encoder-decoder nesting with skip connections to capture both fine-grained details and long-range motion patterns. The training objective is based on a classical variational formulation using total variation (TV) regularization. Specifically, we minimize an energy functional that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness constancy, along with a TV term that promotes spatial smoothness and coherent flow fields. Experiments on synthetic and benchmark datasets show that FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow fields. The model is especially effective for high-resolution data and scenarios with limited annotations.",
        "translated": "我们提出了FractalPINN-Flow——一种无监督深度学习框架，用于直接从连续灰度帧中学习稠密光流估计，无需真实标注数据。该架构的核心是分形形变网络（FDN），这是一个受分形几何和自相似性启发的递归编码器-解码器结构。与传统采用顺序下采样的CNN不同，FDN通过重复的编码器-解码器嵌套结构与跳跃连接，同时捕获细粒度细节和长程运动模式。训练目标基于经典变分公式，采用全变分（TV）正则化：具体通过最小化结合$L^1$和$L^2$数据保真项（用于增强亮度恒定性）与TV项（促进空间平滑性和流场一致性）的能量泛函实现。在合成数据和基准数据集上的实验表明，FractalPINN-Flow能生成精确、平滑且保持边缘细节的光流场。该模型尤其适用于高分辨率数据及标注有限的场景。\n\n（注：专业术语说明：\n- FractalPINN-Flow: 保留英文形式，体现模型命名\n- optical flow: 光流\n- encoder-decoder: 编码器-解码器\n- total variation: 全变分\n- brightness constancy: 亮度恒定性\n- edge-preserving: 边缘保持）"
    },
    {
        "title": "Skeleton-based sign language recognition using a dual-stream\n  spatio-temporal dynamic graph convolutional network",
        "url": "http://arxiv.org/abs/2509.08661v1",
        "pub_date": "2025-09-10",
        "summary": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "translated": "孤立手语识别（ISLR）面临形态相似但语义迥异的手势识别难题，这一挑战源于手部形状与运动轨迹之间复杂的相互作用。现有方法通常依赖单一参考帧，难以解决这种几何模糊性问题。本文提出双参考帧双流架构Dual-SignLanguageNet（DSLNet），通过在两个互补坐标系中解耦并分别建模手势形态和运动轨迹。我们的方法采用腕部中心坐标系实现视角无关的形状分析，以及面部中心坐标系实现上下文感知的轨迹建模——两个分支分别由专用网络处理：采用拓扑感知图卷积网络处理形状信息，基于芬斯勒几何的编码器处理轨迹信息，最终通过几何驱动的最优传输融合机制进行整合。DSLNet以显著更少的参数量，在具有挑战性的WLASL-100、WLASL-300和LSA64数据集上分别达到93.70%、89.97%和99.79%的准确率，创造了新的性能标杆。\n\n（注：专业术语说明：\n1. Finsler geometry保留为\"芬斯勒几何\"，这是微分几何中的标准术语\n2. optimal transport采用学界通用译法\"最优传输\"\n3. graph convolution遵循计算机视觉领域规范译为\"图卷积\"\n4. WLASL/LSA64作为标准数据集名称保留不译\n5. parameters根据上下文译为\"参数量\"而非\"参数\"，更符合深度学习领域的表述习惯）"
    },
    {
        "title": "X-Part: high fidelity and structure coherent shape decomposition",
        "url": "http://arxiv.org/abs/2509.08643v1",
        "pub_date": "2025-09-10",
        "summary": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability and suffer from poor semantically meaningful decomposition. To this end, we introduce X-Part, a controllable generative model designed to decompose a holistic 3D object into semantically meaningful and structurally coherent parts with high geometric fidelity. X-Part exploits the bounding box as prompts for the part generation and injects point-wise semantic features for meaningful decomposition. Furthermore, we design an editable pipeline for interactive part generation. Extensive experimental results show that X-Part achieves state-of-the-art performance in part-level shape generation. This work establishes a new paradigm for creating production-ready, editable, and structurally sound 3D assets. Codes will be released for public research.",
        "translated": "在部件级别生成三维形状对于网格重拓扑、UV映射和三维打印等下游应用至关重要。然而，现有的基于部件的生成方法往往缺乏足够的可控性，且语义化分解效果不佳。为此，我们提出了X-Part——一种可控生成模型，能够将整体三维对象分解为具有高几何保真度的语义化部件，并保持结构一致性。该模型以边界框作为部件生成提示，并通过注入点级语义特征实现有意义的结构分解。此外，我们设计了支持交互式部件生成的可编辑流程。大量实验结果表明，X-Part在部件级形状生成任务上达到了最先进的性能。这项工作为创建生产就绪、可编辑且结构合理的三维资产建立了新范式。代码将开源以供学术研究使用。\n\n（注：根据学术规范，对\"Codes will be released for public research\"采用国内计算机领域常用表述\"代码将开源\"进行意译，既符合中文表达习惯，也准确传递了原意）"
    },
    {
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and\n  Correct Image Interpretation Model Shortcuts",
        "url": "http://arxiv.org/abs/2509.08640v1",
        "pub_date": "2025-09-10",
        "summary": "Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\\% of cases, correctly incorporated the specified finding in 89-99\\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\\% AUC in internal validation and by 1-11\\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.",
        "translated": "胸部X光片（CXR）是临床最常用的检查手段之一。自动化影像解读可减轻放射科医生的工作负担并扩大诊断专业知识的可及性。深度学习多任务与基础模型虽在CXR解读中表现出色，但存在捷径学习缺陷——模型依赖虚假或偏离目标的关联而非临床相关特征进行决策。我们提出RoentMod反事实图像编辑框架，该框架能生成具有用户指定合成病变、且保留原始扫描中无关解剖特征的解剖学真实CXR图像。RoentMod将开源医学图像生成器（RoentGen）与图像修改模型相结合，无需重新训练。经执业放射医师和放射科住院医师参与的阅片研究显示：RoentMod生成的图像真实性获93%认可，89-99%的案例正确融合指定病灶特征，其原生解剖结构保存度与真实随访CXR相当。通过RoentMod实验，我们发现当前最优的多任务与基础模型频繁利用偏离目标的病变特征作为捷径，限制了模型特异性。在训练中加入Roent生成的反事实图像后，模型抗干扰能力显著提升：内部验证中多项病变的判别AUC提升3-19%，外部测试中6类病变有5类提升1-11%。这些发现表明RoentMod可作为普适性工具用于探测和修正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了CXR解读模型的鲁棒性与可解释性，为改进医学影像基础模型提供了可推广的策略。"
    },
    {
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain\n  Translation",
        "url": "http://arxiv.org/abs/2509.08628v1",
        "pub_date": "2025-09-10",
        "summary": "Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "translated": "扩散模型在生成高质量输出方面表现出色，但在数据稀缺领域面临挑战，这些领域通常需要 exhaustive 的重新训练或成本高昂的配对数据。为应对这些局限性，我们提出潜在对齐扩散桥（LADB），这是一种用于样本到样本转换的半监督框架，能够利用部分配对数据有效弥合领域差距。通过在共享潜在空间中对齐源域和目标域分布，LADB 将预训练的源域扩散模型与目标域潜在对齐扩散模型（LADM）无缝集成，后者基于部分配对的潜在表示进行训练。这一方法实现了确定性的领域映射，无需完全监督。与通常缺乏可控性的非配对方法以及需要大型领域特定数据集的完全配对方法相比，LADB 通过结合配对和非配对的潜在-目标耦合，在保真度和多样性之间取得了平衡。我们的实验结果表明，该方法在部分监督下的深度图到图像转换任务中表现优异。此外，我们将 LADB 扩展到多源转换（从深度图和分割掩码）以及类别条件风格迁移任务中的多目标转换，展示了其处理多样化和异构用例的灵活性。最终，我们提出 LADB 作为一种可扩展且通用的解决方案，适用于现实世界的领域转换任务，尤其是在数据标注成本高昂或不完整的场景中。"
    },
    {
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "url": "http://arxiv.org/abs/2509.08827v1",
        "pub_date": "2025-09-10",
        "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "translated": "本文综述了强化学习（RL）在大语言模型（LLM）推理任务中的最新进展。强化学习在拓展大语言模型能力边界方面取得了显著成就，尤其在数学与编程等复杂逻辑任务上表现突出。因此，强化学习已成为将大语言模型升级为逻辑推理模型（LRM）的基础方法论。随着该领域的快速发展，逻辑推理模型的强化学习规模化应用正面临基础性挑战，这些挑战不仅存在于计算资源层面，更涉及算法设计、训练数据与基础设施等多个维度。为此，有必要重新审视该领域的发展路径，评估其演进轨迹，并探索增强强化学习可扩展性以实现人工超智能（ASI）的策略。本文重点分析了自DeepSeek-R1发布以来，强化学习在提升大语言模型与逻辑推理模型推理能力方面的研究进展，涵盖基础组件、核心问题、训练资源及下游应用等维度，以揭示这一快速发展领域的未来机遇与方向。我们希望本综述能推动强化学习在更广泛推理模型中的深入研究。GitHub项目地址：https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs\n\n（注：译文严格遵循学术规范，对RL（强化学习）、LLM（大语言模型）、LRM（逻辑推理模型）、ASI（人工超智能）等专业术语采用括号标注全称的规范译法，保持技术表述的准确性。长难句按中文习惯拆分重组，确保逻辑清晰性。GitHub链接等数字资源信息完整保留。）"
    },
    {
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
        "url": "http://arxiv.org/abs/2509.08825v1",
        "pub_date": "2025-09-10",
        "summary": "Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.   We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.   Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.",
        "translated": "大型语言模型（LLMs）正通过自动化数据标注和文本分析等劳动密集型任务，迅速改变社会科学研究范式。然而，LLM的输出结果会因研究者的实施选择（如模型选择、提示策略或温度参数设置）产生显著差异。这种差异可能引入系统性偏差和随机误差，进而蔓延至下游分析，导致第一类错误、第二类错误、符号错误或量级错误。我们将这种现象称为\"LLM黑客行为\"。  \n\n我们通过复现21项已发表社会科学研究中的37个数据标注任务，使用18种不同模型进行量化分析。基于对1300万个LLM生成标签的检验，我们针对2,361个现实假设测试了研究者不同选择对统计结论的影响。研究发现：使用最先进模型标注数据时，约三分之一的假设会得出错误结论；而使用小型语言模型时，错误结论比例高达二分之一。虽然任务表现提升和模型通用能力增强可降低LLM黑客风险，但即使高精度模型也无法完全消除该风险。随着效应量增大，LLM黑客风险会相应降低，这表明对接近显著性阈值的研究发现需要更严格的验证。  \n\n我们对LLM黑客风险缓解技术的深入分析表明：人工标注对于降低假阳性发现和改进模型选择至关重要。令人惊讶的是，常见的回归估计量校正技术对降低LLM黑客风险基本无效，因为它们需要大幅权衡第一类与第二类错误。  \n\n除意外误差外，我们发现故意实施LLM黑客行为异常简单：仅需使用少量LLM模型和提示词变体，任何结论都可以被呈现为具有统计显著性。"
    },
    {
        "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora",
        "url": "http://arxiv.org/abs/2509.08824v1",
        "pub_date": "2025-09-10",
        "summary": "The performance of large language models (LLMs) is deeply influenced by the quality and composition of their training data. While much of the existing work has centered on English, there remains a gap in understanding how to construct effective training corpora for other languages. We explore scalable methods for building web-based corpora for LLMs. We apply them to build a new 120B token corpus in Portuguese that achieves competitive results to an industrial-grade corpus. Using a continual pretraining setup, we study how different data selection and preprocessing strategies affect LLM performance when transitioning a model originally trained in English to another language. Our findings demonstrate the value of language-specific filtering pipelines, including classifiers for education, science, technology, engineering, and mathematics (STEM), as well as toxic content. We show that adapting a model to the target language leads to performance improvements, reinforcing the importance of high-quality, language-specific data. While our case study focuses on Portuguese, our methods are applicable to other languages, offering insights for multilingual LLM development.",
        "translated": "大型语言模型（LLM）的性能深受其训练数据质量与构成的影响。尽管现有研究多集中于英语语料，但针对其他语言如何构建有效训练语料仍存在认知空白。本研究探索了基于网络数据构建多语言LLM语料库的可扩展方法，并成功构建了包含1200亿词元的葡萄牙语语料库，其性能可与工业级语料库相媲美。通过持续预训练实验框架，我们系统分析了当将原基于英语训练的模型迁移至其他语言时，不同数据筛选与预处理策略对模型性能的影响。研究结果表明：采用针对目标语言的专项过滤流程（包括教育类、STEM领域及有害内容分类器）具有显著价值；模型适应目标语言后性能持续提升，印证了高质量语言专属数据的重要性。虽然本案例研究聚焦葡萄牙语，但所提出的方法适用于其他语言，为多语言LLM开发提供了重要实践洞见。"
    },
    {
        "title": "Merge-of-Thought Distillation",
        "url": "http://arxiv.org/abs/2509.08814v1",
        "pub_date": "2025-09-10",
        "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different \"best teachers,\" and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into student with overcoming conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students.",
        "translated": "尽管实际中存在多个候选教师模型且思维链语料库日益丰富，高效的长链思维推理蒸馏仍长期受限于单一最优教师的假设约束。我们重新审视教师选择机制，发现不同学生存在差异化的\"最佳教师\"，甚至同一学生在不同数据集上的最优教师也会变化。为此，我们提出融合思维蒸馏框架（MoT），通过交替执行教师专属的监督微调分支与权重空间融合，将多位教师的推理能力统一注入学生模型，有效解决多教师监督间的冲突。在数学竞赛基准测试中，仅使用约200个高质量思维链样本，对Qwen3-14B学生模型应用MoT后，其表现超越了DEEPSEEK-R1、QWEN3-30B-A3B、QWEN3-32B及OPENAI-O1等强劲模型，实现显著性能提升。该框架不仅持续优于最佳单教师蒸馏和朴素多教师联合方法，在提升性能上限的同时缓解过拟合现象，还对分布偏移和同级别教师表现出鲁棒性。此外，MoT能减少灾难性遗忘，提升数学领域外的泛化推理能力，甚至培育出更优质的教师模型，表明经过共识过滤的推理特征具有广泛迁移性。这些成果使MoT成为从多元教师向紧凑学生模型高效蒸馏长链思维能力的简洁可扩展方案。"
    },
    {
        "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
        "url": "http://arxiv.org/abs/2509.08812v1",
        "pub_date": "2025-09-10",
        "summary": "Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC",
        "translated": "基于子词的标记化方法往往难以有效保留形态学边界，这一局限在资源匮乏且形态复杂的语言（如使用吉兹字母的文字体系）中尤为明显。为此，我们提出MoVoC（形态素感知的子词词汇构建方法），并训练出集成监督式形态分析的标记器MoVoC-Tok。该混合分词方法结合了基于形态素的分词与字节对编码（BPE）标记，在保持词汇语义的同时维护形态完整性。针对资源稀缺问题，我们整理并发布了四种吉兹字母语言的人工标注形态素数据集，以及其中两种语言的形态素感知词汇表。虽然所提出的标记化方法未显著提升自动翻译质量，但我们观察到内在评估指标MorphoScore和边界精确率（Boundary Precision）的持续改善，这凸显了形态感知分词在提升语言保真度和标记效率方面的价值。我们公开提供形态素标注数据集与标记器，以支持对资源匮乏型形态丰富语言的进一步研究。代码与数据详见GitHub：https://github.com/hailaykidu/MoVoC\n\n（注：Geez script作为专有名词保留\"吉兹字母\"译法，MorphoScore作为专业指标名称保留英文形式，技术术语如\"Byte Pair Encoding (BPE)\"采用学界通用译法\"字节对编码\"）"
    },
    {
        "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals",
        "url": "http://arxiv.org/abs/2509.08809v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have significantly reduced data annotation costs and reliance on human annotators. However, evaluating the quality of their annotations remains challenging in dynamic, unsupervised environments where oracle feedback is scarce and conventional methods fail. To address this challenge, we propose a novel agentic annotation paradigm, where a student model collaborates with a noisy teacher (the LLM) to assess and refine annotation quality without relying on oracle feedback. The student model, acting as an unsupervised feedback mechanism, employs a user preference-based majority voting strategy to evaluate the consistency of the LLM outputs. To systematically measure the reliability of LLM-generated annotations, we introduce the Consistent and Inconsistent (CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only quantifies the annotation quality of the noisy teacher under limited user preferences but also plays a critical role in model selection, enabling the identification of robust LLMs in dynamic, unsupervised environments. Applied to ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a strong positive correlation with LLM accuracy, establishing it as an essential tool for unsupervised evaluation and model selection in real-world settings.",
        "translated": "在基于提示的任务中，大语言模型（LLMs）显著降低了数据标注成本并减少了对人工标注者的依赖。然而，在动态无监督环境中，由于缺乏真实反馈且传统方法失效，评估其标注质量仍具挑战性。针对这一问题，我们提出了一种新型智能标注范式：通过学生模型与噪声教师（即大语言模型）协作，在不依赖真实反馈的情况下评估并优化标注质量。该学生模型作为无监督反馈机制，采用基于用户偏好的多数投票策略来评估大语言模型输出的一致性。为系统衡量大语言模型生成标注的可靠性，我们提出了\"一致性与不一致性比率\"（CAI Ratio）这一新型无监督评估指标。CAI比率不仅能量化噪声教师在有限用户偏好下的标注质量，还在模型选择中发挥关键作用，帮助在动态无监督环境中识别稳健的大语言模型。通过在四个大语言模型和十个开放域NLP数据集上的实验验证，CAI比率与大语言模型准确率呈现强正相关，证明了其作为现实场景中无监督评估与模型选择核心工具的有效性。"
    },
    {
        "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
        "url": "http://arxiv.org/abs/2509.08803v1",
        "pub_date": "2025-09-10",
        "summary": "The rise of misinformation underscores the need for scalable and reliable fact-checking solutions. Large language models (LLMs) hold promise in automating fact verification, yet their effectiveness across global contexts remains uncertain. We systematically evaluate nine established LLMs across multiple categories (open/closed-source, multiple sizes, diverse architectures, reasoning-based) using 5,000 claims previously assessed by 174 professional fact-checking organizations across 47 languages. Our methodology tests model generalizability on claims postdating training cutoffs and four prompting strategies mirroring both citizen and professional fact-checker interactions, with over 240,000 human annotations as ground truth. Findings reveal a concerning pattern resembling the Dunning-Kruger effect: smaller, accessible models show high confidence despite lower accuracy, while larger models demonstrate higher accuracy but lower confidence. This risks systemic bias in information verification, as resource-constrained organizations typically use smaller models. Performance gaps are most pronounced for non-English languages and claims originating from the Global South, threatening to widen existing information inequalities. These results establish a multilingual benchmark for future research and provide an evidence base for policy aimed at ensuring equitable access to trustworthy, AI-assisted fact-checking.",
        "translated": "错误信息的泛滥凸显了对可扩展且可靠的事实核查解决方案的迫切需求。大型语言模型（LLMs）在自动化事实核查领域展现出潜力，但其在全球语境下的有效性仍存疑问。本研究系统评估了九种主流LLMs（涵盖开源/闭源、多种参数量级、不同架构及基于推理的模型），使用由174家专业事实核查机构以47种语言验证过的5,000条声明作为测试集。我们的方法通过训练截止日期后产生的声明测试模型泛化能力，并采用四种提示策略模拟普通用户和专业核查人员的交互场景，以超过24万条人工标注作为基准真值。\n\n研究发现存在类似邓宁-克鲁格效应的风险模式：轻量级开源模型虽准确率较低却呈现高置信度，而大型模型准确率更高却表现更谨慎。这种偏差可能导致系统性风险——资源有限的机构通常使用小型模型进行信息验证。性能差距在非英语语境及源自全球南方的声明中尤为显著，这种技术鸿沟可能加剧现有的信息不平等。本研究为后续研究建立了多语言基准测试框架，并为制定保障公平获取可信AI辅助事实核查政策的制定提供了实证依据。"
    },
    {
        "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A\n  Cross-Architecture Analysis of Recall Mechanisms",
        "url": "http://arxiv.org/abs/2509.08778v1",
        "pub_date": "2025-09-10",
        "summary": "Understanding how Transformer-based language models store and retrieve factual associations is critical for improving interpretability and enabling targeted model editing. Prior work, primarily on GPT-style models, has identified MLP modules in early layers as key contributors to factual recall. However, it remains unclear whether these findings generalize across different autoregressive architectures. To address this, we conduct a comprehensive evaluation of factual recall across several models -- including GPT, LLaMA, Qwen, and DeepSeek -- analyzing where and how factual information is encoded and accessed. Consequently, we find that Qwen-based models behave differently from previous patterns: attention modules in the earliest layers contribute more to factual recall than MLP modules. Our findings suggest that even within the autoregressive Transformer family, architectural variations can lead to fundamentally different mechanisms of factual recall.",
        "translated": "理解基于Transformer的语言模型如何存储和检索事实关联，对于提升模型可解释性和实现定向模型编辑至关重要。先前针对GPT类模型的研究发现，早期层的MLP模块是事实召回的关键贡献者。然而，这些发现是否适用于不同自回归架构仍不明确。为此，我们对包括GPT、LLaMA、Qwen和DeepSeek在内的多个模型进行了事实召回能力的综合评估，分析事实信息编码与访问的位置及机制。研究发现，基于Qwen的模型表现出与既往模式不同的特性：其最早层的注意力模块对事实召回的贡献度超过MLP模块。这一结果表明，即使在自回归Transformer架构家族内部，结构差异也可能导致事实召回机制的根本性不同。"
    },
    {
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.08755v1",
        "pub_date": "2025-09-10",
        "summary": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.",
        "translated": "开发能够通过一系列智能决策解决复杂现实任务的自主大语言模型（LLM）智能体，是当前快速演进的前沿领域。与人类认知发展类似，智能体需要通过与环境探索和交互来获取知识与技能。尽管已有诸多进展，学界仍缺乏一个统一的交互式强化学习（RL）框架，能够在多样化的现实环境中完全从零开始（无需监督微调SFT）有效训练此类智能体。为填补这一空白，我们提出了AgentGym-RL——一个通过强化学习训练多轮交互决策LLM智能体的新型框架。该框架采用模块化解耦架构，确保高度灵活性与可扩展性，涵盖多样化现实场景，并支持主流RL算法。\n\n此外，我们提出ScalingInter-RL训练方法，旨在实现探索-利用的平衡与稳定的RL优化。该方法在早期阶段通过限制交互次数强调利用策略，随后逐步扩大探索范围以鼓励多样化问题解决策略。这种设计使智能体能够发展出更丰富的行为模式，并降低长周期任务中的策略崩溃风险。我们通过大量实验验证了AgentGym-RL框架与ScalingInter-RL方法的稳定性与有效性：训练的智能体在27个跨领域任务中达到或超越了商业模型性能。我们将提供关键洞见并开源完整的AgentGym-RL框架（含代码与数据集），以推动新一代智能体的研发。\n\n（注：专业术语说明：\n- LLM：大语言模型（Large Language Model）\n- RL：强化学习（Reinforcement Learning）\n- SFT：监督微调（Supervised Fine-Tuning）\n- 探索-利用平衡（exploration-exploitation tradeoff）：强化学习中智能体在尝试新策略（探索）与执行已知有效策略（利用）之间的平衡机制）"
    },
    {
        "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
        "url": "http://arxiv.org/abs/2509.08753v1",
        "pub_date": "2025-09-10",
        "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at https://github.com/kyutai-labs/delayed-streams-modeling",
        "translated": "我们提出了延迟流建模（Delayed Streams Modeling, DSM），这是一种面向流式多模态序列到序列学习的灵活框架。传统的序列到序列生成通常以离线方式进行，即模型需完整读取输入序列后才开始生成第一个输出时间步。而流式序列到序列方法则依赖于学习一种策略来决定何时推进输入流或写入输出流。与之不同，DSM采用仅含解码器的语言模型来处理已经时间对齐的流数据。通过将对齐过程移至预处理阶段，并在不同流之间引入适当延迟，DSM能够实现从任意输入组合到任意输出序列的流式推理，使其可广泛应用于多种序列到序列问题。特别地，当给定文本和音频流时，自动语音识别（ASR）对应于文本流延迟的情况，而相反配置则构成文本到语音（TTS）模型。我们针对这两大核心序列到序列任务进行了大量实验，结果表明DSM在支持任意长序列的同时，实现了最先进的性能与延迟表现，甚至可与离线基线模型相媲美。代码、样本及演示详见https://github.com/kyutai-labs/delayed-streams-modeling。\n\n（注：根据学术规范，术语处理说明：\n1. \"streaming\"在计算领域统一译为\"流式\"\n2. \"decoder-only language model\"译为\"仅含解码器的语言模型\"\n3. \"state-of-the-art\"遵循国内学术惯例译为\"最先进的\"\n4. 专业缩写ASR/TTS首次出现时标注英文全称及中文译名）"
    },
    {
        "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
        "url": "http://arxiv.org/abs/2509.08729v1",
        "pub_date": "2025-09-10",
        "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.   Maintaining selection pressure by setting the success threshold to $\\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.   Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.",
        "translated": "多轮转单轮（M2S）方法将迭代式红队测试压缩至单个结构化提示中，但以往研究依赖少量人工编写的模板。我们提出X-Teaming Evolutionary M2S框架，通过语言模型引导的进化自动发现并优化M2S模板。该系统结合了从12个数据源的智能采样策略，采用受StrongREJECT启发的LLM-as-judge评估机制，并记录完全可审计的日志。  \n通过将成功阈值设定为θ=0.70以维持选择压力，我们获得了五代进化结果、两个新模板家族，并在GPT-4.1上实现44.8%的整体成功率（103/230）。针对2,500次实验的平衡跨模型评估（固定评判标准）表明，结构增益具有可迁移性但随目标模型变化；有两个模型在相同阈值下成功率为零。我们还发现提示长度与得分呈正相关，这启发了长度感知的评判机制。  \n本研究证明：结构级搜索是增强单轮探测的有效可复现路径，同时强调了阈值校准和跨模型评估的重要性。代码、配置与实验成果详见https://github.com/hyunjun1121/M2S-x-teaming。\n\n（注：根据学术规范，关键术语处理如下：  \n- \"red-teaming\" 译为\"红队测试\"（安全评估方法）  \n- \"LLM-as-judge\" 译为\"LLM-as-judge评估机制\"（保留英文缩写并添加说明）  \n- \"selection pressure\" 译为\"选择压力\"（进化算法术语）  \n- \"cross-model evaluation\" 译为\"跨模型评估\"）"
    },
    {
        "title": "Generative Data Refinement: Just Ask for Better Data",
        "url": "http://arxiv.org/abs/2509.08653v1",
        "pub_date": "2025-09-10",
        "summary": "For a fixed parameter size, the capabilities of large models are primarily determined by the quality and quantity of its training data. Consequently, training datasets now grow faster than the rate at which new data is indexed on the web, leading to projected data exhaustion over the next decade. Much more data exists as user-generated content that is not publicly indexed, but incorporating such data comes with considerable risks, such as leaking private information and other undesirable content. We introduce a framework, Generative Data Refinement (GDR), for using pretrained generative models to transform a dataset with undesirable content into a refined dataset that is more suitable for training. Our experiments show that GDR can outperform industry-grade solutions for dataset anonymization, as well as enable direct detoxification of highly unsafe datasets. Moreover, we show that by generating synthetic data that is conditioned on each example in the real dataset, GDR's refined outputs naturally match the diversity of web scale datasets, and thereby avoid the often challenging task of generating diverse synthetic data via model prompting. The simplicity and effectiveness of GDR make it a powerful tool for scaling up the total stock of training data for frontier models.",
        "translated": "对于固定参数规模的大模型而言，其能力主要取决于训练数据的质量与数量。当前训练数据集的增长速度已超过网络公开索引数据的增长率，预计未来十年将面临数据枯竭问题。大量用户生成内容虽未公开索引，但直接使用此类数据存在显著风险，包括隐私泄露和不良内容等问题。我们提出生成式数据精炼框架（GDR），通过预训练生成模型将含不良内容的数据集转化为更适合训练的精炼数据集。实验表明，GDR在数据集匿名化处理方面优于工业级解决方案，并能直接对高度不安全数据集进行脱毒处理。通过基于真实数据集样本生成条件化合成数据，GDR的精炼输出自然保持网络规模数据集的多样性，避免了通过模型提示生成多样化合成数据的技术挑战。GDR的简洁性与有效性使其成为扩展前沿模型训练数据总量的有力工具。"
    },
    {
        "title": "OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for\n  Aspect-Based Sentiment Analysis",
        "url": "http://arxiv.org/abs/2509.08612v1",
        "pub_date": "2025-09-10",
        "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and determine their sentiment polarity. While dependency trees combined with contextual semantics effectively identify aspect sentiment, existing methods relying on syntax trees and aspect-aware attention struggle to model complex semantic relationships. Their dependence on linear dot-product features fails to capture nonlinear associations, allowing noisy similarity from irrelevant words to obscure key opinion terms. Motivated by Differentiable Optimal Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN), which introduces a Syntactic-Semantic Collaborative Attention. It comprises a Syntactic Graph-Aware Attention for mining latent syntactic dependencies and modeling global syntactic topology, as well as a Semantic Optimal Transport Attention designed to uncover fine-grained semantic alignments amidst textual noise, thereby accurately capturing sentiment signals obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates these heterogeneous features, and contrastive regularization further improves robustness. Experiments demonstrate that OTESGN achieves state-of-the-art results, outperforming previous best models by +1.01% F1 on Twitter and +1.30% F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its efficacy in precise localization of opinion words and noise resistance.",
        "translated": "基于方面的情感分析（ABSA）旨在识别文本中的方面术语并判定其情感极性。尽管依赖树与上下文语义结合能有效识别方面情感，但现有基于语法树和方面感知注意力机制的方法难以建模复杂语义关系。这些方法对线性点积特征的依赖无法捕捉非线性关联，导致无关词汇产生的噪声相似度干扰关键观点词的识别。受可微分最优匹配理论启发，我们提出基于最优传输增强的语法-语义图网络（OTESGN），创新性地引入语法-语义协同注意力机制：该机制包含语法图感知注意力（挖掘潜在语法依赖并建模全局语法拓扑结构）和语义最优传输注意力（在文本噪声中发现细粒度语义对齐，从而精准捕获被无关词元遮蔽的情感信号）。自适应注意力融合模块整合这些异构特征，对比正则化进一步提升了模型鲁棒性。实验表明OTESNN取得最先进性能，在Twitter数据集上F1值较之前最佳模型提升1.01%，在Laptop14基准上提升1.30%。消融研究与可视化分析验证了该方法在观点词精确定位和噪声抵抗方面的有效性。\n\n（注：根据学术规范，对关键术语进行标准化处理：\n- Differentiable Optimal Matching 译为\"可微分最优匹配\"\n- Optimal Transport Enhanced Syntactic-Semantic Graph Network 保留首字母缩写OTESGN并给出全称\"最优传输增强的语法-语义图网络\"\n- Syntactic Graph-Aware Attention/Semantic Optimal Transport Attention 采用\"语法图感知注意力/语义最优传输注意力\"的译法\n- 技术指标F1值保留原始计量单位%）"
    },
    {
        "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
        "url": "http://arxiv.org/abs/2509.08604v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.",
        "translated": "大型语言模型（LLMs）在医学领域展现出巨大潜力。迄今，LLMs已被广泛应用于辅助诊断、医学问答和临床信息整合等任务。然而，一个关键问题尚未解决：LLMs对医学训练数据的记忆程度究竟如何？本研究首次对医学领域LLMs的记忆现象开展系统性评估，从普遍性（发生频率）、特征（记忆内容类型）、体量（记忆信息量）及潜在下游影响（记忆如何影响医学应用）四个维度进行探究。我们系统分析了三种常见适应场景：（1）基于医学语料的持续预训练；（2）在标准医学基准上的微调；（3）基于真实世界临床数据的微调，包括来自耶鲁纽黑文医疗系统的超13,000条独特住院记录。结果表明：记忆现象在所有适应场景中普遍存在，其程度显著高于通用领域报道值。记忆效应影响医学LLMs的开发与应用，可归为三类：有益记忆（如准确回忆临床指南和生物医学参考文献）、无意义记忆（如重复的免责声明或模板化医疗文书语言）以及有害记忆（如再生数据集特异性内容或敏感临床信息）。基于这些发现，我们提出实践建议：促进有益记忆以增强领域特异性推理和事实准确性，减少无意义记忆以推动超越表面模式的深度学习，并遏制有害记忆以防止敏感或可识别患者信息的泄露。"
    },
    {
        "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge",
        "url": "http://arxiv.org/abs/2509.08596v1",
        "pub_date": "2025-09-10",
        "summary": "Biomedical question answering (QA) poses significant challenges due to the need for precise interpretation of specialized knowledge drawn from a vast, complex, and rapidly evolving corpus. In this work, we explore how large language models (LLMs) can be used for information retrieval (IR), and an ensemble of zero-shot models can accomplish state-of-the-art performance on a domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge tasks, we show that ensembles can outperform individual LLMs and in some cases rival or surpass domain-tuned systems - all while preserving generalizability and avoiding the need for costly fine-tuning or labeled data. Our method aggregates outputs from multiple LLM variants, including models from Anthropic and Google, to synthesize more accurate and robust answers. Moreover, our investigation highlights a relationship between context length and performance: while expanded contexts are meant to provide valuable evidence, they simultaneously risk information dilution and model disorientation. These findings emphasize IR as a critical foundation in Retrieval-Augmented Generation (RAG) approaches for biomedical QA systems. Precise, focused retrieval remains essential for ensuring LLMs operate within relevant information boundaries when generating answers from retrieved documents. Our results establish that ensemble-based zero-shot approaches, when paired with effective RAG pipelines, constitute a practical and scalable alternative to domain-tuned systems for biomedical question answering.",
        "translated": "生物医学问答（QA）面临重大挑战，因为需要从庞大、复杂且快速更新的专业文献中精确解读专业知识。本研究探索了如何利用大语言模型（LLM）进行信息检索（IR），并通过零样本模型的集成方法在特定领域的二元问答任务中实现最先进性能。通过在BioASQ挑战任务上的评估，我们证明集成模型不仅能超越单个LLM的性能，在某些情况下甚至可与经过领域调优的系统相媲美或更优——同时保持泛化能力，无需昂贵的微调或标注数据。我们的方法聚合了包括Anthropic和谷歌多个LLM变体的输出，以生成更准确、更稳健的答案。\n\n研究还揭示了上下文长度与性能之间的关系：扩展的上下文本意是提供更有价值的证据，但同时也可能导致信息稀释和模型方向迷失。这些发现强调了信息检索在生物医学QA系统的检索增强生成（RAG）方法中的关键基础作用。当LLM基于检索到的文档生成答案时，精确且聚焦的检索对于确保模型在相关信息边界内运作至关重要。我们的结果表明：基于集成的零样本方法与有效的RAG流程结合时，可为生物医学问答提供一种兼具实用性、可扩展性的领域调优系统替代方案。\n\n（注：专业术语说明：\n- LLM：大语言模型（Large Language Model）\n- IR：信息检索（Information Retrieval）\n- RAG：检索增强生成（Retrieval-Augmented Generation）\n- 零样本（Zero-shot）：指模型未经特定任务训练即可执行该任务\n- 领域调优（Domain-tuned）：针对特定领域进行参数优化的系统）"
    },
    {
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models",
        "url": "http://arxiv.org/abs/2509.08541v1",
        "pub_date": "2025-09-10",
        "summary": "Current large language models (LLMs) generally show a significant performance gap in alignment between English and other languages. To bridge this gap, existing research typically leverages the model's responses in English as a reference to select the best/worst responses in other languages, which are then used for Direct Preference Optimization (DPO) training. However, we argue that there are two limitations in the current methods that result in noisy multilingual preference data and further limited alignment performance: 1) Not all English responses are of high quality, and using a response with low quality may mislead the alignment for other languages. 2) Current methods usually use biased or heuristic approaches to construct multilingual preference pairs. To address these limitations, we design a consistency-based data selection method to construct high-quality multilingual preference data for improving multilingual alignment (CM-Align). Specifically, our method includes two parts: consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction. Experimental results on three LLMs and three common tasks demonstrate the effectiveness and superiority of our method, which further indicates the necessity of constructing high-quality preference data.",
        "translated": "当前的大型语言模型（LLM）在英语与其他语言的对齐性能上普遍存在显著差距。为弥补这一差距，现有研究通常以模型的英文回复作为参考，筛选其他语言中的最佳/最差回复，进而用于直接偏好优化（DPO）训练。然而，我们认为当前方法存在两个局限性，导致生成的多语言偏好数据存在噪声，进而限制对齐效果：1）并非所有英文回复都具有高质量，使用低质量回复可能误导其他语言的对齐过程；2）现有方法通常依赖有偏或启发式策略构建多语言偏好对。针对这些问题，我们设计了一种基于一致性的数据选择方法，用于构建高质量多语言偏好数据以提升多语言对齐性能（CM-Align）。具体而言，该方法包含两个核心模块：一致性引导的英文参考选择机制和基于跨语言一致性的多语言偏好数据构建机制。在三个大型语言模型和三项常见任务上的实验结果表明，本方法具有显著的有效性和优越性，进一步印证了构建高质量偏好数据的必要性。"
    },
    {
        "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
        "url": "http://arxiv.org/abs/2509.08494v1",
        "pub_date": "2025-09-10",
        "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.",
        "translated": "随着人类将更多任务和决策权委托给人工智能（AI），我们正逐渐丧失对个人及集体未来的掌控。相对简单的算法系统已在引导人类决策，例如社交媒体信息流算法导致用户无意识、机械地浏览 engagement 优化内容。本文通过整合哲学与科学领域的能动性理论，结合AI辅助评估方法——使用大语言模型（LLMs）模拟验证用户查询并评估AI响应，系统阐述了人类能动性理念。我们开发了HumanAgencyBench（HAB），这是一个基于典型AI使用场景、包含六维人类能动性指标的可扩展自适应基准测试框架。HAB通过六大维度衡量AI助手或智能体的行为倾向：提出澄清性问题、避免价值操纵、纠正错误信息、推迟重大决策、鼓励学习以及维护社交边界。研究发现，当前基于LLM的助手对人类能动性的支持程度为低至中等水平，且不同系统开发者和维度间存在显著差异。例如，虽然Anthropic的LLM整体对人类能动性支持度最高，但在\"避免价值操纵\"维度却是支持度最低的模型。研究还表明，能动性支持度的提升并非源于LLM能力增强或指令遵循行为（如RLHF）的线性发展，我们呼吁转向更鲁棒的安全性与对齐目标建设。\n\n（注：专业术语处理说明：\n1. \"human agency\"译为\"人类能动性\"（哲学与社会科学领域标准译法）\n2. \"engagement-optimized content\"译为\"engagement优化内容\"（保留专业表述）\n3. \"LLMs\"统一译为\"大语言模型\"并标注英文缩写\n4. \"RLHF\"译为\"人类反馈强化学习\"并保留英文缩写\n5. \"alignment\"译为\"对齐\"（AI安全领域专业术语）\n6. \"Anthropic\"等企业名按行业惯例保留英文）"
    },
    {
        "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
        "url": "http://arxiv.org/abs/2509.08486v1",
        "pub_date": "2025-09-10",
        "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range of NLP tasks, yet aligning their outputs with the principles of Helpfulness, Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing methods often optimize for individual alignment dimensions in isolation, leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE) architectures offer modularity, they suffer from poorly calibrated routing, limiting their effectiveness in alignment tasks. We propose TrinityX, a modular alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE) within the Transformer architecture. TrinityX leverages separately trained experts for each HHH dimension, integrating their outputs through a calibrated, task-adaptive routing mechanism that combines expert signals into a unified, alignment-aware representation. Extensive experiments on three standard alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines, achieving relative improvements of 32.5% in win rate, 33.9% in safety score, and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and inference latency by over 40% compared to prior MoE-based approaches. Ablation studies highlight the importance of calibrated routing, and cross-model evaluations confirm TrinityX's generalization across diverse LLM backbones.",
        "translated": "大型语言模型（LLMs）在众多自然语言处理任务中展现出强大性能，但其输出与有用性（Helpfulness）、无害性（Harmlessness）和真实性（Honesty）的\"HHH\"原则对齐仍存在持续挑战。现有方法往往孤立地优化单一对齐维度，导致性能权衡与行为不一致。虽然混合专家（MoE）架构提供模块化能力，但其路由机制校准不足，限制了对齐任务的有效性。我们提出TrinityX框架，通过在Transformer架构中引入校准专家混合模块（MoCaE），为每个HHH维度独立训练专家模型，并通过任务自适应的校准路由机制整合专家输出，形成统一且具有对齐意识的表征。在三大标准对齐基准测试——Alpaca（有用性）、BeaverTails（无害性）和TruthfulQA（真实性）上的实验表明，TrinityX显著优于基线模型，在胜率、安全评分和真实性指标上分别实现32.5%、33.9%和28.4%的相对提升。此外，相比现有基于MoE的方法，该框架降低40%以上的内存占用与推理延迟。消融实验验证了校准路由机制的关键作用，跨模型评估则证实了TrinityX在不同LLM骨干网络上的泛化能力。\n\n（注：专业术语说明：\n1. Mixture-of-Experts (MoE)：混合专家模型\n2. Transformer architecture：Transformer架构\n3. Win rate：胜率（模型输出优于基线的比例）\n4. Ablation studies：消融实验（通过移除组件验证其重要性））"
    },
    {
        "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text",
        "url": "http://arxiv.org/abs/2509.08484v1",
        "pub_date": "2025-09-10",
        "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating particular perspectives or linguistic styles through the lens of a specified identity. While this method is often used to personalize outputs, its impact on how LLMs represent social groups remains underexplored. In this paper, we investigate whether persona-prompting leads to different levels of linguistic abstraction - an established marker of stereotyping - when generating short texts linking socio-demographic categories with stereotypical or non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias framework, we analyze outputs from six open-weight LLMs under three prompting conditions, comparing 11 persona-driven responses to those of a generic AI assistant. To support this analysis, we introduce Self-Stereo, a new dataset of self-reported stereotypes from Reddit. We measure abstraction through three metrics: concreteness, specificity, and negation. Our results highlight the limits of persona-prompting in modulating abstraction in language, confirming criticisms about the ecology of personas as representative of socio-demographic groups and raising concerns about the risk of propagating stereotypes even when seemingly evoking the voice of a marginalized group.",
        "translated": "角色提示是一种新兴策略，通过指定身份视角引导大语言模型模拟特定观点或语言风格。虽然该方法常用于个性化输出，但其对大语言模型表征社会群体的影响仍待深入探究。本文研究在生成关联社会人口类别与刻板/非刻板属性的短文本时，角色提示是否会引发不同层级的语言抽象化——这是衡量刻板印象的既定指标。基于语言预期偏差理论框架，我们分析六种开源权重LLM在三种提示条件下的输出，将11种角色驱动响应与通用AI助手响应进行对比。为此我们引入Self-Stereo新数据集，该数据集收集来自Reddit平台的自我报告型刻板印象。我们通过具体性、特定性和否定性三项指标测量抽象化程度。研究结果揭示了角色提示在调节语言抽象化方面的局限性，既印证了关于\"角色作为社会人口群体代表性\"的生态效度批评，也警示了即使看似调用边缘群体声音仍可能传播刻板印象的风险。"
    },
    {
        "title": "FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
        "url": "http://arxiv.org/abs/2509.09680v1",
        "pub_date": "2025-09-11",
        "summary": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
        "translated": "开源文本生成图像（T2I）模型的发展长期受制于缺乏大规模推理导向数据集和综合评估基准，导致其性能与领先闭源系统存在显著差距。为应对这一挑战，我们推出FLUX-Reason-6M数据集与PRISM-Bench（精准鲁棒图像合成测量基准）。FLUX-Reason-6M包含600万张高质量FLUX生成图像及2000万条中英双语描述，专门针对复杂推理能力训练设计。该数据集依据六大核心特性进行组织：想象力（Imagination）、实体（Entity）、文本渲染（Text rendering）、风格（Style）、情感（Affection）和构图（Composition），并通过显式生成思维链（GCoT）技术对图像生成步骤进行细粒度解析。整个数据构建过程耗费15,000个A100 GPU日，为学术界提供了以往仅大型工业实验室才能获得的资源。PRISM-Bench提出包含七大评估维度的新型评测标准，其中采用GCoT的长文本挑战任务尤为突出。通过精心设计的提示词，该基准利用先进视觉语言模型对文本-图像对齐度和图像美学进行类人化精细评估。我们对19个主流模型开展的全面测试揭示了关键性能差距，并明确了需重点改进的领域。现已公开数据集、基准测试框架及评估代码，以推动下一代推理导向T2I生成技术的发展。项目页面：https://flux-reason-6m.github.io/\n\n（注：翻译过程中对以下术语采用专业译法：\n- Reasoning-focused：推理导向\n- Generation Chain-of-Thought (GCoT)：生成思维链\n- Vision-language models：视觉语言模型\n- Prompt-image alignment：文本-图像对齐度\n- Human-aligned assessment：类人化评估\n严格保持技术术语准确性，同时确保中文表达符合学术论文摘要的正式文体特征。）"
    },
    {
        "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
        "url": "http://arxiv.org/abs/2509.09679v1",
        "pub_date": "2025-09-11",
        "summary": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$ entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \\log n)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
        "translated": "大型语言模型需要巨大的内存占用，这严重限制了其在消费级硬件上的部署。量化技术通过降低数值精度来减少内存需求，但极端2位量化会因激活值中的异常值而遭受灾难性的性能损失。基于旋转的方法（如QuIP和QuaRot）利用计算不变性原理：对于正交矩阵$\\mathbf{Q}$满足$\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$，在量化前通过正交变换消除异常值。然而这些方法采用固定变换——使用达到最优最差情况相干性$\\mu = 1/\\sqrt{n}$的哈达玛矩阵——无法适配特定权重分布。我们发现不同Transformer层展现出截然不同的异常值模式，这启发了我们采用层自适应旋转而非通用方案。我们提出ButterflyQuant方法，用可学习的蝴蝶变换替代哈达玛旋转，该变换通过连续Givens旋转角进行参数化。与哈达玛矩阵不可微的离散$\\{+1, -1\\}$元素（阻碍基于梯度的学习）不同，蝴蝶变换的连续参数化在保证构造正交性的同时支持平滑优化。这种正交约束既确保了异常值抑制的理论保证，又以$O(n \\log n)$计算复杂度和仅$\\frac{n \\log n}{2}$可学习参数实现。我们进一步对变换后的激活值引入均匀性正则化，以促进更平滑的量化友好分布。该方法仅需128个校准样本即可完成学习，并在单GPU上数分钟内收敛——这种一次性成本可忽略不计。在LLaMA-2-7B模型的2位量化测试中，ButterflyQuant实现了15.4的困惑度，显著优于QuaRot的22.1。"
    },
    {
        "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2509.09674v1",
        "pub_date": "2025-09-11",
        "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$ on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
        "translated": "视觉-语言-动作（VLA）模型近年来已成为机器人操控领域的重要范式。尽管通过大规模预训练和监督微调（SFT）取得了显著进展，这类模型仍面临两个核心挑战：（1）SFT扩展所需的大规模人工操作机器人轨迹数据稀缺且成本高昂；（2）对存在分布偏移任务的泛化能力有限。大型推理模型（LRMs）的最新突破表明，强化学习（RL）能显著增强逐步推理能力，这引出一个关键问题：RL是否同样能提升VLA模型的长期分层动作规划能力？本研究提出SimpleVLA-RL——一个专为VLA模型设计的高效RL框架。基于veRL架构，我们引入了VLA特化的轨迹采样、可扩展并行化、多环境渲染及优化损失计算。当应用于OpenVLA-OFT模型时，SimpleVLA-RL在LIBERO基准上达到最先进性能，并通过我们提出的探索增强策略，在RoboTwin 1.0和2.0环境中甚至超越了$\\pi_0$基线。该框架不仅降低了对大规模数据的依赖并实现了强泛化能力，更在真实任务中显著优于SFT方法。此外，我们在RL训练过程中发现了一种名为\"pushcut\"的新现象——智能体能发现训练历史中未曾出现的新行为模式。项目地址：https://github.com/PRIME-RL/SimpleVLA-RL\n\n（注：专业术语说明：\n1. VLA：视觉-语言-动作多模态模型\n2. SFT：监督微调（Supervised Fine-Tuning）\n3. RL：强化学习（Reinforcement Learning）\n4. LRM：大型推理模型（Large Reasoning Models）\n5. SoTA：最先进水平（State-of-the-Art）\n6. $\\pi_0$：强化学习中的初始策略基线\n7. pushcut：本文发现的新型训练现象，暂译为\"推切现象\"）"
    },
    {
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
        "url": "http://arxiv.org/abs/2509.09675v1",
        "pub_date": "2025-09-11",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
        "translated": "【论文核心思想】  \n针对强化学习可验证奖励框架（RLVR）中存在的探索不足、早熟收敛和熵崩溃问题，本研究提出好奇心驱动探索（CDE）框架，通过智能体自身的内在好奇心信号引导探索过程。该方法从行动者和评论者双视角量化好奇心：行动者侧采用生成响应的困惑度，评论者侧利用多头架构的价值估计方差，两者共同构成探索奖励项。理论分析表明，行动者奖励项能有效抑制过度自信错误并提升正确答案的多样性，而评论者奖励项与RL中经典的基于计数的探索奖励具有理论等价性。在AIME基准测试中，该方法较GRPO/PPO标准RLVR实现约3个百分点的性能提升，同时揭示了RLVR中存在的校准崩溃机制。\n\n【关键技术贡献】  \n1. 提出双路径好奇心量化机制：  \n   - 行动者路径：基于生成响应的困惑度（perplexity）衡量不确定性  \n   - 评论者路径：通过多头价值网络输出的方差表征认知分歧  \n2. 建立理论连接：  \n   - 证明评论者方差奖励与基于计数的探索方法数学等价  \n   - 揭示行动者困惑度奖励对过度自信的惩罚机制和答案多样性的促进效应  \n3. 实证效果：  \n   - 在AIME基准上显著提升RLVR性能  \n   - 首次发现并解析RLVR中的校准崩溃现象  \n\n【学术价值】  \n本研究通过引入受人类认知启发的内在好奇心机制，有效解决了大语言模型在强化学习中的探索-利用权衡难题，为理解LLM失败模式提供了新的理论视角，同时为多模态大模型的探索策略设计提供了可迁移的框架。"
    },
    {
        "title": "Steering MoE LLMs via Expert (De)Activation",
        "url": "http://arxiv.org/abs/2509.09660v1",
        "pub_date": "2025-09-11",
        "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.",
        "translated": "专家混合（MoE）架构通过将每个令牌路由至特定的前馈网络（FFN）专家子集来实现大语言模型的高效计算。本文提出SteerMoE框架，通过检测和控制行为关联专家来实现对MoE模型的定向调控。我们的检测方法能够识别在具有对立行为特征的配对输入中呈现显著激活差异的专家。通过在推理过程中选择性（解）激活此类专家，我们无需重新训练或修改权重即可控制模型的忠实性和安全性等行为特征。在11个基准测试和6个大语言模型上的实验表明，我们的调控方法最高可提升20%的安全性指标和27%的忠实性指标。在对抗攻击模式下，该方法单独使用可降低41%的安全性指标，与现有越狱方法结合时甚至能完全突破安全防护（-100%），暴露出专家网络中隐藏的\"对齐伪装\"新维度。\n\n（注：译文采用以下专业术语处理：\n- Mixture-of-Experts: 专家混合架构\n- routing: 路由\n- Feed-Forward Networks: 前馈网络\n- activation patterns: 激活模式\n- faithfulness: 忠实性\n- jailbreak methods: 越狱方法\n- alignment faking: 对齐伪装\n严格保持技术术语准确性，同时通过\"对立行为特征\"\"显著激活差异\"等表述确保学术文本的精确性，最后使用破折号补充说明-100%的具体含义，符合中文学术表达规范。）"
    },
    {
        "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations",
        "url": "http://arxiv.org/abs/2509.09651v1",
        "pub_date": "2025-09-11",
        "summary": "We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.",
        "translated": "我们针对无线电法规这一法律敏感且高风险的领域展开问答系统研究。提出了一种电信领域专用的检索增强生成（RAG）流程，并基于权威资料通过自动化筛选和人工验证构建了该领域首个多选评估数据集。为评估检索质量，我们定义了领域特异性检索指标，在该指标下我们的检索器达到约97%的准确率。除检索性能外，该方法在所有测试模型上均持续提升生成准确率。特别值得注意的是，当未经结构化检索直接插入文档时，GPT-4o仅获得边际增益（低于1%），而应用我们的流程可实现近12%的相对提升。这些发现表明，经过精准定位的基准确立既提供了简单而强大的基线，也为法规问答提供了有效的领域特异性解决方案。所有代码、评估脚本及衍生的问答数据集已开源：https://github.com/Zakaria010/Radio-RAG。\n\n（注：根据学术规范，对原文中\"grounding\"的翻译采用\"基准确立\"这一符合计算机领域术语的译法，其在此语境中指通过检索增强为模型提供准确的事实依据）"
    },
    {
        "title": "All for One: LLMs Solve Mental Math at the Last Token With Information\n  Transferred From Other Tokens",
        "url": "http://arxiv.org/abs/2509.09650v1",
        "pub_date": "2025-09-11",
        "summary": "Large language models (LLMs) demonstrate proficiency across numerous computational tasks, yet their inner workings remain unclear. In theory, the combination of causal self-attention and multilayer perceptron layers allows every token to access and compute information based on all preceding tokens. In practice, to what extent are such operations present? In this paper, on mental math tasks (i.e., direct math calculation via next-token prediction without explicit reasoning), we investigate this question in three steps: inhibiting input-specific token computations in the initial layers, restricting the routes of information transfer across token positions in the next few layers, and forcing all computation to happen at the last token in the remaining layers. With two proposed techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with high accuracy on a wide variety of mental math tasks, where meaningful computation occurs very late (in terms of layer depth) and only at the last token, which receives information of other tokens in few specific middle layers. Experiments on a variety of models and arithmetic expressions show that this subgraph is sufficient and necessary for high model performance, transfers across different models, and works on a variety of input styles. Ablations on different CAMA and ABP alternatives reveal their unique advantages over other methods, which may be of independent interest.",
        "translated": "大型语言模型（LLMs）在众多计算任务中展现出卓越能力，但其内部工作机制仍不明确。理论上，因果自注意力机制与多层感知器的结合使得每个词元都能访问并基于所有前序词元进行信息计算。但在实际运行中，此类操作究竟以何种程度存在？本文通过心算任务（即通过下一词元预测直接进行数学计算，无需显式推理）分三步探究该问题：抑制初始层中针对特定输入的词元计算，限制后续若干层中跨词元位置的信息传递路径，以及在剩余层中强制所有计算集中于末位词元。通过提出的两种技术——上下文感知均值消融（CAMA）和基于注意力的窥探（ABP）——我们识别出一个\"万为一\"子图（AF1），该结构在多种心算任务中均呈现高精度，其显著特征在于：有意义计算发生时间极晚（就网络深度而言），且仅发生于末位词元；该词元通过少数特定中间层接收其他词元的信息。在不同模型和算术表达式上的实验表明，该子图是实现高模型性能的充分必要条件，具有跨模型迁移能力，且适用于多种输入格式。对CAMA与ABP替代方案的消融实验揭示了二者相较于其他方法的独特优势，这一发现可能具有独立研究价值。\n\n（注：译文严格遵循以下技术要点：\n1. \"mental math\"译为专业术语\"心算\"\n2. \"next-token prediction\"保留技术概念译为\"下一词元预测\"\n3. \"All-for-One subgraph\"采用意译\"万为一子图\"并保留英文缩写AF1\n4. \"Context-Aware Mean Ablation\"等专业术语保持英文缩写并给出完整中文译名\n5. 复杂句式按中文习惯拆分重组，如将原文三个步骤的英文分词结构转换为中文分号并列结构\n6. 保持学术论文的客观表述风格，避免口语化表达）"
    },
    {
        "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for\n  Low-Latency Zero-Shot Text-To-Speech",
        "url": "http://arxiv.org/abs/2509.09631v1",
        "pub_date": "2025-09-11",
        "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
        "translated": "零样本文本到语音合成（Zero-shot TTS）旨在仅通过短时参考样本合成模仿未见说话人声音的高质量语音，这不仅需要说话人自适应，还需精确建模韵律属性。基于语言模型、扩散模型和流匹配的最新方法在零样本TTS中展现出潜力，但仍存在推理速度慢和重复伪影的问题。离散编解码表示已被广泛用于语音合成，近期研究开始探索纯离散场景下的扩散模型，这揭示了离散生成模型在语音合成中的潜力。然而现有流匹配方法通常将离散标记嵌入连续空间并应用连续流匹配，可能未能充分发挥离散表示的优势。\n\n为解决这些挑战，我们提出了DiFlow-TTS——据我们所知，这是首个探索纯离散流匹配（Discrete Flow Matching）的语音合成模型。该模型通过紧凑统一的架构显式建模分解的语音属性，利用上下文学习机制，以文本内容及从参考语音中提取的韵律和声学属性为条件，实现零样本场景下的有效属性克隆。此外，模型采用具有独立韵律头和声学头的分解流预测机制，能够学习特定方面的分布。实验结果表明，DiFlow-TTS在自然度、韵律表现、说话人风格保持和能量控制等关键指标上均取得优异表现，同时保持紧凑的模型规模，推理延迟显著降低，生成速度较现有最新基线提升达25.8倍。\n\n（注：专业术语说明：\n1. Zero-shot Text-to-Speech：零样本文本转语音\n2. prosodic attributes：韵律属性\n3. diffusion models：扩散模型\n4. flow matching：流匹配\n5. discrete codec representations：离散编解码表示\n6. in-context learning：上下文学习\n7. acoustic attributes：声学属性\n8. factorized flow prediction：分解流预测）"
    },
    {
        "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems",
        "url": "http://arxiv.org/abs/2509.09629v1",
        "pub_date": "2025-09-11",
        "summary": "The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.",
        "translated": "大型语言模型（LLMs）的发展推动了多智能体系统的构建，通过将职责分配给专业化智能体（如负责生成子目标的规划智能体和执行工具使用操作的落地智能体）来解决复杂任务。现有方法通常独立微调各智能体，导致其能力存在差距且协作效率低下。为此，我们提出MOAT——一种多智能体联合对齐调优框架，通过迭代对齐机制提升智能体协作能力。MOAT交替执行两个关键阶段：（1）规划智能体对齐：优化规划智能体以生成能更好指导落地智能体的子目标序列；（2）落地智能体改进：利用智能体自身生成的多样化子目标-动作对微调落地智能体，增强其泛化能力。理论分析证明MOAT能确保训练过程具有非递减且逐步收敛的特性。在六个基准测试上的实验表明，MOAT优于现有最先进基线方法，在已知任务和未知任务上分别实现了3.1%和4.4%的平均性能提升。\n\n（注：专业术语说明：\n1. \"grounding agent\"译为\"落地智能体\"，强调其将抽象规划转化为具体执行动作的特性；\n2. \"held-in/held-out tasks\"采用\"已知任务/未知任务\"的译法，体现模型对训练见过/未见任务的泛化能力；\n3. \"non-decreasing and progressively convergent\"译为\"非递减且逐步收敛\"，保持数学描述的精确性；\n4. 保留英文缩写MOAT，符合学术惯例）"
    },
    {
        "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death\n  Determination",
        "url": "http://arxiv.org/abs/2509.09602v1",
        "pub_date": "2025-09-11",
        "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings.",
        "translated": "言语尸检（VA）是在缺乏医疗死亡证明的资源有限地区进行死因推断的重要工具。本研究提出LA-VA概念验证流程，通过结合大语言模型（LLM）与传统算法方法及基于嵌入的分类技术，显著提升死因预测准确率。基于人口健康指标研究联盟（PHMRC）数据集的三类年龄组数据（成人7,580例；儿童1,960例；新生儿2,438例），我们评估了多种方法：GPT-5预测、LCVA基线模型、文本嵌入分类以及元学习集成模型。实验结果表明，GPT-5在测试集上取得最优单体性能，平均准确率分别达到成人48.6%、儿童50.5%、新生儿53.5%，较传统统计机器学习基线方法提升5-10%。研究发现表明，现成的轻量级LLM辅助方法可显著提升言语尸检准确率，这对资源匮乏地区的全球健康监测具有重要实践价值。\n\n（注：根据学术规范，对GPT-5的表述保留模型原名，因原文未明确说明是否为假设模型或笔误，若实际应为GPT-3.5/4需根据上下文调整）"
    },
    {
        "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
        "url": "http://arxiv.org/abs/2509.09593v1",
        "pub_date": "2025-09-11",
        "summary": "The versatility of Large Language Models (LLMs) in natural language understanding has made them increasingly popular in mental health research. While many studies explore LLMs' capabilities in emotion recognition, a critical gap remains in evaluating whether LLMs align with human emotions at a fine-grained level. Existing research typically focuses on classifying emotions into predefined, limited categories, overlooking more nuanced expressions. To address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit communities featuring 251 fine-grained, self-disclosed emotion labels. Our comprehensive evaluation framework examines predicted emotion terms and decomposes them into eight basic emotions using established emotion theories, enabling a fine-grained comparison. Systematic testing of prevalent LLMs under various prompt settings reveals that accurately predicting emotions that align with human self-disclosed emotions remains challenging. Qualitative analysis further shows that while certain LLMs generate emotion terms consistent with established emotion theories and definitions, they sometimes fail to capture contextual cues as effectively as human self-disclosures. These findings highlight the limitations of LLMs in fine-grained emotion alignment and offer insights for future research aimed at enhancing their contextual understanding.",
        "translated": "大型语言模型（LLMs）在自然语言理解方面的多功能性使其在心理健康研究中日益受到关注。尽管许多研究探索了LLMs在情绪识别方面的能力，但在评估这些模型是否能在细粒度层面与人类情绪保持一致方面仍存在关键空白。现有研究通常将情绪分类为预定义的有限类别，忽略了更细微的情绪表达。为填补这一空白，我们推出了EXPRESS——一个从Reddit社区精选的基准数据集，包含251个细粒度的自我披露情绪标签。我们通过综合评估框架，不仅分析预测的情绪术语，还借助成熟的情绪理论将其分解为八种基本情绪，从而实现细粒度对比。在不同提示设置下对主流LLMs进行的系统测试表明，准确预测与人类自我披露情绪相符的情感仍然具有挑战性。定性分析进一步发现，虽然某些LLMs能生成符合既有情绪理论和定义的情绪术语，但它们有时难以像人类自我披露那样有效捕捉上下文线索。这些发现凸显了LLMs在细粒度情绪对齐方面的局限性，并为未来提升其上下文理解能力的研究提供了重要参考。"
    },
    {
        "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role\n  of Personality Detection in Matchmaking",
        "url": "http://arxiv.org/abs/2509.09583v1",
        "pub_date": "2025-09-11",
        "summary": "Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.",
        "translated": "社交互动是学习过程中的重要组成部分，然而在线课程环境为自然社交群体的形成设置了障碍。SAMI通过促进学生间的连接提供了一种解决方案，但其有效性受限于不完善的心理理论（Theory of Mind），导致难以构建有效的学生心智模型。其中一个关键局限是无法推断用户性格，而性格特征可能影响推荐结果的关联性。为此，我们提出一种人格检测模型，利用GPT的零样本能力从在线课程中常见的论坛自我介绍帖子中推断大五人格特质。通过与传统基准模型对比，验证了该方法在此任务中的有效性。进一步地，我们将该模型集成至SAMI基于实体的匹配系统中，实现融合人格特征的社交推荐。初步集成结果表明，人格特质能够对现有匹配因素形成有效补充，但仍需进一步评估以确定其对学习者参与度和匹配质量的完整影响。\n\n（注：专业术语说明：\n1. Theory of Mind：译为\"心理理论\"，指系统理解他人心理状态的能力\n2. Big-Five personality traits：采用心理学界通用译法\"大五人格特质\"，包含开放性、尽责性、外向性、宜人性和神经质性五个维度\n3. zero-shot capability：译为\"零样本能力\"，指模型无需特定训练即可处理新任务的能力\n4. entity-based matchmaking system：译为\"基于实体的匹配系统\"，强调以用户实体特征为核心的匹配机制）"
    },
    {
        "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance\n  NLP (2022-2025)",
        "url": "http://arxiv.org/abs/2509.09544v1",
        "pub_date": "2025-09-11",
        "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling new tasks and driving a proliferation of datasets and diversification of data sources. Yet, this transformation has outpaced traditional surveys. In this paper, we present MetaGraph, a generalizable methodology for extracting knowledge graphs from scientific literature and analyzing them to obtain a structured, queryable view of research trends. We define an ontology for financial NLP research and apply an LLM-based extraction pipeline to 681 papers (2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals three key phases: early LLM adoption and task/dataset innovation; critical reflection on LLM limitations; and growing integration of peripheral techniques into modular systems. This structured view offers both practitioners and researchers a clear understanding of how financial NLP has evolved - highlighting emerging trends, shifting priorities, and methodological shifts-while also demonstrating a reusable approach for mapping scientific progress in other domains.",
        "translated": "大型语言模型（LLMs）正在快速重塑金融自然语言处理领域，不仅催生了新型任务，还推动了数据集的激增与数据来源的多元化。然而，这种变革速度已超越传统文献综述的追踪能力。本文提出MetaGraph——一种可泛化的方法论，能够从科学文献中提取知识图谱，并通过分析获得结构化、可查询的研究趋势视图。我们定义了金融自然语言处理研究的本体框架，并基于LLM构建提取管道，对681篇文献（2022-2025年）进行大规模数据驱动分析。MetaGraph揭示了三个关键发展阶段：早期LLM采用与任务/数据集创新阶段；对LLM局限性的批判性反思阶段；以及外围技术逐渐融入模块化系统的整合阶段。这种结构化视角既为从业者和研究者提供了对金融NLP演进路径的清晰认知——突出新兴趋势、优先级转变与方法论变迁——同时也展示了可用于其他学科领域科学进展图谱构建的可复用方法。\n\n（注：译文严格遵循以下技术处理原则：\n1. 专业术语准确对应：\"ontology\"译作\"本体框架\"，\"extraction pipeline\"译为\"提取管道\"\n2. 复杂句式重构：将英文长句拆解为符合中文表达习惯的短句结构\n3. 概念显化处理：\"peripheral techniques\"意译为\"外围技术\"而非字面直译\n4. 学术语境保持：使用\"方法论\"\"可泛化\"\"模块化系统\"等符合学术论文风格的表述\n5. 时间跨度处理：保留\"2022-2025\"原格式符合学术文献惯例）"
    },
    {
        "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning\n  and Label Distribution Learning",
        "url": "http://arxiv.org/abs/2509.09524v1",
        "pub_date": "2025-09-11",
        "summary": "This system paper presents the DeMeVa team's approaches to the third edition of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et al., 2025). We explore two directions: in-context learning (ICL) with large language models, where we compare example sampling strategies; and label distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we evaluate several fine-tuning methods. Our contributions are twofold: (1) we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance; and (2) we argue that LDL methods are promising for soft label predictions and merit further exploration by the perspectivist community.",
        "translated": "本系统论文介绍了DeMeVa团队针对第三届\"学习中的分歧\"共享任务（LeWiDi 2025；Leonardelli等人，2025）提出的解决方案。我们探索了两个研究方向：基于大语言模型的上下文学习（ICL）方法——重点比较了不同的示例采样策略；以及基于RoBERTa（Liu等人，2019b）的标签分布学习（LDL）方法——评估了多种微调技术。我们的主要贡献包括：（1）证明了ICL能够有效预测标注者特异性注释（视角主义标注），且将这些预测聚合为软标签后可获得具有竞争力的性能；（2）论证了LDL方法在软标签预测方面的潜力，值得视角主义研究社区进一步探索。"
    },
    {
        "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual\n  Relatedness and Knowledge Graphs",
        "url": "http://arxiv.org/abs/2509.09522v1",
        "pub_date": "2025-09-11",
        "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between texts that extend beyond superficial lexical similarity. In this study, we investigate STR in the context of job title matching - a key challenge in resume recommendation systems, where overlapping terms are often limited or misleading. We introduce a self-supervised hybrid architecture that combines dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to improve both semantic alignment and explainability. Unlike previous work that evaluated models on aggregate performance, our approach emphasizes data stratification by partitioning the STR score continuum into distinct regions: low, medium, and high semantic relatedness. This stratified evaluation enables a fine-grained analysis of model performance across semantically meaningful subspaces. We evaluate several embedding models, both with and without KG integration via graph neural networks. The results show that fine-tuned SBERT models augmented with KGs produce consistent improvements in the high-STR region, where the RMSE is reduced by 25% over strong baselines. Our findings highlight not only the benefits of combining KGs with text embeddings, but also the importance of regional performance analysis in understanding model behavior. This granular approach reveals strengths and weaknesses hidden by global metrics, and supports more targeted model selection for use in Human Resources (HR) systems and applications where fairness, explainability, and contextual matching are essential.",
        "translated": "语义文本关联性（Semantic Textual Relatedness, STR）能够捕捉文本间超越表层词汇相似度的深层语义关系。本研究针对简历推荐系统中的关键挑战——职位名称匹配问题展开STR分析，该场景中文本间的重合术语往往有限且具有误导性。我们提出一种自监督混合架构，通过将稠密句子嵌入与领域知识图谱（KG）相结合，同步提升语义对齐能力和可解释性。与既往仅关注整体性能评估的研究不同，本方法采用数据分层策略，将STR分数连续体划分为低、中、高三个语义关联区间。这种分层评估机制实现了对语义子空间模型性能的细粒度解析。\n\n我们评估了多种嵌入模型（包含结合图神经网络的KG增强版本与独立版本）。实验结果表明：经过微调的SBERT模型在集成KG后，在高STR区域持续表现出性能提升，其均方根误差（RMSE）较基线模型降低25%。研究发现不仅验证了知识图谱与文本嵌入结合的优势，更揭示了区域性能分析对理解模型行为的重要性。这种精细化分析方法能够发现被全局指标掩盖的模型优缺点，为人力资源系统等对公平性、可解释性和上下文匹配要求严格的应用场景提供更具针对性的模型选择依据。\n\n（注：专业术语说明：\n- STR: 语义文本关联性\n- KG: 知识图谱\n- SBERT:  Sentence-BERT句子嵌入模型\n- RMSE: 均方根误差\n- 图神经网络: Graph Neural Networks\n- 自监督: self-supervised\n- 语义对齐: semantic alignment）"
    },
    {
        "title": "Mitigating Language Barriers in Education: Developing Multilingual\n  Digital Learning Materials with Machine Translation",
        "url": "http://arxiv.org/abs/2509.09473v1",
        "pub_date": "2025-09-11",
        "summary": "The EdUKate project combines digital education, linguistics, translation studies, and machine translation to develop multilingual learning materials for Czech primary and secondary schools. Launched through collaboration between a major Czech academic institution and the country's largest educational publisher, the project is aimed at translating up to 9,000 multimodal interactive exercises from Czech into Ukrainian, English, and German for an educational web portal. It emphasizes the development and evaluation of a direct Czech-Ukrainian machine translation system tailored to the educational domain, with special attention to processing formatted content such as XML and PDF and handling technical and scientific terminology. We present findings from an initial survey of Czech teachers regarding the needs of non-Czech-speaking students and describe the system's evaluation and implementation on the web portal. All resulting applications are freely available to students, educators, and researchers.",
        "translated": "EdUKate项目融合数字教育、语言学、翻译研究与机器翻译技术，致力于为捷克中小学开发多语言学习材料。该项目由捷克顶尖学术机构与国内最大教育出版商联合启动，旨在将多达9000个多模态交互式习题从捷克语翻译成乌克兰语、英语和德语，并集成至教育网络门户。项目重点开发并评估针对教育领域定制的捷克语-乌克兰语直接机器翻译系统，特别关注XML和PDF等格式化内容的处理技术，以及科技专业术语的翻译方案。我们通过初步调研呈现了捷克教师对非母语学生需求的评估结果，并详细说明了该系统在网络门户中的实施与评估过程。所有最终应用均向学生、教育工作者及研究人员免费开放。\n\n（注：根据学术论文摘要的规范要求，译文采用客观严谨的学术表述风格，确保以下专业要素的准确传达：\n1. 跨学科特性：数字教育/语言学/翻译研究的学科交叉性\n2. 技术细节：明确机器翻译系统类型（直接翻译）、处理格式（XML/PDF）、术语处理等关键技术点\n3. 项目规模：量化数据（9000个练习）和语言对（4种语言）的精确表述\n4. 学术价值：强调系统评估、需求调研等研究环节\n5. 社会效益：突出免费开放的服务模式）"
    },
    {
        "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.09438v1",
        "pub_date": "2025-09-11",
        "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence elicitation is a prominent approach to AI safety in high-stakes applications, such as healthcare and finance. Existing methods either require expensive computational overhead or suffer from poor calibration, making them impractical and unreliable for real-world deployment. In this work, we propose GrACE, a Generative Approach to Confidence Elicitation that enables scalable and reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in which the model expresses confidence by the similarity between the last hidden state and the embedding of a special token appended to the vocabulary, in real-time. We fine-tune the model for calibrating the confidence with calibration targets associated with accuracy. Experiments with three LLMs and two benchmark datasets show that the confidence produced by GrACE achieves the best discriminative capacity and calibration on open-ended generation tasks, outperforming six competing methods without resorting to additional sampling or an auxiliary model. Moreover, we propose two strategies for improving test-time scaling based on confidence induced by GrACE. Experimental results show that using GrACE not only improves the accuracy of the final decision but also significantly reduces the number of required samples in the test-time scaling scheme, indicating the potential of GrACE as a practical solution for deploying LLMs with scalable, reliable, and real-time confidence estimation.",
        "translated": "在医疗健康与金融等高风险应用领域，通过置信度激发来评估大语言模型（LLUMs）的可靠性是人工智能安全领域的重要研究方向。现有方法要么需要高昂的计算开销，要么存在校准效果不佳的问题，导致其在实际部署中缺乏实用性和可靠性。本研究提出GrACE（生成式置信度激发方法），一种可实现规模化、高可靠性置信度激发的新方案。GrACE采用创新机制：模型通过实时计算最后一层隐藏状态与词表中特殊标记嵌入的相似度来表达置信度。我们通过关联准确率的校准目标对模型进行微调以实现置信度校准。在三个大语言模型和两个基准数据集上的实验表明：在开放式生成任务中，GrACE产生的置信度在判别能力和校准效果方面均优于六种对比方法，且无需额外采样或辅助模型。此外，我们提出两种基于GrACE置信度的测试时缩放优化策略。实验结果表明，使用GrACE不仅能提升最终决策的准确率，还能显著减少测试时缩放方案所需的样本量，这证明GrACE具备作为实用解决方案的潜力——能够以可扩展、高可靠性且实时的方式部署具备置信度评估能力的大语言模型。\n\n（注：本文翻译严格遵循以下技术规范：\n1. 专业术语统一：\"confidence elicitation\"译为\"置信度激发\"，\"hidden state\"译为\"隐藏状态\"，\"fine-tune\"译为\"微调\"\n2. 技术概念准确处理：\"test-time scaling\"译为\"测试时缩放\"，\"discriminative capacity\"译为\"判别能力\"\n3. 长难句拆分重构：将原文复合句按中文表达习惯分解为多个短句\n4. 被动语态转换：将英文被动结构转换为中文主动表述\n5. 逻辑连接显性化：通过\"不仅...还能...\"等连接词明确技术优势的递进关系）"
    },
    {
        "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of\n  Self-Generated Counterfactual Explanations",
        "url": "http://arxiv.org/abs/2509.09396v1",
        "pub_date": "2025-09-11",
        "summary": "To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",
        "translated": "为实现与人类的有效协作，语言模型需具备用自然语言解释其决策的能力。本研究聚焦于一类特殊的自我解释形式：自生成反事实解释（SCEs），即模型通过修改输入内容来阐释为何原本会做出不同预测。我们评估了大语言模型能否生成既有效（达成预期结果）又最小化（仅进行必要修改）的SCEs。实验发现：当被要求生成反事实解释时，大语言模型通常能产生有效的SCEs，但远未达到最小化标准，这导致其难以揭示模型的决策机制。更令人担忧的是，当被要求生成最小化反事实时，模型往往进行过度细微的编辑，以致无法真正改变预测结果。这种有效性与最小化之间的权衡关系在多个大语言模型、数据集和评估设置中均保持一致。我们的研究结果表明，SCEs往好了说是一种低效的可解释性工具，往坏了说可能对模型行为产生误导性解读。在高风险场景中部署大语言模型时，必须考虑不可靠的自我解释对下游决策的影响。代码已开源：https://github.com/HarryMayne/SCEs。\n\n（注：翻译过程中对以下要点进行了专业处理：\n1. \"self-generated counterfactual explanations (SCEs)\" 译为专业术语\"自生成反事实解释（SCEs）\"\n2. 保持\"validity\"和\"minimality\"在机器学习可解释性领域的标准译法\"有效性\"和\"最小化\"\n3. \"high-stakes settings\"译为\"高风险场景\"以符合学术文献表述\n4. 采用中文长句拆分技巧处理英语复合句结构\n5. 保留技术术语一致性（如LLMs统一译为\"大语言模型\"））"
    },
    {
        "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs",
        "url": "http://arxiv.org/abs/2509.09388v1",
        "pub_date": "2025-09-11",
        "summary": "We revisit hierarchical bracketing encodings from a practical perspective in the context of dependency graph parsing. The approach encodes graphs as sequences, enabling linear-time parsing with $n$ tagging actions, and still representing reentrancies, cycles, and empty nodes. Compared to existing graph linearizations, this representation substantially reduces the label space while preserving structural information. We evaluate it on a multilingual and multi-formalism benchmark, showing competitive results and consistent improvements over other methods in exact match accuracy.",
        "translated": "我们以实用角度重新审视了依存图解析中的层次括号编码方法。该方法将图结构编码为序列形式，仅需n次标注操作即可实现线性时间解析，同时仍能表示重入边、循环结构和空节点。与现有图线性化方法相比，该表征在保持结构信息完整的前提下显著缩小了标签空间。我们在多语言多形式主义基准测试中进行评估，结果显示该方法在精确匹配准确率上取得了具有竞争力的结果，并持续优于其他方法。\n\n（注：专业术语说明：\n1. reentrancies：重入边，指图中多个边指向同一节点的现象\n2. cycles：循环结构，指图中形成环路的依赖关系\n3. linear-time parsing：线性时间解析，指时间复杂度与输入长度成线性关系\n4. multi-formalism：多形式主义，指支持不同语法理论体系\n5. exact match accuracy：精确匹配准确率，指整个解析结构完全正确的评估指标）"
    },
    {
        "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive\n  Science Theory and NLP Research",
        "url": "http://arxiv.org/abs/2509.09381v1",
        "pub_date": "2025-09-11",
        "summary": "Analogical reasoning is an essential aspect of human cognition. In this paper, we summarize key theory about the processes underlying analogical reasoning from the cognitive science literature and relate it to current research in natural language processing. While these processes can be easily linked to concepts in NLP, they are generally not viewed through a cognitive lens. Furthermore, we show how these notions are relevant for several major challenges in NLP research, not directly related to analogy solving. This may guide researchers to better optimize relational understanding in text, as opposed to relying heavily on entity-level similarity.",
        "translated": "类比推理是人类认知的重要组成部分。本文从认知科学文献中总结了关于类比推理过程的核心理论，并将其与自然语言处理领域的研究现状相联系。尽管这些认知过程能够与自然语言处理中的概念建立明确关联，但现有研究通常未从认知视角进行解读。进一步地，我们论证了这些理论认知如何适用于自然语言处理研究中若干与类比求解不直接相关的重要挑战。这将引导研究者优化文本关系理解机制，而非过度依赖实体层面的相似性匹配。"
    },
    {
        "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
        "url": "http://arxiv.org/abs/2509.09676v1",
        "pub_date": "2025-09-11",
        "summary": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
        "translated": "在空间智能领域，空间重建与世界探索均取得了显著进展。然而当前模型的可扩展性和现实世界保真度仍受制于大规模高质量训练数据的稀缺性。虽然现有若干数据集提供相机位姿信息，但其在规模、多样性和标注丰富度方面存在局限，尤其缺乏具有真实相机运动标注的现实动态场景数据。为此，我们构建了\\textbf{SpatialVID}数据集——一个包含海量真实场景视频的语料库，涵盖多样化场景、相机运动模式以及密集的3D标注（包括逐帧相机位姿、深度信息和运动指令）。具体而言，我们收集超过21,000小时的原始视频，通过分级过滤流程处理形成270万个视频片段，总计7,089小时动态内容。后续标注流程为这些片段注入详细的空间与语义信息，包括相机位姿、深度图、动态遮罩、结构化描述文本和序列化运动指令。对SpatialVID的数据统计分析显示，其丰富性和多样性将直接促进模型泛化能力与性能提升，使之成为视频与三维视觉研究领域的重要资源。"
    },
    {
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "url": "http://arxiv.org/abs/2509.09672v1",
        "pub_date": "2025-09-11",
        "summary": "Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
        "translated": "在生成模型中，扩散模型因其训练目标存在闭式最优最小化器（通常称为最优去噪器）而独具研究价值。然而，使用该最优去噪器的扩散过程仅能复现训练集中的图像，无法捕捉深度扩散模型的实际行为。近期研究尝试刻画最优去噪器与深度扩散模型之间的性能差异，提出了无需训练的分析模型，其生成图像与训练后的UNet输出结果相似。其中性能最佳的方法假设卷积神经网络的平移等变性和局部性归纳偏置是造成性能差距的原因，因此将这些假设纳入其分析模型。本文通过证据表明，深度扩散模型中的局部性源于图像数据集的统计特性，而非卷积神经网络的归纳偏置。具体而言，我们证明参数化线性最优去噪器展现出与深度神经去噪器相似的局部特性，并通过理论与实验验证这种局部性直接源自自然图像数据集中存在的像素相关性。最终，基于这些发现，我们构建了一个分析型去噪器，其与深度扩散模型预测得分的匹配度优于先前专家构建的替代方案。"
    },
    {
        "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration",
        "url": "http://arxiv.org/abs/2509.09671v1",
        "pub_date": "2025-09-11",
        "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale, contact-rich demonstrations and hold promise for scaling dexterous robotic manipulation. Yet demonstration inaccuracies and embodiment gaps between human and robot hands limit the straightforward use of these data. Existing methods adopt a three-stage workflow, including retargeting, tracking, and residual correction, which often leaves demonstrations underused and compound errors across stages. We introduce Dexplore, a unified single-loop optimization that jointly performs retargeting and tracking to learn robot control policies directly from MoCap at scale. Rather than treating demonstrations as ground truth, we use them as soft guidance. From raw trajectories, we derive adaptive spatial scopes, and train with reinforcement learning to keep the policy in-scope while minimizing control effort and accomplishing the task. This unified formulation preserves demonstration intent, enables robot-specific strategies to emerge, improves robustness to noise, and scales to large demonstration corpora. We distill the scaled tracking policy into a vision-based, skill-conditioned generative controller that encodes diverse manipulation skills in a rich latent representation, supporting generalization across objects and real-world deployment. Taken together, these contributions position Dexplore as a principled bridge that transforms imperfect demonstrations into effective training signals for dexterous manipulation.",
        "translated": "手-物体运动捕捉（MoCap）数据库提供了大规模、高接触度的演示数据，为扩展灵巧机器人操控能力带来了希望。然而演示数据的不精确性以及人手与机器人手之间的本体差异限制了这些数据的直接使用。现有方法采用三阶段工作流程（包括重定向、跟踪和残差校正），往往导致演示数据利用不足且误差在多阶段中累积。我们提出Dexplore方法，通过统一的单循环优化联合执行重定向与跟踪，直接从大规模运动捕捉数据中学习机器人控制策略。该方法将演示数据视为软性指导而非绝对真值，从原始轨迹中推导出自适应空间范围，并通过强化学习训练策略使其在保持控制范围内最小化控制力并完成任务。这种统一框架既保留了演示意图，又催生了机器人专属策略，增强了对噪声的鲁棒性，并能扩展到大规模演示库。我们将规模化跟踪策略蒸馏为基于视觉的技能条件生成控制器，该控制器将多样化的操控技能编码为丰富的潜在表征，支持跨物体泛化和现实世界部署。这些贡献共同使Dexplore成为将不完美演示转化为灵巧操控有效训练信号的理论桥梁。\n\n（注：专业术语说明：\n1. motion-capture (MoCap) 译为\"运动捕捉\"\n2. retargeting 译为\"重定向\"（指将人体运动数据映射到机器人模型的过程）\n3. embodiment gaps 译为\"本体差异\"\n4. reinforcement learning 译为\"强化学习\"\n5. skill-conditioned generative controller 译为\"技能条件生成控制器\"\n6. latent representation 译为\"潜在表征\"\n译文严格保持技术准确性，同时符合中文学术表达规范。）"
    },
    {
        "title": "Geometric Neural Distance Fields for Learning Human Motion Priors",
        "url": "http://arxiv.org/abs/2509.09667v1",
        "pub_date": "2025-09-11",
        "summary": "We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to \"roll out\" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.",
        "translated": "我们提出了神经黎曼运动场（NRMF），这是一种新颖的3D生成式人体运动先验模型，能够实现鲁棒、时间一致且物理合理的3D运动恢复。与现有的基于VAE或扩散模型的方法不同，我们的高阶运动先验显式地将人体运动建模为对应姿态、过渡（速度）和加速度动力学的神经距离场（NDFs）的零水平集。我们的框架具有严谨的理论基础：NDFs构建在关节旋转、角速度及角加速度的乘积空间上，严格遵循底层关节结构的几何特性。我们还进一步引入了：（i）一种新颖的自适应步长混合算法，用于投影到合理运动集合；（ii）一种创新的几何积分器，在测试时优化和生成过程中实现真实运动轨迹的\"展开\"。实验结果表明显著且一致的性能提升：在AMASS数据集上训练的NRMF模型，能够出色地泛化到多种输入模态，并适应从去噪、运动插值到部分2D/3D观测数据拟合等多样化任务。"
    },
    {
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?",
        "url": "http://arxiv.org/abs/2509.09666v1",
        "pub_date": "2025-09-11",
        "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.",
        "translated": "本文提出了一种富有洞见的范式——通过自编码器视角进行理解：将图像理解视为编码器（I2T）把图像压缩为文本，将生成过程视为解码器（T2I）从文本重建图像。我们以重建保真度作为统一训练目标，强制理解与生成过程之间形成连贯的双向信息流，从而实现相互增益。为实现这一目标，我们提出了创新性统一多模态学习框架UAE。首先通过大规模长上下文图像描述数据对解码器进行预训练，以捕捉细粒度语义和复杂空间关系。随后提出基于强化学习（RL）的Unified-GRPO训练框架，包含三个阶段：（1）冷启动阶段通过语义重建损失温和初始化编码器和解码器；（2）\"生成促进理解\"阶段训练编码器生成信息丰富的描述文本，以最大化解码器的重建质量，从而增强其视觉理解能力；（3）\"理解辅助生成\"阶段优化解码器根据这些描述进行重建，迫使其利用每个细节，提升长上下文指令遵循能力和生成保真度。为评估模型性能，我们推出了首个专门评估统一多模态模型（UMM）融合程度的基准测试Unified-Bench。实验揭示了多模态学习领域的惊人发现：随着强化学习的推进，编码器能自主生成更具描述性的文本，而解码器同时展现出理解这些复杂描述的卓越能力，最终实现具有惊人保真度的图像重建。"
    },
    {
        "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2509.09658v1",
        "pub_date": "2025-09-11",
        "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a \"None of the above\" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench.",
        "translated": "多模态大语言模型（MLLMs）中的幻觉问题——即模型生成与输入图像不一致的内容——在现实应用中存在显著风险，可能引发从视觉问答错误信息到决策过程中的安全隐患。现有基准测试主要关注识别准确度，即评估模型能否在干扰项中选择正确答案，却忽视了可信AI同样关键的能力：识别所有给定选项均不正确的情况，这种行为反映的是认知谦逊（epistemic humility）。我们提出HumbleBench，这是一个专用于评估MLLMs在三种幻觉类型（物体、关系和属性）中拒绝看似合理但错误答案能力的新型基准测试平台。基于全景场景图数据集，我们利用细粒度场景图标注提取真实实体与关系，通过GPT-4-Turbo生成多项选择题，并经过严格人工筛选流程。每个问题均包含\"以上皆非\"选项，要求模型不仅能识别正确视觉信息，还需具备判断无有效答案的能力。我们在HumbleBench上评估了包括通用模型和专用推理模型在内的多种前沿MLLMs，并向社区分享了具有价值的研究发现与洞察。通过引入显式错误选项拒绝机制，HumbleBench填补了现有评估体系的关键空白，为安全关键场景中的MLLM可靠性提供了更真实的衡量标准。代码与数据集已开源，可通过https://github.com/maifoundations/HumbleBench 获取。\n\n（注：译文严格遵循以下技术处理原则：\n1. 专业术语准确对应：\"panoptic scene graph\"译为\"全景场景图\"，\"epistemic humility\"采用学界通用译法\"认知谦逊\"\n2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如对基准测试构建流程的描述\n3. 被动语态转化：将\"are released publicly\"等被动结构转换为中文主动式\"已开源\"\n4. 概念显性化处理：\"safety-critical settings\"译为\"安全关键场景\"以突出其重要性\n5. 逻辑连接优化：使用\"不仅...还需...\"等关联词强化技术要求的递进关系）"
    },
    {
        "title": "Mechanistic Learning with Guided Diffusion Models to Predict\n  Spatio-Temporal Brain Tumor Growth",
        "url": "http://arxiv.org/abs/2509.09610v1",
        "pub_date": "2025-09-11",
        "summary": "Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
        "translated": "预测脑肿瘤的时空演变对于指导神经肿瘤学的临床决策至关重要。我们提出了一种混合机制学习框架，将数学肿瘤生长模型与引导式去噪扩散隐式模型（DDIM）相结合，通过前期扫描数据合成解剖学上可行的未来MRI影像。该机制模型通过常微分方程组描述，能够捕捉包括放疗效应在内的肿瘤时序动态并预估未来肿瘤负荷。这些预测结果作为梯度引导DDIM的条件输入，确保生成的影像既符合预测生长规律又保留患者解剖特征。我们在BraTS成人和儿童胶质瘤数据集上训练模型，并在60个内部纵向采集的儿童弥漫性中线胶质瘤（DMG）轴位切片上进行评估。该框架基于空间相似度指标生成了具有高度真实性的随访扫描影像，同时通过第95百分位豪斯多夫距离证明，其生成的肿瘤生长概率图能有效捕捉临床相关的肿瘤生长范围与方向性。这种方法在数据有限场景下实现了基于生物学原理的图像生成，提供了兼顾机制先验的生成式时空预测能力。"
    },
    {
        "title": "Graph Alignment via Dual-Pass Spectral Encoding and Latent Space\n  Communication",
        "url": "http://arxiv.org/abs/2509.09597v1",
        "pub_date": "2025-09-11",
        "summary": "Graph alignment-the problem of identifying corresponding nodes across multiple graphs-is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations.",
        "translated": "图对齐——即识别多个图间对应节点的问题——是众多应用的基础。大多数现有无监督方法将节点特征嵌入潜在表示空间，以实现无需真实对应关系的跨图比较。但这些方法存在两个关键局限：基于GNN的嵌入因过度平滑导致节点区分度下降，以及结构噪声、特征异质性和训练不稳定性引发的潜在空间错位，最终导致节点对应关系不可靠。我们提出了一种新颖的图对齐框架，可同时增强节点区分度并强制潜在空间间的几何一致性。该方法采用双通道编码器，结合低通与高通谱滤波器生成兼具结构感知和高区分度的嵌入表示。针对潜在空间错位问题，我们引入几何感知功能映射模块，学习图嵌入间的双射等距变换，确保不同表征间几何关系的一致性。在图基准测试上的大量实验表明，我们的方法持续优于现有无监督对齐基线，对结构不一致性和具有挑战性的对齐场景展现出卓越的鲁棒性。此外，通过使用多样化预训练模型在视觉-语言基准上的综合评估表明，该框架能有效泛化至图领域之外，实现视觉与语言表征的无监督对齐。\n\n（注：译文严格遵循了以下技术要点：\n1. 专业术语准确：\"graph alignment\"译为\"图对齐\"，\"unsupervised methods\"译为\"无监督方法\"，\"GNN-based embeddings\"译为\"基于GNN的嵌入\"\n2. 技术概念完整保留：\"spectral filters\"译为\"谱滤波器\"，\"bijective and isometric transformations\"译为\"双射等距变换\"\n3. 学术表达规范：采用\"表征\"\"鲁棒性\"\"泛化\"等符合计算机领域论文规范的表述\n4. 长难句处理：将原文复合句按中文习惯拆分为多个短句，如对两个关键局限的并列说明处理\n5. 逻辑关系显化：通过\"即\"\"针对\"\"此外\"等连接词明确技术逻辑链条）"
    },
    {
        "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
        "url": "http://arxiv.org/abs/2509.09595v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
        "translated": "近期音频驱动虚拟形象视频生成技术显著提升了视听真实感，但现有方法仅将指令条件视为声学或视觉线索驱动的低级追踪，未能对指令传达的交流意图进行建模。这一局限影响了生成内容的叙事连贯性与角色表现力。为此，我们提出Kling-Avatar——一种创新级联框架，通过统一多模态指令理解与超写实人像生成来解决该问题。\n\n我们的方法采用两阶段流程：第一阶段设计多模态大语言模型（MLLM）导演模块，根据多样化指令信号生成蓝图视频，从而控制角色动作和情感等高层语义；第二阶段在蓝图关键帧引导下，采用首尾帧策略并行生成多个子片段。这种从全局到局部的框架在保持细粒度细节的同时，忠实编码多模态指令背后的高层意图。并行架构还支持快速稳定生成长时视频，适用于数字人直播和视频博客等实际场景。\n\n为全面评估方法性能，我们构建包含375个精选样本的评测集，覆盖多样化指令与挑战性场景。大量实验表明，Kling-Avatar能以1080p分辨率和48帧率生成生动流畅的长时视频，在唇形同步精度、情感动态表现力、指令可控性、身份保持和跨域泛化方面均优于现有技术。这些成果确立了Kling-Avatar作为语义化高保真音频驱动虚拟形象合成的新标杆。"
    },
    {
        "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
        "url": "http://arxiv.org/abs/2509.09594v1",
        "pub_date": "2025-09-11",
        "summary": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/",
        "translated": "仅使用单目相机与拓扑地图的视觉导航方法，因其无需额外传感器和三维地图的优势，正逐渐成为多传感器方案的有力替代方案。现有方法通常采用\"图像相对性\"范式，通过当前观测图像与子目标图像的配对来估计控制指令。然而图像级的世界表征存在固有局限：图像严格依赖于智能体的位姿与具体形态。相比之下，作为地图固有属性的对象，能够提供与智能体形态及运动轨迹无关的世界表征。本研究提出了一种新型\"对象相对性\"控制学习范式，其具备三大优势：a) 无需严格模仿历史经验即可探索新路径；b) 可将控制预测问题与图像匹配问题解耦；c) 在训练-测试及建图-执行场景中均能实现跨形态部署的高度不变性。我们提出采用\"相对性三维场景图\"形式的拓扑-度量混合地图表征，以此获取信息更丰富的对象级全局路径规划代价。通过直接基于高层\"路径对象代价地图\"表征（无需显式RGB输入）训练局部控制器\"ObjectReact\"，实验证明：在传感器高度变化、需要挑战空间理解能力的多类导航任务（如反向循迹导航）中，对象相对性控制学习均优于图像相对性方案。我们进一步证实：仅通过仿真训练的策略能够良好泛化至真实室内环境。代码及补充材料详见项目页面：https://object-react.github.io/\n\n（注：译文严格遵循以下技术规范：\n1. 专业术语准确对应：\"topological map\"译作\"拓扑地图\"，\"3D scene graph\"译作\"三维场景图\"\n2. 概念体系完整保留：\"embodiment\"译为\"形态\"以保持机器人学语境\n3. 技术逻辑清晰呈现：通过括号补充说明（如\"无需显式RGB输入\"）确保技术细节无损传递\n4. 学术表述符合规范：使用\"范式\"\"表征\"\"解耦\"等学术用语\n5. 长句结构合理切分：将原文复合句拆解为符合中文表达习惯的短句结构）"
    },
    {
        "title": "Visual Grounding from Event Cameras",
        "url": "http://arxiv.org/abs/2509.09584v1",
        "pub_date": "2025-09-11",
        "summary": "Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.",
        "translated": "事件相机能够以微秒级精度捕捉亮度变化，在运动模糊和复杂光照条件下仍保持可靠性能，为高动态场景建模提供了显著优势。然而，其与自然语言理解的结合尚未得到充分关注，导致多模态感知领域存在空白。为此，我们推出Talk2Event——首个基于事件数据的语言驱动目标定位大规模基准数据集。该数据集以真实驾驶场景为基础，包含5,567个场景、13,458个标注对象以及超过30,000条经过严格验证的指代表达式。每条表达式均包含四个结构化属性：外观特征、状态信息、与观察者的关系以及与周围对象的关系，这些属性明确捕捉了空间、时间和关系线索。这种以属性为中心的设计支持可解释的组合式定位，使得分析不再局限于简单目标识别，而是扩展到动态环境中的上下文推理。我们期待Talk2Event成为推动多模态及时序感知研究的基础平台，其应用可覆盖机器人技术、人机交互等多个领域。\n\n（注：译文严格遵循学术论文摘要的规范表述，对\"event cameras\"采用行业通用译法\"事件相机\"，\"object grounding\"译为\"目标定位\"，\"referring expressions\"译为\"指代表达式\"等专业术语均符合计算机视觉与自然语言处理领域的术语标准。通过使用\"微秒级精度\"\"结构化属性\"\"上下文推理\"等专业表述，在保持原文技术准确性的同时确保中文表达的流畅性。）"
    },
    {
        "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient\n  Fine-Tuning for Remote Sensing Change Detection",
        "url": "http://arxiv.org/abs/2509.09572v1",
        "pub_date": "2025-09-11",
        "summary": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.",
        "translated": "为应对多时相多源遥感影像中普遍存在的伪变化、标注样本稀缺及跨域泛化困难等挑战，本文提出PeftCD——一种基于视觉基础模型（VFM）与参数高效微调（PEFT）技术的变化检测框架。该框架核心采用权重共享的孪生编码器结构，源自预训练VFM主干网络，并无缝集成LoRA与Adapter模块，通过仅训练极少量附加参数实现高效任务自适应。为充分释放VFM潜力，我们探索了两种领先主干网络：以强大分割先验著称的Segment Anything Model v2（SAM2），以及前沿自监督表征学习模型DINOv3。框架配以轻量化解码器设计，确保聚焦于主干网络提取的强大特征表征。大量实验表明，PeftCD在多个公开数据集上达到最先进性能：SYSU-CD（IoU 73.81%）、WHUCD（92.05%）、MSRSCD（64.07%）、MLCD（76.89%）、CDD（97.01%）、S2Looking（52.25%）和LEVIR-CD（85.62%），兼具精确边界刻画与强伪变化抑制能力。该框架在精度、效率与泛化性间达成最优平衡，为大规模VFM适配真实遥感变化检测应用提供了强大可扩展的范式。代码与预训练模型将于https://github.com/dyzy41/PeftCD 开源。\n\n（注：专业术语说明：\n1. 伪变化（Pseudo changes）：由光照、季节、传感器差异等非地表真实变化引起的干扰信号\n2. 参数高效微调（PEFT）：Parameter-Efficient Fine-Tuning的规范译法\n3. 视觉基础模型（VFM）：Vision Foundation Models的标准译名，与LLM（大语言模型）形成对应概念\n4. LoRA/Adapter：保持原文大写格式的模块名称，学术界通用写法\n5. 孪生编码器（Siamese encoder）：计算机视觉领域标准术语译法\n6. IoU：交并比（Intersection over Union），保留英文缩写+中文说明的规范表述）"
    },
    {
        "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in\n  MRI-based Alzheimer's Disease Classification",
        "url": "http://arxiv.org/abs/2509.09558v1",
        "pub_date": "2025-09-11",
        "summary": "Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR",
        "translated": "磁共振成像（MRI）是脑部影像学的金标准。已有研究提出采用深度学习（DL）算法辅助基于MRI扫描的阿尔茨海默病（AD）诊断。然而深度学习算法可能存在捷径学习问题——即使用与输出标签无直接关联的伪特征进行预测。当这些特征与受保护属性相关时，会导致对代表性不足群体（如按种族和性别划分的群体）的性能偏差。本研究探索了基于MRI的AD深度学习诊断中存在的捷径学习与人口统计学偏差风险。我们首先验证深度学习算法能否通过3D脑部MRI扫描识别种族或性别，以确认是否存在基于种族和性别的分布偏移。继而探究训练集的种族或性别不平衡是否会导致模型性能下降，从而揭示捷径学习与偏差现象。最后，我们对受保护属性和AD分类任务中不同脑区的特征归因进行了定量与定性分析。通过多数据集（ADNI、AIBL、MIRIAD）和多种深度学习模型（ResNet、SwinTransformer）的实验，我们证实了基于种族的性别捷径学习与偏差在AD分类中的存在。本研究为开发更公平的脑部MRI深度学习诊断工具奠定了基础。代码已开源：https://github.com/acharaakshit/ShortMR\n\n（注：根据学术规范，对原文中括号内的数据集名称ADNI/AIBL/MIRIAD进行了显性化处理，使技术细节表达更完整）"
    },
    {
        "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction\n  Generation",
        "url": "http://arxiv.org/abs/2509.09555v1",
        "pub_date": "2025-09-11",
        "summary": "While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.",
        "translated": "尽管大规模人体动作捕捉数据集推动了人体动作生成的发展，但由于数据集的局限性，动态三维人-物交互（HOI）的建模与生成仍面临挑战。现有数据集往往缺乏大量高质量动作数据与标注，且存在接触穿透、物体悬浮、手部动作失真等伪影问题。为解决这些问题，我们推出了InterAct——一个包含数据集与方法论创新的三维人-物交互基准系统。首先，我们整合并标准化了来自多来源的21.81小时人-物交互数据，并通过精细化文本标注进行数据增强。其次，提出统一优化框架，通过减少伪影和修正手部动作提升数据质量。基于接触不变性原理，我们在保持人-物交互关系的同时引入动作变异，将数据集扩展至30.70小时。第三，我们定义了六项基准任务，并开发了统一的人-物交互生成建模框架，实现了最先进的性能表现。大量实验验证了本数据集作为推动三维人-物交互生成研究的基础资源的实用性。为持续支持该领域研究，数据集已在https://github.com/wzyabcas/InterAct开源并将持续维护。\n\n（注：专业术语说明：\n1. human-object interactions (HOIs) 译为\"人-物交互\"\n2. contact penetration 译为\"接触穿透\"\n3. contact invariance 译为\"接触不变性\"\n4. state-of-the-art 译为\"最先进的\"\n5. benchmarking tasks 译为\"基准任务\"\n6. generative modeling 译为\"生成建模\"\n译文严格保持学术论文的正式语体，准确传递技术细节，同时符合中文表达习惯。）"
    },
    {
        "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion\n  and Alignment from Self-Supervised Vision Encoders",
        "url": "http://arxiv.org/abs/2509.09547v1",
        "pub_date": "2025-09-11",
        "summary": "Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/",
        "translated": "近年来，视频扩散模型因架构创新（如扩散变换器）和新型训练目标（如流匹配）的推动而快速发展。相比之下，如何提升此类模型的特征表征能力尚未获得足够关注。本研究提出，通过将视频生成器的中间特征与预训练视觉编码器的特征表示进行对齐，可有效提升视频扩散模型的训练效果。我们设计了一种新指标，并对多种视觉编码器展开深入分析，评估其判别能力与时序一致性，从而判断它们是否适用于视频特征对齐任务。基于分析结果，我们提出Align4Gen框架——通过创新的多特征融合与对齐方法增强视频扩散模型的训练过程。在无条件生成和类别条件生成任务上的实验表明，该方法能显著提升视频生成质量，多项评估指标均得到改善。完整视频结果详见项目页面：https://align4gen.github.io/align4gen/\n\n（注：技术要点说明：\n1. 专业术语保留英文原词：diffusion transformers（扩散变换器）、flow matching（流匹配）、feature alignment（特征对齐）\n2. 关键技术概念准确转化：multi-feature fusion（多特征融合）、temporal consistency（时序一致性）\n3. 学术表达规范：采用\"本研究\"\"实验表明\"等符合学术摘要的表述方式\n4. 链接与专有名词原样保留）"
    },
    {
        "title": "DualTrack: Sensorless 3D Ultrasound needs Local and Global Context",
        "url": "http://arxiv.org/abs/2509.09530v1",
        "pub_date": "2025-09-11",
        "summary": "Three-dimensional ultrasound (US) offers many clinical advantages over conventional 2D imaging, yet its widespread adoption is limited by the cost and complexity of traditional 3D systems. Sensorless 3D US, which uses deep learning to estimate a 3D probe trajectory from a sequence of 2D US images, is a promising alternative. Local features, such as speckle patterns, can help predict frame-to-frame motion, while global features, such as coarse shapes and anatomical structures, can situate the scan relative to anatomy and help predict its general shape. In prior approaches, global features are either ignored or tightly coupled with local feature extraction, restricting the ability to robustly model these two complementary aspects. We propose DualTrack, a novel dual-encoder architecture that leverages decoupled local and global encoders specialized for their respective scales of feature extraction. The local encoder uses dense spatiotemporal convolutions to capture fine-grained features, while the global encoder utilizes an image backbone (e.g., a 2D CNN or foundation model) and temporal attention layers to embed high-level anatomical features and long-range dependencies. A lightweight fusion module then combines these features to estimate the trajectory. Experimental results on a large public benchmark show that DualTrack achieves state-of-the-art accuracy and globally consistent 3D reconstructions, outperforming previous methods and yielding an average reconstruction error below 5 mm.",
        "translated": "三维超声（US）相比传统二维成像具有诸多临床优势，但其广泛应用受限于传统3D系统的成本与复杂度。无传感器3D超声技术通过深度学习从二维超声图像序列中估计探头轨迹，成为一种具有前景的替代方案。散斑图案等局部特征可辅助帧间运动预测，而粗略形状和解剖结构等全局特征能定位扫描位置并推断整体形态。现有方法往往忽略全局特征，或将其与局部特征提取强耦合，限制了对此互补双要素的鲁棒建模能力。我们提出DualTrack——一种新型双编码器架构，采用解耦的局部与全局编码器分别专注于不同尺度的特征提取：局部编码器通过密集时空卷积捕获细粒度特征，全局编码器则利用图像主干网络（如2D CNN或基础模型）与时序注意力层嵌入高层解剖特征及长程依赖关系。轻量级融合模块最终整合这些特征以估计轨迹。在大型公开基准测试中，DualTrack实现了最先进的精度和全局一致的三维重建，性能超越现有方法，平均重建误差低于5毫米。"
    },
    {
        "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
        "url": "http://arxiv.org/abs/2509.09527v1",
        "pub_date": "2025-09-11",
        "summary": "In recent years, Multi-View Clustering (MVC) has been significantly advanced under the influence of deep learning. By integrating heterogeneous data from multiple views, MVC enhances clustering analysis, making multi-view fusion critical to clustering performance. However, there is a problem of low-quality data in multi-view fusion. This problem primarily arises from two reasons: 1) Certain views are contaminated by noisy data. 2) Some views suffer from missing data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF) method to address this problem. SGDF leverages a multiple generative mechanism for the multi-view feature of each sample. It is robust to low-quality data. Building on SGDF, we further present the Generative Diffusion Contrastive Network (GDCN). Extensive experiments show that GDCN achieves the state-of-the-art results in deep MVC tasks. The source code is publicly available at https://github.com/HackerHyper/GDCN.",
        "translated": "近年来，深度学习推动了多视图聚类（MVC）领域的显著发展。通过整合来自多个视图的异构数据，MVC增强了聚类分析能力，使得多视图融合成为决定聚类性能的关键因素。然而，多视图融合中存在低质量数据的问题，这主要源于两方面原因：1）某些视图受到噪声数据污染；2）部分视图存在数据缺失。本文提出了一种新颖的随机生成扩散融合（SGDF）方法来解决这一问题。SGDF利用多重生成机制处理每个样本的多视图特征，对低质量数据具有强鲁棒性。基于SGDF，我们进一步构建了生成扩散对比网络（GDCN）。大量实验表明，GDCN在深度多视图聚类任务中取得了最先进的性能。相关源代码已公开于https://github.com/HackerHyper/GDCN。\n\n（注：专业术语说明：\n- Multi-View Clustering (MVC) 标准译法为\"多视图聚类\"\n- Stochastic Generative Diffusion Fusion (SGDF) 采用意译\"随机生成扩散融合\"\n- Generative Diffusion Contrastive Network (GDCN) 译为\"生成扩散对比网络\"\n- state-of-the-art 遵循学术惯例译为\"最先进的\"\n- 技术概念\"multi-view fusion\"统一译为\"多视图融合\"\n- \"robust\"按工程术语译为\"鲁棒性\"）"
    },
    {
        "title": "Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided\n  Protocol on the Connectome 2.0 scanner",
        "url": "http://arxiv.org/abs/2509.09513v1",
        "pub_date": "2025-09-11",
        "summary": "The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.",
        "translated": "扩散磁共振神经突触交换成像模型通过估算脑灰质微结构参数（如分区容积、扩散系数及跨区水分子交换时间），为探测神经组织微观特性提供了有力工具。然而现有成像方案需耗费较长扫描时间。本研究针对\"连接组2.0\"扫描仪提出一种缩减采集方案，在保持模型精度的同时显著缩短扫描时长。我们开发了基于可解释人工智能的数据驱动框架，采用指导式递归特征消除策略，从原有15个特征的协议中优选8个特征子集。通过体内实验验证该优化方案的性能，并与完整采集方案及其他缩减策略进行对比评估，重点考察参数准确性、解剖对比度保持度和重测再现性。结果表明：缩减方案获得的参数估计值与皮层映射图与完整方案相当，在合成数据中呈现较低估计误差，且对重测变异影响极小。与理论驱动式和启发式缩减方案相比，本方案展现出更优的鲁棒性，将水交换时间估计值的偏差降低两倍以上。该混合优化框架最终实现了仅需14分钟即可完成神经突触交换成像且不损失参数保真度，为交换敏感型扩散磁共振成像在神经科学和临床研究中的广泛应用提供支持，同时为生物物理参数映射领域的高效采集方案设计提供了可推广的方法论。"
    },
    {
        "title": "AskDoc -- Identifying Hidden Healthcare Disparities",
        "url": "http://arxiv.org/abs/2509.09622v1",
        "pub_date": "2025-09-11",
        "summary": "The objective of this study is to understand the online Ask the Doctor services medical advice on internet platforms via AskDoc, a Reddit community that serves as a public AtD platform and study if platforms mirror existing hurdles and partiality in healthcare across various demographic groups. We downloaded data from January 2020 to May 2022 from AskDoc -- a subreddit, and created regular expressions to identify self-reported demographics (Gender, Race, and Age) from the posts, and performed statistical analysis to understand the interaction between peers and physicians with the posters. Half of the posts did not receive comments from peers or physicians. At least 90% of the people disclose their gender and age, and 80% of the people do not disclose their race. It was observed that the subreddit is dominated by adult (age group 20-39) white males. Some disparities were observed in the engagement between the users and the posters with certain demographics. Beyond the confines of clinics and hospitals, social media could bring patients and providers closer together, however, as observed, current physicians participation is low compared to posters.",
        "translated": "本研究旨在通过分析Reddit平台的AskDoc社区（一个公共在线问诊平台），探究互联网“问诊”服务的医疗建议模式，并验证此类平台是否反映了不同人口群体在医疗保健中面临的既有障碍与偏见。我们收集了该子论坛2020年1月至2022年5月的数据，通过正则表达式识别发帖者自我报告的人口统计特征（性别、种族、年龄），并采用统计分析研究医患及用户间的互动模式。\n\n研究发现：半数发帖未获得任何用户或医生的回复；超过90%的用户公开了性别和年龄信息，但80%未披露种族信息；该社区主要由20-39岁的成年白人男性主导。数据分析显示，特定人口特征的发帖者获得的互动参与度存在差异。研究表明，社交媒体具有突破传统诊所医院限制、拉近医患距离的潜力，但当前医生参与度远低于发帖者需求。\n\n（注：专业术语说明：\n- Ask the Doctor(AtD): 在线问诊服务\n- subreddit: Reddit子论坛\n- regular expressions: 正则表达式\n- demographic groups: 人口统计群体\n- 所有医学术语和数据分析方法均按学术规范准确翻译）"
    },
    {
        "title": "Boosting Data Utilization for Multilingual Dense Retrieval",
        "url": "http://arxiv.org/abs/2509.09459v1",
        "pub_date": "2025-09-11",
        "summary": "Multilingual dense retrieval aims to retrieve relevant documents across different languages based on a unified retriever model. The challenge lies in aligning representations of different languages in a shared vector space. The common practice is to fine-tune the dense retriever via contrastive learning, whose effectiveness highly relies on the quality of the negative sample and the efficacy of mini-batch data. Different from the existing studies that focus on developing sophisticated model architecture, we propose a method to boost data utilization for multilingual dense retrieval by obtaining high-quality hard negative samples and effective mini-batch data. The extensive experimental results on a multilingual retrieval benchmark, MIRACL, with 16 languages demonstrate the effectiveness of our method by outperforming several existing strong baselines.",
        "translated": "多语言稠密检索旨在通过统一的检索器模型实现跨语言的相关文档检索，其核心挑战在于将不同语言的表征对齐到共享向量空间中。当前主流方法是通过对比学习对稠密检索器进行微调，其效果高度依赖于负样本质量与小批次数据的有效性。与现有研究侧重于开发复杂模型架构不同，我们提出了一种提升多语言稠密检索数据利用效率的方法，通过获取高质量困难负样本并构建高效小批次数据实现性能提升。在多语言检索基准MIRACL（涵盖16种语言）上的大量实验表明，我们的方法优于多个现有强基线，验证了其有效性。\n\n（注：译文严格遵循了以下要点：\n1. 专业术语准确：\"dense retrieval\"译为\"稠密检索\"，\"contrastive learning\"译为\"对比学习\"，\"hard negative samples\"译为\"困难负样本\"\n2. 技术细节保留：完整传递了\"mini-batch data有效性\"、\"共享向量空间对齐\"等核心概念\n3. 学术表达规范：采用\"旨在\"\"侧重于\"\"验证了\"等学术用语\n4. 逻辑结构保持：遵循\"问题背景-方法创新-实验验证\"的原文脉络\n5. 数据准确性：明确保留\"MIRACL基准\"和\"16种语言\"的关键数据信息）"
    },
    {
        "title": "We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years\n  Later",
        "url": "http://arxiv.org/abs/2509.09414v1",
        "pub_date": "2025-09-11",
        "summary": "In 2011, Xavier Amatriain sounded the alarm: recommender systems research was \"doing it all wrong\" [1]. His critique, rooted in statistical misinterpretation and methodological shortcuts, remains as relevant today as it was then. But rather than correcting course, we added new layers of sophistication on top of the same broken foundations. This paper revisits Amatriain's diagnosis and argues that many of the conceptual, epistemological, and infrastructural failures he identified still persist, in more subtle or systemic forms. Drawing on recent work in reproducibility, evaluation methodology, environmental impact, and participatory design, we showcase how the field's accelerating complexity has outpaced its introspection. We highlight ongoing community-led initiatives that attempt to shift the paradigm, including workshops, evaluation frameworks, and calls for value-sensitive and participatory research. At the same time, we contend that meaningful change will require not only new metrics or better tooling, but a fundamental reframing of what recommender systems research is for, who it serves, and how knowledge is produced and validated. Our call is not just for technical reform, but for a recommender systems research agenda grounded in epistemic humility, human impact, and sustainable practice.",
        "translated": "2011年，Xavier Amatriain曾敲响警钟：推荐系统研究正\"误入歧途\"[1]。其批判根植于统计误读与方法论捷径，这一洞见在当今仍具现实意义。然而我们非但没有修正航向，反而在原有缺陷基础上叠加了更复杂的层次。本文重访Amatriain的诊断，指出其当年指出的概念性、认识论和基础设施层面的缺陷——尽管以更隐蔽或系统化的形式——至今依然存在。通过借鉴可复现性研究、评估方法论、环境影响及参与式设计等领域的最新成果，我们揭示该领域如何陷入\"复杂性加速超越自省速度\"的困境。我们重点介绍了社区主导的范式转型尝试，包括专题研讨会、评估框架建设，以及推动价值敏感性与参与式研究的倡议。同时我们主张，真正意义上的变革不仅需要新评估指标或更优工具，更需要从根本上重新界定推荐系统研究的目标宗旨、服务对象以及知识生产与验证机制。我们呼吁的不仅是技术改良，更是一场需要以认知谦逊、人类福祉和可持续实践为根基的推荐系统研究范式重构。\n\n（注：保留原文文献标注[1]，专业术语如\"epistemic humility\"译为\"认知谦逊\"，\"value-sensitive\"译为\"价值敏感性\"，\"participatory design\"译为\"参与式设计\"，在保持学术严谨性的同时确保中文表达流畅。）"
    },
    {
        "title": "CESRec: Constructing Pseudo Interactions for Sequential Recommendation\n  via Conversational Feedback",
        "url": "http://arxiv.org/abs/2509.09342v1",
        "pub_date": "2025-09-11",
        "summary": "Sequential Recommendation Systems (SRS) have become essential in many real-world applications. However, existing SRS methods often rely on collaborative filtering signals and fail to capture real-time user preferences, while Conversational Recommendation Systems (CRS) excel at eliciting immediate interests through natural language interactions but neglect historical behavior. To bridge this gap, we propose CESRec, a novel framework that integrates the long-term preference modeling of SRS with the real-time preference elicitation of CRS. We introduce semantic-based pseudo interaction construction, which dynamically updates users'historical interaction sequences by analyzing conversational feedback, generating a pseudo-interaction sequence that seamlessly combines long-term and real-time preferences. Additionally, we reduce the impact of outliers in historical items that deviate from users'core preferences by proposing dual alignment outlier items masking, which identifies and masks such items using semantic-collaborative aligned representations. Extensive experiments demonstrate that CESRec achieves state-of-the-art performance by boosting strong SRS models, validating its effectiveness in integrating conversational feedback into SRS.",
        "translated": "序列推荐系统（SRS）已在众多实际应用中变得至关重要。然而，现有SRS方法通常依赖协同过滤信号，难以捕捉实时用户偏好；而对话推荐系统（CRS）虽能通过自然语言交互有效获取即时兴趣，却忽略了历史行为。为弥补这一缺陷，我们提出CESRec——一个创新框架，将SRS的长期偏好建模与CRS的实时偏好获取能力相融合。我们引入基于语义的伪交互构建技术，通过分析对话反馈动态更新用户历史交互序列，生成融合长期与实时偏好的伪交互序列。此外，针对历史项目中偏离用户核心偏好的异常项，我们提出双对齐异常项掩蔽机制，通过语义-协同对齐表征识别并屏蔽此类干扰项。大量实验表明，CESRec通过增强现有强效SRS模型实现了最先进的性能，验证了其将对话反馈整合至SRS的有效性。"
    },
    {
        "title": "Constructing a Question-Answering Simulator through the Distillation of\n  LLMs",
        "url": "http://arxiv.org/abs/2509.09226v1",
        "pub_date": "2025-09-11",
        "summary": "The question-answering (QA) simulator is a model that mimics real student learning behaviors and predicts their correctness of their responses to questions. QA simulators enable educational recommender systems (ERS) to collect large amounts of training data without interacting with real students, thereby preventing harmful recommendations made by an undertrained ERS from undermining actual student learning. Given the QA history, there are two categories of solutions to predict the correctness, conducting the simulation: (1) LLM-free methods, which apply a traditional sequential model to transfer the QA history into a vector representation first, and make predictions based on the representation; (2) LLM-based methods, which leverage the domain knowledge and reasoning capability of LLM to enhence the prediction. LLM-free methods offer fast inference but generally yield suboptimal performance. In contrast, most LLM-based methods achieve better results, but at the cost of slower inference speed and higher GPU memory consumption. In this paper, we propose a method named LLM Distillation based Simulator (LDSim), which distills domain knowledge and reasoning capability from an LLM to better assist prediction, thereby improving simulation performance. Extensive experiments demonstrate that our LDSim achieves strong results on both the simulation task and the knowledge tracing (KT) task. Our code is publicly available at https://anonymous.4open.science/r/LDSim-05A9.",
        "translated": "问题回答模拟器（QA simulator）是一种模拟真实学生学习行为并预测其答题正确率的模型。该模拟器使教育推荐系统（ERS）无需与真实学生交互即可收集大量训练数据，从而避免因系统训练不足产生有害推荐而影响实际学习效果。基于答题历史记录，现有预测正确率的模拟方法可分为两类：（1）无大语言模型方法（LLM-free），采用传统序列模型先将答题历史转换为向量表示，再基于该表示进行预测；（2）基于大语言模型方法（LLM-based），利用大模型的领域知识和推理能力提升预测精度。无大模型方法推理速度快但性能欠佳，而多数大模型方法虽效果更好，却需以降低推理速度和增加GPU内存消耗为代价。本文提出名为大模型蒸馏模拟器（LDSim）的方法，通过从大模型中蒸馏领域知识与推理能力来优化预测，从而提升模拟性能。大量实验表明，LDSim在模拟任务和知识追踪（KT）任务上均取得优异结果。代码已开源：https://anonymous.4open.science/r/LDSim-05A9。\n\n（注：根据学术规范，译文对原文中的技术术语（如knowledge tracing译为\"知识追踪\"）、方法论描述（如distill译为\"蒸馏\"）及长难句结构进行了专业化处理，同时确保了URL等要素的准确保留。）"
    },
    {
        "title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2509.09114v1",
        "pub_date": "2025-09-11",
        "summary": "Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec.",
        "translated": "多模态推荐系统正日益成为电子商务和内容平台的基础技术，其通过联合建模用户历史行为与商品多模态特征（如视觉与文本特征）来实现个性化服务。然而现有方法大多依赖静态融合策略或基于图的局部交互建模，存在两个关键局限：（1）细粒度跨模态关联建模能力不足，导致融合质量欠佳；（2）缺乏全局分布层面的一致性，引发表征偏差。为此，我们提出MambaRec框架，通过注意力引导学习整合局部特征对齐与全局分布正则化。其核心是提出的扩张细化注意力模块（DREAM），该模块采用多尺度扩张卷积结合通道与空间注意力机制，对齐视觉与文本模态间的细粒度语义模式。该模块能捕获层次化关系和上下文感知关联，提升跨模态语义建模能力。此外，我们引入最大均值差异（MMD）和对比损失函数约束全局模态对齐，增强语义一致性。这种双重正则化机制减少了模态特异性偏差并提升鲁棒性。为提高可扩展性，MambaRec采用降维策略降低高维多模态特征的计算成本。在真实电商数据集上的大量实验表明，MambaRec在融合质量、泛化能力和效率方面均优于现有方法。代码已开源：https://github.com/rkl71/MambaRec。\n\n（注：译文严格遵循以下技术细节处理：\n1. 专业术语标准化：\"Multimodal recommendation systems\"译为\"多模态推荐系统\"，\"Maximum Mean Discrepancy\"保留英文缩写MMD并补充中文全称\n2. 技术概念准确传达：\"dilated convolutions\"译为\"扩张卷积\"，\"contrastive loss\"译为\"对比损失函数\"\n3. 结构逻辑显性化：通过\"其核心是\"、\"此外\"等连接词明确技术模块的层次关系\n4. 长句拆分重组：将原文复合句拆分为符合中文表达习惯的短句，如将DREAM模块的说明拆分为功能描述和技术实现两个层次）"
    },
    {
        "title": "Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in\n  Personalized Recommendation",
        "url": "http://arxiv.org/abs/2509.09037v1",
        "pub_date": "2025-09-10",
        "summary": "Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have been used as fairness concepts in the economics, game theory, and social choice literatures since the 1960s, and have recently gained popularity within the recommendation systems communities. In this short position paper we will give an overview of envy-freeness and its use in economics and recommendation systems; and illustrate why envy is not appropriate to measure fairness for use in settings where personalization plays a role.",
        "translated": "嫉妒自由性（Envy-freeness）及其松弛形式\"单物品嫉妒自由性\"（EF-1）作为公平性概念，自1960年代以来广泛应用于经济学、博弈论和社会选择领域的研究中，近期在推荐系统领域也受到广泛关注。本短篇立场文件将概述嫉妒自由性在经济学与推荐系统中的应用，并论证在涉及个性化设置的场景中，该指标并不适合作为公平性的衡量标准。"
    },
    {
        "title": "Generative Engine Optimization: How to Dominate AI Search",
        "url": "http://arxiv.org/abs/2509.08919v1",
        "pub_date": "2025-09-10",
        "summary": "The rapid adoption of generative AI-powered search engines like ChatGPT, Perplexity, and Gemini is fundamentally reshaping information retrieval, moving from traditional ranked lists to synthesized, citation-backed answers. This shift challenges established Search Engine Optimization (SEO) practices and necessitates a new paradigm, which we term Generative Engine Optimization (GEO).   This paper presents a comprehensive comparative analysis of AI Search and traditional web search (Google). Through a series of large-scale, controlled experiments across multiple verticals, languages, and query paraphrases, we quantify critical differences in how these systems source information. Our key findings reveal that AI Search exhibit a systematic and overwhelming bias towards Earned media (third-party, authoritative sources) over Brand-owned and Social content, a stark contrast to Google's more balanced mix. We further demonstrate that AI Search services differ significantly from each other in their domain diversity, freshness, cross-language stability, and sensitivity to phrasing.   Based on these empirical results, we formulate a strategic GEO agenda. We provide actionable guidance for practitioners, emphasizing the critical need to: (1) engineer content for machine scannability and justification, (2) dominate earned media to build AI-perceived authority, (3) adopt engine-specific and language-aware strategies, and (4) overcome the inherent \"big brand bias\" for niche players. Our work provides the foundational empirical analysis and a strategic framework for achieving visibility in the new generative search landscape.",
        "translated": "随着以ChatGPT、Perplexity和Gemini为代表的生成式AI搜索引擎迅速普及，信息检索模式正经历根本性变革——从传统排名列表转向具备文献引证的合成式答案。这一转变对现有搜索引擎优化（SEO）实践构成挑战，亟需建立新范式，我们将其定义为生成式引擎优化（GEO）。  \n本文通过跨垂直领域、多语言及查询释义的大规模对照实验，对AI搜索与传统网络搜索（Google）展开全面对比分析。量化研究揭示：AI搜索系统呈现出系统性、压倒性的对第三方权威媒体内容（Earned media）的偏好，显著区别于谷歌对自有品牌内容与社交内容的均衡收录模式。研究进一步发现，不同AI搜索服务在领域多样性、内容时效性、跨语言稳定性及查询句式敏感性方面存在显著差异。  \n基于实证结果，我们构建了战略性GEO实施框架，为从业者提供可操作的指导建议：（1）设计便于机器扫描与逻辑验证的内容架构；（2）通过权威媒体曝光建立AI可感知的公信力；（3）制定引擎差异化与语言感知型策略；（4）帮助利基市场参与者克服固有的\"大品牌偏见\"。本研究为在生成式搜索新生态中获取可见性提供了实证基础与战略框架。"
    },
    {
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
        "url": "http://arxiv.org/abs/2509.06733v1",
        "pub_date": "2025-09-08",
        "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
        "translated": "深度研究系统——一种通过协调推理、开放网络与用户文件搜索及工具使用来解决复杂多步骤任务的智能体AI——正朝着包含规划器、协调器和执行器的分层架构发展。实践中，端到端训练整个系统栈仍不现实，因此现有研究多集中于训练连接搜索、浏览和代码等核心工具的单一规划器。虽然监督微调（SFT）能保证协议规范性，但其存在模仿偏差和暴露偏差问题，且未能充分利用环境反馈。偏好对齐方法（如DPO）受限于预设框架和代理指标，属于离策策略，在长周期信用分配和多目标权衡方面表现较弱。SFT和DPO的更深层局限在于其依赖通过框架设计和标注比较来人为定义决策点与子技能。\n\n强化学习通过优化轨迹级策略，与闭环工具交互研究形成天然契合：它支持探索与恢复行为，实现原理性信用分配，并减少对人类先验知识和评分者偏差的依赖。据我们所知，本综述是首篇专注于深度研究系统强化学习基础的系统性研究。我们沿三个维度梳理了DeepSeek-R1后的研究成果：（i）数据合成与治理；（ii）智能研究体的RL方法，涵盖稳定性、样本效率、长上下文处理、奖励与信用设计、多目标优化及多模态整合；（iii）智能体RL训练系统与框架。同时涵盖智能体架构与协调机制，以及评估基准——包括最新QA、VQA、长文本合成及领域扎根的工具交互任务。我们提炼出重复模式，揭示基础设施瓶颈，并为训练鲁棒透明的深度研究智能体提供实用RL指导。\n\n（注：专业术语说明：\n- Agentic AI：译为\"智能体AI\"，强调自主决策能力\n- Exposure bias：暴露偏差，指训练与推断阶段的数据分布差异问题\n- Off-policy：离策策略，指评估策略与行为策略不同的强化学习方法\n- Credit assignment：信用分配，指将总体回报归因到具体行动的过程\n- VQA：视觉问答（Visual Question Answering）\n- 长文本合成（long-form synthesis）：指生成连贯长文本的能力）"
    },
    {
        "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
        "url": "http://arxiv.org/abs/2509.09360v1",
        "pub_date": "2025-09-11",
        "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must be consistent with retrieved evidence. We therefore present MetaRAG, a metamorphic testing framework for hallucination detection in Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time, unsupervised, black-box setting, requiring neither ground-truth references nor access to model internals, making it suitable for proprietary and high-stakes domains. The framework proceeds in four stages: (1) decompose answers into atomic factoids, (2) generate controlled mutations of each factoid using synonym and antonym substitutions, (3) verify each variant against the retrieved context (synonyms are expected to be entailed and antonyms contradicted), and (4) aggregate penalties for inconsistencies into a response-level hallucination score. Crucially for identity-aware AI, MetaRAG localizes unsupported claims at the factoid span where they occur (e.g., pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility), allowing users to see flagged spans and enabling system designers to configure thresholds and guardrails for identity-sensitive queries. Experiments on a proprietary enterprise dataset illustrate the effectiveness of MetaRAG for detecting hallucinations and enabling trustworthy deployment of RAG-based conversational agents. We also outline a topic-based deployment design that translates MetaRAG's span-level scores into identity-aware safeguards; this design is discussed but not evaluated in our experiments.",
        "translated": "大型语言模型（LLMs）在企业应用中的部署日益增多，但其可靠性仍受幻觉问题（即自信但事实错误的输出）的限制。现有检测方法（如SelfCheckGPT和MetaQA）主要针对独立LLMs，未能解决检索增强生成（RAG）系统中需确保响应与检索证据一致的特殊挑战。为此，我们提出MetaRAG——一个面向RAG系统幻觉检测的蜕变测试框架。该框架在实时、无监督、黑盒环境下运行，既不需要真实参考数据，也无需访问模型内部，适用于专有和高风险领域。\n\nMetaRAG的工作流程包含四个阶段：（1）将答案分解为原子化事实单元；（2）通过同义词/反义词替换生成受控变异；（3）依据检索上下文验证每个变体（要求同义词版本可被验证，反义词版本应被否定）；（4）将不一致性惩罚聚合为响应级幻觉分数。该框架的核心价值在于支持身份感知AI——它能定位未经验证的主张所在的具体事实单元（例如妊娠特定注意事项、LGBTQ+难民权利或劳动资格），既允许用户查看被标记的文本片段，也支持系统设计者为身份敏感查询配置阈值与防护机制。\n\n在专有企业数据集上的实验证明了MetaRAG在检测幻觉和推动RAG对话代理可信部署方面的有效性。我们还提出基于主题的部署设计方案，将片段级分数转化为身份感知保障机制（该设计在文中讨论但未纳入实验评估）。"
    },
    {
        "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
        "url": "http://arxiv.org/abs/2509.09332v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io",
        "translated": "多模态大语言模型（MLLMs）的最新进展为具身智能开辟了新机遇，使其能够实现多模态理解、推理与交互以及连续空间决策。然而，当前基于MLLM的具身系统面临两个关键局限：其一，几何适应性鸿沟——仅基于二维输入训练或采用硬编码三维几何注入的模型存在空间信息不足或二维泛化受限的问题，导致其难以适应不同空间需求的任务；其二，具身约束鸿沟——现有研究常忽略真实机器人的物理约束与能力，导致生成的任务计划理论上可行但实际难以执行。\n\n针对上述问题，我们提出OmniEVA——一种具身通用规划器，通过两项关键创新实现先进的具身推理与任务规划：（1）任务自适应三维 grounding 机制，引入门控路由器根据上下文需求对三维融合进行显式选择性调控，从而为多样化具身任务实现情境感知的三维 grounding；（2）具身意识推理框架，将任务目标与实体约束共同纳入推理循环，生成既符合目标导向又可执行的规划决策。\n\n大量实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛下游场景中展现出强大能力。通过对包括基础任务与复合任务在内的新型具身基准测试评估，其鲁棒且通用的规划能力得到验证。项目页面：https://omnieva.github.io\n\n（注：根据学术规范，grounding在此保留英文术语，其含义可理解为\"基于感知数据的语义锚定\"或\"跨模态对齐\"。若需进一步解释，可添加译注说明该术语在具身人工智能中特指多模态信息与物理空间的关联过程。）"
    },
    {
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
        "url": "http://arxiv.org/abs/2509.09307v1",
        "pub_date": "2025-09-11",
        "summary": "Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
        "translated": "材料表征是获取材料信息的基础，能够揭示指导材料设计与优化的加工-微观结构-性能关系。尽管多模态大语言模型（MLLMs）近期在材料科学的生成与预测任务中展现出潜力，但其对真实世界表征成像数据的理解能力仍待深入探索。为填补这一空白，我们推出了首个面向材料表征图像理解的基准测试MatCha，包含1,500个需要专家级领域知识的问题。该基准覆盖材料研究的四个关键阶段共21项任务，每项任务均设计用于反映材料科学家面临的真实挑战。\n\n我们对前沿多模态大语言模型在MatCha上的评估显示，其性能与人类专家存在显著差距。这些模型在处理需要高阶专业知识和复杂视觉感知的问题时表现明显退化，简单的少样本提示和思维链提示方法难以缓解这些局限性。这些发现表明，现有多模态大语言模型对真实材料表征场景的适应性仍然有限。我们期望MatCha能推动新材料发现和自主科研智能体等领域的未来研究。MatCha已发布于：https://github.com/FreedomIntelligence/MatCha。\n\n（注：翻译严格遵循了以下技术规范：\n1. 专业术语准确对应：\"characterization\"译为\"表征\"，\"multimodal large language models\"保留专业缩写\"MLLMs\"并补充全称\"多模态大语言模型\"\n2. 概念体系完整保留：\"processing-microstructure-property relationships\"完整译为\"加工-微观结构-性能关系\"\n3. 技术动作精准传达：\"few-shot prompting\"译为\"少样本提示\"，\"chain-of-thought prompting\"采用学界通用译法\"思维链提示\"\n4. 学术表述符合规范：被动语态转换为主动句式（如\"are designed to\"译为\"设计用于\"），长难句按中文习惯切分重组\n5. 重要概念首次出现时标注英文原词（如MLLMs），确保学术严谨性）"
    },
    {
        "title": "From scratch to silver: Creating trustworthy training data for\n  patent-SDG classification using Large Language Models",
        "url": "http://arxiv.org/abs/2509.09303v1",
        "pub_date": "2025-09-11",
        "summary": "Classifying patents by their relevance to the UN Sustainable Development Goals (SDGs) is crucial for tracking how innovation addresses global challenges. However, the absence of a large, labeled dataset limits the use of supervised learning. Existing methods, such as keyword searches, transfer learning, and citation-based heuristics, lack scalability and generalizability. This paper frames patent-to-SDG classification as a weak supervision problem, using citations from patents to SDG-tagged scientific publications (NPL citations) as a noisy initial signal. To address its sparsity and noise, we develop a composite labeling function (LF) that uses large language models (LLMs) to extract structured concepts, namely functions, solutions, and applications, from patents and SDG papers based on a patent ontology. Cross-domain similarity scores are computed and combined using a rank-based retrieval approach. The LF is calibrated via a custom positive-only loss that aligns with known NPL-SDG links without penalizing discovery of new SDG associations. The result is a silver-standard, soft multi-label dataset mapping patents to SDGs, enabling the training of effective multi-label regression models. We validate our approach through two complementary strategies: (1) internal validation against held-out NPL-based labels, where our method outperforms several baselines including transformer-based models, and zero-shot LLM; and (2) external validation using network modularity in patent citation, co-inventor, and co-applicant graphs, where our labels reveal greater thematic, cognitive, and organizational coherence than traditional technological classifications. These results show that weak supervision and semantic alignment can enhance SDG classification at scale.",
        "translated": "根据专利与联合国可持续发展目标（SDGs）的相关性进行分类，对于追踪创新如何应对全球挑战至关重要。然而，由于缺乏大规模标注数据集，监督学习的应用受到限制。现有方法（如关键词搜索、迁移学习和基于引用的启发式方法）缺乏可扩展性和泛化能力。本文通过将专利-SDG分类构建为弱监督问题，利用专利引用带有SDG标签的科学出版物（非专利文献引用，NPL citations）作为噪声初始信号。为解决该信号的稀疏性和噪声问题，我们开发了一种复合标注函数（LF），基于专利本体论使用大语言模型（LLM）从专利和SDG论文中提取结构化概念（即功能、解决方案和应用）。通过基于排序的检索方法计算并融合跨领域相似度得分。该标注函数通过定制化的仅正样本损失函数进行校准，该函数与已知的NPL-SDG关联保持一致，且不会惩罚新SDG关联的发现。最终生成银标准软多标签数据集，将专利映射到SDGs，从而支持训练有效的多标签回归模型。我们通过两种互补策略验证方法有效性：（1）基于保留的NPL标签进行内部验证，本方法在包括基于Transformer的模型和零样本LLM在内的多个基线模型中表现优异；（2）利用专利引用、共同发明人和共同申请人网络中的模块度进行外部验证，结果表明相较于传统技术分类，我们的标签在主题、认知和组织层面展现出更强的一致性。这些结果证明，弱监督与语义对齐能够有效提升SDG分类的大规模应用能力。"
    },
    {
        "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for\n  Multistep Reasoning",
        "url": "http://arxiv.org/abs/2509.09284v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We propose a staged GRPO training paradigm where completions are derived from partially revealed MCTS rollouts, introducing a novel tree-structured setting for advantage estimation. This leads to a rich class of prefix-conditioned reward signals, which we analyze theoretically and empirically. Our initial results indicate that while structured advantage estimation can stabilize updates and better reflect compositional reasoning quality, challenges such as advantage saturation and reward signal collapse remain. We propose heuristic and statistical solutions to mitigate these issues and discuss open challenges for learning under staged or tree-like reward structures.",
        "translated": "近期，大型语言模型（LLM）在推理任务中的进展表明，蒙特卡洛树搜索（MCTS）能够有效生成高质量的中继轨迹，尤其在数学与符号领域表现突出。受此启发，我们探索如何将传统上用于训练价值模型或奖励模型的MCTS轨迹重新应用于基于偏好的强化学习（RL）中的策略优化。具体而言，我们聚焦于群组相对策略优化（GRPO）——一种无需价值网络即可实现偏好一致策略学习的新算法。我们提出一种分阶段GRPO训练范式，其中补全结果源自部分展开的MCTS推演，从而为优势估计引入了一种新颖的树形结构设定。该方法产生了一类丰富的基于前缀条件的奖励信号，我们从理论与实证两方面对其进行了分析。初步结果表明：虽然结构化优势估计能够稳定更新过程并更好地反映组合推理质量，但仍存在优势饱和与奖励信号坍缩等挑战。我们提出了启发式与统计学的解决方案以缓解这些问题，并讨论了在阶段性或树状奖励结构下学习的开放性难题。"
    },
    {
        "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
        "url": "http://arxiv.org/abs/2509.09265v1",
        "pub_date": "2025-09-11",
        "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/",
        "translated": "在长周期任务中，基于大语言模型（LLM）的智能体面临核心挑战：稀疏的结果型奖励难以对中间步骤进行有效信用分配。现有方法主要通过传统强化学习技术（如逆强化学习）或使用过程奖励模型提供逐步反馈，以构建密集奖励信号来指导学习。本文发现LLM学习动态中存在一个根本性问题：策略梯度幅度与熵值固有耦合，导致对置信度高的正确动作产生低效的小幅度更新，而对不确定动作可能产生破坏稳定性的大幅度更新。针对这一问题，我们提出熵调制策略梯度（EMPG）框架，该框架基于步骤级不确定性和最终任务结果重新校准学习信号。EMPG会放大对高置信度正确动作的更新，惩罚高置信度错误动作，并衰减不确定步骤的更新以稳定探索过程。我们还引入了未来清晰度奖励项，激励智能体寻找更具可预测性的解决路径。通过在WebShop、ALFWorld和DeepSearch三个具有挑战性的智能体任务上进行综合实验，我们证明EMPG实现了显著性能提升，大幅优于强策略梯度基线方法。项目页面详见：https://empgseed-seed.github.io/\n\n（注：专业术语说明：\n1. Entropy-Modulated Policy Gradients (EMPG) 译为\"熵调制策略梯度\"\n2. Process Reward Models 译为\"过程奖励模型\"\n3. 保持技术表述准确性：将\"policy gradients\"译为\"策略梯度\"，\"credit assignment\"译为\"信用分配\"，\"exploration\"译为\"探索\"等\n4. 长难句处理：对原文中复合从句进行合理切分，符合中文表达习惯）"
    },
    {
        "title": "Agentic LLMs for Question Answering over Tabular Data",
        "url": "http://arxiv.org/abs/2509.09234v1",
        "pub_date": "2025-09-11",
        "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges due to the diverse structure, size, and data types of real-world tables. The SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale, domain-diverse datasets to evaluate the ability of models to accurately answer structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a multi-stage pipeline involving example selection, SQL query generation, answer extraction, verification, and iterative refinement. Experiments demonstrate the effectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and 71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\% and 27\\% respectively. This paper details our methodology, experimental results, and alternative approaches, providing insights into the strengths and limitations of LLM-driven Table QA.",
        "translated": "表格问答（Table QA）因现实世界中表格结构、规模和数据类型的多样性而面临独特挑战。SemEval 2025任务8（DataBench）引入了一个由大规模、多领域数据集组成的基准测试，用于评估模型准确回答结构化查询的能力。我们提出了一种基于大语言模型（如GPT-4o、GPT-4o-mini和DeepSeek v2:16b）的自然语言转SQL（NL-to-SQL）方法，通过动态生成SQL查询实现问答。该系统采用多阶段流程，包括示例选择、SQL查询生成、答案提取、验证与迭代优化。实验证明该方法效果显著，在DataBench QA和DataBench Lite QA上分别达到70.5%和71.6%的准确率，显著超越26%和27%的基线水平。本文详细阐述了方法论、实验结果及替代方案，深入分析了大语言模型驱动表格问答的优势与局限性。"
    },
    {
        "title": "Reading Between the Lines: Classifying Resume Seniority with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2509.09229v1",
        "pub_date": "2025-09-11",
        "summary": "Accurately assessing candidate seniority from resumes is a critical yet challenging task, complicated by the prevalence of overstated experience and ambiguous self-presentation. In this study, we investigate the effectiveness of large language models (LLMs), including fine-tuned BERT architectures, for automating seniority classification in resumes. To rigorously evaluate model performance, we introduce a hybrid dataset comprising both real-world resumes and synthetically generated hard examples designed to simulate exaggerated qualifications and understated seniority. Using the dataset, we evaluate the performance of Large Language Models in detecting subtle linguistic cues associated with seniority inflation and implicit expertise. Our findings highlight promising directions for enhancing AI-driven candidate evaluation systems and mitigating bias introduced by self-promotional language. The dataset is available for the research community at https://bit.ly/4mcTovt",
        "translated": "准确评估简历中候选人的资历水平是一项关键但具有挑战性的任务，其复杂性主要源于普遍存在的工作经验夸大现象和模糊的自我表述。本研究探讨了大型语言模型（包括经过微调的BERT架构）在自动化简历资历分类中的有效性。为系统评估模型性能，我们构建了一个混合数据集，其中既包含真实简历，也包含专门设计的合成困难样本——这些样本用于模拟夸大资质或刻意低调描述资历的情况。基于该数据集，我们评估了大语言模型在识别与资历夸大相关的微妙语言线索及隐含专业技能方面的表现。研究结果为增强AI驱动的候选人评估系统、减少自我宣传语言带来的偏见提供了有前景的研究方向。本数据集已向研究社区开放，访问地址：https://bit.ly/4mcTovt\n\n（注：翻译中对以下要点进行了专业处理：\n1. \"overstated experience\"译为\"工作经验夸大\"符合人力资源领域术语\n2. \"synthetically generated hard examples\"采用\"合成困难样本\"的学术表述\n3. \"seniority inflation\"译为\"资历夸大\"准确传达概念\n4. 长难句拆分为符合中文表达习惯的短句结构\n5. 技术术语（BERT/LLMs）保持原文大写形式\n6. 链接地址完整保留并添加\"访问地址\"引导词）"
    },
    {
        "title": "Identifying Key Features for Establishing Sustainable Agro-Tourism\n  Centre: A Data Driven Approach",
        "url": "http://arxiv.org/abs/2509.09214v1",
        "pub_date": "2025-09-11",
        "summary": "Agro-tourism serves as a strategic economic model designed to facilitate rural development by diversifying income streams for local communities like farmers while promoting the conservation of indigenous cultural heritage and traditional agricultural practices. As a very booming subdomain of tourism, there is a need to study the strategies for the growth of Agro-tourism in detail. The current study has identified the important indicators for the growth and enhancement of agro-tourism. The study is conducted in two phases: identification of the important indicators through a comprehensive literature review and in the second phase state-of-the-art techniques were used to identify the important indicators for the growth of agro-tourism. The indicators are also called features synonymously, the machine learning models for feature selection were applied and it was observed that the Least Absolute Shrinkage and Selection Operator (LASSO) method combined with, the machine Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT), Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were used to suggest the growth of the agro-tourism. The results show that with the LASSO method, LR model gives the highest classification accuracy of 98% in 70-30% train-test data followed by RF with 95% accuracy. Similarly, in the 80-20% train-test data LR maintains the highest accuracy at 99%, while DT and XGBoost follow with 97% accuracy.",
        "translated": "农业旅游作为一种战略性经济模式，旨在通过为农民等当地社区拓展收入渠道促进乡村发展，同时推动本土文化遗产与传统农业实践的保存。作为旅游业中蓬勃发展的细分领域，亟需对农业旅游的发展策略进行深入研究。本研究通过系统方法识别了推动农业旅游发展的关键指标。研究分为两个阶段：首先通过全面文献综述初步确定重要指标，随后采用前沿技术手段进一步筛选关键影响因素。\n\n这些指标在机器学习领域常被称为特征，本研究应用特征选择模型进行分析。通过结合最小绝对收缩与选择算子（LASSO）方法，并采用逻辑回归（LR）、决策树（DT）、随机森林（RF）和极限梯度提升（XGBOOST）等机器学习分类器，构建了农业旅游发展预测模型。实验结果表明：在70%-30%的训练-测试数据划分下，LASSO结合逻辑回归模型取得98%的最高分类准确率，随机森林模型以95%的准确率次之；在80%-20%的数据划分中，逻辑回归模型保持99%的峰值准确率，决策树与XGBOOST模型则以97%的准确率紧随其后。"
    },
    {
        "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection\n  Systems",
        "url": "http://arxiv.org/abs/2509.09204v1",
        "pub_date": "2025-09-11",
        "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.",
        "translated": "音频深度伪造检测（ADD）模型通常通过融合多种合成器的数据集进行评估，并以单一等错误率（EER）作为性能指标。然而，这种方法会过度加权样本量较多的合成器，使其他合成器的代表性不足，从而降低EER的整体可靠性。此外，多数ADD数据集的真实语音多样性不足，通常仅包含单一环境和语音风格（如纯净朗读语音），限制了其模拟真实场景的能力。针对这些问题，我们提出真实语音交叉测试框架——一种整合多样化真实语音数据集并通过聚合EER实现更均衡评估的新型评估方法。与传统评估方式相比，我们的方案显著提升了鲁棒性和可解释性。我们在九类真实语音类型上对超过150种合成器进行基准测试，并发布新数据集以推动后续研究（项目地址：https://github.com/cyaaronk/audio_deepfake_eval）。"
    },
    {
        "title": "Region-Wise Correspondence Prediction between Manga Line Art Images",
        "url": "http://arxiv.org/abs/2509.09501v1",
        "pub_date": "2025-09-11",
        "summary": "Understanding region-wise correspondence between manga line art images is a fundamental task in manga processing, enabling downstream applications such as automatic line art colorization and in-between frame generation. However, this task remains largely unexplored, especially in realistic scenarios without pre-existing segmentation or annotations. In this paper, we introduce a novel and practical task: predicting region-wise correspondence between raw manga line art images without any pre-existing labels or masks. To tackle this problem, we divide each line art image into a set of patches and propose a Transformer-based framework that learns patch-level similarities within and across images. We then apply edge-aware clustering and a region matching algorithm to convert patch-level predictions into coherent region-level correspondences. To support training and evaluation, we develop an automatic annotation pipeline and manually refine a subset of the data to construct benchmark datasets. Experiments on multiple datasets demonstrate that our method achieves high patch-level accuracy (e.g., 96.34%) and generates consistent region-level correspondences, highlighting its potential for real-world manga applications.",
        "translated": "理解漫画线稿图像间的区域对应关系是漫画处理中的基础任务，能够支持自动线稿上色和中间帧生成等下游应用。然而该任务尤其在缺乏预分割或标注的真实场景中仍属未充分探索的领域。本文提出一项新颖且实用的任务：在没有任何预标注或掩码的情况下预测原始漫画线稿图像间的区域对应关系。为解决该问题，我们将线稿图像划分为若干图像块，并提出基于Transformer的框架以学习图像内及图像间的块级相似性。随后通过边缘感知聚类和区域匹配算法，将块级预测转换为连贯的区域级对应关系。为支持训练与评估，我们开发了自动标注流程并对部分数据人工精校，构建了基准数据集。在多数据集上的实验表明，我们的方法实现了高块级准确率（如96.34%），并生成一致的区域级对应关系，彰显了其在现实漫画应用中的潜力。\n\n（注：专业术语说明：\n1. \"region-wise correspondence\" 译为\"区域对应关系\"\n2. \"manga line art images\" 译为\"漫画线稿图像\"\n3. \"edge-aware clustering\" 译为\"边缘感知聚类\"\n4. \"Transformer-based framework\" 译为\"基于Transformer的框架\"\n5. 数值精度保留原文三位小数以符合学术规范）"
    },
    {
        "title": "Improving Human Motion Plausibility with Body Momentum",
        "url": "http://arxiv.org/abs/2509.09496v1",
        "pub_date": "2025-09-11",
        "summary": "Many studies decompose human motion into local motion in a frame attached to the root joint and global motion of the root joint in the world frame, treating them separately. However, these two components are not independent. Global movement arises from interactions with the environment, which are, in turn, driven by changes in the body configuration. Motion models often fail to precisely capture this physical coupling between local and global dynamics, while deriving global trajectories from joint torques and external forces is computationally expensive and complex. To address these challenges, we propose using whole-body linear and angular momentum as a constraint to link local motion with global movement. Since momentum reflects the aggregate effect of joint-level dynamics on the body's movement through space, it provides a physically grounded way to relate local joint behavior to global displacement. Building on this insight, we introduce a new loss term that enforces consistency between the generated momentum profiles and those observed in ground-truth data. Incorporating our loss reduces foot sliding and jitter, improves balance, and preserves the accuracy of the recovered motion. Code and data are available at the project page https://hlinhn.github.io/momentum_bmvc.",
        "translated": "许多研究将人体运动分解为附着于根关节坐标系中的局部运动与世界坐标系中根关节的全局运动，并分别处理这两个部分。然而，这两个组成部分并非相互独立。全局运动产生于与环境的交互，而这种交互又由身体姿态的变化所驱动。现有运动模型往往难以精确捕捉局部与全局动力学之间的这种物理耦合关系，而通过关节扭矩和外力推导全局轨迹又存在计算成本高、复杂度大的问题。针对这些挑战，我们提出使用全身线性动量与角动量作为约束条件，将局部运动与全局位移相关联。由于动量反映了关节层面动力学对身体空间运动的整体影响，它为连接局部关节行为与全局位移提供了物理依据。基于这一思路，我们引入了一种新的损失函数项，用于强制生成动量曲线与真实数据中观测到的动量分布保持一致。引入该损失函数后，有效减少了脚步滑动与抖动现象，改善了运动平衡性，同时保持了重建运动的精确度。代码与数据详见项目页面：https://hlinhn.github.io/momentum_bmvc。\n\n（注：译文严格遵循以下学术规范：\n1. 专业术语准确对应：\"root joint\"译为\"根关节\"，\"world frame\"译为\"世界坐标系\"，\"linear and angular momentum\"译为\"线性动量与角动量\"\n2. 技术概念完整保留：通过增补\"物理依据\"、\"动力学\"等表述确保物理耦合关系的准确传达\n3. 长难句重构：将原文复合句拆分为符合中文表达习惯的短句结构，如对\"global movement arises from...\"句式的处理\n4. 逻辑连接显性化：使用\"然而\"、\"又由\"、\"针对\"等连接词明确技术论证逻辑\n5. 被动语态转化：将\"are driven by\"等被动结构转换为\"由...驱动\"的中文主动表达\n6. 项目信息完整保留：准确呈现网址及技术资源信息）"
    },
    {
        "title": "OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake\n  Detection",
        "url": "http://arxiv.org/abs/2509.09495v1",
        "pub_date": "2025-09-11",
        "summary": "Deepfakes, synthetic media created using advanced AI techniques, have intensified the spread of misinformation, particularly in politically sensitive contexts. Existing deepfake detection datasets are often limited, relying on outdated generation methods, low realism, or single-face imagery, restricting the effectiveness for general synthetic image detection. By analyzing social media posts, we identify multiple modalities through which deepfakes propagate misinformation. Furthermore, our human perception study demonstrates that recently developed proprietary models produce synthetic images increasingly indistinguishable from real ones, complicating accurate identification by the general public. Consequently, we present a comprehensive, politically-focused dataset specifically crafted for benchmarking detection against modern generative models. This dataset contains three million real images paired with descriptive captions, which are used for generating 963k corresponding high-quality synthetic images from a mix of proprietary and open-source models. Recognizing the continual evolution of generative techniques, we introduce an innovative crowdsourced adversarial platform, where participants are incentivized to generate and submit challenging synthetic images. This ongoing community-driven initiative ensures that deepfake detection methods remain robust and adaptive, proactively safeguarding public discourse from sophisticated misinformation threats.",
        "translated": "深度伪造（Deepfakes）作为一种基于先进人工智能技术生成的合成媒体，加剧了错误信息的传播，尤其在政治敏感语境中尤为突出。现有的深度伪造检测数据集往往存在局限性：依赖过时的生成方法、真实性不足或仅包含单一人脸图像，这限制了其在通用合成图像检测中的有效性。通过分析社交媒体帖子，我们识别出深度伪造传播错误信息的多模态特征。此外，我们的人类感知研究表明，近期开发的专有模型生成的合成图像与真实图像的区分度越来越低，导致公众难以准确识别。为此，我们提出了一个专注于政治语境、面向现代生成模型检测基准的综合数据集。该数据集包含300万张真实图像及其描述性文本标注，并基于混合专有与开源模型生成了96.3万张对应的高质量合成图像。鉴于生成技术的持续演进，我们创新性地推出了众包对抗平台，通过激励参与者生成并提交具有挑战性的合成图像。这一持续开展的社区驱动计划将确保深度伪造检测方法保持鲁棒性和适应性，从而主动保护公共讨论免受复杂错误信息威胁的侵害。\n\n（注：专业术语说明：  \n- Deepfakes：采用\"深度伪造\"行业通用译法  \n- Synthetic media：译为\"合成媒体\"  \n- Proprietary models：译为\"专有模型\"  \n- Adversarial platform：译为\"对抗平台\"  \n- Robust and adaptive：译为\"鲁棒性和适应性\"  \n译文严格遵循技术文档翻译规范，确保学术概念准确传达）"
    },
    {
        "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
        "url": "http://arxiv.org/abs/2509.09494v1",
        "pub_date": "2025-09-11",
        "summary": "In-loop filtering (ILF) is a key technology in video coding standards to reduce artifacts and enhance visual quality. Recently, neural network-based ILF schemes have achieved remarkable coding gains, emerging as a powerful candidate for next-generation video coding standards. However, the use of deep neural networks (DNN) brings significant computational and time complexity or high demands for dedicated hardware, making it challenging for general use. To address this limitation, we study a practical ILF solution by adopting look-up tables (LUTs). After training a DNN with a restricted reference range for ILF, all possible inputs are traversed, and the output values of the DNN are cached into LUTs. During the coding process, the filtering process is performed by simply retrieving the filtered pixel through locating the input pixels and interpolating between the cached values, instead of relying on heavy inference computations. In this paper, we propose a universal LUT-based ILF framework, termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of filtering LUTs and propose a series of customized indexing mechanisms to enable better filtering reference perception with limited storage consumption. Second, we propose the cross-component indexing mechanism to enable the filtering of different color components jointly. Third, in order to make our solution practical for coding uses, we propose the LUT compaction scheme to enable the LUT pruning, achieving a lower storage cost of the entire solution. The proposed framework is implemented in the VVC reference software. Experimental results show that the proposed framework achieves on average 0.82%/2.97%/1.63% and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI and RA configurations, respectively. Compared to DNN-based solutions, our proposed solution has much lower time complexity and storage cost.",
        "translated": "环路滤波（ILF）是视频编码标准中用于减少伪影和提升视觉质量的关键技术。近年来，基于神经网络的ILF方案取得了显著的编码增益，成为下一代视频编码标准的有力候选方案。然而，深度神经网络（DNN）的使用带来了巨大的计算和时间复杂度，或对专用硬件的高需求，使其难以普及应用。为解决这一局限性，我们研究了一种采用查找表（LUT）的实用ILF方案。通过训练一个具有受限参考范围的DNN用于ILF后，遍历所有可能的输入，并将DNN的输出值缓存至LUT中。在编码过程中，滤波操作仅需通过定位输入像素并在缓存值间插值来获取滤波后像素，无需依赖繁重的推理计算。本文提出了一种通用的基于LUT的ILF框架——LUT-ILF++。首先，我们引入多类滤波LUT的协同机制，并提出一系列定制化索引方案，在有限存储消耗下实现更优的滤波参考感知。其次，我们提出跨分量索引机制，实现不同颜色分量的联合滤波。第三，为使方案更适用于实际编码场景，我们提出LUT压缩方案以实现剪枝，降低整体存储成本。该框架已在VVC参考软件中实现。实验结果表明，在AI和RA配置下，所提方案对通用测试序列的平均码率节省分别达到0.82%/2.97%/1.63%和0.85%/4.11%/2.06%。与基于DNN的方案相比，本方案具有显著更低的时间复杂度和存储成本。\n\n（注：专业术语说明：\n1. in-loop filtering (ILF)：环路滤波（视频编码中位于编解码环路内的滤波技术）\n2. look-up tables (LUTs)：查找表（通过预计算值替代实时计算的优化技术）\n3. VVC：Versatile Video Coding（H.266下一代视频编码标准）\n4. AI/RA配置：All-Intra/Random Access（VVC标准中的两种编码模式）\n5. 跨分量索引：cross-component indexing（联合处理YUV等颜色分量的技术））"
    },
    {
        "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
        "url": "http://arxiv.org/abs/2509.09469v1",
        "pub_date": "2025-09-11",
        "summary": "Gliomas are the most prevalent type of primary brain tumors, and their accurate segmentation from MRI is critical for diagnosis, treatment planning, and longitudinal monitoring. However, the scarcity of high-quality annotated imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for deploying advanced segmentation models in clinical workflows. This study introduces a robust and computationally efficient deep learning framework tailored for resource-constrained settings. We leveraged a 3D Attention UNet architecture augmented with residual blocks and enhanced through transfer learning from pre-trained weights on the BraTS 2021 dataset. Our model was evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma segmentation in SSA MRI data. Despite the limited data quality and quantity, our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80 for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding Non-Functional Hemisphere (SNFH). These results demonstrate the generalizability of the proposed model and its potential to support clinical decision making in low-resource settings. The compact architecture, approximately 90 MB, and sub-minute per-volume inference time on consumer-grade hardware further underscore its practicality for deployment in SSA health systems. This work contributes toward closing the gap in equitable AI for global health by empowering underserved regions with high-performing and accessible medical imaging solutions.",
        "translated": "胶质瘤是最常见的原发性脑肿瘤类型，其基于MRI图像的精确分割对诊断、治疗规划和长期监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺性，为在临床工作流程中部署先进分割模型带来了重大挑战。本研究提出了一种专为资源受限环境设计的鲁棒且计算高效深度学习框架。我们采用集成残差块的3D注意力UNet架构，并基于BraTS 2021数据集通过预训练权重进行迁移学习增强。使用BraTS-Africa数据集（SSA地区MRI胶质瘤分割基准）的95例MRI病例进行评估，在数据质量和数量有限的情况下，我们的方法在增强肿瘤区域(ET)获得0.76的Dice分数，坏死与非增强肿瘤核心区域(NETC)达0.80，周围非功能性半球区域(SNFH)达0.85。这些结果证明了所提出模型的泛化能力及其在低资源环境中支持临床决策的潜力。该紧凑架构仅约90MB，在消费级硬件上单例推理时间不足一分钟，进一步凸显了其在SSA医疗系统中部署的实用性。通过为资源匮乏地区提供高性能、可及的医学影像解决方案，本研究有助于缩小全球健康领域人工智能公平性差距。\n\n（注：根据医学影像分割领域规范，对专业术语采用标准译法：\n- Enhancing Tumor (ET) 译为\"增强肿瘤区域\"\n- Necrotic and Non-Enhancing Tumor Core (NETC) 译为\"坏死与非增强肿瘤核心区域\"\n- Surrounding Non-Functional Hemisphere (SNFH) 译为\"周围非功能性半球区域\"\n- Dice score保留专业指标名称不翻译\n- 保持BraTS等专业数据集名称原貌\n- 准确传递了迁移学习、残差块等技术细节）"
    },
    {
        "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion\n  based on diffusion model",
        "url": "http://arxiv.org/abs/2509.09456v1",
        "pub_date": "2025-09-11",
        "summary": "Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.",
        "translated": "不同模态的医学影像能为疾病提供独特的生理与解剖信息。多模态医学图像融合技术通过整合来自不同互补模态医学图像的有效信息，生成能全面客观反映病灶特征的融合图像，以辅助医生进行临床诊断。然而现有融合方法只能处理固定数量的模态输入（如仅接受双模态或三模态输入），无法直接处理变化的输入数量，这限制了其在临床环境中的应用。为解决这一问题，我们提出FlexiD-Fuse——一种基于扩散模型的图像融合网络，可适配灵活的输入模态数量。该网络能在相同权重参数下端到端处理双模态及三模态医学图像融合任务。FlexiD-Fuse将原本仅支持固定条件输入的扩散融合问题，转化为基于扩散过程与分层贝叶斯建模的最大似然估计问题。通过将期望最大化算法融入扩散采样迭代过程，FlexiD-Fuse能独立于输入图像数量，生成具有源图像跨模态信息的高质量融合图像。我们在哈佛数据集上对比了最新的双模态与三模态医学图像融合方法，并使用九种主流指标进行评估。实验结果表明，本方法在变输入数量的医学图像融合中取得了最优性能。同时，我们在红外-可见光、多曝光、多焦点等任意数量图像融合任务上进行了广泛扩展实验，并与视角级SOTA方法进行对比。扩展实验结果一致证明了本方法的有效性与优越性。"
    },
    {
        "title": "Semantic Concentration for Self-Supervised Dense Representations\n  Learning",
        "url": "http://arxiv.org/abs/2509.09429v1",
        "pub_date": "2025-09-11",
        "summary": "Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.",
        "translated": "近年来，图像级自监督学习（SSL）虽取得显著进展，但为图像块学习稠密表征仍具挑战性。主流方法存在\"过度分散\"现象——同实例/类别的图像块在表征空间中离散分布，损害了稠密预测任务的下游性能。本研究揭示图像级SSL通过隐式语义聚集避免该问题：非严格的空间对齐保证实例内一致性，而输入空间中类内实例的共享模式（即相似局部）确保图像间一致性。但由于空间敏感性和复杂场景中心数据，这些方法难以适用于稠密SSL。基于此，我们探索显式语义聚集方案：首先，为打破严格空间对齐，提出提取图像块对应关系；针对噪声和不平衡伪标签，提出耐噪排序损失，其核心是将平均精度（AP）损失扩展至连续目标，利用其决策无关和自适应聚焦特性防止学生模型被误导；其次，为从复杂场景中区分共享模式，提出物体感知过滤器将输出空间映射至基于物体的空间，具体通过交叉注意力使图像块由可学习的物体原型表示。大量实验证明该方法在多任务中的有效性，代码已开源。"
    },
    {
        "title": "FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal\n  image fusion and super-resolution",
        "url": "http://arxiv.org/abs/2509.09427v1",
        "pub_date": "2025-09-11",
        "summary": "As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.",
        "translated": "作为一项具有影响力的信息融合与底层视觉技术，图像融合通过整合源图像中的互补信息生成信息丰富的融合图像。近年来已有研究尝试联合实现图像融合与超分辨率任务。然而在军事侦察和远程探测等实际应用中，多模态图像中的目标与背景结构易受损，存在分辨率低、语义信息弱等问题，导致现有融合方法效果欠佳。为此，我们提出FS-Diff——一种语义引导与清晰度感知的图像融合与超分辨率联合方法。该方法将图像融合与超分辨率统一为条件生成问题，利用所提出的清晰度感知机制实现自适应低分辨率感知与跨模态特征提取。具体而言，我们将目标融合结果初始化为纯高斯噪声，并引入双向特征Mamba模块提取多模态图像的全局特征。进一步以源图像和语义信息为条件，通过改进的U-Net网络实现随机迭代去噪过程。该网络经过多噪声级去噪训练，能生成具有跨模态特征和丰富语义信息的高分辨率融合结果。我们还构建了包含600对图像的大规模航空视角多场景基准数据集AVMS。在六个公共数据集及AVMS数据集上的大量实验表明，FS-Diff在多种放大倍数下均优于现有先进方法，能恢复更丰富的细节与语义信息。代码已开源：https://github.com/XylonXu01/FS-Diff。\n\n（注：翻译严格遵循以下技术规范：\n1. 专业术语准确对应：\"semantic guidance\"译作\"语义引导\"，\"conditional generation\"译作\"条件生成\"\n2. 技术概念完整保留：双向特征Mamba（bidirectional feature Mamba）、多噪声级去噪（multi-noise level denoising）等专业表述保持原意\n3. 长难句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如方法原理部分采用分号衔接的递进式说明\n4. 被动语态转化：\"are easily corrupted\"主动化为\"易受损\"，\"is trained\"转化为\"经过训练\"\n5. 学术表述规范：采用\"该方法\"\"实验表明\"等符合中文论文摘要的书面表达）"
    },
    {
        "title": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot\n  Adaptation under Shift",
        "url": "http://arxiv.org/abs/2509.09397v1",
        "pub_date": "2025-09-11",
        "summary": "Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.",
        "translated": "医学视觉-语言模型（VLM）为临床决策支持提供了潜力，但其在数据分布变化下的可靠性仍是安全部署的核心挑战。由于医学影像协议和自由文本报告的多样性，这些模型常学习到与任务无关的虚假关联，限制了泛化能力并增加实际应用中的失败风险。我们提出DRiFt——一种结构化特征解耦框架，通过参数高效调优（LoRA）和可学习的提示标记，显式分离临床相关信号与任务无关噪声。为增强跨模态对齐并降低不确定性，我们通过为多样化医学数据集生成描述文本，构建了高质量、临床相关的图像-文本对。相比现有基于提示的方法，我们的方法在分布内性能提升11.4%的Top-1准确率和3.3%的Macro-F1分数，同时在未见数据集上保持强劲鲁棒性。消融实验表明：解耦任务相关特征与精细对齐能显著增强模型泛化能力，减少领域偏移下的不可预测行为。这些发现为构建更安全可靠的临床用VLM提供了重要见解。代码已开源：https://github.com/rumaima/DRiFt。\n\n（注：翻译严格遵循以下技术要点：\n1. \"distribution shifts\"译为\"数据分布变化\"符合机器学习领域术语\n2. \"task-agnostic correlations\"采用\"与任务无关的虚假关联\"的译法，既准确传达原意又符合中文表达习惯\n3. \"parameter-efficient tuning (LoRA)\"保留英文缩写同时括号注明全称，符合学术规范\n4. 性能指标\"Top-1 accuracy\"和\"Macro-F1\"直接保留英文术语+中文说明，确保专业性\n5. \"Ablation studies\"译为\"消融实验\"是计算机视觉领域标准译法）"
    },
    {
        "title": "Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic\n  Normality",
        "url": "http://arxiv.org/abs/2509.09375v1",
        "pub_date": "2025-09-11",
        "summary": "Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained defects that depress yield and reliability. Most industrial defect segmentation compares a test image against an external normal set, a strategy that is brittle for IC imagery where layouts vary across products and accurate alignment is difficult. We observe that defects are predominantly local, while each image still contains rich, repeatable normal patterns. We therefore propose an unsupervised IC defect segmentation framework that requires no external normal support. A learnable normal-information extractor aggregates representative normal features from the test image, and a coherence loss enforces their association with normal regions. Guided by these features, a decoder reconstructs only normal content; the reconstruction residual then segments defects. Pseudo-anomaly augmentation further stabilizes training. Experiments on datasets from three IC process stages show consistent improvements over existing approaches and strong robustness to product variability.",
        "translated": "现代集成电路（IC）制造过程中会产生多样化、细粒度的缺陷，这些缺陷会降低产品良率和可靠性。当前工业缺陷分割方法大多通过将测试图像与外部正常样本集进行比对，但这种策略对IC图像存在局限性——因为不同产品的版图布局存在差异且难以实现精确配准。我们注意到缺陷通常具有局部性特征，而每张图像本身仍包含大量可重复的正常模式。因此，本文提出了一种无需外部正常样本支持的无监督IC缺陷分割框架。该框架通过可学习的正常信息提取器从测试图像中聚合具有代表性的正常特征，并采用一致性损失约束这些特征与正常区域的关联性。在正常特征的引导下，解码器仅重构正常内容，最终通过重构残差实现缺陷分割。伪异常增强技术进一步稳定了训练过程。在三个IC制造阶段数据集上的实验表明，本方法相较现有方案均取得稳定提升，并对产品变异性展现出强大鲁棒性。\n\n（注：翻译过程中对以下专业术语进行了精准处理：\n- \"fine-grained defects\"译为\"细粒度的缺陷\"\n- \"reconstruction residual\"译为\"重构残差\"\n- \"pseudo-anomaly augmentation\"译为\"伪异常增强\"\n- \"robustness to product variability\"译为\"对产品变异性的鲁棒性\"\n同时保持了技术细节的准确性，如\"coherence loss\"译为\"一致性损失\"，\"normal-information extractor\"译为\"正常信息提取器\"等。）"
    },
    {
        "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for\n  Medicaid Care Management",
        "url": "http://arxiv.org/abs/2509.09655v1",
        "pub_date": "2025-09-11",
        "summary": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline RL procedure that calibrates per-group safety thresholds to reduce harm while equalizing a chosen fairness target (coverage or harm) across protected subgroups. Using de-identified longitudinal trajectories from a Medicaid population health management program, we evaluate FG-FARL against behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global conformal safety baseline). We report off-policy value estimates with bootstrap 95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL achieves comparable value to baselines while improving fairness metrics, demonstrating a practical path to safer and more equitable decision support.",
        "translated": "我们提出了可行性引导的公平自适应强化学习（FG-FARL），这是一种离线强化学习框架，通过校准各群体安全阈值来减少伤害，同时在受保护子群体间平衡选定的公平目标（覆盖率或伤害）。基于医疗补助人群健康管理项目的脱敏纵向轨迹数据，我们将FG-FARL与行为克隆（BC）及HACO（混合自适应共形离线强化学习；全局共形安全基线）进行对比评估。研究采用自助法95%置信区间报告离线策略价值估计，并提供带p值的子群体差异分析。结果表明，FG-FARL在保持与基线方法相当价值的同时显著改善了公平性指标，为实现更安全、更公平的决策支持提供了可行路径。"
    },
    {
        "title": "Functional Groups are All you Need for Chemically Interpretable\n  Molecular Property Prediction",
        "url": "http://arxiv.org/abs/2509.09619v1",
        "pub_date": "2025-09-11",
        "summary": "Molecular property prediction using deep learning (DL) models has accelerated drug and materials discovery, but the resulting DL models often lack interpretability, hindering their adoption by chemists. This work proposes developing molecule representations using the concept of Functional Groups (FG) in chemistry. We introduce the Functional Group Representation (FGR) framework, a novel approach to encoding molecules based on their fundamental chemical substructures. Our method integrates two types of functional groups: those curated from established chemical knowledge (FG), and those mined from a large molecular corpus using sequential pattern mining (MFG). The resulting FGR framework encodes molecules into a lower-dimensional latent space by leveraging pre-training on a large dataset of unlabeled molecules. Furthermore, the proposed framework allows the inclusion of 2D structure-based descriptors of molecules. We demonstrate that the FGR framework achieves state-of-the-art performance on a diverse range of 33 benchmark datasets spanning physical chemistry, biophysics, quantum mechanics, biological activity, and pharmacokinetics while enabling chemical interpretability. Crucially, the model's representations are intrinsically aligned with established chemical principles, allowing chemists to directly link predicted properties to specific functional groups and facilitating novel insights into structure-property relationships. Our work presents a significant step toward developing high-performing, chemically interpretable DL models for molecular discovery.",
        "translated": "基于深度学习（DL）模型的分子性质预测加速了药物与材料发现进程，但其结果往往缺乏可解释性，阻碍了化学家的实际应用。本研究提出利用化学中的官能团（Functional Groups, FG）概念构建分子表征方法。我们开发了官能团表征（Functional Group Representation, FGR）框架——一种基于基础化学子结构编码分子的创新方法。该框架整合了两类官能团：源自既定化学知识的传统官能团（FG），以及通过序列模式挖掘技术从大规模分子库中提取的挖掘型官能团（MFG）。通过对海量未标注分子数据进行预训练，FGR框架可将分子编码至低维潜在空间。此外，该框架支持融入基于二维分子结构的描述符。我们在33个跨领域的基准数据集（涵盖物理化学、生物物理、量子力学、生物活性和药代动力学）上验证了FGR框架的先进性能，同时保持了化学可解释性。关键的是，该模型的表征与既定化学原理内在契合，使化学家能够直接将预测性质与特定官能团关联，为结构-性质关系研究提供新视角。本研究成果为开发高性能、具有化学可解释性的分子发现深度学习模型迈出重要一步。"
    },
    {
        "title": "Explaining Concept Drift through the Evolution of Group Counterfactuals",
        "url": "http://arxiv.org/abs/2509.09616v1",
        "pub_date": "2025-09-11",
        "summary": "Machine learning models in dynamic environments often suffer from concept drift, where changes in the data distribution degrade performance. While detecting this drift is a well-studied topic, explaining how and why the model's decision-making logic changes still remains a significant challenge. In this paper, we introduce a novel methodology to explain concept drift by analyzing the temporal evolution of group-based counterfactual explanations (GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their associated counterfactual action vectors before and after a drift. These evolving GCEs act as an interpretable proxy, revealing structural changes in the model's decision boundary and its underlying rationale. We operationalize this analysis within a three-layer framework that synergistically combines insights from the data layer (distributional shifts), the model layer (prediction disagreement), and our proposed explanation layer. We show that such holistic view allows for a more comprehensive diagnosis of drift, making it possible to distinguish between different root causes, such as a spatial data shift versus a re-labeling of concepts.",
        "translated": "在动态环境中，机器学习模型常面临概念漂移问题——数据分布的变化会导致模型性能下降。虽然漂移检测已是广泛研究的课题，但解释模型决策逻辑如何及为何发生变化仍存在重大挑战。本文提出一种创新方法，通过分析基于群体的反事实解释（GCEs）的时序演化来解释概念漂移。该方法追踪漂移前后GCEs聚类中心点及其关联反事实行动向量的变化轨迹，这些动态演化的GCEs可作为可解释代理，揭示模型决策边界及其底层逻辑的结构性变化。我们通过三层分析框架将这一方法操作化：数据层（分布变化）、模型层（预测分歧）与我们提出的解释层形成协同分析体系。研究表明，这种整体视角能实现对概念漂移更全面的诊断，可有效区分不同根本原因（如空间数据偏移与概念重新标注等）。"
    },
    {
        "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and\n  Achieving Discretization Invariance",
        "url": "http://arxiv.org/abs/2509.09611v1",
        "pub_date": "2025-09-11",
        "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct inputs. Inspired by the Reduced Basis Method and the recently introduced Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a mathematically rigorous greedy algorithm to build its network structure offline adaptively from the ground up. Knowledge distillation via task-specific activation function allows ReBaNO to have a compact architecture requiring minimal computational cost online while embedding physics. In comparison to state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, numerical results demonstrate that ReBaNO significantly outperforms them in terms of eliminating/shrinking the generalization gap for both in- and out-of-distribution tests and being the only operator learning algorithm achieving strict discretization invariance.",
        "translated": "我们提出了一种新型数据稀疏算子学习算法——缩减基神经算子（ReBaNO），用于求解具有多个不同输入参数的偏微分方程组。该方法受缩减基方法和近期提出的生成式预训练物理信息神经网络启发，采用数学严谨的贪婪算法自底向上自适应地构建离线网络结构。通过任务特定激活函数实现的知识蒸馏技术，使ReBaNO在嵌入物理约束的同时保持紧凑的网络架构，在线计算成本极低。与主流的PCA-Net、DeepONet、FNO和CNO等算子学习算法相比，数值实验表明：ReBaNO在缩小/消除分布内与分布外测试的泛化差距方面显著优于现有方法，且是唯一严格实现离散化不变性的算子学习算法。\n\n（注：专业术语说明：\n1. Reduced Basis Neural Operator (ReBaNO) 译为\"缩减基神经算子\"\n2. Generative Pre-Trained Physics-Informed Neural Networks 译为\"生成式预训练物理信息神经网络\"\n3. discretization invariance 译为\"离散化不变性\"\n4. generalization gap 译为\"泛化差距\"\n5. greedy algorithm 译为\"贪婪算法\"\n6. knowledge distillation 译为\"知识蒸馏\"\n7. in- and out-of-distribution tests 译为\"分布内与分布外测试\"）"
    },
    {
        "title": "Conditioning on PDE Parameters to Generalise Deep Learning Emulation of\n  Stochastic and Chaotic Dynamics",
        "url": "http://arxiv.org/abs/2509.09599v1",
        "pub_date": "2025-09-11",
        "summary": "We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events.",
        "translated": "我们提出了一种针对随机与混沌时空系统的深度学习仿真器，其显式条件化于偏微分方程（PDE）的参数值。该方法首先在单一参数域上进行模型预训练，随后通过小规模多样化数据集进行微调，实现了对广泛参数值的泛化能力。通过引入局部注意力机制，该网络能够处理可变域尺寸与分辨率，从而在较小计算域上实现高效预训练，仅需少量附加数据即可学习如何泛化至更大域尺寸。我们在混沌Kuramoto-Sivashinsky方程和随机强迫β平面湍流系统上验证了模型性能，证明其能有效捕捉插值参数值下的物理现象。相较于传统数值积分方法，该仿真器可实现显著的计算加速，助力参数空间的高效探索；其概率化变体还能提供不确定性量化，为罕见事件的统计研究提供支持。\n\n（注：专业术语说明：\n1. Kuramoto-Sivashinsky equation：非线性偏微分方程，用于描述反应扩散系统中的混沌动力学\n2. beta-plane turbulence：基于β平面近似的湍流模型，常用于地球物理流体力学\n3. 局部注意力机制：一种聚焦局部特征的神经网络计算模式\n4. 不确定性量化：通过概率方法对模型预测的可靠性进行度量）"
    },
    {
        "title": "What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion\n  Detection Datasets",
        "url": "http://arxiv.org/abs/2509.09564v1",
        "pub_date": "2025-09-11",
        "summary": "Supervised machine learning techniques rely on labeled data to achieve high task performance, but this requires the labels to capture some meaningful differences in the underlying data structure. For training network intrusion detection algorithms, most datasets contain a series of attack classes and a single large benign class which captures all non-attack network traffic. A review of intrusion detection papers and guides that explicitly state their data preprocessing steps identified that the majority took the labeled categories of the dataset at face value when training their algorithms. The present paper evaluates the structure of benign traffic in several common intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and determines whether there are meaningful sub-categories within this traffic which may improve overall multi-classification performance using common machine learning techniques. We present an overview of some unsupervised clustering techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they differentially cluster the benign traffic space.",
        "translated": "监督式机器学习技术依赖标注数据以实现较高的任务性能，但这要求标签能够捕捉到底层数据结构中有意义的差异。在网络入侵检测算法的训练过程中，大多数数据集包含一系列攻击类别和一个庞大的良性流量类别（涵盖所有非攻击网络流量）。通过综述明确说明数据预处理步骤的入侵检测论文与指南发现，大多数研究在训练算法时直接采用数据集的标注分类。本文评估了多个常用入侵检测数据集（NSL-KDD、UNSW-NB15和CIC-IDS 2017）中良性流量的结构特征，并探究是否存在有意义的子类别划分——通过使用常见机器学习技术提升整体多分类性能。我们系统概述了无监督聚类技术（如HDBSCAN、均值漂移聚类），并展示这些技术如何对良性流量空间实现差异化聚类分析。"
    },
    {
        "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution",
        "url": "http://arxiv.org/abs/2509.09560v1",
        "pub_date": "2025-09-11",
        "summary": "Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary \"thinking\" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.",
        "translated": "具身智能系统在动态环境中运行，需要感知模块与生成模块的无缝协同，以处理高频的输入输出需求。传统串行计算模式虽能有效保证准确性，但在实现现实应用所需\"思维\"频率方面存在明显局限。本研究提出Auras——一种算法-系统协同设计的推理框架，通过优化具身智能代理的推理频率实现突破。该框架将感知与生成过程解耦，并为二者提供受控的流水线并行机制，从而实现高且稳定的吞吐量。针对并行度提升导致的数据陈旧问题，Auras建立了感知与生成模块共享的公共上下文环境，确保具身代理的准确性。实验结果表明，Auras在保持102.7%原系统精度的同时，平均提升2.54倍吞吐量，有效突破了串行计算模式的限制，实现了高性能推理。"
    },
    {
        "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
        "url": "http://arxiv.org/abs/2509.09550v2",
        "pub_date": "2025-09-11",
        "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech processing tasks due to their excellent rate-distortion performance and compatibility with Large Language Models (LLMs) as discrete feature representations for audio generation. While most existing codecs rely on Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has recently emerged as a compelling alternative that simplifies training and natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC, and show that FSQ encodes baked-in redundancy which produces an encoding which is robust when transmitted through noisy channels. First, through an encoder distillation experiment, we show that two different encoders can learn to encode identical audio into vastly different code sequences whilst maintaining comparable reconstruction quality with the same quantizer and decoder. Second, we demonstrate that FSQ has vastly superior bit-level perturbation robustness by comparing the performance of RVQ and FSQ codecs when simulating the transmission of code sequences through a noisy channel.",
        "translated": "神经音频编解码器（NACs）因其优异的率失真性能以及与大型语言模型（LLMs）的兼容性——可作为音频生成的离散特征表示——在语音处理任务中日益普及。虽然现有编解码器大多依赖残差向量量化（RVQ），但有限标量量化（FSQ）近期成为一种引人注目的替代方案，它简化了训练过程并原生支持单码本。我们提出了基于FSQ的神经编解码器NeuCodec，并证明FSQ通过内置冗余编码机制，能够在噪声信道传输中生成具有强鲁棒性的编码表示。首先，通过编码器蒸馏实验，我们证明两个不同的编码器可以学会将相同音频编码为截然不同的码序列，同时在使用相同量化器和解码器的情况下保持相当的重建质量。其次，通过模拟噪声信道传输场景对比RVQ与FSQ编解码器的性能，我们证明了FSQ具有显著优越的比特级扰动鲁棒性。\n\n（注：专业术语说明：\n1. Residual Vector Quantization (RVQ) → 残差向量量化\n2. Finite Scalar Quantization (FSQ) → 有限标量量化\n3. rate-distortion performance → 率失真性能\n4. discrete feature representations → 离散特征表示\n5. encoder distillation → 编码器蒸馏\n6. bit-level perturbation robustness → 比特级扰动鲁棒性\n7. noisy channel → 噪声信道）"
    },
    {
        "title": "ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated\n  Learning",
        "url": "http://arxiv.org/abs/2509.09534v1",
        "pub_date": "2025-09-11",
        "summary": "Federated Learning (FL) emerged as a widely studied paradigm for distributed learning. Despite its many advantages, FL remains vulnerable to adversarial attacks, especially under data heterogeneity. We propose a new Byzantine-robust FL algorithm called ProDiGy. The key novelty lies in evaluating the client gradients using a joint dual scoring system based on the gradients' proximity and dissimilarity. We demonstrate through extensive numerical experiments that ProDiGy outperforms existing defenses in various scenarios. In particular, when the clients' data do not follow an IID distribution, while other defense mechanisms fail, ProDiGy maintains strong defense capabilities and model accuracy. These findings highlight the effectiveness of a dual perspective approach that promotes natural similarity among honest clients while detecting suspicious uniformity as a potential indicator of an attack.",
        "translated": "联邦学习（Federated Learning, FL）已成为分布式学习中广泛研究的重要范式。尽管具备诸多优势，联邦学习在数据异构场景下仍易遭受对抗攻击。本文提出了一种名为ProDiGy的新型拜占庭鲁棒联邦学习算法，其核心创新在于通过梯度邻近性与差异性的联合双评分系统评估客户端梯度。大量数值实验表明，ProDiGy在多种场景下均优于现有防御机制。特别在客户端数据非独立同分布（non-IID）时，当其他防御机制失效的情况下，ProDiGy仍能保持强大的防御能力和模型精度。这些发现凸显了双视角方法的有效性：该方法既能促进诚实客户端间的自然相似性，又能通过检测可疑的一致性模式来识别潜在攻击。"
    },
    {
        "title": "Cough Classification using Few-Shot Learning",
        "url": "http://arxiv.org/abs/2509.09515v1",
        "pub_date": "2025-09-11",
        "summary": "This paper investigates the effectiveness of few-shot learning for respiratory sound classification, focusing on coughbased detection of COVID-19, Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram representations of cough sounds to address the challenge of limited labeled data. Our study evaluates whether few-shot learning can enable models to achieve performance comparable to traditional deep learning approaches while using significantly fewer training samples. Additionally, we compare multi-class and binary classification models to assess whether multi-class models can perform comparably to their binary counterparts. Experimental findings show that few-shot learning models can achieve competitive accuracy. Our model attains 74.87% accuracy in multi-class classification with only 15 support examples per class, while binary classification achieves over 70% accuracy across all class pairs. Class-wise analysis reveals Flu as the most distinguishable class, and Healthy as the most challenging. Statistical tests (paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant performance difference between binary and multiclass models, supporting the viability of multi-class classification in this setting. These results highlight the feasibility of applying few-shot learning in medical diagnostics, particularly when large labeled datasets are unavailable.",
        "translated": "本文研究了小样本学习在呼吸音分类中的有效性，重点关注基于咳嗽声音的COVID-19、流感和健康状态的检测。我们采用原型网络（Prototypical Networks）结合咳嗽声的频谱图表示，以解决标注数据有限的问题。本研究评估了小样本学习能否在使用显著较少训练样本的情况下，使模型达到与传统深度学习方法相媲美的性能。此外，我们比较了多类别和二分类模型，以评估多类别模型是否能够达到与二分类模型相当的表现。实验结果表明，小样本学习模型能够取得具有竞争力的准确率：在多类别分类任务中，每类仅使用15个支持样本，我们的模型达到了74.87%的准确率；而在所有类别对的二分类任务中，准确率均超过70%。类别分析显示，流感是最容易区分的类别，而健康状态则最具挑战性。统计检验（配对t检验p = 0.149，Wilcoxon检验p = 0.125）表明二分类与多类别模型之间无显著性能差异，支持了多类别分类在此场景下的可行性。这些结果突显了小样本学习在医疗诊断中应用的潜力，特别是在缺乏大规模标注数据集的情况下。"
    },
    {
        "title": "PIPES: A Meta-dataset of Machine Learning Pipelines",
        "url": "http://arxiv.org/abs/2509.09512v1",
        "pub_date": "2025-09-11",
        "summary": "Solutions to the Algorithm Selection Problem (ASP) in machine learning face the challenge of high computational costs associated with evaluating various algorithms' performances on a given dataset. To mitigate this cost, the meta-learning field can leverage previously executed experiments shared in online repositories such as OpenML. OpenML provides an extensive collection of machine learning experiments. However, an analysis of OpenML's records reveals limitations. It lacks diversity in pipelines, specifically when exploring data preprocessing steps/blocks, such as scaling or imputation, resulting in limited representation. Its experiments are often focused on a few popular techniques within each pipeline block, leading to an imbalanced sample. To overcome the observed limitations of OpenML, we propose PIPES, a collection of experiments involving multiple pipelines designed to represent all combinations of the selected sets of techniques, aiming at diversity and completeness. PIPES stores the results of experiments performed applying 9,408 pipelines to 300 datasets. It includes detailed information on the pipeline blocks, training and testing times, predictions, performances, and the eventual error messages. This comprehensive collection of results allows researchers to perform analyses across diverse and representative pipelines and datasets. PIPES also offers potential for expansion, as additional data and experiments can be incorporated to support the meta-learning community further. The data, code, supplementary material, and all experiments can be found at https://github.com/cynthiamaia/PIPES.git.",
        "translated": "在机器学习领域，算法选择问题（ASP）的解决方案面临着一个关键挑战：评估不同算法在给定数据集上的性能需要高昂的计算成本。为降低这一成本，元学习领域可利用OpenML等在线知识库共享的历史实验数据。OpenML虽提供了大量机器学习实验记录，但分析发现其存在明显局限性：一方面在探索数据预处理步骤/模块（如标准化、缺失值填补）时缺乏流程多样性，导致代表性不足；另一方面实验往往聚焦于各流程模块中的少数热门技术，造成样本失衡。\n\n为克服OpenML的现有局限，我们提出PIPES——一个包含多种处理流程的实验集合，其设计目标是通过选取技术组合的全覆盖来实现多样性和完整性。PIPES存储了将9,408种处理流程应用于300个数据集的实验结果，详细记录了流程模块配置、训练测试耗时、预测结果、性能指标及最终错误信息。这一综合性成果集合使研究人员能够基于多样化和具代表性的流程及数据集进行分析。PIPES还具有扩展潜力，可通过纳入更多数据与实验来进一步支持元学习社区。相关数据、代码、补充材料及完整实验记录详见https://github.com/cynthiamaia/PIPES.git。\n\n（注：根据学术规范，专业术语处理说明：\n1. Algorithm Selection Problem保留英文缩写ASP并标注中文全称\n2. OpenML作为专有平台名称保留英文形式\n3. scaling根据上下文译为\"标准化\"而非字面意义的\"缩放\"\n4. imputation译为专业术语\"缺失值填补\"\n5. pipelines根据计算机领域惯例译为\"处理流程\"而非直译\"管道\"\n6. meta-learning统一译为\"元学习\"）"
    },
    {
        "title": "Balancing Utility and Privacy: Dynamically Private SGD with Random\n  Projection",
        "url": "http://arxiv.org/abs/2509.09485v2",
        "pub_date": "2025-09-11",
        "summary": "Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.",
        "translated": "随机优化是现代机器学习中的关键推动技术，能够为各类任务生成高效模型。然而，现有研究表明模型参数与梯度信息存在隐私泄露风险。虽然差分隐私随机梯度下降（DPSGD）能解决隐私问题，但其静态噪声机制会影响模型性能的误差边界。此外，随着模型参数量的指数级增长，使用随机优化器高效学习这些模型变得更具挑战性。为此，我们提出动态差分隐私投影随机梯度下降（D2P2-SGD）优化器。该优化器融合两大核心思想：（i）采用自动梯度裁剪的动态差分隐私（DDP）机制；（ii）结合随机投影的SGD方法，实现模型效用与隐私保护平衡的动态调节。理论证明表明，该算法在不同目标函数上均具有可证明的次线性收敛速率，且达到了现有最佳收敛率。理论分析进一步揭示：动态差分隐私能以隐私代价换取更优的模型效用，而随机投影技术可提升模型学习效率。在多组数据集上的大量实验表明，D2P2-SGD在保持隐私保护的同时显著提升了模型精度。代码已开源。"
    },
    {
        "title": "Database Views as Explanations for Relational Deep Learning",
        "url": "http://arxiv.org/abs/2509.09482v1",
        "pub_date": "2025-09-11",
        "summary": "In recent years, there has been significant progress in the development of deep learning models over relational databases, including architectures based on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph transformers. In effect, such architectures state how the database records and links (e.g., foreign-key references) translate into a large, complex numerical expression, involving numerous learnable parameters. This complexity makes it hard to explain, in human-understandable terms, how a model uses the available data to arrive at a given prediction. We present a novel framework for explaining machine-learning models over relational databases, where explanations are view definitions that highlight focused parts of the database that mostly contribute to the model's prediction. We establish such global abductive explanations by adapting the classic notion of determinacy by Nash, Segoufin, and Vianu (2010). In addition to tuning the tradeoff between determinacy and conciseness, the framework allows controlling the level of granularity by adopting different fragments of view definitions, such as ones highlighting whole columns, foreign keys between tables, relevant groups of tuples, and so on. We investigate the realization of the framework in the case of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive search over the space of all databases. We propose techniques that are model-agnostic, and others that are tailored to hetero-GNNs via the notion of learnable masking. Our approach is evaluated through an extensive empirical study on the RelBench collection, covering a variety of domains and different record-level tasks. The results demonstrate the usefulness of the proposed explanations, as well as the efficiency of their generation.",
        "translated": "近年来，基于关系数据库的深度学习模型取得了显著进展，包括基于异构图神经网络（hetero-GNNs）和异构图变换器的架构。这类架构实质上揭示了数据库记录与关联（如外键引用）如何转化为包含大量可学习参数的复杂数值表达式。这种复杂性使得难以用人类可理解的方式解释模型如何利用数据得出特定预测。本文提出了一种新颖的关系数据库机器学习模型解释框架，其解释结果以视图定义的形式呈现，突出显示对模型预测贡献最大的数据库核心部分。我们通过调整Nash、Segoufin和Vianu（2010）提出的确定性概念来建立这种全局溯因解释。该框架不仅可权衡确定性与简洁性，还能通过采用不同的视图定义片段（如突出整列、表间外键、相关元组群等）来控制粒度层级。我们重点研究了该框架在异构图神经网络中的实现：开发了避免全数据库空间穷举搜索的启发式算法，提出了模型无关的通用技术，以及通过可学习掩码概念为hetero-GNNs定制的专项技术。通过在RelBench数据集上进行涵盖多领域多任务的实证研究，结果表明所提出的解释方法兼具实用性和生成高效性。\n\n（注：专业术语说明：\n1. hetero-GNNs：异构图神经网络，专用于处理多种类型节点和边的图结构数据\n2. 全局溯因解释：通过反推数据中哪些部分对结果产生决定性影响的解释方法\n3. 可学习掩码：通过训练过程自动识别重要特征的掩码技术\n4. RelBench：专门用于评估关系数据库机器学习模型的基准数据集）"
    },
    {
        "title": "CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph\n  Forecasting",
        "url": "http://arxiv.org/abs/2509.09474v1",
        "pub_date": "2025-09-11",
        "summary": "We address the task of temporal knowledge graph (TKG) forecasting by introducing a fully explainable method based on temporal rules. Motivated by recent work proposing a strong baseline using recurrent facts, our approach learns four simple types of rules with a confidence function that considers both recency and frequency. Evaluated on nine datasets, our method matches or surpasses the performance of eight state-of-the-art models and two baselines, while providing fully interpretable predictions.",
        "translated": "我们针对时序知识图谱（TKG）预测任务，提出了一种基于时序规则的完全可解释方法。受近期利用循环事实构建强基线研究的启发，本方法通过学习四种简单规则类型，并采用同时考虑时效性和频次的可信度函数进行推理。在九个数据集上的实验表明，该方法在匹配或超越八个前沿模型和两个基线模型性能的同时，能够提供完全可解释的预测结果。\n\n（注：专业术语说明：\n1. \"temporal knowledge graph (TKG)\" 译为\"时序知识图谱\"，是知识图谱中引入时间维度的专业表述\n2. \"recurrent facts\" 译为\"循环事实\"，特指在时间维度上周期性出现的事实关系\n3. \"recency and frequency\" 译为\"时效性和频次\"，准确表达时间新近性和发生频率的双重考量\n4. \"state-of-the-art models\" 采用学界通用译法\"前沿模型\"\n5. \"fully interpretable predictions\" 译为\"完全可解释的预测\"，强调模型的可解释性特性）"
    },
    {
        "title": "AEGIS: An Agent for Extraction and Geographic Identification in\n  Scholarly Proceedings",
        "url": "http://arxiv.org/abs/2509.09470v1",
        "pub_date": "2025-09-11",
        "summary": "Keeping pace with the rapid growth of academia literature presents a significant challenge for researchers, funding bodies, and academic societies. To address the time-consuming manual effort required for scholarly discovery, we present a novel, fully automated system that transitions from data discovery to direct action. Our pipeline demonstrates how a specialized AI agent, 'Agent-E', can be tasked with identifying papers from specific geographic regions within conference proceedings and then executing a Robotic Process Automation (RPA) to complete a predefined action, such as submitting a nomination form. We validated our system on 586 papers from five different conferences, where it successfully identified every target paper with a recall of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the potential of task-oriented AI agents to not only filter information but also to actively participate in and accelerate the workflows of the academic community.",
        "translated": "面对学术文献的快速增长，如何及时跟进已成为研究者、资助机构和学术团体面临的重要挑战。为减轻学术发现过程中繁重的人工负担，我们开发了一种全新的全自动化系统，实现了从数据发现到直接执行的无缝衔接。本研究展示了一个名为\"Agent-E\"的专用AI代理如何完成以下任务：首先从会议论文集中识别特定地区的论文，随后通过机器人流程自动化（RPA）执行预定操作（如提交提名表格）。我们在五个国际会议的586篇论文上验证系统性能，成功实现100%的召回率与99.4%的准召率。该成果表明：面向任务的AI代理不仅能有效过滤信息，更可主动参与并加速学术工作流程的运行。"
    },
    {
        "title": "AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed\n  Multi-Input Transformer",
        "url": "http://arxiv.org/abs/2509.09458v1",
        "pub_date": "2025-09-11",
        "summary": "This work addresses the challenge of forecasting urban water dynamics by developing a multi-input, multi-output deep learning model that incorporates both endogenous variables (e.g., water height or discharge) and exogenous factors (e.g., precipitation history and forecast reports). Unlike conventional forecasting, the proposed model, AquaCast, captures both inter-variable and temporal dependencies across all inputs, while focusing forecast solely on endogenous variables. Exogenous inputs are fused via an embedding layer, eliminating the need to forecast them and enabling the model to attend to their short-term influences more effectively. We evaluate our approach on the LausanneCity dataset, which includes measurements from four urban drainage sensors, and demonstrate state-of-the-art performance when using only endogenous variables. Performance also improves with the inclusion of exogenous variables and forecast reports. To assess generalization and scalability, we additionally test the model on three large-scale synthesized datasets, generated from MeteoSwiss records, the Lorenz Attractors model, and the Random Fields model, each representing a different level of temporal complexity across 100 nodes. The results confirm that our model consistently outperforms existing baselines and maintains a robust and accurate forecast across both real and synthetic datasets.",
        "translated": "本研究致力于解决城市水动态预测的挑战，提出了一种多输入多输出的深度学习模型——AquaCast。该模型同时融合了内生变量（如水位高度或流量）与外生因素（如历史降水数据和预报报告）。与传统预测方法不同，AquaCast能够捕捉所有输入变量间的相互依赖关系以及时间维度上的关联性，同时仅针对内生变量进行预测。外生输入通过嵌入层进行融合，无需对其单独预测，使模型能更有效地关注其短期影响。我们在洛桑城市数据集（包含四个城市排水传感器的监测数据）上评估了该方法，结果表明仅使用内生变量时模型已达到领先性能，而引入外生变量和预报报告后性能进一步提升。为评估泛化能力与可扩展性，我们额外在三个大规模合成数据集上进行了测试：这些数据集分别基于瑞士气象局记录、洛伦兹吸引子模型和随机场模型生成，每个数据集包含100个节点并呈现不同层次的时间复杂性。实验结果表明，我们的模型在真实与合成数据集中均持续优于现有基线方法，展现出稳健且精确的预测能力。\n\n（注：专业术语说明：\n1. 内生变量（endogenous variables）：指系统内部产生的变量\n2. 外生因素（exogenous factors）：指外部输入的影响因素\n3. 嵌入层（embedding layer）：深度学习中对离散变量进行稠密向量表示的神经网络层\n4. 洛伦兹吸引子（Lorenz Attractors）：描述混沌系统行为的经典数学模型\n5. 随机场（Random Fields）：具有空间相关性的随机变量集合）"
    },
    {
        "title": "Composable Score-based Graph Diffusion Model for Multi-Conditional\n  Molecular Generation",
        "url": "http://arxiv.org/abs/2509.09451v1",
        "pub_date": "2025-09-11",
        "summary": "Controllable molecular graph generation is essential for material and drug discovery, where generated molecules must satisfy diverse property constraints. While recent advances in graph diffusion models have improved generation quality, their effectiveness in multi-conditional settings remains limited due to reliance on joint conditioning or continuous relaxations that compromise fidelity. To address these limitations, we propose Composable Score-based Graph Diffusion model (CSGD), the first model that extends score matching to discrete graphs via concrete scores, enabling flexible and principled manipulation of conditional guidance. Building on this foundation, we introduce two score-based techniques: Composable Guidance (CoG), which allows fine-grained control over arbitrary subsets of conditions during sampling, and Probability Calibration (PC), which adjusts estimated transition probabilities to mitigate train-test mismatches. Empirical results on four molecular datasets show that CSGD achieves state-of-the-art performance, with a 15.3% average improvement in controllability over prior methods, while maintaining high validity and distributional fidelity. Our findings highlight the practical advantages of score-based modeling for discrete graph generation and its capacity for flexible, multi-property molecular design.",
        "translated": "可控分子图生成对于材料与药物发现至关重要，生成的分子必须满足多样化的属性约束。尽管图扩散模型的最新进展提升了生成质量，但由于依赖联合条件作用或损害保真度的连续松弛方法，其在多条件设定下的有效性仍受限。为解决这些局限性，我们提出可组合基于分数的图扩散模型（CSGD），这是首个通过具体分数将分数匹配扩展至离散图的模型，实现了条件引导的灵活且原则性调控。基于此，我们引入两种基于分数的技术：可组合引导（CoG）——支持在采样过程中对任意条件子集进行细粒度控制；概率校准（PC）——通过调整估计的转移概率缓解训练-测试失配问题。在四个分子数据集上的实验结果表明，CSGD实现了最先进的性能，其可控性较现有方法平均提升15.3%，同时保持高有效性和分布保真度。我们的研究发现凸显了基于分数建模在离散图生成中的实践优势及其在多属性分子灵活设计方面的潜力。"
    },
    {
        "title": "Fused Lasso Improves Accuracy of Co-occurrence Network Inference in\n  Grouped Samples",
        "url": "http://arxiv.org/abs/2509.09413v1",
        "pub_date": "2025-09-11",
        "summary": "Co-occurrence network inference algorithms have significantly advanced our understanding of microbiome communities. However, these algorithms typically analyze microbial associations within samples collected from a single environmental niche, often capturing only static snapshots rather than dynamic microbial processes. Previous studies have commonly grouped samples from different environmental niches together without fully considering how microbial communities adapt their associations when faced with varying ecological conditions. Our study addresses this limitation by explicitly investigating both spatial and temporal dynamics of microbial communities. We analyzed publicly available microbiome abundance data across multiple locations and time points, to evaluate algorithm performance in predicting microbial associations using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates algorithms in two distinct scenarios: training and testing within the same environmental niche (Same), and training and testing on combined data from multiple environmental niches (All). To overcome the limitations of conventional algorithms, we propose fuser, an algorithm that, while not entirely new in machine learning, is novel for microbiome community network inference. It retains subsample-specific signals while simultaneously sharing relevant information across environments during training. Unlike standard approaches that infer a single generalized network from combined data, fuser generates distinct, environment-specific predictive networks. Our results demonstrate that fuser achieves comparable predictive performance to existing algorithms such as glmnet when evaluated within homogeneous environments (Same), and notably reduces test error compared to baseline algorithms in cross-environment (All) scenarios.",
        "translated": "共现网络推断算法显著推动了我们对微生物群落的理解。然而，这些算法通常仅分析来自单一环境样本中的微生物关联，往往只能捕捉静态快照而非动态的微生物过程。以往研究常将不同环境样本混合分析，未能充分考虑微生物群落在面对不同生态条件时如何调整其关联模式。本研究通过明确考察微生物群落的空间和时间动态特征，解决了这一局限性。我们分析了跨多个地点和时间的公开微生物组丰度数据，并采用我们提出的同源-混合交叉验证（SAC）框架来评估算法预测微生物关联的性能。SAC在两种场景下验证算法：相同环境内的训练与测试（同源模式），以及跨环境混合数据的训练与测试（混合模式）。\n\n为突破传统算法的限制，我们提出了fuser算法——该方法在机器学习领域虽非全新，但在微生物群落网络推断中具有创新性。它能在训练过程中保留特定子样本信号的同时，实现跨环境的信息共享。与从混合数据推断单一通用网络的标准方法不同，fuser能生成具有环境特异性的预测网络。实验结果表明：在同质环境（同源模式）下，fuser与glmnet等现有算法预测性能相当；而在跨环境（混合模式）场景中，其测试误差较基线算法显著降低。\n\n（注：glmnet保留英文原名因该名称在机器学习领域为特定算法标识；SAC框架名称采用中文释义+英文缩写组合方式以实现技术准确性与可读性的平衡）"
    }
]