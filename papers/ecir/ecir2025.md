# ECIR2025 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[kANNolo: Sweet and Smooth Approximate k-Nearest Neighbors Search](https://doi.org/10.1007/978-3-031-88717-8_29)|Leonardo Delfino, Domenico Erriquez, Silvio Martinico, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini||Approximate Nearest Neighbors (ANN) search is a crucial task in several applications like recommender systems and information retrieval. Current state-of-the-art ANN libraries, although being performance-oriented, often lack modularity and ease of use. This translates into them not being fully suitable for easy prototyping and testing of research ideas, an important feature to enable. We address these limitations by introducing kANNolo, a novel research-oriented ANN library written in Rust and explicitly designed to combine usability with performance effectively. kANNolo is the first ANN library that supports dense and sparse vector representations made available on top of different similarity measures, e.g., euclidean distance and inner product. Moreover, it also supports vector quantization techniques, e.g., Product Quantization, on top of the indexing strategies implemented. These functionalities are managed through Rust traits, allowing shared behaviors to be handled abstractly. This abstraction ensures flexibility and facilitates an easy integration of new components. In this work, we detail the architecture of kANNolo and demonstrate that its flexibility does not compromise performance. The experimental analysis shows that kANNolo achieves state-of-the-art performance in terms of speed-accuracy trade-off while allowing fast and easy prototyping, thus making kANNolo a valuable tool for advancing ANN research. Source code available on GitHub: https://github.com/TusKANNy/kannolo.|近似最近邻（ANN）搜索是推荐系统和信息检索等应用中的关键任务。当前最先进的ANN库虽然注重性能，但往往缺乏模块化和易用性。这使得它们并不完全适合快速原型设计和研究想法的测试，而这恰恰是需要支持的重要特性。针对这些局限性，我们推出了kANNolo——一个基于Rust语言编写的新型研究导向ANN库，其设计理念是有效兼顾可用性与性能。作为首个支持稠密/稀疏向量表示、并可基于不同相似性度量（如欧氏距离和内积）进行检索的ANN库，kANNolo还创新性地在索引策略之上实现了向量量化技术（如乘积量化）。这些功能通过Rust特性（trait）进行管理，使得共享行为能被抽象化处理。这种抽象机制既确保了灵活性，又便于新组件的快速集成。本文详细阐述了kANNolo的架构，并证明其灵活性不会以牺牲性能为代价。实验分析表明，kANNolo在速度-准确率权衡方面达到了业界领先水平，同时支持快速简易的原型开发，这使其成为推动ANN研究的宝贵工具。源代码已发布于GitHub：https://github.com/TusKANNy/kannolo。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=kANNolo:+Sweet+and+Smooth+Approximate+k-Nearest+Neighbors+Search)|1|
|[Set-Encoder: Permutation-Invariant Inter-passage Attention for Listwise Passage Re-ranking with Cross-Encoders](https://doi.org/10.1007/978-3-031-88711-6_1)|Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen||Existing cross-encoder models can be categorized as pointwise, pairwise, or listwise. Pairwise and listwise models allow passage interactions, which typically makes them more effective than pointwise models but less efficient and less robust to input passage order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder architecture with inter-passage attention: the Set-Encoder. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is as effective as state-of-the-art listwise models while being more efficient and invariant to input passage order permutations. Compared to pointwise models, the Set-Encoder is particularly more effective when considering inter-passage information, such as novelty, and retains its advantageous properties compared to other listwise models. Our code is publicly available at https://github.com/webis-de/ECIR-25.|现有的交叉编码器模型可分为逐点式、成对式和列表式三类。成对式和列表式模型支持段落交互，这使得它们通常比逐点式模型更有效，但效率较低且对输入段落顺序排列的鲁棒性较差。为了实现重排序过程中高效且排列不变的段落交互，我们提出了一种具有跨段落注意力机制的新型交叉编码器架构：集合编码器（Set-Encoder）。在TREC深度学习和TIREx数据集上的实验表明，集合编码器在保持输入段落顺序排列不变性的同时，其效果与最先进的列表式模型相当，且具有更高效率。相较于逐点式模型，集合编码器在考虑段落间信息（如新颖性）时表现尤为突出，同时保留了相对于其他列表式模型的优势特性。我们的代码已开源：https://github.com/webis-de/ECIR-25。

（注：根据学术翻译规范，技术术语处理如下：
1. "cross-encoder"译为"交叉编码器"（NLP领域标准译法）
2. "pointwise/pairwise/listwise"分别译为"逐点式/成对式/列表式"（信息检索领域通用译法）
3. "permutation-invariant"译为"排列不变"（数学特性标准表述）
4. "novelty"在此语境下译为"新颖性"（信息检索术语）
5. 技术机构名称"TREC"保留英文缩写形式
6. 补充括号说明格式保持中文标点规范）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Set-Encoder:+Permutation-Invariant+Inter-passage+Attention+for+Listwise+Passage+Re-ranking+with+Cross-Encoders)|1|
|[Improving RAG for Personalization with Author Features and Contrastive Examples](https://doi.org/10.1007/978-3-031-88714-7_40)|Mert Yazan, Suzan Verberne, Frederik Situmeang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+RAG+for+Personalization+with+Author+Features+and+Contrastive+Examples)|1|
|[Leveraging High-Resolution Features for Improved Deep Hashing-Based Image Retrieval](https://doi.org/10.1007/978-3-031-88711-6_28)|Aymene Berriche, Mehdi Zakaria Adjal, Riyadh Baghdadi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+High-Resolution+Features+for+Improved+Deep+Hashing-Based+Image+Retrieval)|1|
|[LIBRA: Measuring Bias of Large Language Model from a Local Context](https://doi.org/10.1007/978-3-031-88708-6_1)|Bo Pang, Tingrui Qiao, Caroline Walker, Chris Cunningham, Yun Sing Koh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LIBRA:+Measuring+Bias+of+Large+Language+Model+from+a+Local+Context)|1|
|[A Reproducibility Study for Joint Information Retrieval and Recommendation in Product Search](https://doi.org/10.1007/978-3-031-88717-8_10)|Simone Merlo, Guglielmo Faggioli, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reproducibility+Study+for+Joint+Information+Retrieval+and+Recommendation+in+Product+Search)|0|
|[BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A](https://doi.org/10.1007/978-3-031-88720-8_1)|Samy Ateia, Udo Kruschwitz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BioRAGent:+A+Retrieval-Augmented+Generation+System+for+Showcasing+Generative+Query+Expansion+and+Domain-Specific+Search+for+Scientific+Q&A)|0|
|[Conversational Information Retrieval and Recommender Systems](https://doi.org/10.1007/978-3-031-88720-8_44)|Guglielmo Faggioli, Nicola Ferro, Simone Merlo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Information+Retrieval+and+Recommender+Systems)|0|
|[Evaluating Sequential Recommendations in the Wild: A Case Study on Offline Accuracy, Click Rates, and Consumption](https://doi.org/10.1007/978-3-031-88711-6_5)|Anastasiia Klimashevskaia, Snorre Alvsvåg, Christoph Trattner, Alain D. Starke, Astrid Tessem, Dietmar Jannach||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Sequential+Recommendations+in+the+Wild:+A+Case+Study+on+Offline+Accuracy,+Click+Rates,+and+Consumption)|0|
|[DiffGR: A Discrete Diffusion-Based Model for Personalised Recommendation by Reconstructing User-Item Bipartite Graphs](https://doi.org/10.1007/978-3-031-88714-7_23)|Zheng Ju, Honghui Du, Elias Z. Tragos, Neil Hurley, Aonghus Lawlor||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffGR:+A+Discrete+Diffusion-Based+Model+for+Personalised+Recommendation+by+Reconstructing+User-Item+Bipartite+Graphs)|0|
|[Guiding Retrieval Using LLM-Based Listwise Rankers](https://doi.org/10.1007/978-3-031-88708-6_15)|Mandeep Rathee, Sean MacAvaney, Avishek Anand||Large Language Models (LLMs) have shown strong promise as rerankers, especially in “listwise” settings where an LLM is prompted to rerank several search results at once. However, this “cascading” retrieve-and-rerank approach is limited by the bounded recall problem: relevant documents not retrieved initially are permanently excluded from the final ranking. Adaptive retrieval techniques address this problem, but do not work with listwise rerankers because they assume a document's score is computed independently from other documents. In this paper, we propose an adaptation of an existing adaptive retrieval method that supports the listwise setting and helps guide the retrieval process itself (thereby overcoming the bounded recall problem for LLM rerankers). Specifically, our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point. Through extensive experiments across diverse LLM rerankers, first stage retrievers, and feedback sources, we demonstrate that our method can improve nDCG@10 by up to 13.23 while keeping the total number of LLM inferences constant and overheads due to the adaptive process minimal. The work opens the door to leveraging LLM-based search in settings where the initial pool of results is limited, e.g., by legacy systems, or by the cost of deploying a semantic first-stage.|大型语言模型（LLMs）作为重排序器已展现出强大潜力，尤其在"列表式"场景中——通过单次提示即可对多个搜索结果进行重新排序。然而这种"级联式"检索-重排序方法受限于有界召回率问题：初始未被检索到的相关文档将永久排除在最终排序之外。自适应检索技术虽能解决该问题，却无法与列表式重排序器协同工作，因其假设文档评分独立于其他文档。本文提出对现有自适应检索方法的改进方案，使其支持列表式场景并指导检索过程本身（从而解决LLM重排序器的有界召回问题）。具体而言，我们的算法融合了初始排序结果与当前最相关文档提供的反馈文档。通过在不同LLM重排序器、首阶段检索器和反馈源上的大量实验表明，该方法在保持LLM推理总量不变且自适应过程开销最小化的前提下，能将nDCG@10提升最高达13.23%。这项研究为在初始结果池受限的场景（如受遗留系统制约或语义首阶段部署成本限制时）部署基于LLM的搜索系统开辟了新途径。

注：
1. "listwise"译为"列表式"，符合信息检索领域对排序学习范式的命名惯例（对应pointwise/pairwise/listwise）
2. "bounded recall problem"译为"有界召回率问题"，准确传达原文技术含义
3. "feedback documents"译为"反馈文档"而非"反馈文件"，符合信息检索术语规范
4. 保留nDCG@10等专业指标原名，符合学术论文翻译惯例
5. "legacy systems"译为"遗留系统"是计算机领域的标准译法
6. 通过增补"（从而解决...）"的括号说明，既保持句式流畅又确保技术准确性
7. 最后长句通过拆分处理，符合中文多用短句的表达习惯|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Guiding+Retrieval+Using+LLM-Based+Listwise+Rankers)|0|
|[Leveraging Query Terms for Efficient Legal Document Recommendation](https://doi.org/10.1007/978-3-031-88714-7_6)|André Rolim, Leandro Balby Marinho, Edleno Silva de Moura, Marcos Aurélio Domingues, Ricardo S. Oliveira||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Query+Terms+for+Efficient+Legal+Document+Recommendation)|0|
|[Examining the Impact of Transcript Variation on Podcast Search and Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_9)|Watheq Mansour, J. Shane Culpepper, Joel Mackenzie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Examining+the+Impact+of+Transcript+Variation+on+Podcast+Search+and+Re-ranking)|0|
|[Counterfactual Query Rewriting to Use Historical Relevance Feedback](https://doi.org/10.1007/978-3-031-88714-7_11)|Jüri Keller, Maik Fröbe, Gijs Hendriksen, Daria Alexander, Martin Potthast, Matthias Hagen, Philipp Schaer||When a retrieval system receives a query it has encountered before, previous relevance feedback, such as clicks or explicit judgments can help to improve retrieval results. However, the content of a previously relevant document may have changed, or the document might not be available anymore. Despite this evolved corpus, we counterfactually use these previously relevant documents as relevance signals. In this paper we proposed approaches to rewrite user queries and compare them against a system that directly uses the previous qrels for the ranking. We expand queries with terms extracted from the previously relevant documents or derive so-called keyqueries that rank the previously relevant documents to the top of the current corpus. Our evaluation in the CLEF LongEval scenario shows that rewriting queries with historical relevance feedback improves the retrieval effectiveness and even outperforms computationally expensive transformer-based approaches.|当检索系统再次遇到曾经处理过的查询时，先前获取的相关性反馈（如点击数据或显式判断）可用于提升检索效果。然而，先前相关文档的内容可能已发生变更，甚至文档本身可能已不可用。尽管语料库处于动态演变状态，我们仍反事实地将这些历史相关文档作为相关性信号加以利用。本文提出多种查询重构方法，并与直接使用历史相关标注（qrels）进行排序的系统进行对比。我们通过从历史相关文档中提取术语来扩展查询，或生成能将历史相关文档重新排至当前语料库前列的"关键查询"。在CLEF LongEval场景下的评估表明，利用历史相关性反馈重构查询不仅能提升检索效能，其表现甚至优于计算成本高昂的基于Transformer的方法。

（说明：根据学术论文摘要的翻译规范，我进行了以下处理：
1. 专业术语统一："relevance feedback"译为"相关性反馈"，"qrels"保留英文并补充中文注释"相关标注"
2. 技术概念准确传达："counterfactually"译为"反事实地"以保持方法论特征
3. 句式结构调整：将英语长句拆分为符合中文表达习惯的短句，如处理"Despite this evolved corpus..."的让步状语从句
4. 创新术语处理："keyqueries"采用直译加引号的"关键查询"译法
5. 项目名称保留："CLEF LongEval"不作翻译以保持国际会议标准命名）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Query+Rewriting+to+Use+Historical+Relevance+Feedback)|0|
|[Patience in Proximity: A Simple Early Termination Strategy for HNSW Graph Traversal in Approximate k-Nearest Neighbor Search](https://doi.org/10.1007/978-3-031-88714-7_39)|Tommaso Teofili, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Patience+in+Proximity:+A+Simple+Early+Termination+Strategy+for+HNSW+Graph+Traversal+in+Approximate+k-Nearest+Neighbor+Search)|0|
|[Towards Intent-Driven Transparency in Conversational Search Systems](https://doi.org/10.1007/978-3-031-88720-8_37)|Yumeng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Intent-Driven+Transparency+in+Conversational+Search+Systems)|0|
|[Combining Dissimilarity Spaces to Improve Approximate Similarity Search](https://doi.org/10.1007/978-3-031-88720-8_30)|Elena GarcíaMorato, Felipe Ortega, Javier Gómez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Dissimilarity+Spaces+to+Improve+Approximate+Similarity+Search)|0|
|[News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-Lingual News Recommendation](https://doi.org/10.1007/978-3-031-88711-6_8)|Andreea Iana, Fabian David Schmidt, Goran Glavas, Heiko Paulheim||Rapidly growing numbers of multilingual news consumers pose an increasing challenge to news recommender systems in terms of providing customized recommendations. First, existing neural news recommenders, even when powered by multilingual language models (LMs), suffer substantial performance losses in zero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of fine-tuning the backbone LM of a neural recommender on task-specific data is computationally expensive and infeasible in few-shot recommendation and cold-start setups, where data is scarce or completely unavailable. In this work, we propose a news-adapted sentence encoder (NaSE), domain-specialized from a pretrained massively multilingual sentence encoder (SE). To this end, we construct and leverage PolyNews and PolyNewsParallel, two multilingual news-specific corpora. With the news-adapted multilingual SE in place, we test the effectiveness of (i.e., question the need for) supervised fine-tuning for news recommendation, and propose a simple and strong baseline based on (i) frozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE achieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot news recommendation.|多语言新闻用户数量的快速增长，为新闻推荐系统提供个性化推荐服务带来了日益严峻的挑战。首先，现有的神经新闻推荐模型即使采用多语言预训练模型（LM）作为支撑，在零样本跨语言迁移（ZS-XLT）场景中仍会遭受显著的性能损失。其次，当前基于任务特定数据对神经推荐模型主干LM进行微调的范式，在计算资源消耗巨大，难以适用于数据稀缺或完全缺失的少样本推荐和冷启动场景。本研究提出一种新闻领域适配的句向量编码器（NaSE），该模型通过对预训练的大规模多语言句向量编码器（SE）进行领域专项优化得到。为此，我们构建并利用了PolyNews和PolyNewsParallel两个多语言新闻专用语料库。基于优化后的多语言新闻SE，我们验证了（即质疑了）监督式微调在新闻推荐中的必要性，并提出一个简单而强大的基线方案：该方案基于（i）冻结的NaSE嵌入向量和（ii）后期点击行为融合。实验表明，NaSE在真实冷启动和少样本新闻推荐场景的ZS-XLT任务中实现了最先进的性能表现。

（注：根据学术翻译规范，关键术语首次出现时保留英文缩写并在括号内标注全称，如"零样本跨语言迁移（zero-shot cross-lingual transfer, ZS-XLT）"。后续出现时可直接使用缩写。专业术语如"state-of-the-art"译为"最先进的"符合国内计算机领域惯例。"late click-behavior fusion"译为"后期点击行为融合"准确传达了技术含义。）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News+Without+Borders:+Domain+Adaptation+of+Multilingual+Sentence+Embeddings+for+Cross-Lingual+News+Recommendation)|0|
|[LLM is Knowledge Graph Reasoner: LLM's Intuition-Aware Knowledge Graph Reasoning for Cold-Start Sequential Recommendation](https://doi.org/10.1007/978-3-031-88711-6_17)|Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama||Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items. However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios. On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems. Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues. To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings. By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies. Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios.|知识图谱（KG）以图结构表示实体间关系，作为能精准考量物品内容信息的推荐工具已得到广泛研究。然而传统基于KG的推荐方法面临根本性挑战：时间信息考量不足，以及在冷启动场景下表现欠佳。另一方面，大型语言模型（LLM）可视为蕴含网络数据海量知识的数据库，近期因其作为推荐系统的应用潜力备受关注。虽然将LLM视为推荐系统的方法能利用其强大的推荐理解能力，但其输入标记限制导致无法处理整个推荐领域数据集，进而引发可扩展性问题。为解决这些挑战，我们提出LLM直觉感知的知识图谱推理模型（LIKR）。核心思路是将LLM作为输出KG直观探索策略的推理机。为整合LLM与KG的知识，我们通过强化学习训练推荐代理，采用融合不同推荐策略（包括LLM直觉与KG嵌入）的奖励函数。LIKR通过提示工程实现时间感知，并从有限交互中生成用户偏好的文本表征，从而提升冷启动场景的推荐性能。此外，LIKR通过KG表示推荐领域数据集并将LLM输出限定为KG探索策略，有效避免了可扩展性问题。真实数据集实验表明，本模型在冷启动序列推荐场景中优于现有最先进的推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM+is+Knowledge+Graph+Reasoner:+LLM's+Intuition-Aware+Knowledge+Graph+Reasoning+for+Cold-Start+Sequential+Recommendation)|0|
|[Embedding Cultural Diversity in Prototype-Based Recommender Systems](https://doi.org/10.1007/978-3-031-88708-6_2)|Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi||Popularity bias in recommender systems can increase cultural overrepresentation by favoring norms from dominant cultures and marginalizing underrepresented groups. This issue is critical for platforms offering cultural products, as they influence consumption patterns and human perceptions. In this work, we address popularity bias by identifying demographic biases within prototype-based matrix factorization methods. Using the country of origin as a proxy for cultural identity, we link this demographic attribute to popularity bias by refining the embedding space learning process. First, we propose filtering out irrelevant prototypes to improve representativity. Second, we introduce a regularization technique to enforce a uniform distribution of prototypes within the embedding space. Across four datasets, our results demonstrate a 27% reduction in the average rank of long-tail items and a 2% reduction in the average rank of items from underrepresented countries. Additionally, our model achieves a 2% improvement in HitRatio@10 compared to the state-of-the-art, highlighting that fairness is enhanced without compromising recommendation quality. Moreover, the distribution of prototypes leads to more inclusive explanations by better aligning items with diverse prototypes.|推荐系统中的流行度偏差会通过偏向主流文化规范、边缘化弱势群体，加剧文化过度代表问题。该问题对提供文化产品的平台至关重要，因其会影响消费模式和人类认知。本研究通过识别基于原型的矩阵分解方法中的人口统计偏差来解决流行度偏差问题。我们以原产国作为文化身份的代理指标，通过改进嵌入空间学习过程，将这一人口统计属性与流行度偏差相关联。首先，我们提出过滤不相关原型以提高代表性；其次，我们引入正则化技术确保嵌入空间中原型的均匀分布。在四个数据集上的实验表明：长尾物品的平均排名提升27%，弱势国家物品的平均排名提升2%。相比现有最优模型，我们的方法在HitRatio@10指标上提升2%，证明在保证推荐质量的同时提升了公平性。此外，原型分布通过使物品与多样化原型更好对齐，产生了更具包容性的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Cultural+Diversity+in+Prototype-Based+Recommender+Systems)|0|
|[Is Relevance Propagated from Retriever to Generator in RAG?](https://doi.org/10.1007/978-3-031-88708-6_3)|Fangzheng Tian, Debasis Ganguly, Craig Macdonald||Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task's objective of maximising the relevance of a set of top-ranked documents, a RAG system's objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.|检索增强生成（Retrieval Augmented Generation，RAG）是一种通过整合外部知识（通常以文档集合中检索到的文档集形式呈现）作为大型语言模型（LLM）提示词组成部分的框架，旨在提升下游任务（如问答系统）的性能。与传统检索任务以最大化顶层文档相关性为目标不同，RAG系统的核心目标是最大化文档集的整体效用——其中文档效用指将其作为LLM提示词附加上下文信息时能否改善下游任务表现。现有研究主要探讨RAG上下文相关性在知识密集型语言任务（KILT）中的作用，这类场景中相关性本质体现为答案包含性。而本研究则聚焦信息检索任务中查询与文档主题重叠度的相关性。具体而言，我们利用信息检索测试集实证分析：由主题相关文档构成的RAG上下文是否能提升下游性能。实验得出以下结论：(a) 相关性与效用呈弱正相关；(b) 随着上下文规模扩大（k-shot中k值增大），这种相关性逐渐减弱；(c) 更高效的检索模型通常能带来更优的RAG下游性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Relevance+Propagated+from+Retriever+to+Generator+in+RAG?)|0|
|[Improving the Reusability of Conversational Search Test Collections](https://doi.org/10.1007/978-3-031-88708-6_13)|Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi||Incomplete relevance judgments limit the reusability of test collections. When new systems are compared to previous systems that contributed to the pool, they often face a disadvantage. This is due to pockets of unjudged documents (called holes) in the test collection that the new systems return. The very nature of Conversational Search (CS) means that these holes are potentially larger and more problematic when evaluating systems. In this paper, we aim to extend CS test collections by employing Large Language Models (LLMs) to fill holes by leveraging existing judgments. We explore this problem using TREC iKAT 23 and TREC CAsT 22 collections, where information needs are highly dynamic and the responses are much more varied, leaving bigger holes to fill. Our experiments reveal that CS collections show a trend towards less reusability in deeper turns. Also, fine-tuning the Llama 3.1 model leads to high agreement with human assessors, while few-shot prompting the ChatGPT results in low agreement with humans. Consequently, filling the holes of a new system using ChatGPT leads to a higher change in the location of the new system. While regenerating the assessment pool with few-shot prompting the ChatGPT model and using it for evaluation achieves a high rank correlation with human-assessed pools. We show that filling the holes using few-shot training the Llama 3.1 model enables a fairer comparison between the new system and the systems contributed to the pool. Our hole-filling model based on few-shot training of the Llama 3.1 model can improve the reusability of test collections.|不完整的相关性标注会限制测试集的可复用性。当新系统与曾参与构建标注池的旧系统进行比较时，新系统往往处于劣势，这是因为测试集中存在新系统返回但未被标注的文档区域（称为"空洞"）。对话式搜索（CS）的特性决定了这些空洞在系统评估时会更加显著且更具破坏性。本文旨在利用大语言模型（LLMs），基于现有标注来填补这些空洞以扩展CS测试集。我们选用TREC iKAT 23和TREC CAsT 22数据集展开研究——这两个集合的信息需求高度动态化且响应结果差异极大，因而存在更严重的待填补空洞。实验表明：CS测试集在更深层对话轮次中呈现可复用性下降趋势；微调Llama 3.1模型能达到与人工标注者高度一致，而使用少量示例提示的ChatGPT则与人工一致性较低。因此，使用ChatGPT填补新系统的空洞会导致该系统排名位置发生较大偏移，而通过少量示例提示ChatGPT重新生成评估池却能获得与人工标注池高度一致的排序相关性。我们证明采用少量示例训练的Llama 3.1模型填补空洞，能更公平地比较新系统与标注池贡献系统。基于少量示例训练的Llama 3.1空洞填补模型可有效提升测试集的复用价值。

（注：根据学术翻译规范，对以下术语进行了统一处理：
1. "holes"译为"空洞"而非字面意义的"洞"，符合计算机领域术语惯例
2. "Conversational Search"保留英文缩写"CS"并在首次出现时标注全称
3. "few-shot prompting"译为"少量示例提示"，准确体现提示工程方法论
4. 被动语态转换为中文主动句式（如"are compared"译为"进行比较"）
5. 长难句拆分重组（如最后两句话的逻辑重组）以符合中文表达习惯）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Reusability+of+Conversational+Search+Test+Collections)|0|
|[Evaluating Auto-complete Ranking for Diversity and Relevance](https://doi.org/10.1007/978-3-031-88708-6_21)|Sonali Singh, Sachin Farfade, Prakash Mandayam Comar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Auto-complete+Ranking+for+Diversity+and+Relevance)|0|
|[Zero-Shot and Efficient Clarification Need Prediction in Conversational Search](https://doi.org/10.1007/978-3-031-88708-6_25)|Lili Lu, Chuan Meng, Federico Ravenda, Mohammad Aliannejadi, Fabio Crestani||Clarification need prediction (CNP) is a key task in conversational search, aiming to predict whether to ask a clarifying question or give an answer to the current user query. However, current research on CNP suffers from the issues of limited CNP training data and low efficiency. In this paper, we propose a zero-shot and efficient CNP framework (Zef-CNP), in which we first prompt large language models (LLMs) in a zero-shot manner to generate two sets of synthetic queries: ambiguous and specific (unambiguous) queries. We then use the generated queries to train efficient CNP models. Zef-CNP eliminates the need for human-annotated clarification-need labels during training and avoids the use of LLMs with high query latency at query time. To further improve the generation quality of synthetic queries, we devise a topic-, information-need-, and query-aware chain-of-thought (CoT) prompting strategy (TIQ-CoT). Moreover, we enhance TIQ-CoT with counterfactual query generation (CoQu), which guides LLMs first to generate a specific/ambiguous query and then sequentially generate its corresponding ambiguous/specific query. Experimental results show that Zef-CNP achieves superior CNP effectiveness and efficiency compared with zero- and few-shot LLM-based CNP predictors.|澄清需求预测（CNP）是对话式搜索中的核心任务，旨在判断当前用户查询应当触发澄清提问还是直接返回答案。然而，现有CNP研究面临训练数据稀缺和预测效率低下的双重挑战。本文提出零样本高效CNP框架（Zef-CNP）：首先以零样本方式提示大语言模型（LLMs）生成两组合成查询——模糊查询与明确查询，继而利用生成数据训练高效CNP模型。该框架既无需训练阶段的人工标注数据，又能在查询阶段避免高延迟的LLMs推理。为进一步提升合成查询质量，我们设计融合话题、信息需求与查询意识的思维链提示策略（TIQ-CoT），并通过反事实查询生成（CoQu）进行增强——引导LLMs首先生成明确/模糊查询，再递进式生成对应的模糊/明确查询。实验表明，相较于零样本和少样本的LLM基线预测器，Zef-CNP在预测效能与效率上均展现显著优势。

（翻译说明：1. 专业术语统一处理："clarifying question"译为"澄清提问"，"query latency"译为"查询延迟"；2. 复杂句式拆分：将原文复合句按中文表达习惯分解为短句；3. 被动语态转化："are generated"译为主动态"生成"；4. 概念显化："counterfactual"译为"反事实"并补充说明性文字；5. 技术名词保留原缩写形式同时首次出现标注全称；6. 动词动态处理："devise"译为"设计"，"enhance"译为"增强"以符合技术文本特征）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-Shot+and+Efficient+Clarification+Need+Prediction+in+Conversational+Search)|0|
|[FAIR-QR: Enhancing Fairness-Aware Information Retrieval Through Query Refinement](https://doi.org/10.1007/978-3-031-88717-8_1)|Fumian Chen, Hui Fang||Information retrieval systems such as open web search and recommendation systems are ubiquitous and significantly impact how people receive and consume online information. Previous research has shown the importance of fairness in information retrieval systems to combat the issue of echo chambers and mitigate the rich-get-richer effect. Therefore, various fairness-aware information retrieval methods have been proposed. Score-based fairness-aware information retrieval algorithms, focusing on statistical parity, are interpretable but could be mathematically infeasible and lack generalizability. In contrast, learning-to-rank-based fairness-aware information retrieval algorithms using fairness-aware loss functions demonstrate strong performance but lack interpretability. In this study, we proposed a novel and interpretable framework that recursively refines query keywords to retrieve documents from underrepresented groups and achieve group fairness. Retrieved documents using refined queries will be re-ranked to ensure relevance. Our method not only shows promising retrieval results regarding relevance and fairness but also preserves interpretability by showing refined keywords used at each iteration.|开放网络搜索与推荐系统等信息检索技术已无处不在，其显著影响着人们获取与消费在线信息的方式。现有研究表明，信息检索系统的公平性对于打破信息茧房效应、缓解"马太效应"具有重要作用。为此，学界已提出多种公平感知的信息检索方法。基于统计平分的评分式公平检索算法虽具可解释性，但存在数学实现困难且泛化能力不足的缺陷；而采用公平损失函数的排序学习式公平检索算法虽表现优异，却缺乏可解释性。本研究提出了一种创新且可解释的框架，该框架通过递归优化查询关键词来获取代表性不足群体的文档，从而实现群体公平性。基于优化查询检索到的文档将经过重排序以确保相关性。我们的方法不仅在相关性与公平性指标上展现出优越的检索效果，还能通过展示每轮迭代使用的优化关键词来保持算法的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FAIR-QR:+Enhancing+Fairness-Aware+Information+Retrieval+Through+Query+Refinement)|0|
|[How Child-Friendly is Web Search? An Evaluation of Relevance vs. Harm](https://doi.org/10.1007/978-3-031-88717-8_16)|Maik Fröbe, Sophie Charlotte Bartholly, Matthias Hagen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Child-Friendly+is+Web+Search?+An+Evaluation+of+Relevance+vs.+Harm)|0|
|[Poison-RAG: Adversarial Data Poisoning Attacks on Retrieval-Augmented Generation in Recommender Systems](https://doi.org/10.1007/978-3-031-88717-8_18)|Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia||This study presents Poison-RAG, a framework for adversarial data poisoning attacks targeting retrieval-augmented generation (RAG)-based recommender systems. Poison-RAG manipulates item metadata, such as tags and descriptions, to influence recommendation outcomes. Using item metadata generated through a large language model (LLM) and embeddings derived via the OpenAI API, we explore the impact of adversarial poisoning attacks on provider-side, where attacks are designed to promote long-tail items and demote popular ones. Two attack strategies are proposed: local modifications, which personalize tags for each item using BERT embeddings, and global modifications, applying uniform tags across the dataset. Experiments conducted on the MovieLens dataset in a black-box setting reveal that local strategies improve manipulation effectiveness by up to 50%, while global strategies risk boosting already popular items. Results indicate that popular items are more susceptible to attacks, whereas long-tail items are harder to manipulate. Approximately 70% of items lack tags, presenting a cold-start challenge; data augmentation and synthesis are proposed as potential defense mechanisms to enhance RAG-based systems' resilience. The findings emphasize the need for robust metadata management to safeguard recommendation frameworks. Code and data are available at https://github.com/atenanaz/Poison-RAG.|本研究提出Poison-RAG框架，针对基于检索增强生成（RAG）的推荐系统实施对抗性数据投毒攻击。该框架通过操纵项目元数据（如标签和描述）来影响推荐结果。我们利用大语言模型（LLM）生成的元数据及OpenAI API生成的嵌入向量，探究了攻击者从供应端发起的对抗性投毒攻击对推荐系统的影响——此类攻击旨在提升长尾项目曝光率同时抑制热门项目。研究提出两种攻击策略：基于BERT嵌入向量为每个项目定制化修改标签的局部策略，以及在整个数据集中统一应用标签的全局策略。在MovieLens数据集上进行的黑盒实验表明，局部策略可使操纵效果提升高达50%，而全局策略存在意外提升已热门项目排名的风险。结果显示热门项目更易受攻击影响，而长尾项目操纵难度较高。约70%的项目缺乏标签数据，形成冷启动挑战；研究建议采用数据增强和合成技术作为潜在防御机制来增强RAG系统的鲁棒性。这些发现强调了强化元数据管理对保护推荐框架的重要性。相关代码与数据详见https://github.com/atenanaz/Poison-RAG。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poison-RAG:+Adversarial+Data+Poisoning+Attacks+on+Retrieval-Augmented+Generation+in+Recommender+Systems)|0|
|[How to Diversify any Personalized Recommender?](https://doi.org/10.1007/978-3-031-88717-8_23)|Manel Slokom, Savvina Daniil, Laura Hollink||In this paper, we introduce a novel approach to improve the diversity of Top-N recommendations while maintaining accuracy. Our approach employs a user-centric pre-processing strategy aimed at exposing users to a wide array of content categories and topics. We personalize this strategy by selectively adding and removing a percentage of interactions from user profiles. This personalization ensures we remain closely aligned with user preferences while gradually introducing distribution shifts. Our pre-processing technique offers flexibility and can seamlessly integrate into any recommender architecture. We run extensive experiments on two publicly available data sets for news and book recommendations to evaluate our approach. We test various standard and neural network-based recommender system algorithms. Our results show that our approach generates diverse recommendations, ensuring users are exposed to a wider range of items. Furthermore, using pre-processed data for training leads to recommender systems achieving performance levels comparable to, and in some cases, better than those trained on original, unmodified data. Additionally, our approach promotes provider fairness by facilitating exposure to minority categories. Our GitHub code is available at: https://github.com/SlokomManel/How-to-Diversify-any-Personalized-Recommender-|本文提出了一种在保持推荐准确性的同时提升Top-N推荐多样性的创新方法。该方法采用以用户为中心的预处理策略，旨在使用户接触到更广泛的内容类别与主题。我们通过对用户画像中的交互记录进行选择性增减来实现策略个性化，在紧密贴合用户偏好的同时逐步引入分布偏移。这种预处理技术具有高度灵活性，可无缝集成至任何推荐系统架构。我们在新闻和图书推荐两个公开数据集上进行了大量实验，测试了多种标准推荐算法与基于神经网络的推荐模型。实验结果表明，本方法能有效生成多样化推荐结果，确保用户接触到更丰富的项目类型。此外，使用预处理数据训练后的推荐系统性能与原数据训练效果相当，部分场景下甚至表现更优。该方法还能通过促进小众类目的曝光来提升供应商公平性。相关代码已开源：https://github.com/SlokomManel/How-to-Diversify-any-Personalized-Recommender-

（注：根据学术论文摘要翻译规范，我们做出了以下专业处理：
1. 将"Top-N recommendations"译为专业术语"Top-N推荐"
2. "user profiles"译为行业通用表述"用户画像"
3. "distribution shifts"保留技术含义译为"分布偏移"
4. 对Github链接等数字对象标识符保持原格式
5. 通过"类目""项目"等电商推荐系统领域术语保持语境一致性
6. 使用"供应商公平性"准确传达"provider fairness"的推荐系统评价维度）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+Diversify+any+Personalized+Recommender?)|0|
|[Improving Minimax Group Fairness in Sequential Recommendation](https://doi.org/10.1007/978-3-031-88717-8_26)|Krishna Acharya, David Wardrope, Timos Korres, Aleksandr V. Petrov, Anders Uhrenholt||Training sequential recommenders such as SASRec with uniform sample weights achieves good overall performance but can fall short on specific user groups. One such example is popularity bias, where mainstream users receive better recommendations than niche content viewers. To improve recommendation quality across diverse user groups, we explore three Distributionally Robust Optimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at Risk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and struggle with users belonging to multiple groups, CVaR does not require such annotations and can naturally handle overlapping groups. In experiments on two real-world datasets, we show that the DRO methods outperform standard training, with CVaR delivering the best results. Additionally, we find that Group and Streaming DRO are sensitive to the choice of group used for loss computation. Our contributions include (i) a novel application of CVaR to recommenders, (ii) showing that the DRO methods improve group metrics as well as overall performance, and (iii) demonstrating CVaR's effectiveness in the practical scenario of intersecting user groups.|对SASRec等序列推荐模型采用均匀样本权重训练虽可获得整体良好性能，但在特定用户群体上可能表现欠佳。以流行度偏差为例，主流内容消费者获得的推荐质量往往优于小众内容受众。为提升跨用户群体的推荐质量，我们探索了三种分布鲁棒优化（DRO）方法：群体DRO、流式DRO和条件风险价值（CVaR）DRO。前两种方法依赖群体标注且难以处理多归属用户，而CVaR无需此类标注即可自然处理重叠群体。在两个真实数据集上的实验表明：DRO方法均优于标准训练方式，其中CVaR表现最佳；同时发现群体DRO与流式DRO对损失计算所选群体敏感。本文贡献在于：（1）首次将CVaR应用于推荐系统（2）证实DRO方法能同步提升群体指标与整体性能（3）验证了CVaR在用户群体交叉场景下的实践有效性。

（注：根据学术翻译规范，对以下术语进行了标准化处理：
1. "sequential recommenders"译为"序列推荐模型"而非字面直译
2. "popularity bias"采用领域通用译法"流行度偏差"
3. "Distributionally Robust Optimization"保留英文缩写DRO并首次出现时标注全称
4. 技术术语"Conditional Value at Risk"使用金融领域既定译名"条件风险价值"
5. 将英文长句按中文表达习惯拆分为多个短句，如实验发现部分拆分为两个分句）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Minimax+Group+Fairness+in+Sequential+Recommendation)|0|
|[Call for Research on the Impact of Information Retrieval on Social Norms](https://doi.org/10.1007/978-3-031-88717-8_27)|Tim Gollub, Pierre Achkar, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Call+for+Research+on+the+Impact+of+Information+Retrieval+on+Social+Norms)|0|
|[CountNet: Utilising Repetition Counts in Sequential Recommendation](https://doi.org/10.1007/978-3-031-88714-7_4)|Aleksandr V. Petrov, Efi Karra Taniskidou, Sean Murphy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CountNet:+Utilising+Repetition+Counts+in+Sequential+Recommendation)|0|
|[Ranking Generated Answers - On the Agreement of Retrieval Models with Humans on Consumer Health Questions](https://doi.org/10.1007/978-3-031-88714-7_10)|Sebastian Heineking, Jonas Probst, Daniel Steinbach, Martin Potthast, Harrisen Scells||Evaluating the output of generative large language models (LLMs) is challenging and difficult to scale. Most evaluations of LLMs focus on tasks such as single-choice question-answering or text classification. These tasks are not suitable for assessing open-ended question-answering capabilities, which are critical in domains where expertise is required, such as health, and where misleading or incorrect answers can have a significant impact on a user's health. Using human experts to evaluate the quality of LLM answers is generally considered the gold standard, but expert annotation is costly and slow. We present a method for evaluating LLM answers that uses ranking signals as a substitute for explicit relevance judgements. Our scoring method correlates with the preferences of human experts. We validate it by investigating the well-known fact that the quality of generated answers improves with the size of the model as well as with more sophisticated prompting strategies.|评估生成式大语言模型（LLM）的输出具有挑战性且难以规模化。目前大多数LLM评估聚焦于单项选择题回答或文本分类等任务，这些任务并不适用于评估开放式问答能力——这种能力在需要专业知识的领域（如医疗健康）至关重要，因为误导性或错误答案可能对用户健康产生重大影响。虽然人工专家评估通常被视为LLM答案质量的金标准，但专家标注成本高昂且效率低下。我们提出了一种创新评估方法，利用排序信号替代显式相关性判断。实验表明，我们的评分方法与人类专家偏好具有显著相关性。该方法通过验证两个公认结论得到有效验证：生成答案的质量随模型规模扩大而提升，并随提示策略的优化而改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+Generated+Answers+-+On+the+Agreement+of+Retrieval+Models+with+Humans+on+Consumer+Health+Questions)|0|
|[Gradual Negative Matching for LLM Unlearning](https://doi.org/10.1007/978-3-031-88714-7_16)|Hrishikesh Kulkarni, Nazli Goharian, Ophir Frieder||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gradual+Negative+Matching+for+LLM+Unlearning)|0|
|[Efficient and Effective Conversational Search with Tail Entity Selection](https://doi.org/10.1007/978-3-031-88714-7_26)|Hai Dang Tran, Andrew Yates, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Conversational+Search+with+Tail+Entity+Selection)|0|
|[Investigating the Scalability of Approximate Sparse Retrieval Algorithms to Massive Datasets](https://doi.org/10.1007/978-3-031-88714-7_43)|Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini, Leonardo Venuta||Learned sparse text embeddings have gained popularity due to their effectiveness in top-k retrieval and inherent interpretability. Their distributional idiosyncrasies, however, have long hindered their use in real-world retrieval systems. That changed with the recent development of approximate algorithms that leverage the distributional properties of sparse embeddings to speed up retrieval. Nonetheless, in much of the existing literature, evaluation has been limited to datasets with only a few million documents such as MSMARCO. It remains unclear how these systems behave on much larger datasets and what challenges lurk in larger scales. To bridge that gap, we investigate the behavior of state-of-the-art retrieval algorithms on massive datasets. We compare and contrast the recently-proposed Seismic and graph-based solutions adapted from dense retrieval. We extensively evaluate Splade embeddings of 138M passages from MsMarco-v2 and report indexing time and other efficiency and effectiveness metrics.|学习型稀疏文本嵌入因其在top-k检索中的高效性及内在可解释性而广受欢迎。然而，其分布特性上的独特性长期以来阻碍了其在现实检索系统中的应用。随着近期近似算法的发展——这些算法利用稀疏嵌入的分布特性来加速检索——这一局面得以改变。尽管如此，现有文献中的评估大多局限于MSMARCO等仅包含数百万文档的数据集。对于这些系统在超大规模数据集上的表现及可能面临的挑战，目前仍缺乏清晰认知。为填补这一空白，我们研究了前沿检索算法在海量数据集上的行为表现。我们对比分析了近期提出的Seismic算法与源自稠密检索的图基解决方案，并对MsMarco-v2中1.38亿段落生成的Splade嵌入进行了全面评估，报告了索引时间及其他效率与效果指标。  

（注：根据学术翻译规范，对部分术语处理如下：  
1. "top-k retrieval"译为专业术语"top-k检索"  
2. "Splade"作为算法名称保留不译  
3. "MsMarco-v2"作为标准数据集名称保留原写法  
4. "138M"转换为中文数字单位"1.38亿"  
5. 被动语态"have long been hindered"转化为主动句式"长期以来阻碍"  
6. 长难句分拆为符合中文表达习惯的短句结构）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+the+Scalability+of+Approximate+Sparse+Retrieval+Algorithms+to+Massive+Datasets)|0|
|[FinPersona: An LLM-Driven Conversational Agent for Personalized Financial Advising](https://doi.org/10.1007/978-3-031-88720-8_3)|Takehiro Takayanagi, Masahiro Suzuki, Kiyoshi Izumi, Javier SanzCruzado, Richard McCreadie, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FinPersona:+An+LLM-Driven+Conversational+Agent+for+Personalized+Financial+Advising)|0|
|[MedLink: Retrieval and Ranking of Case Reports to Assist Clinical Decision Making](https://doi.org/10.1007/978-3-031-88720-8_13)|Luís Filipe Cunha, Nuno Guimarães, Alexandra Mendes, Ricardo Campos, Alípio Jorge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MedLink:+Retrieval+and+Ranking+of+Case+Reports+to+Assist+Clinical+Decision+Making)|0|
|[Jina Embeddings V3: Multilingual Text Encoder with Low-Rank Adaptations](https://doi.org/10.1007/978-3-031-88720-8_21)|Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Jina+Embeddings+V3:+Multilingual+Text+Encoder+with+Low-Rank+Adaptations)|0|
|[Personalizing Enterprise Search with LLM Populated Attributes in Graph Models](https://doi.org/10.1007/978-3-031-88720-8_24)|Christopher Liu, Varsha Embar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalizing+Enterprise+Search+with+LLM+Populated+Attributes+in+Graph+Models)|0|
|[Towards Query Obfuscation Strategies for Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_31)|Francesco Luigi De Faveri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Query+Obfuscation+Strategies+for+Information+Retrieval)|0|
|[Enhancing Reproducibility and Replicability in Information Retrieval: A Path Towards Scientific Integrity and Effective Research](https://doi.org/10.1007/978-3-031-88720-8_42)|Antonio Ferrara, Claudio Pomo, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Reproducibility+and+Replicability+in+Information+Retrieval:+A+Path+Towards+Scientific+Integrity+and+Effective+Research)|0|
|[ImageCLEF 2025: Multimedia Retrieval in Medical, Social Media and Content Recommendation Applications](https://doi.org/10.1007/978-3-031-88720-8_60)|Bogdan Ionescu, Henning Müller, DanCristian Stanciu, Ahmad IdrissiYaghir, Ahmedkhan Radzhabov, Alba García Seco de Herrera, Alexandra Andrei, Andrea M. Storås, Asma Ben Abacha, Benjamin Bracke, Benjamin Lecouteux, Benno Stein, Cécile Macaire, Christoph M. Friedrich, Cynthia Sabrina Schmidt, Diandra Fabre, Didier Schwab, Dimitar Dimitrov, Emmanuelle EsperançaRodier, Mihai Gabriel Constantin, Helmut Becker, Hendrik Damm, Henning Schäfer, Ivan Rodkin, Ivan Koychev, Johannes Kiesel, Johannes Rückert, Josep Malvehy, LiviuDaniel Stefan, Louise Bloch, Martin Potthast, Maximilian Heinrich, Michael A. Riegler, Mihai Dogariu, Noel Codella, Pål Halvorsen, Preslav Nakov, Raphael Brüngel, Roberto A. Novoa, Rocktim Jyoti Das, Steven Alexander Hicks, Sushant Gautam, Tabea Margareta Grace Pakull, Vajira Thambawita, Vassili Kovalev, Wenwai Yim, Zhuohan Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ImageCLEF+2025:+Multimedia+Retrieval+in+Medical,+Social+Media+and+Content+Recommendation+Applications)|0|
|[Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval](https://doi.org/10.1007/978-3-031-88711-6_10)|Jesús LovónMelgarejo, Martin Mouysset, Jo Oleiwan, José G. Moreno, Christine DamaseMichel, Lynda Tamine||Electronic Health Record (EHR) tables pose unique challenges among which is the presence of hidden contextual dependencies between medical features with a high level of data dimensionality and sparsity. This study presents the first investigation into the abilities of LLMs to comprehend EHRs for patient data extraction and retrieval. We conduct extensive experiments using the MIMICSQL dataset to explore the impact of the prompt structure, instruction, context, and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task performance. Through quantitative and qualitative analyses, our findings show that optimal feature selection and serialization methods can enhance task performance by up to 26.79 learning setups with relevant example selection improve data extraction performance by 5.95 we believe would help the design of LLM-based models to support health search.|电子健康记录（EHR）表格在医学特征间存在隐含的上下文关联性、数据维度高且稀疏性强等独特挑战。本研究首次探索大语言模型（LLMs）在患者数据提取与检索任务中对EHR的理解能力。基于Llama2和Meditron两个主干模型，我们利用MIMICSQL数据集开展系统实验，从任务性能角度分析提示结构、指令设计、上下文关联和示例演示的影响机制。定量与定性分析表明：最优特征选择与序列化方法可使任务性能提升达26.79%；而结合相关示例的学习配置能使数据提取准确率提高5.95%。本文提出的框架设计原则，将为开发支持健康搜索的LLM模型提供重要参考。  

（注：根据学术摘要翻译规范，采用以下处理：  
1. 专业术语统一："hidden contextual dependencies"译为"隐含的上下文关联性"以保持医学数据特性  
2. 技术指标保留原始数据精度："26.79%"与"5.95%"严格对应原文  
3. 被动语态转化："we conduct"转为主动式"开展"符合中文表达习惯  
4. 长句拆分：将复合实验描述分解为因果逻辑链  
5. 概念显化："prompt structure, instruction, context, and demonstration"整合为"提示结构、指令设计、上下文关联和示例演示"四要素  
6. 术语首次出现标注英文缩写，如"大语言模型（LLMs）"）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+LLM+Abilities+to+Understand+Tabular+Electronic+Health+Records:+A+Comprehensive+Study+of+Patient+Data+Extraction+and+Retrieval)|0|
|[Maybe You Are Looking for CroQS [inline-graphic not available: see fulltext] Cross-Modal Query Suggestion for Text-to-Image Retrieval](https://doi.org/10.1007/978-3-031-88711-6_9)|Giacomo Pacini, Fabio Carrara, Nicola Messina, Nicola Tonellotto, Giuseppe Amato, Fabrizio Falchi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maybe+You+Are+Looking+for+CroQS+[inline-graphic+not+available:+see+fulltext]+Cross-Modal+Query+Suggestion+for+Text-to-Image+Retrieval)|0|
|[Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation](https://doi.org/10.1007/978-3-031-88708-6_10)|Kasra Hosseini, Thomas Kober, Josip Krapac, Roland Vollgraf, Weiwei Cheng, Ana PeleteiroRamallo||Evaluating production-level retrieval systems at scale is a crucial yet challenging task due to the limited availability of a large pool of well-trained human annotators. Large Language Models (LLMs) have the potential to address this scaling issue and offer a viable alternative to humans for the bulk of annotation tasks. In this paper, we propose a framework for assessing the product search engines in a large-scale e-commerce setting, leveraging Multimodal LLMs for (i) generating tailored annotation guidelines for individual queries, and (ii) conducting the subsequent annotation task. Our method, validated through deployment on a large e-commerce platform, demonstrates comparable quality to human annotations, significantly reduces time and cost, facilitates rapid problem discovery, and provides an effective solution for production-level quality control at scale.|由于训练有素的人类标注人员规模有限，大规模评估生产级检索系统是一项关键而富有挑战性的任务。大型语言模型（LLMs）有望解决这一规模化难题，为大部分标注任务提供可靠的人类替代方案。本文提出一个面向大型电商场景的产品搜索引擎评估框架，通过多模态LLMs实现：（1）为每个查询生成定制化的标注指南；（2）执行后续标注任务。该方法在大型电商平台的实际部署验证表明，其标注质量与人工相当，能显著降低时间和成本，加速问题发现，并为生产级质量控制的规模化实施提供了有效解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieve,+Annotate,+Evaluate,+Repeat:+Leveraging+Multimodal+LLMs+for+Large-Scale+Product+Retrieval+Evaluation)|0|
|[Corpus Subsampling: Estimating the Effectiveness of Neural Retrieval Models on Large Corpora](https://doi.org/10.1007/978-3-031-88708-6_29)|Maik Fröbe, Andrew Parry, Harrisen Scells, Shuai Wang, Shengyao Zhuang, Guido Zuccon, Martin Potthast, Matthias Hagen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Corpus+Subsampling:+Estimating+the+Effectiveness+of+Neural+Retrieval+Models+on+Large+Corpora)|0|
|[LiT and Lean: Distilling Listwise Rerankers Into Encoder-Decoder Models](https://doi.org/10.1007/978-3-031-88714-7_13)|Manveer Singh Tamber, Ronak Pradeep, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiT+and+Lean:+Distilling+Listwise+Rerankers+Into+Encoder-Decoder+Models)|0|
|[MURR: Model Updating with Regularized Replay for Searching a Document Stream](https://doi.org/10.1007/978-3-031-88708-6_6)|Eugene Yang, Nicola Tonellotto, Dawn J. Lawrie, Sean MacAvaney, James Mayfield, Douglas W. Oard, Scott Miller||The Internet produces a continuous stream of new documents and user-generated queries. These naturally change over time based on events in the world and the evolution of language. Neural retrieval models that were trained once on a fixed set of query-document pairs will quickly start misrepresenting newly-created content and queries, leading to less effective retrieval. Traditional statistical sparse retrieval can update collection statistics to reflect these changes in the use of language in documents and queries. In contrast, continued fine-tuning of the language model underlying neural retrieval approaches such as DPR and ColBERT creates incompatibility with previously-encoded documents. Re-encoding and re-indexing all previously-processed documents can be costly. In this work, we explore updating a neural dual encoder retrieval model without reprocessing past documents in the stream. We propose MURR, a model updating strategy with regularized replay, to ensure the model can still faithfully search existing documents without reprocessing, while continuing to update the model for the latest topics. In our simulated streaming environments, we show that fine-tuning models using MURR leads to more effective and more consistent retrieval results than other strategies as the stream of documents and queries progresses.|互联网持续不断地产生新文档和用户生成的查询。这些内容会基于世界事件和语言的演变而自然变化。在固定查询-文档对上训练一次的神经检索模型很快就会开始错误表征新创建的内容和查询，导致检索效果下降。传统的统计稀疏检索可以通过更新集合统计量来反映文档和查询中语言使用的变化。相比之下，持续微调神经检索方法（如DPR和ColBERT）所依赖的语言模型会导致与先前编码文档的不兼容。重新编码和重新索引所有已处理文档可能成本高昂。在本研究中，我们探索了无需重新处理流式数据中历史文档的神经双编码器检索模型更新方法。我们提出了MURR（带正则化回放的模型更新策略），该策略能确保模型在不重新处理的情况下仍能准确检索现有文档，同时持续针对最新主题更新模型。在模拟的流式环境中，我们证明随着文档和查询流的演进，使用MURR进行模型微调比其他策略能带来更有效且更稳定的检索结果。  

（翻译说明：  
1. 专业术语处理："neural retrieval models"译为"神经检索模型"，"dual encoder"译为"双编码器"，"regularized replay"译为"正则化回放"  
2. 技术概念保留：保持"DPR""ColBERT"等模型名称原貌，符合学术惯例  
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"models that were trained..."从句独立处理  
4. 动态表达："streaming environments"译为"流式环境"而非字面直译，准确反映技术场景  
5. 一致性："re-encoding and re-indexing"统一处理为"重新编码和重新索引"，保持动词结构对称）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MURR:+Model+Updating+with+Regularized+Replay+for+Searching+a+Document+Stream)|0|
|[Repeat-Bias-Aware Optimization of Beyond-Accuracy Metrics for Next Basket Recommendation](https://doi.org/10.1007/978-3-031-88708-6_14)|Yuanna Liu, Ming Li, Mohammad Aliannejadi, Maarten de Rijke||In next basket recommendation (NBR) a set of items is recommended to users based on their historical basket sequences. In many domains, the recommended baskets consist of both repeat items and explore items. Some state-of-the-art NBR methods are heavily biased to recommend repeat items so as to maximize utility. The evaluation and optimization of beyond-accuracy objectives for NBR, such as item fairness and diversity, has attracted increasing attention. How can such beyond-accuracy objectives be pursued in the presence of heavy repeat bias? We find that only optimizing diversity or item fairness without considering repeat bias may cause NBR algorithms to recommend more repeat items. To solve this problem, we propose a model-agnostic repeat-bias-aware optimization algorithm to post-process the recommended results obtained from NBR methods with the objective of mitigating repeat bias when optimizing diversity or item fairness. We consider multiple variations of our optimization algorithm to cater to multiple NBR methods. Experiments on three real-world grocery shopping datasets show that the proposed algorithms can effectively improve diversity and item fairness, and mitigate repeat bias at acceptable Recall loss.|在下一次购物篮推荐（NBR）任务中，系统会根据用户历史购物篮序列向其推荐一组商品。在许多应用场景中，被推荐的购物篮既包含重复购买商品也包含探索性商品。当前最先进的NBR方法存在严重的重复推荐倾向，以期实现效用最大化。近年来，针对NBR系统中超越准确率的评估指标（如商品公平性和多样性）的优化研究日益受到关注。当存在严重重复推荐偏差时，应如何实现这些超越准确率的目标？我们发现若仅优化多样性或商品公平性而不考虑重复偏差，反而可能导致NBR算法推荐更多重复商品。为解决该问题，我们提出一种与模型无关的重复偏差感知优化算法，通过对NBR方法生成的推荐结果进行后处理，在优化多样性或商品公平性的同时降低重复偏差。我们设计了优化算法的多种变体以适配不同NBR方法。在三个真实杂货购物数据集上的实验表明，所提算法能在可接受的召回率损失范围内有效提升多样性、改善商品公平性并缓解重复推荐偏差。

（注：根据学术论文翻译规范，对以下术语进行了标准化处理：
1. "next basket recommendation"译为"下一次购物篮推荐"并标注专业缩写NBR
2. "repeat items/explore items"统一译为"重复购买商品/探索性商品"
3. "beyond-accuracy objectives"译为"超越准确率的评估指标"
4. 保持"Recall"等专业指标名称英文原貌
5. 对长难句进行符合中文表达习惯的拆分重组，如将"in the presence of..."从句转换为条件状语前置）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Repeat-Bias-Aware+Optimization+of+Beyond-Accuracy+Metrics+for+Next+Basket+Recommendation)|0|
|[Lost but Not Only in the Middle - Positional Bias in Retrieval Augmented Generation](https://doi.org/10.1007/978-3-031-88708-6_16)|Jan Hutter, David Rau, Maarten Marx, Jaap Kamps||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lost+but+Not+Only+in+the+Middle+-+Positional+Bias+in+Retrieval+Augmented+Generation)|0|
|[A Multi-modal Recipe for Improved Multi-domain Recommendation](https://doi.org/10.1007/978-3-031-88708-6_27)|Zixuan Yi, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-modal+Recipe+for+Improved+Multi-domain+Recommendation)|0|
|[Improving Novelty and Diversity of Nearest-Neighbors Recommendation by Exploiting Dissimilarities](https://doi.org/10.1007/978-3-031-88717-8_14)|Pablo Sánchez, Javier SanzCruzado, Alejandro Bellogín||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Novelty+and+Diversity+of+Nearest-Neighbors+Recommendation+by+Exploiting+Dissimilarities)|0|
|[Improving Low-Resource Retrieval Effectiveness Using Zero-Shot Linguistic Similarity Transfer](https://doi.org/10.1007/978-3-031-88717-8_22)|Andreas Chari, Sean MacAvaney, Iadh Ounis||Globalisation and colonisation have led the vast majority of the world to use only a fraction of languages, such as English and French, to communicate, excluding many others. This has severely affected the survivability of many now-deemed vulnerable or endangered languages, such as Occitan and Sicilian. These languages often share some characteristics, such as elements of their grammar and lexicon, with other high-resource languages, e.g. French or Italian. They can be clustered into groups of language varieties with various degrees of mutual intelligibility. Current search systems are not usually trained on many of these low-resource varieties, leading search users to express their needs in a high-resource language instead. This problem is further complicated when most information content is expressed in a high-resource language, inhibiting even more retrieval in low-resource languages. We show that current search systems are not robust across language varieties, severely affecting retrieval effectiveness. Therefore, it would be desirable for these systems to leverage the capabilities of neural models to bridge the differences between these varieties. This can allow users to express their needs in their low-resource variety and retrieve the most relevant documents in a high-resource one. To address this, we propose fine-tuning neural rankers on pairs of language varieties, thereby exposing them to their linguistic similarities. We find that this approach improves the performance of the varieties upon which the models were directly trained, thereby regularising these models to generalise and perform better even on unseen language variety pairs. We also explore whether this approach can transfer across language families and observe mixed results that open doors for future research.|全球化和殖民化进程导致世界绝大多数人口仅使用英语、法语等少数语言进行交流，致使众多其他语言被边缘化。这一现象严重威胁了诸如奥克语、西西里语等当前被列为脆弱或濒危语种的存续。这些语言通常与法语、意大利语等高资源语言在语法结构和词汇元素上存在共性特征，可依据互通程度划分为不同层级的方言集群。现有检索系统大多未针对这些低资源语言变体进行训练，迫使检索用户不得不转用高资源语言表达需求。当绝大多数信息内容均以高资源语言呈现时，该问题会进一步恶化，从而更加抑制低资源语言的检索效能。我们的研究表明，现有检索系统在跨语言变体场景下缺乏鲁棒性，这将严重影响检索效果。因此，亟需利用神经模型的强大能力来弥合这些语言变体之间的差异，使用户能够使用低资源变体表达需求，同时检索出高资源变体中的最相关文档。为此，我们提出基于语言变体对微调神经排序模型的方法，使其充分学习语言间的相似性特征。实验证明，该方法不仅能提升模型在训练涉及语言变体上的表现，还能通过正则化效应使其对未见过的语言变体对也展现出更好的泛化性能。我们还探究了该方法在跨语系场景下的迁移能力，观察到的差异化结果为未来研究提供了新的探索方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Low-Resource+Retrieval+Effectiveness+Using+Zero-Shot+Linguistic+Similarity+Transfer)|0|
|[FlashCheck: Exploration of Efficient Evidence Retrieval for Fast Fact-Checking](https://doi.org/10.1007/978-3-031-88717-8_28)|Kevin Nanhekhan, Venktesh V, Erik Martin, Henrik Vatndal, Vinay Setty, Avishek Anand||The advances in digital tools have led to the rampant spread of misinformation. While fact-checking aims to combat this, manual fact-checking is cumbersome and not scalable. It is essential for automated fact-checking to be efficient for aiding in combating misinformation in real-time and at the source. Fact-checking pipelines primarily comprise a knowledge retrieval component which extracts relevant knowledge to fact-check a claim from large knowledge sources like Wikipedia and a verification component. The existing works primarily focus on the fact-verification part rather than evidence retrieval from large data collections, which often face scalability issues for practical applications such as live fact-checking. In this study, we address this gap by exploring various methods for indexing a succinct set of factual statements from large collections like Wikipedia to enhance the retrieval phase of the fact-checking pipeline. We also explore the impact of vector quantization to further improve the efficiency of pipelines that employ dense retrieval approaches for first-stage retrieval. We study the efficiency and effectiveness of the approaches on fact-checking datasets such as HoVer and WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the real-world utility of the efficient retrieval approaches by fact-checking 2024 presidential debate and also open source the collection of claims with corresponding labels identified in the debate. Through a combination of indexed facts together with Dense retrieval and Index compression, we achieve up to a 10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the classical fact-checking pipelines over large collections.|数字工具的进步导致虚假信息泛滥。虽然事实核查旨在应对这一问题，但人工核查效率低下且难以扩展。自动化事实核查必须高效运行，才能从源头实时遏制虚假信息传播。典型的事实核查流程主要包括两个环节：从维基百科等大型知识源检索相关证据的知识检索组件，以及进行事实核验的验证组件。现有研究主要集中于事实验证环节，而面向大规模数据集的证据检索往往存在可扩展性问题，难以满足实时核查等实际应用需求。本研究通过探索多种索引方法填补这一空白——从维基百科等海量数据中构建精简事实陈述集合以优化检索环节，并研究向量量化技术对采用稠密检索的一阶段检索流程的增效作用。基于HoVer和WiCE等事实核查数据集，我们以维基百科作为知识源，系统评估了各方法在效率与效果上的表现。为验证实用价值，我们还对2024年总统辩论进行实时事实核查，并开源了标注过的辩论主张数据集。实验表明：通过索引事实库结合稠密检索与索引压缩技术，相比传统大规模事实核查流程，我们在CPU上实现了10.0倍加速，GPU上更获得超过20.0倍的性能提升。

（注：根据技术文档翻译规范进行了以下专业处理：
1. "pipeline"译为"流程"而非字面直译"管道"
2. "dense retrieval"采用学界通用译法"稠密检索"
3. "vector quantization"译为专业术语"向量量化"
4. 长难句按中文习惯拆分为短句，如将原文最后复合长句分解为因果逻辑清晰的表述
5. 保持技术指标数字精确性，如"10.0x"译为"10.0倍"
6. 专业数据集名称HoVer/WiCE保留英文原名）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FlashCheck:+Exploration+of+Efficient+Evidence+Retrieval+for+Fast+Fact-Checking)|0|
|[Sim4Rec: Flexible and Extensible Simulator for Recommender Systems for Large-Scale Data](https://doi.org/10.1007/978-3-031-88717-8_33)|Anna Volodkevich, Veronika Ivanova, Alexey Vasilev, Dmitry Bugaychenko, Maxim Savchenko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sim4Rec:+Flexible+and+Extensible+Simulator+for+Recommender+Systems+for+Large-Scale+Data)|0|
|[The Impact of Mainstream-Driven Algorithms on Recommendations for Children](https://doi.org/10.1007/978-3-031-88714-7_5)|Robin Ungruh, Alejandro Bellogín, Maria Soledad Pera||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Mainstream-Driven+Algorithms+on+Recommendations+for+Children)|0|
|[Rank-DistiLLM: Closing the Effectiveness Gap Between Cross-Encoders and LLMs for Passage Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_31)|Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen|Friedrich-Schiller-Universität Jena; CSIRO; University of Queensland; University of Kassel ScadDS.AI; Bauhaus-Universität Weimar; Leipzig University|Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, distilled models do not match the effectiveness of their teacher LLMs. We hypothesize that this effectiveness gap is due to the fact that previous work has not applied the best-suited methods for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss functions). To close this gap, we create a new dataset, Rank-DistiLLM. Cross-encoders trained on Rank-DistiLLM achieve the effectiveness of LLMs while being up to 173 times faster and 24 times more memory efficient. Our code and data is available at https://github.com/webis-de/ECIR-25.|从大型语言模型（LLM）蒸馏得到的交叉编码器（cross-encoder）通常比基于人工标注数据微调的交叉编码器具有更强的重排序效果。然而，蒸馏模型始终无法达到其教师LLM的效能水平。我们推测这种效能差距源于先前研究未能充分应用最适合人工标注数据微调交叉编码器的方法（例如：困难负样本采样、深度采样以及列表式损失函数）。为消除这一差距，我们构建了全新数据集Rank-DistiLLM。基于该数据集训练的交叉编码器在效能上可媲美LLM，同时推理速度提升达173倍，内存效率提高24倍。代码与数据集已开源：https://github.com/webis-de/ECIR-25。

（翻译说明：  
1. 专业术语处理："hard-negative sampling"译为"困难负样本采样"符合NLP领域惯例，"listwise loss functions"采用"列表式损失函数"的学术译法  
2. 技术细节保留：精确转化"173 times faster"为"173倍"，避免"快173倍"的口语化表达  
3. 句式结构调整：将英文长句拆分为符合中文科技论文表达习惯的短句，如将假设从句转换为独立陈述句  
4. 被动语态转化："are often more effective"译为"通常具有更强效果"符合中文主动语态偏好  
5. 数据一致性：严格保持数值准确性，包括倍数关系和效率指标  
6. 学术规范：完整保留技术网址，采用角标数字呈现）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rank-DistiLLM:+Closing+the+Effectiveness+Gap+Between+Cross-Encoders+and+LLMs+for+Passage+Re-ranking)|0|
|[Exploring the Effectiveness of Multi-stage Fine-Tuning for Cross-Encoder Re-rankers](https://doi.org/10.1007/978-3-031-88714-7_45)|Francesca Pezzuti, Sean MacAvaney, Nicola Tonellotto||State-of-the-art cross-encoders can be fine-tuned to be highly effective in passage re-ranking. The typical fine-tuning process of cross-encoders as re-rankers requires large amounts of manually labelled data, a contrastive learning objective, and a set of heuristically sampled negatives. An alternative recent approach for fine-tuning instead involves teaching the model to mimic the rankings of a highly effective large language model using a distillation objective. These fine-tuning strategies can be applied either individually, or in sequence. In this work, we systematically investigate the effectiveness of point-wise cross-encoders when fine-tuned independently in a single stage, or sequentially in two stages. Our experiments show that the effectiveness of point-wise cross-encoders fine-tuned using contrastive learning is indeed on par with that of models fine-tuned with multi-stage approaches. Code is available for reproduction at https://github.com/fpezzuti/multistage-finetuning.|现有的先进交叉编码器经过微调后，在段落重排序任务中能表现出卓越性能。传统交叉编码器作为重排序器的微调流程通常需要：大量人工标注数据、对比学习目标函数以及一组启发式采样的负例样本。近期出现的一种替代性微调方法，是通过知识蒸馏目标让模型学习模仿高效大型语言模型的排序行为。这些微调策略既可单独实施，也可分阶段组合使用。本研究系统性地探究了点式交叉编码器在单阶段独立微调与两阶段顺序微调中的性能表现。实验结果表明：采用对比学习微调的点式交叉编码器，其效果确实与多阶段微调方法获得的模型性能相当。重现实验的代码已发布于https://github.com/fpezzuti/multistage-finetuning。

（翻译说明：
1. "State-of-the-art"译为"现有的先进"既保留原意又符合中文表达习惯
2. "passage re-ranking"统一译为专业术语"段落重排序"
3. "contrastive learning objective"译为"对比学习目标函数"准确体现技术细节
4. "distillation objective"译为"知识蒸馏目标"保持领域术语一致性
5. "point-wise cross-encoders"译为"点式交叉编码器"精准对应论文专有概念
6. 长难句拆分处理（如第一段第二句），通过冒号和分号保持逻辑连贯性
7. 被动语态转换为主动句式（如"can be fine-tuned"译为"经过微调"）
8. 技术路线描述保持精确性（如"heuristically sampled negatives"译为"启发式采样的负例样本"）
9. 补充"性能表现"等范畴词使中文更通顺
10. 代码仓库链接保留原始格式，符合学术规范）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Effectiveness+of+Multi-stage+Fine-Tuning+for+Cross-Encoder+Re-rankers)|0|
|[Semantic Search and Filtering with AI Agents](https://doi.org/10.1007/978-3-031-88720-8_4)|Martin Bulín, Jan Svec, Filip Polák, Lubos Smídl||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Search+and+Filtering+with+AI+Agents)|0|
|[Adapting LLMs for Domain-Specific Retrieval: A Case Study in Nuclear Safety](https://doi.org/10.1007/978-3-031-88720-8_20)|Federico Borazio, Danilo Croce, Roberto Basili||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+LLMs+for+Domain-Specific+Retrieval:+A+Case+Study+in+Nuclear+Safety)|0|
|[On the Longitudinal Impact of Exposure Bias in Recommender Systems](https://doi.org/10.1007/978-3-031-88720-8_29)|Andrea Pisani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Longitudinal+Impact+of+Exposure+Bias+in+Recommender+Systems)|0|
|[Mitigating Gender Bias in Information Retrieval Systems](https://doi.org/10.1007/978-3-031-88720-8_36)|Shirin Seyedsalehi|Univ Waterloo, Waterloo, ON, Canada; Toronto Metropolitan Univ, Toronto, ON, Canada|Recent studies have shown that information retrieval systems may exhibit stereotypical gender biases in outcomes which may lead to discrimination against minority groups, such as different genders, and impact users' decision making and judgements. In this tutorial, we inform the audience of studies that have systematically reported the presence of stereotypical gender biases in Information Retrieval (IR) systems and different pre-trained Natural Language Processing (NLP) models. We further classify existing work on gender biases in IR systems and NLP models as being related to (1) relevance judgement datasets, (2) structure of retrieval methods, (3) representations learnt for queries and documents, (4) and pre-trained embedding models. Based on the aforementioned categories, we present a host of methods from the literature that can be leveraged to measure, control, or mitigate the existence of stereotypical biases within IR systems and different NLP models that are used for down-stream tasks. Besides, we introduce available datasets and collections that are widely used for studying the existence of gender biases in IR systems and NLP models, the evaluation metrics that can be used for measuring the level of bias and utility of the models, and de-biasing methods that can be leveraged to mitigate gender biases within those models.|近期研究表明，信息检索系统在输出结果中可能呈现刻板性别偏见，这种偏见可能导致对不同性别等少数群体的歧视，进而影响用户的决策与判断。本教程向听众系统性地展示关于信息检索（IR）系统与各类预训练自然语言处理（NLP）模型中存在刻板性别偏见的研究成果。我们进一步将现有关于IR系统与NLP模型中性别偏见的研究工作归类为四大方面：(1) 相关性判定数据集，(2) 检索方法的结构设计，(3) 查询与文档的表征学习，(4) 预训练嵌入模型。基于上述分类体系，我们综述了文献中可用于测量、控制或消除IR系统及下游任务NLP模型中刻板偏见的多种方法。此外，我们还介绍了广泛应用于IR系统与NLP模型性别偏见研究的公开数据集资源、用于衡量模型偏见程度与实用性的评估指标，以及可用于减轻这些模型中性别偏见的去偏见技术方法。

（注：译文严格遵循以下专业处理原则：
1. 技术术语统一："stereotypical biases"译为"刻板偏见"，"pre-trained embedding models"译为"预训练嵌入模型"
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转化："have been classified"译为主动式"归类为"
4. 概念显化："down-stream tasks"补充译为"下游任务NLP模型"
5. 专业表述："de-biasing methods"采用学界通用译法"去偏见技术方法"）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Gender+Bias+in+Information+Retrieval+Systems)|0|
|[Advanced Methods for Visual Information Retrieval and Exploration in Large Multimedia Collections](https://doi.org/10.1007/978-3-031-88720-8_43)|Kai Uwe Barthel, Nico Hezel, Konstantin Schall||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advanced+Methods+for+Visual+Information+Retrieval+and+Exploration+in+Large+Multimedia+Collections)|0|
|[Efficient Session Retrieval Using Topical Index Shards](https://doi.org/10.1007/978-3-031-88711-6_3)|Gijs Hendriksen, Djoerd Hiemstra, Arjen P. de Vries||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Session+Retrieval+Using+Topical+Index+Shards)|0|
|[Feature Attribution Explanations of Session-Based Recommendations](https://doi.org/10.1007/978-3-031-88711-6_4)|Simone Borg Bruun, Maria Maistro, Christina Lioma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Feature+Attribution+Explanations+of+Session-Based+Recommendations)|0|
|[Exploring the Relationship Between Listener Receptivity and Source of Music Recommendations](https://doi.org/10.1007/978-3-031-88711-6_7)|John Paul Vargheese, Marianne Wilson, Katherine Stephen, Rachel Salzano, David Brazier||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Relationship+Between+Listener+Receptivity+and+Source+of+Music+Recommendations)|0|
|[An Investigation of Prompt Variations for Zero-Shot LLM-Based Rankers](https://doi.org/10.1007/978-3-031-88711-6_12)|Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Investigation+of+Prompt+Variations+for+Zero-Shot+LLM-Based+Rankers)|0|
|[Query Performance Prediction Using Dimension Importance Estimators](https://doi.org/10.1007/978-3-031-88711-6_13)|Guglielmo Faggioli, Nicola Ferro, Raffaele Perego, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Performance+Prediction+Using+Dimension+Importance+Estimators)|0|
|[Rank-Without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models](https://doi.org/10.1007/978-3-031-88711-6_15)|Crystina Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rank-Without-GPT:+Building+GPT-Independent+Listwise+Rerankers+on+Open-Source+Large+Language+Models)|0|
|[Measuring Actual Privacy of Obfuscated Queries in Information Retrieval](https://doi.org/10.1007/978-3-031-88708-6_4)|Francesco Luigi De Faveri, Guglielmo Faggioli, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Actual+Privacy+of+Obfuscated+Queries+in+Information+Retrieval)|0|
|[Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track](https://doi.org/10.1007/978-3-031-88708-6_9)|Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ragnarök:+A+Reusable+RAG+Framework+and+Baselines+for+TREC+2024+Retrieval-Augmented+Generation+Track)|0|
|[Advancing Math Formula Search Using Diverse Structural and Symbolic Representations](https://doi.org/10.1007/978-3-031-88708-6_8)|Sumedh Vemuganti, Ayu Seiya, Nickvash Kani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Math+Formula+Search+Using+Diverse+Structural+and+Symbolic+Representations)|0|
|[Token Pruning Optimization for Efficient Multi-vector Dense Retrieval](https://doi.org/10.1007/978-3-031-88708-6_7)|Shanxiu He, Mutasem AlDarabsah, Suraj Nair, Jonathan May, Tarun Agarwal, Tao Yang, Choon Hui Teo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Token+Pruning+Optimization+for+Efficient+Multi-vector+Dense+Retrieval)|0|
|[Higher Order Knowledge Graph Embeddings](https://doi.org/10.1007/978-3-031-88708-6_12)|Giuseppe Pirrò||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Higher+Order+Knowledge+Graph+Embeddings)|0|
|[Graph Representation of Tables+Text and Compact Subgraph Retrieval for QA Tasks](https://doi.org/10.1007/978-3-031-88708-6_11)|Vishwajeet Kumar, Jaydeep Sen, Bhawna Chelani, Soumen Chakrabarti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Representation+of+Tables+Text+and+Compact+Subgraph+Retrieval+for+QA+Tasks)|0|
|[Context Example Selection for LLM Generated Relevance Assessments](https://doi.org/10.1007/978-3-031-88708-6_19)|Jack McKechnie, Graham McDonald, Craig Macdonald||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context+Example+Selection+for+LLM+Generated+Relevance+Assessments)|0|
|[LSTM-Based Selective Dense Text Retrieval Guided by Sparse Lexical Retrieval](https://doi.org/10.1007/978-3-031-88708-6_18)|Yingrui Yang, Parker Carlson, Yifan Qiao, Wentai Xie, Shanxiu He, Tao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LSTM-Based+Selective+Dense+Text+Retrieval+Guided+by+Sparse+Lexical+Retrieval)|0|
|[Opt-in Transparent Fairness for Recommender Systems](https://doi.org/10.1007/978-3-031-88708-6_23)|Bjørnar Vassøy, Benjamin Kille, Helge Langseth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Opt-in+Transparent+Fairness+for+Recommender+Systems)|0|
|[Towards Identity-Aware Cross-Modal Retrieval: A Dataset and a Baseline](https://doi.org/10.1007/978-3-031-88708-6_28)|Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Identity-Aware+Cross-Modal+Retrieval:+A+Dataset+and+a+Baseline)|0|
|[CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval](https://doi.org/10.1007/978-3-031-88717-8_2)|Mohammad Mahdi Abootorabi, Ehsaneddin Asgari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLASP:+Contrastive+Language-Speech+Pretraining+for+Multilingual+Multimodal+Information+Retrieval)|0|
|[Fact vs. Fiction: Are the Reportedly "Magical" LLM-Based Recommenders Reproducible?](https://doi.org/10.1007/978-3-031-88717-8_7)|Shirin Tahmasebi, Narjes Nikzad, Amir Hossein Payberah, Meysam AsgariChenaghlu, Mihhail Matskin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fact+vs.+Fiction:+Are+the+Reportedly+"Magical"+LLM-Based+Recommenders+Reproducible?)|0|
|[Reproducing HotFlip for Corpus Poisoning Attacks in Dense Retrieval](https://doi.org/10.1007/978-3-031-88717-8_8)|Yongkang Li, Panagiotis Eustratiadis, Evangelos Kanoulas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reproducing+HotFlip+for+Corpus+Poisoning+Attacks+in+Dense+Retrieval)|0|
|[On the Reproducibility of Learned Sparse Retrieval Adaptations for Long Documents](https://doi.org/10.1007/978-3-031-88717-8_6)|Emmanouil Georgios Lionis, JiaHuei Ju||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Reproducibility+of+Learned+Sparse+Retrieval+Adaptations+for+Long+Documents)|0|
|[Are Representation Disentanglement and Interpretability Linked in Recommendation Models? - A Critical Review and Reproducibility Study](https://doi.org/10.1007/978-3-031-88717-8_4)|Ervin Dervishaj, Tuukka Ruotsalo, Maria Maistro, Christina Lioma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Representation+Disentanglement+and+Interpretability+Linked+in+Recommendation+Models?+-+A+Critical+Review+and+Reproducibility+Study)|0|
|[Combining Query Performance Predictors: A Reproducibility Study](https://doi.org/10.1007/978-3-031-88717-8_9)|Sourav Saha, Suchana Datta, Dwaipayan Roy, Mandar Mitra, Derek Greene||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Query+Performance+Predictors:+A+Reproducibility+Study)|0|
|[Revisiting Language Models in Neural News Recommender Systems](https://doi.org/10.1007/978-3-031-88717-8_12)|Yuyue Zhao, Jin Huang, David Vos, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Language+Models+in+Neural+News+Recommender+Systems)|0|
|[Towards Reproducibility of Interactive Retrieval Experiments: Framework and Case Study](https://doi.org/10.1007/978-3-031-88717-8_11)|Jana Isabelle Friese, Norbert Fuhr||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reproducibility+of+Interactive+Retrieval+Experiments:+Framework+and+Case+Study)|0|
|[LambdaFair for Fair and Effective Ranking](https://doi.org/10.1007/978-3-031-88717-8_15)|Federico Marcuzzi, Claudio Lucchese, Salvatore Orlando||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LambdaFair+for+Fair+and+Effective+Ranking)|0|
|[Fair Exposure Allocation Using Generative Query Expansion](https://doi.org/10.1007/978-3-031-88717-8_20)|Thomas Jänich, Graham McDonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Exposure+Allocation+Using+Generative+Query+Expansion)|0|
|[Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO](https://doi.org/10.1007/978-3-031-88717-8_21)|Umer Butt, Stalin Veranasi, Günter Neumann||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Low-Resource+Language+Retrieval:+Establishing+Baselines+for+Urdu+MS+MARCO)|0|
|[Unraveling the Impact of Visual Complexity on Search as Learning](https://doi.org/10.1007/978-3-031-88714-7_2)|Wolfgang Gritz, Anett Hoppe, Ralph Ewerth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+the+Impact+of+Visual+Complexity+on+Search+as+Learning)|0|
|[Enhancing Utility in Differentially Private Recommendation Data Release via Exponential Mechanism](https://doi.org/10.1007/978-3-031-88714-7_3)|Antonio Ferrara, Angela Di Fazio, Alberto Carlo Maria Mancino, Tommaso Di Noia, Eugenio Di Sciascio||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Utility+in+Differentially+Private+Recommendation+Data+Release+via+Exponential+Mechanism)|0|
|[Inducing Diversity in Differentiable Search Indexing](https://doi.org/10.1007/978-3-031-88714-7_7)|Abhijeet Phatak, Jayant Sachdev, Sean D. Rosario, Swati Kirti, Chittaranjan Tripathy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inducing+Diversity+in+Differentiable+Search+Indexing)|0|
|[The Impact of Incidental Multilingual Text on Cross-Lingual Transfer in Monolingual Retrieval](https://doi.org/10.1007/978-3-031-88714-7_14)|Andrew Liu, Edward Xu, Crystina Zhang, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Incidental+Multilingual+Text+on+Cross-Lingual+Transfer+in+Monolingual+Retrieval)|0|
|[Approximate Bag-of-Words Top-k Corpus Graphs](https://doi.org/10.1007/978-3-031-88714-7_15)|Lachlan Dunn, Luke Gallagher, Joel Mackenzie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximate+Bag-of-Words+Top-k+Corpus+Graphs)|0|
|[Iterative Self-training for Code Generation via Reinforced Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_21)|Nikita Sorokin, Ivan Sedykh, Valentin Malykh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Iterative+Self-training+for+Code+Generation+via+Reinforced+Re-ranking)|0|
|[Fact-Driven Health Information Retrieval: Integrating LLMs and Knowledge Graphs to Combat Misinformation](https://doi.org/10.1007/978-3-031-88714-7_17)|Gian Carlo Milanese, Georgios Peikos, Gabriella Pasi, Marco Viviani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fact-Driven+Health+Information+Retrieval:+Integrating+LLMs+and+Knowledge+Graphs+to+Combat+Misinformation)|0|
|[Investigating the Performance of Dense Retrievers for Queries with Numerical Conditions](https://doi.org/10.1007/978-3-031-88714-7_19)|Haruki Fujimaki, Makoto P. Kato||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+the+Performance+of+Dense+Retrievers+for+Queries+with+Numerical+Conditions)|0|
|[Efficient Constant-Space Multi-vector Retrieval](https://doi.org/10.1007/978-3-031-88714-7_22)|Sean MacAvaney, Antonio Mallia, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Constant-Space+Multi-vector+Retrieval)|0|
|[Large Language Model Can Be a Foundation for Hidden Rationale-Based Retrieval](https://doi.org/10.1007/978-3-031-88714-7_27)|Luo Ji, Feixiang Guo, Teng Chen, Qingqing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin Jiang, Yong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+Can+Be+a+Foundation+for+Hidden+Rationale-Based+Retrieval)|0|
|[SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation](https://doi.org/10.1007/978-3-031-88714-7_28)|Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAFERec:+Self-Attention+and+Frequency+Enriched+Model+for+Next+Basket+Recommendation)|0|
|[Can Generative AI Adequately Protect Queries? Analyzing the Trade-Off Between Privacy Awareness and Retrieval Effectiveness](https://doi.org/10.1007/978-3-031-88714-7_34)|Luca HerranzCelotti, Blessing Guembe, Giovanni Livraga, Marco Viviani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Generative+AI+Adequately+Protect+Queries?+Analyzing+the+Trade-Off+Between+Privacy+Awareness+and+Retrieval+Effectiveness)|0|
|[A Test Collection for Dataset Retrieval](https://doi.org/10.1007/978-3-031-88714-7_36)|Nikolay Kolyada, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Test+Collection+for+Dataset+Retrieval)|0|
|[E2Rank: Efficient and Effective Layer-Wise Reranking](https://doi.org/10.1007/978-3-031-88714-7_41)|Cesare Campagnano, Antonio Mallia, Jack Pertschuk, Fabrizio Silvestri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=E2Rank:+Efficient+and+Effective+Layer-Wise+Reranking)|0|
|[Retrieval-Augmented Neural Team Formation](https://doi.org/10.1007/978-3-031-88714-7_35)|Mohammad Dara, Radin Hamidi Rad, Fattane Zarrinkalam, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Neural+Team+Formation)|0|
|[A Comparative Analysis of Retrieval-Augmented Generation and Crowdsourcing for Fact-Checking](https://doi.org/10.1007/978-3-031-88714-7_44)|Francesco Bombassei De Bona, David La Barbera, Stefano Mizzaro, Kevin Roitero||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Comparative+Analysis+of+Retrieval-Augmented+Generation+and+Crowdsourcing+for+Fact-Checking)|0|
|[PIE-Med: Predicting, Interpreting and Explaining Medical Recommendations](https://doi.org/10.1007/978-3-031-88720-8_2)|Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PIE-Med:+Predicting,+Interpreting+and+Explaining+Medical+Recommendations)|0|
|[MindWell: A Conversational Agent for Professional Depression Screening on Social Media](https://doi.org/10.1007/978-3-031-88720-8_9)|Eliseo Bao, Anxo Pérez, Javier Parapar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MindWell:+A+Conversational+Agent+for+Professional+Depression+Screening+on+Social+Media)|0|
|[DenseReviewer: A Screening Prioritisation Tool for Systematic Review Based on Dense Retrieval](https://doi.org/10.1007/978-3-031-88720-8_11)|Xinyu Mao, Teerapong Leelanupab, Harrisen Scells, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DenseReviewer:+A+Screening+Prioritisation+Tool+for+Systematic+Review+Based+on+Dense+Retrieval)|0|
|[MechIR: A Mechanistic Interpretability Framework for Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_16)|Andrew Parry, Catherine Chen, Carsten Eickhoff, Sean MacAvaney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MechIR:+A+Mechanistic+Interpretability+Framework+for+Information+Retrieval)|0|
|[Combining Knowledge Graphs and Retrieval Augmented Generation for Enterprise Resource Planning](https://doi.org/10.1007/978-3-031-88720-8_19)|Amar Viswanathan, Felix Sasaki||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Knowledge+Graphs+and+Retrieval+Augmented+Generation+for+Enterprise+Resource+Planning)|0|
|[Web-Scale Retrieval Experimentation with chatnoir-pyterrier](https://doi.org/10.1007/978-3-031-88720-8_17)|Jan Heinrich Merker, Janek Bevendorff, Maik Fröbe, Tim Hagen, Harrisen Scells, Matti Wiegmann, Benno Stein, Matthias Hagen, Martin Potthast||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web-Scale+Retrieval+Experimentation+with+chatnoir-pyterrier)|0|
|[Contextualizing Spotify's Audiobook List Recommendations with Descriptive Shelves](https://doi.org/10.1007/978-3-031-88720-8_26)|Gustavo Penha, Alice Wang, Martin Achenbach, Kristen Sheets, Sahitya Mantravadi, Remi Galvez, Nico GuettaJeanrenaud, Divya Narayanan, Ofeliya Kalaydzhyan, Hugues Bouchard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextualizing+Spotify's+Audiobook+List+Recommendations+with+Descriptive+Shelves)|0|
|[Text2Playlist: Generating Personalized Playlists from Text on Deezer](https://doi.org/10.1007/978-3-031-88720-8_27)|Mathieu Delcluze, Antoine Khoury, Clémence Vast, Valerio Arnaudo, Léa Briand, Walid Bendada, Thomas Bouabça||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text2Playlist:+Generating+Personalized+Playlists+from+Text+on+Deezer)|0|
|[Cooperative and Competitive LLM-Based Multi-Agent Systems for Recommendation](https://doi.org/10.1007/978-3-031-88720-8_33)|Marco Valentini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cooperative+and+Competitive+LLM-Based+Multi-Agent+Systems+for+Recommendation)|0|
|[Advancing Query Performance Prediction: Challenges and Adaptive Solutions](https://doi.org/10.1007/978-3-031-88720-8_38)|Abbas Saleminezhad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Query+Performance+Prediction:+Challenges+and+Adaptive+Solutions)|0|
|[Explainable Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_40)|Avishek Anand, Sourav Saha, Venktesh V||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Information+Retrieval)|0|
|[QPP++ 2025: Query Performance Prediction and Its Applications in the Era of Large Language Models](https://doi.org/10.1007/978-3-031-88720-8_49)|Chuan Meng, Guglielmo Faggioli, Mohammad Aliannejadi, Nicola Ferro, Josiane Mothe||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QPP+++2025:+Query+Performance+Prediction+and+Its+Applications+in+the+Era+of+Large+Language+Models)|0|
|[eRisk 2025: Contextual and Conversational Approaches for Depression Challenges](https://doi.org/10.1007/978-3-031-88720-8_62)|Javier Parapar, Anxo Pérez, Xi Wang, Fabio Crestani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=eRisk+2025:+Contextual+and+Conversational+Approaches+for+Depression+Challenges)|0|
|[The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval](https://doi.org/10.1007/978-3-031-88720-8_68)|Firoj Alam, Julia Maria Struß, Tanmoy Chakraborty, Stefan Dietze, Salim Hafid, Katerina Korre, Arianna Muti, Preslav Nakov, Federico Ruggeri, Sebastian Schellhammer, Vinay Setty, Megha Sundriyal, Konstantin Todorov, Venktesh V||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+CLEF-2025+CheckThat!+Lab:+Subjectivity,+Fact-Checking,+Claim+Normalization,+and+Retrieval)|0|
|[Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering](https://doi.org/10.1007/978-3-031-88711-6_6)|Imed Keraghel, Mohamed Nadif||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Convolutional+Networks:+Named+Entity+Recognition+and+Large+Language+Model+Embedding+in+Document+Clustering)|0|
|[MVAM: Multi-View Attention Method for Fine-Grained Image-Text Matching](https://doi.org/10.1007/978-3-031-88711-6_11)|Wanqing Cui, Rui Cheng, Jiafeng Guo, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MVAM:+Multi-View+Attention+Method+for+Fine-Grained+Image-Text+Matching)|0|
|[PEIR: Modeling Performance in Neural Information Retrieval](https://doi.org/10.1007/978-3-031-88711-6_18)|Pooya Khandel, Andrew Yates, Ana Lucia Varbanescu, Maarten de Rijke, Andy D. Pimentel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEIR:+Modeling+Performance+in+Neural+Information+Retrieval)|0|
|[mFollowIR: A Multilingual Benchmark for Instruction Following in Retrieval](https://doi.org/10.1007/978-3-031-88711-6_19)|Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Samuel Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn J. Lawrie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=mFollowIR:+A+Multilingual+Benchmark+for+Instruction+Following+in+Retrieval)|0|
|[Leveraging Retrieval-Augmented Generation for Keyphrase Synonym Suggestion](https://doi.org/10.1007/978-3-031-88711-6_20)|Jorge Gabín, Javier Parapar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Retrieval-Augmented+Generation+for+Keyphrase+Synonym+Suggestion)|0|
|[Can Large Language Models Effectively Rerank News Articles for Background Linking?](https://doi.org/10.1007/978-3-031-88711-6_21)|Marwa Essam, Tamer Elsayed||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Large+Language+Models+Effectively+Rerank+News+Articles+for+Background+Linking?)|0|
|[OKRA: An Explainable, Heterogeneous, Multi-stakeholder Job Recommender System](https://doi.org/10.1007/978-3-031-88711-6_22)|Roan Schellingerhout, Francesco Barile, Nava Tintarev||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OKRA:+An+Explainable,+Heterogeneous,+Multi-stakeholder+Job+Recommender+System)|0|
|[CUP: A Framework for Resource-Efficient Review-Based Recommenders](https://doi.org/10.1007/978-3-031-88711-6_23)|Ghazaleh H. Torbati, Anna Tigunova, Gerhard Weikum, Andrew Yates||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CUP:+A+Framework+for+Resource-Efficient+Review-Based+Recommenders)|0|
|[On the Robustness of Generative Information Retrieval Models: An Out-of-Distribution Perspective](https://doi.org/10.1007/978-3-031-88711-6_26)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Changjiang Zhou, Maarten de Rijke, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Robustness+of+Generative+Information+Retrieval+Models:+An+Out-of-Distribution+Perspective)|0|
|[Towards Reliable Testing for Multiple Information Retrieval System Comparisons](https://doi.org/10.1007/978-3-031-88711-6_27)|David Otero, Javier Parapar, Álvaro Barreiro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+Testing+for+Multiple+Information+Retrieval+System+Comparisons)|0|
|[BioASQ at CLEF2025: The Thirteenth Edition of the Large-Scale Biomedical Semantic Indexing and Question Answering Challenge](https://doi.org/10.1007/978-3-031-88720-8_61)|Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Martin Krallinger, Miguel RodríguezOrtega, Natalia V. Loukachevitch, Andrey Sakhovskiy, Elena Tutubalina, Grigorios Tsoumakas, George Giannakoulas, Alexandra Bekiaridou, Athanasios Samaras, Giorgio Maria Di Nunzio, Nicola Ferro, Stefano Marchesin, Laura Menotti, Gianmaria Silvello, Georgios Paliouras||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BioASQ+at+CLEF2025:+The+Thirteenth+Edition+of+the+Large-Scale+Biomedical+Semantic+Indexing+and+Question+Answering+Challenge)|0|
|[Biased PromptORE: Enhancing Relation Extraction in Gendered Languages and Complex Texts The Case of Spanish Documents from the XVIbf th Century](https://doi.org/10.1007/978-3-031-88708-6_17)|Michel Boeglin, David Kahn, Héctor López Hidalgo, Josiane Mothe, Diégo Ortiz, David Panzoli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Biased+PromptORE:+Enhancing+Relation+Extraction+in+Gendered+Languages+and+Complex+Texts+The+Case+of+Spanish+Documents+from+the+XVIbf+th+Century)|0|
|[Semantically Proportioned nDCG for Explaining ColBERT's Learning Process](https://doi.org/10.1007/978-3-031-88708-6_22)|Ariane Mueller, Craig Macdonald||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantically+Proportioned+nDCG+for+Explaining+ColBERT's+Learning+Process)|0|
|[Tales and Truths: Exploring the Linguistic Journey of 19th Century Literature and Non-fiction](https://doi.org/10.1007/978-3-031-88717-8_19)|Suchana Datta, Dwaipayan Roy, Derek Greene, Gerardine Meaney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tales+and+Truths:+Exploring+the+Linguistic+Journey+of+19th+Century+Literature+and+Non-fiction)|0|
|[TROPIC - Trustworthiness Rating of Online Publishers Through Online Interactions Calculation](https://doi.org/10.1007/978-3-031-88717-8_30)|Manuel Pratelli, Fabio Saracco, Marinella Petrocchi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TROPIC+-+Trustworthiness+Rating+of+Online+Publishers+Through+Online+Interactions+Calculation)|0|
|[LS-Dashboard: A Tool for Monitoring and Analyzing Data Annotation in Machine Learning Classification Tasks](https://doi.org/10.1007/978-3-031-88717-8_31)|Vinicius Monteiro de Lira, Peng Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LS-Dashboard:+A+Tool+for+Monitoring+and+Analyzing+Data+Annotation+in+Machine+Learning+Classification+Tasks)|0|
|[Prabodhini: Making Large Language Models Inclusive for Low-Text Literate Users](https://doi.org/10.1007/978-3-031-88717-8_35)|Vivan Jain, Srivant Vishnuvajjala, Pranathi Voora, Bhaskar Ruthvik Bikkina, Bharghavaram Boddapati, C. R. Chaitra, Dipanjan Chakraborty, Prajna Upadhyay||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prabodhini:+Making+Large+Language+Models+Inclusive+for+Low-Text+Literate+Users)|0|
|[BAAF: A Framework for Media Bias Detection](https://doi.org/10.1007/978-3-031-88714-7_24)|Soumyadeep Sar, Subinay Adhikary, Dwaipayan Roy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BAAF:+A+Framework+for+Media+Bias+Detection)|0|
|[BiasScanner: Automatic News Bias Classification for Strengthening Democracy](https://doi.org/10.1007/978-3-031-88720-8_18)|Tim Menzner, Jochen L. Leidner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BiasScanner:+Automatic+News+Bias+Classification+for+Strengthening+Democracy)|0|
|[Automatic Evaluation of Online News Outlets' Reliability](https://doi.org/10.1007/978-3-031-88720-8_32)|John Bianchi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Evaluation+of+Online+News+Outlets'+Reliability)|0|
|[Uncertainty Estimation in the Real World: A Study on Music Emotion Recognition](https://doi.org/10.1007/978-3-031-88711-6_14)|Karn N. Watcharasupat, Yiwei Ding, T. Aleksandra Ma, Pavan Seshadri, Alexander Lerch||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty+Estimation+in+the+Real+World:+A+Study+on+Music+Emotion+Recognition)|0|
|[Towards Efficient and Explainable Hate Speech Detection via Model Distillation](https://doi.org/10.1007/978-3-031-88711-6_24)|Paloma Piot, Javier Parapar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Efficient+and+Explainable+Hate+Speech+Detection+via+Model+Distillation)|0|
|[One Size Doesn't Fit All: Predicting the Number of Examples for In-Context Learning](https://doi.org/10.1007/978-3-031-88708-6_5)|Manish Chandra, Debasis Ganguly, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+Size+Doesn't+Fit+All:+Predicting+the+Number+of+Examples+for+In-Context+Learning)|0|
|[Enhancing FEVER-Style Claim Fact-Checking Against Wikipedia: A Diagnostic Taxonomy and a Generative Framework](https://doi.org/10.1007/978-3-031-88708-6_20)|Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+FEVER-Style+Claim+Fact-Checking+Against+Wikipedia:+A+Diagnostic+Taxonomy+and+a+Generative+Framework)|0|
|[Decoding the Hierarchy: A Hybrid Approach to Hierarchical Multi-label Text Classification](https://doi.org/10.1007/978-3-031-88708-6_26)|Fatos Torba, Christophe Gravier, Charlotte Laclau, Abderrhammen Kammoun, Julien Subercaze||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+the+Hierarchy:+A+Hybrid+Approach+to+Hierarchical+Multi-label+Text+Classification)|0|
|[Malevolence Attacks Against Pretrained Dialogue Models](https://doi.org/10.1007/978-3-031-88708-6_24)|Pengjie Ren, Ruiqi Li, Zhaochun Ren, Zhumin Chen, Maarten de Rijke, Yangjun Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Malevolence+Attacks+Against+Pretrained+Dialogue+Models)|0|
|[ColBERT-Serve: Efficient Multi-stage Memory-Mapped Scoring](https://doi.org/10.1007/978-3-031-88717-8_3)|Kaili Huang, Thejas Venkatesh, Uma Dingankar, Antonio Mallia, Daniel Campos, Jian Jiao, Christopher Potts, Matei Zaharia, Kwabena Boahen, Omar Khattab, Saarthak Sarup, Keshav Santhanam||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ColBERT-Serve:+Efficient+Multi-stage+Memory-Mapped+Scoring)|0|
|[A Reproducibility Study on Consistent LLM Reasoning for Natural Language Inference over Clinical Trials](https://doi.org/10.1007/978-3-031-88717-8_5)|Artur Guimarães, João Magalhães, Bruno Martins||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reproducibility+Study+on+Consistent+LLM+Reasoning+for+Natural+Language+Inference+over+Clinical+Trials)|0|
|[Multimodal Feature Extraction for Assistive Technology: Evaluation and Dataset](https://doi.org/10.1007/978-3-031-88717-8_13)|Hunter Briegel, Maya Pagal, Jacki Liddle, J. Shane Culpepper||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Feature+Extraction+for+Assistive+Technology:+Evaluation+and+Dataset)|0|
|[GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://doi.org/10.1007/978-3-031-88717-8_17)|Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GASCADE:+Grouped+Summarization+of+Adverse+Drug+Event+for+Enhanced+Cancer+Pharmacovigilance)|0|
|[Verifying Cross-Modal Entity Consistency in News Using Vision-Language Models](https://doi.org/10.1007/978-3-031-88717-8_25)|Sahar Tahmasebi, Eric MüllerBudack, Ralph Ewerth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Verifying+Cross-Modal+Entity+Consistency+in+News+Using+Vision-Language+Models)|0|
|[Nano-ESG: Extracting Corporate Sustainability Information from News Articles](https://doi.org/10.1007/978-3-031-88717-8_24)|Fabian Billert, Stefan Conrad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Nano-ESG:+Extracting+Corporate+Sustainability+Information+from+News+Articles)|0|
|[TimIR: Time-Traveling Through IR History](https://doi.org/10.1007/978-3-031-88717-8_34)|Moritz Staudinger, Wojciech Kusa, Florina Piroi, Andreas Rauber, Allan Hanbury||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TimIR:+Time-Traveling+Through+IR+History)|0|
|[SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://doi.org/10.1007/978-3-031-88717-8_32)|Michael Färber, Parisa Aghdam, Kyuri Im, Mario Tawfelis, Hardik Ghoshal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SimplifyMyText:+An+LLM-Based+System+for+Inclusive+Plain+Language+Text+Simplification)|0|
|[exHarmony: Authorship and Citations for Benchmarking the Reviewer Assignment Problem](https://doi.org/10.1007/978-3-031-88714-7_1)|Sajad Ebrahimi, Sara Salamat, Negar Arabzadeh, Mahdi Bashari, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=exHarmony:+Authorship+and+Citations+for+Benchmarking+the+Reviewer+Assignment+Problem)|0|
|[EGL-DST: Error-Guided Learning for Multidimensional Evaluation Method of Dialogue State Tracking via GPT-4](https://doi.org/10.1007/978-3-031-88714-7_8)|Wenjie Dong, Sirong Chen, Ming Gu, Yan Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EGL-DST:+Error-Guided+Learning+for+Multidimensional+Evaluation+Method+of+Dialogue+State+Tracking+via+GPT-4)|0|
|[Improving Language Model Performance by Training on Prototypical Contradictions](https://doi.org/10.1007/978-3-031-88714-7_12)|Maren Pielka, MarieChristin Freischlad, Svetlana Schmidt, Rafet Sifa||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Language+Model+Performance+by+Training+on+Prototypical+Contradictions)|0|
|[Hierarchical Skip Decoding for Efficient Autoregressive Language Model](https://doi.org/10.1007/978-3-031-88714-7_20)|Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Skip+Decoding+for+Efficient+Autoregressive+Language+Model)|0|
|[Towards Interpretable Radiology Report Generation via Concept Bottlenecks Using a Multi-agentic RAG](https://doi.org/10.1007/978-3-031-88714-7_18)|Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Interpretable+Radiology+Report+Generation+via+Concept+Bottlenecks+Using+a+Multi-agentic+RAG)|0|
|[A Simple but Effective Closed-Form Solution for Extreme Multi-label Learning](https://doi.org/10.1007/978-3-031-88714-7_25)|Kazuma Onishi, Katsuhiko Hayashi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Simple+but+Effective+Closed-Form+Solution+for+Extreme+Multi-label+Learning)|0|
|[Benchmarking Prompt Sensitivity in Large Language Models](https://doi.org/10.1007/978-3-031-88714-7_29)|Amir Hossein Razavi, Mina Soltangheis, Negar Arabzadeh, Sara Salamat, Morteza Zihayat, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+Prompt+Sensitivity+in+Large+Language+Models)|0|
|[Do LLMs Provide Consistent Answers to Health-Related Questions Across Languages?](https://doi.org/10.1007/978-3-031-88714-7_30)|Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Do+LLMs+Provide+Consistent+Answers+to+Health-Related+Questions+Across+Languages?)|0|
|[Benchmark Creation for Narrative Knowledge Delta Extraction Tasks: Can LLMs Help?](https://doi.org/10.1007/978-3-031-88714-7_32)|Alaa ElEbshihy, Annisa Maulida Ningtyas, Florina Piroi, Andreas Rauber||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmark+Creation+for+Narrative+Knowledge+Delta+Extraction+Tasks:+Can+LLMs+Help?)|0|
|[Passage Segmentation of Documents for Extractive Question Answering](https://doi.org/10.1007/978-3-031-88714-7_33)|Zuhong Liu, CharlesElie Simon, Fabien Caspani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Passage+Segmentation+of+Documents+for+Extractive+Question+Answering)|0|
|[A New Dataset for Keyword Extraction from IT Job Descriptions](https://doi.org/10.1007/978-3-031-88714-7_37)|Nisan Fichman, Hadar Isaacson, Natalia Vanetik||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+New+Dataset+for+Keyword+Extraction+from+IT+Job+Descriptions)|0|
|[Entity-Aware Cross-Modal Pretraining for Knowledge-Based Visual Question Answering](https://doi.org/10.1007/978-3-031-88714-7_38)|Omar Adjali, Olivier Ferret, Sahar Ghannay, Hervé Le Borgne||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity-Aware+Cross-Modal+Pretraining+for+Knowledge-Based+Visual+Question+Answering)|0|
|[Token-Level Graphs for Short Text Classification](https://doi.org/10.1007/978-3-031-88714-7_42)|Gregor Donabauer, Udo Kruschwitz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Token-Level+Graphs+for+Short+Text+Classification)|0|
|[Checky, the Paper-Submission Checklist Generator for Authors, Reviewers and LLMs](https://doi.org/10.1007/978-3-031-88720-8_6)|Joeran Beel, Bela Gipp, Dietmar Jannach, Alan Said, Lukas Wegmeth, Tobias Vente||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Checky,+the+Paper-Submission+Checklist+Generator+for+Authors,+Reviewers+and+LLMs)|0|
|[TheoremView: A Framework for Extracting Theorem-Like Environments from Raw PDFs](https://doi.org/10.1007/978-3-031-88720-8_5)|Shrey Mishra, Neil Sharma, Antoine Gauquier, Pierre Senellart||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TheoremView:+A+Framework+for+Extracting+Theorem-Like+Environments+from+Raw+PDFs)|0|
|[AirTOWN: A Privacy-Preserving Mobile App for Real-Time Pollution-Aware POI Suggestion](https://doi.org/10.1007/978-3-031-88720-8_7)|Giuseppe Fasano, Yashar Deldjoo, Tommaso Di Noia||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AirTOWN:+A+Privacy-Preserving+Mobile+App+for+Real-Time+Pollution-Aware+POI+Suggestion)|0|
|[Spoken Question Answering on Municipal Council Meetings](https://doi.org/10.1007/978-3-031-88720-8_8)|Pepijn van Wijk, Maarten Marx||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spoken+Question+Answering+on+Municipal+Council+Meetings)|0|
|[Leveraging LLMs to Improve Human Annotation Efficiency with INCEpTION](https://doi.org/10.1007/978-3-031-88720-8_10)|Luís Filipe Cunha, Nana Yu, Purificação Silvano, Ricardo Campos, Alípio Jorge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+to+Improve+Human+Annotation+Efficiency+with+INCEpTION)|0|
|[Forecasting Prescription Efficacy](https://doi.org/10.1007/978-3-031-88720-8_14)|HaoRen Yao, Oskar Mencer, HanSun Chiang, DerChen Chang, Ophir Frieder||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Forecasting+Prescription+Efficacy)|0|
|[ASPIRE: Assistive System for Performance Evaluation in IR](https://doi.org/10.1007/978-3-031-88720-8_12)|Georgios Peikos, Wojciech Kusa, Symeon Symeonidis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ASPIRE:+Assistive+System+for+Performance+Evaluation+in+IR)|0|
|[Rebuilding the Past: Reconstructing Portuguese News Outlets with Web Archives](https://doi.org/10.1007/978-3-031-88720-8_15)|Rodrigo Silva, Ricardo Campos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rebuilding+the+Past:+Reconstructing+Portuguese+News+Outlets+with+Web+Archives)|0|
|[PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents](https://doi.org/10.1007/978-3-031-88720-8_22)|Kanika Goswami, Puneet Mathur, Ryan A. Rossi, Franck Dernoncourt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PlotEdit:+Natural+Language-Driven+Accessible+Chart+Editing+in+PDFs+via+Multimodal+LLM+Agents)|0|
|[RURAGE: Robust Universal RAG Evaluator for Fast and Affordable QA Performance Testing](https://doi.org/10.1007/978-3-031-88720-8_23)|Nikita Krayko, Ivan Sidorov, Fedor Laputin, Alexander Panchenko, Daria Galimzianova, Vasily Konovalov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RURAGE:+Robust+Universal+RAG+Evaluator+for+Fast+and+Affordable+QA+Performance+Testing)|0|
|[Leveraging LLMs for Energy Forecasting: The AcegasApsAmga Case Study](https://doi.org/10.1007/978-3-031-88720-8_25)|Kevin Roitero, Andrea Zancola, Vincenzo Della Mea, Stefano Mizzaro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+for+Energy+Forecasting:+The+AcegasApsAmga+Case+Study)|0|
|[Hierarchical Prefixes for Long Document Representations](https://doi.org/10.1007/978-3-031-88720-8_28)|Iskandar Boucharenc||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Prefixes+for+Long+Document+Representations)|0|
|[SynKGP: Knowledge Graph Population with Syntactic-LLM Hybridation for Question-Answering](https://doi.org/10.1007/978-3-031-88720-8_34)|Eve Sauvage||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SynKGP:+Knowledge+Graph+Population+with+Syntactic-LLM+Hybridation+for+Question-Answering)|0|
|[Understanding Numerical Context by Asking Quantitative Questions](https://doi.org/10.1007/978-3-031-88720-8_35)|R. Gayathri, Koninika Pal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Numerical+Context+by+Asking+Quantitative+Questions)|0|
|[Fairness in Information Access Conceptual Foundations and New Directions](https://doi.org/10.1007/978-3-031-88720-8_41)|Michael D. Ekstrand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+Information+Access+Conceptual+Foundations+and+New+Directions)|0|
|[Enhancing Generative Models for Scientific Text Simplification](https://doi.org/10.1007/978-3-031-88720-8_39)|Benjamin Vendeville||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Generative+Models+for+Scientific+Text+Simplification)|0|
|[Large Language Models Are Human-Like Annotators](https://doi.org/10.1007/978-3-031-88720-8_45)|Mounika Marreddy, Subba Reddy Oota, Manish Gupta||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+Are+Human-Like+Annotators)|0|
|[Rapid Prototyping for AI-Based Applications: A Hands-on Tutorial for Connecting the Dots](https://doi.org/10.1007/978-3-031-88720-8_46)|Omar Alonso, Kenneth Church||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rapid+Prototyping+for+AI-Based+Applications:+A+Hands-on+Tutorial+for+Connecting+the+Dots)|0|
|[ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition](https://doi.org/10.1007/978-3-031-88720-8_56)|Jussi Karlgren, Ekaterina Artemova, Ondrej Bojar, Vladislav Mikhailov, Magnus Sahlgren, Erik Velldal, Lilja Øvrelid||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ELOQUENT+CLEF+Shared+Tasks+for+Evaluation+of+Generative+Language+Model+Quality,+2025+Edition)|0|
|[LongEval at CLEF 2025: Longitudinal Evaluation of IR Model Performance](https://doi.org/10.1007/978-3-031-88720-8_58)|Matteo Cancellieri, Alaa ElEbshihy, Tobias Fink, Petra Galuscáková, Gabriela González Sáez, Lorraine Goeuriot, David Iommi, Jüri Keller, Petr Knoth, Philippe Mulhem, Florina Piroi, David Pride, Philipp Schaer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LongEval+at+CLEF+2025:+Longitudinal+Evaluation+of+IR+Model+Performance)|0|
|[LifeCLEF 2025 Teaser: Challenges on Species Presence Prediction and Identification, and Individual Animal Identification](https://doi.org/10.1007/978-3-031-88720-8_57)|Alexis Joly, Lukás Picek, Stefan Kahl, Hervé Goëau, Lukás Adam, Christophe Botella, Maximilien Servajean, Diego Marcos, César Leblanc, Théo Larcher, Jirí Matas, Klára Janousková, Vojtech Cermák, Kostas Papafitsoros, Robert Planqué, WillemPier Vellinga, Holger Klinck, Tom Denton, Pierre Bonnet, Henning Müller||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LifeCLEF+2025+Teaser:+Challenges+on+Species+Presence+Prediction+and+Identification,+and+Individual+Animal+Identification)|0|
|[CLEF 2025 JOKER Lab: Humour in the Machine](https://doi.org/10.1007/978-3-031-88720-8_59)|Liana Ermakova, AnneGwenn Bosser, Tristan Miller, Ricardo Campos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLEF+2025+JOKER+Lab:+Humour+in+the+Machine)|0|
|[CLEF 2025 SimpleText Track - Simplify Scientific Text (and Nothing More)](https://doi.org/10.1007/978-3-031-88720-8_63)|Liana Ermakova, Hosein Azarbonyad, Jan Bakker, Benjamin Vendeville, Jaap Kamps||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLEF+2025+SimpleText+Track+-+Simplify+Scientific+Text+(and+Nothing+More))|0|
|[Overview of PAN 2025: Generative AI Detection, Multilingual Text Detoxification, Multi-author Writing Style Analysis, and Generative Plagiarism Detection - Extended Abstract](https://doi.org/10.1007/978-3-031-88720-8_64)|Janek Bevendorff, Daryna Dementieva, Maik Fröbe, Bela Gipp, André GreinerPetter, Jussi Karlgren, Maximilian Mayerl, Preslav Nakov, Alexander Panchenko, Martin Potthast, Artem Shelmanov, Efstathios Stamatatos, Benno Stein, Yuxia Wang, Matti Wiegmann, Eva Zangerle||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+PAN+2025:+Generative+AI+Detection,+Multilingual+Text+Detoxification,+Multi-author+Writing+Style+Analysis,+and+Generative+Plagiarism+Detection+-+Extended+Abstract)|0|
|[EXIST 2025: Learning with Disagreement for Sexism Identification and Characterization in Tweets, Memes, and TikTok Videos](https://doi.org/10.1007/978-3-031-88720-8_65)|Laura Plaza, Jorge CarrillodeAlbornoz, Iván Árcos, Paolo Rosso, Damiano Spina, Enrique Amigó, Julio Gonzalo, Roser Morante||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXIST+2025:+Learning+with+Disagreement+for+Sexism+Identification+and+Characterization+in+Tweets,+Memes,+and+TikTok+Videos)|0|
|[QuantumCLEF 2025 - The Second Edition of the Quantum Computing Lab at CLEF](https://doi.org/10.1007/978-3-031-88720-8_66)|Andrea Pasin, Maurizio Ferrari Dacrema, Paolo Cremonesi, Washington Cunha, Marcos André Gonçalves, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QuantumCLEF+2025+-+The+Second+Edition+of+the+Quantum+Computing+Lab+at+CLEF)|0|
|[Overview of Touché 2025: Argumentation Systems - Extended Abstract](https://doi.org/10.1007/978-3-031-88720-8_67)|Johannes Kiesel, Çagri Çöltekin, Marcel Gohsen, Sebastian Heineking, Maximilian Heinrich, Maik Fröbe, Tim Hagen, Mohammad Aliannejadi, Tomaz Erjavec, Matthias Hagen, Matyás Kopp, Nikola Ljubesic, Katja Meden, Nailia Mirzakhmedova, Vaidas Morkevicius, Harrisen Scells, Ines Zelch, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+Touché+2025:+Argumentation+Systems+-+Extended+Abstract)|0|
|[TalentCLEF at CLEF2025: Skill and Job Title Intelligence for Human Capital Management](https://doi.org/10.1007/978-3-031-88720-8_69)|Luis Gascó, Hermenegildo Fabregat, Laura GarcíaSardiña, Daniel Deniz, Álvaro Rodrigo, Paula Estrella, Rabih Zbib||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TalentCLEF+at+CLEF2025:+Skill+and+Job+Title+Intelligence+for+Human+Capital+Management)|0|
|[Patent Figure Classification Using Large Vision-Language Models](https://doi.org/10.1007/978-3-031-88711-6_2)|Sushil Awale, Eric MüllerBudack, Ralph Ewerth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Patent+Figure+Classification+Using+Large+Vision-Language+Models)|0|
|[Semi-Supervised Image-Based Narrative Extraction: A Case Study with Historical Photographic Records](https://doi.org/10.1007/978-3-031-88711-6_16)|Fausto German, Brian Keith, Mauricio Matus, Diego Urrutia, Claudio Meneses||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Image-Based+Narrative+Extraction:+A+Case+Study+with+Historical+Photographic+Records)|0|
|[Visual Latent Captioning - Towards Verbalizing Vision Transformer Encoders](https://doi.org/10.1007/978-3-031-88711-6_25)|Sogol Haghighat, Tim Daniel Metzler, Santosh Thoduka, Sebastian Houben||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Visual+Latent+Captioning+-+Towards+Verbalizing+Vision+Transformer+Encoders)|0|
