# KDD2025 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[LLMLight: Large Language Models as Traffic Signal Control Agents](https://doi.org/10.1145/3690624.3709379)|Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLMLight:+Large+Language+Models+as+Traffic+Signal+Control+Agents)|2|
|[Exploring Preference-Guided Diffusion Model for Cross-Domain Recommendation](https://doi.org/10.1145/3690624.3709220)|Xiaodong Li, Hengzhu Tang, Jiawei Sheng, Xinghua Zhang, Li Gao, Suqi Cheng, Dawei Yin, Tingwen Liu||Cross-domain recommendation (CDR) has been proven as a promising way to alleviate the cold-start issue, in which the most critical problem is how to draw an informative user representation in the target domain via the transfer of user preference existing in the source domain. Prior efforts mostly follow the embedding-and-mapping paradigm, which first integrate the preference into user representation in the source domain, and then perform a mapping function on this representation to the target domain. However, they focus on mapping features across domains, neglecting to explicitly model the preference integration process, which may lead to learning coarse user representation. Diffusion models (DMs), which contribute to more accurate user/item representations due to their explicit information injection capability, have achieved promising performance in recommendation systems. Nevertheless, these DMs-based methods cannot directly account for valuable user preference in other domains, leading to challenges in adapting to the transfer of preference for cold-start users. Consequently, the feasibility of DMs for CDR remains underexplored. To this end, we explore to utilize the explicit information injection capability of DMs for user preference integration and propose a Preference-Guided Diffusion Model for CDR to cold-start users, termed as DMCDR. Specifically, we leverage a preference encoder to establish the preference guidance signal with the user's interaction history in the source domain. Then, we explicitly inject the preference guidance signal into the user representation step by step to guide the reverse process, and ultimately generate the personalized user representation in the target domain, thus achieving the transfer of user preference across domains. Furthermore, we comprehensively explore the impact of six DMs-based variants on CDR.|跨领域推荐（CDR）已被证明是缓解冷启动问题的有效途径，其核心挑战在于如何通过源领域的用户偏好迁移，在目标领域构建信息丰富的用户表征。现有研究主要遵循"嵌入-映射"范式：先在源领域将偏好整合为用户表征，再通过映射函数将其转换至目标领域。然而，这些方法侧重于跨领域特征映射，未能显式建模偏好整合过程，可能导致学得的用户表征较为粗糙。扩散模型（DMs）凭借其显式信息注入能力，能生成更精确的用户/物品表征，已在推荐系统中展现出优越性能。但现有基于DMs的方法无法直接利用其他领域的用户偏好信息，难以适应冷启动用户的偏好迁移需求，导致DMs在CDR中的应用潜力尚未得到充分探索。为此，我们创新性地利用DMs的显式信息注入能力进行用户偏好整合，提出面向冷启动用户的偏好引导扩散模型DMCDR。具体而言：1）通过偏好编码器，基于用户在源领域的交互历史生成偏好引导信号；2）在反向过程中逐步注入该信号，引导生成目标领域的个性化用户表征，实现跨领域偏好迁移。此外，我们系统探究了六种DMs变体对CDR任务的影响机制。

（注：根据学术摘要的文体特征，译文在保持专业术语准确性的前提下进行了以下处理：
1. 将长复合句拆分为符合中文表达习惯的短句结构
2. 关键术语采用"领域内推荐译法+括号标注原词"的双重确认方式（如CDR/DMs）
3. 被动语态转换为主动表述（如"have been proven"译为"已被证明"）
4. 技术流程描述采用"序数词+动词"的递进式结构（如"1）通过...2）在..."）
5. 保留英文缩写首次出现时的全称标注，符合中文论文规范）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Preference-Guided+Diffusion+Model+for+Cross-Domain+Recommendation)|1|
|[Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective](https://doi.org/10.1145/3690624.3709177)|Yuchen Fang, Yuxuan Liang, Bo Hui, Zezhi Shao, Liwei Deng, Xu Liu, Xinke Jiang, Kai Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Large-Scale+Traffic+Forecasting+with+Transformers:+A+Spatial+Data+Management+Perspective)|1|
|[Diffusion-Inspired Cold Start with Sufficient Prior in Computerized Adaptive Testing](https://doi.org/10.1145/3690624.3709317)|Haiping Ma, Aoqing Xia, Changqian Wang, Hai Wang, Xingyi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion-Inspired+Cold+Start+with+Sufficient+Prior+in+Computerized+Adaptive+Testing)|1|
|[Lorentzian Residual Neural Networks](https://doi.org/10.1145/3690624.3709292)|Neil He, Menglin Yang, Rex Ying||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lorentzian+Residual+Neural+Networks)|1|
|[Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models](https://doi.org/10.1145/3690624.3709179)|Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+and+Defending+against+Indirect+Prompt+Injection+Attacks+on+Large+Language+Models)|1|
|[Non-Homophilic Graph Pre-Training and Prompt Learning](https://doi.org/10.1145/3690624.3709219)|Xingtong Yu, Jie Zhang, Yuan Fang, Renhe Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Non-Homophilic+Graph+Pre-Training+and+Prompt+Learning)|1|
|[Understanding and Mitigating Hyperbolic Dimensional Collapse in Graph Contrastive Learning](https://doi.org/10.1145/3690624.3709249)|Yifei Zhang, Hao Zhu, Menglin Yang, Jiahong Liu, Rex Ying, Irwin King, Piotr Koniusz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+and+Mitigating+Hyperbolic+Dimensional+Collapse+in+Graph+Contrastive+Learning)|1|
|[Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?](https://doi.org/10.1145/3690624.3709256)|Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Large+Language+Models+Improve+the+Adversarial+Robustness+of+Graph+Neural+Networks?)|1|
|[SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking](https://doi.org/10.1145/3690624.3709412)|Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, Bowen Huang, Yuanyao Li, Huihui Yuan, Lei Hou, Juanzi Li, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SoAy:+A+Solution-based+LLM+API-using+Methodology+for+Academic+Information+Seeking)|1|
|[Scalable Area Difficulty Assessment with Knowledge-enhanced AI for Nationwide Logistics Systems](https://doi.org/10.1145/3690624.3709407)|Zejun Xie, Wenjun Lyu, Yiwei Song, Haotian Wang, Guang Yang, Yunhuai Liu, Tian He, Desheng Zhang, Guang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Area+Difficulty+Assessment+with+Knowledge-enhanced+AI+for+Nationwide+Logistics+Systems)|1|
|[RankElectra: Semi-supervised Pre-training of Learning-to-Rank Electra for Web-scale Search](https://doi.org/10.1145/3690624.3709395)|Yuchen Li, Haoyi Xiong, Yongqi Zhang, Jiang Bian, Tianhao Peng, Xuhong Li, Shuaiqiang Wang, Linghe Kong, Dawei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RankElectra:+Semi-supervised+Pre-training+of+Learning-to-Rank+Electra+for+Web-scale+Search)|0|
|[Towards Web-scale Recommendations with LLMs: From Quality-aware Ranking to Candidate Generation](https://doi.org/10.1145/3690624.3709413)|Jaidev Shah, Iman Barjasteh, Amey Barapatre, Rana Forsati, Gang Luo, Fan Wu, Yuan Fang, Xue Deng, Blake Shepard, Ronak Shah, Linjun Yang, Hongzhi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Web-scale+Recommendations+with+LLMs:+From+Quality-aware+Ranking+to+Candidate+Generation)|0|
|[Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction](https://doi.org/10.1145/3690624.3709438)|Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, Enhong Chen||Click-through Rate (CTR) prediction is crucial for online personalization platforms. Recent advancements have shown that modeling rich user behaviors can significantly improve the performance of CTR prediction. Current long-term user behavior modeling algorithms predominantly follow two cascading stages. The first stage retrieves subsequence related to the target item from the long-term behavior sequence, while the second stage models the relationship between the subsequence and the target item. Despite significant progress, these methods have two critical flaws. First, the retrieval query typically includes only target item information, limiting the ability to capture the user's diverse interests. Second, relational information, such as sequential and interactive information within the subsequence, is frequently overlooked. Therefore, it requires to be further mined to more accurately model user interests. To this end, we propose Multi-granularity Interest Retrieval and Refinement Network (MIRRN). Specifically, we first construct queries based on behaviors observed at different time scales to obtain subsequences, each capturing users' interest at various granularities. We then introduce an noval multi-head Fourier transformer to efficiently learn sequential and interactive information within the subsequences, leading to more accurate modeling of user interests. Finally, we employ multi-head target attention to adaptively assess the impact of these multi-granularity interests on the target item. Extensive experiments have demonstrated that MIRRN significantly outperforms state-of-the-art baselines. Furthermore, an A/B test shows that MIRRN increases the average number of listening songs by 1.32 0.55 available at https://github.com/psycho-demon/MIRRN.|点击率（CTR）预测对于在线个性化平台至关重要。最新研究表明，对丰富用户行为进行建模能显著提升CTR预测性能。当前长期用户行为建模算法主要遵循两个级联阶段：第一阶段从长期行为序列中检索与目标项目相关的子序列，第二阶段建模该子序列与目标项目的关系。尽管取得重大进展，这些方法仍存在两个关键缺陷：首先，检索查询通常仅包含目标项目信息，限制了捕捉用户多元化兴趣的能力；其次，子序列内部的时序关系和交互信息等关键关联特征常被忽视，需要进一步挖掘以更精准建模用户兴趣。

为此，我们提出多粒度兴趣检索与精炼网络（MIRRN）。具体而言：首先基于不同时间尺度的观测行为构建查询，获取表征用户多粒度兴趣的子序列；随后采用新型多头傅里叶变换器高效学习子序列内的时序与交互信息，实现更精确的用户兴趣建模；最后通过多头目标注意力机制自适应评估这些多粒度兴趣对目标项目的影响。大量实验证明MIRRN显著优于现有最优基线模型，在线A/B测试表明该模型使用户平均收听歌曲数提升1.32%。代码已开源在https://github.com/psycho-demon/MIRRN。

（注：根据学术论文翻译规范，对原文最后半句"increases the average number of listening songs by 1.32 0.55"中疑似存在的数据格式问题进行了合理处理，补充了百分号单位以符合中文表达习惯。若实际应为其他数值形式，请以原始数据为准进行调整。）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-granularity+Interest+Retrieval+and+Refinement+Network+for+Long-Term+User+Behavior+Modeling+in+CTR+Prediction)|0|
|[MOPI-HFRS: A Multi-objective Personalized Health-aware Food Recommendation System with LLM-enhanced Interpretation](https://doi.org/10.1145/3690624.3709382)|Zheyuan Zhang, Zehong Wang, Tianyi Ma, Varun Sameer Taneja, Sofia Nelson, Nhi Ha Lan Le, Keerthiram Murugesan, Mingxuan Ju, Nitesh V. Chawla, Chuxu Zhang, Yanfang Ye||The prevalence of unhealthy eating habits has become an increasingly concerning issue in the United States. However, major food recommendation platforms (e.g., Yelp) continue to prioritize users' dietary preferences over the healthiness of their choices. Although efforts have been made to develop health-aware food recommendation systems, the personalization of such systems based on users' specific health conditions remains under-explored. In addition, few research focus on the interpretability of these systems, which hinders users from assessing the reliability of recommendations and impedes the practical deployment of these systems. In response to this gap, we first establish two large-scale personalized health-aware food recommendation benchmarks at the first attempt. We then develop a novel framework, Multi-Objective Personalized Interpretable Health-aware Food Recommendation System (MOPI-HFRS), which provides food recommendations by jointly optimizing the three objectives: user preference, personalized healthiness and nutritional diversity, along with an large language model (LLM)-enhanced reasoning module to promote healthy dietary knowledge through the interpretation of recommended results. Specifically, this holistic graph learning framework first utilizes two structure learning and a structure pooling modules to leverage both descriptive features and health data. Then it employs Pareto optimization to achieve designed multi-facet objectives. Finally, to further promote the healthy dietary knowledge and awareness, we exploit an LLM by utilizing knowledge-infusion, prompting the LLMs with knowledge obtained from the recommendation model for interpretation.|在美国，不健康饮食习惯的盛行已成为日益严峻的社会问题。然而主流餐饮推荐平台（如Yelp）仍将用户饮食偏好置于健康考量之上。尽管学界已开始研发健康感知的饮食推荐系统，但针对用户特定健康状况的个性化推荐机制仍待探索。此外，现有研究鲜少关注系统可解释性，这既阻碍用户评估推荐可靠性，也制约了系统的实际应用。

为填补这一空白，本研究首次构建了两个大规模个性化健康饮食推荐基准数据集，并提出创新框架——多目标个性化可解释健康饮食推荐系统（MOPI-HFRS）。该系统通过联合优化用户偏好、个性化健康指数与营养多样性三大目标进行推荐，并配备大语言模型（LLM）增强的推理模块，通过解释推荐结果传播健康饮食知识。具体而言，该图学习框架首先通过双重结构学习模块和结构池化模块整合描述性特征与健康数据；继而采用帕累托优化实现多维度目标平衡；最后通过知识注入技术，将推荐模型获取的知识输入LLM生成解释，从而强化健康饮食知识的传播。

（注：根据学术翻译规范，对技术术语保持统一："structure pooling modules"译为"结构池化模块"符合计算机领域术语；"Pareto optimization"保留专业称谓"帕累托优化"；"knowledge-infusion"译为"知识注入"是人工智能领域标准译法。长难句按中文习惯拆分为多个短句，如将原文最后复合句分解为三个递进短句，确保专业性与可读性平衡。）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MOPI-HFRS:+A+Multi-objective+Personalized+Health-aware+Food+Recommendation+System+with+LLM-enhanced+Interpretation)|0|
|[CATER: A Cluster-Based Alternative-Term Recommendation Framework for Large-Scale Web Search at NAVER](https://doi.org/10.1145/3690624.3709426)|Jiwon Son, Jaeyoon Kim, Taekin Kim, YeonChang Lee, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CATER:+A+Cluster-Based+Alternative-Term+Recommendation+Framework+for+Large-Scale+Web+Search+at+NAVER)|0|
|[Learning Attribute as Explicit Relation for Sequential Recommendation](https://doi.org/10.1145/3690624.3709267)|Gang Liu, Fan Yang, Yang Jiao, Alireza Bagheri Garakani, Tian Tong, Yan Gao, Meng Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Attribute+as+Explicit+Relation+for+Sequential+Recommendation)|0|
|[From Missteps to Mastery: Enhancing Low-Resource Dense Retrieval through Adaptive Query Generation](https://doi.org/10.1145/3690624.3709225)|Zhenyu Tong, Chuan Qin, Chuyu Fang, Kaichun Yao, Xi Chen, Jingshuai Zhang, Chen Zhu, Hengshu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Missteps+to+Mastery:+Enhancing+Low-Resource+Dense+Retrieval+through+Adaptive+Query+Generation)|0|
|[Embedding Prior Task-specific Knowledge into Language Models for Context-aware Document Ranking](https://doi.org/10.1145/3690624.3709282)|Shuting Wang, Yutao Zhu, Zhicheng Dou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Prior+Task-specific+Knowledge+into+Language+Models+for+Context-aware+Document+Ranking)|0|
|[GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems](https://doi.org/10.1145/3690624.3709304)|Sheng Zhang, Maolin Wang, Wanyu Wang, Jingtong Gao, Xiangyu Zhao, Yu Yang, Xuetao Wei, Zitao Liu, Tong Xu||Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches struggle to embed high-quality semantic and positional information into latent representations. To tackle these challenges, this paper introduces GLINT-RU, a lightweight and efficient SRS leveraging a single-layer dense selective Gated Recurrent Units (GRU) module to accelerate inference. By incorporating a dense selective gate, GLINT-RU adaptively captures temporal dependencies and fine-grained positional information, generating high-quality latent representations. Additionally, a parallel mixing block infuses fine-grained positional features into user-item interactions, enhancing both recommendation quality and efficiency. Extensive experiments on three datasets demonstrate that GLINT-RU achieves superior prediction accuracy and inference speed, outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These results establish GLINT-RU as a powerful and efficient solution for SRSs.|基于Transformer的模型凭借其有效捕捉用户-项目交互的能力，在序列推荐系统（SRS）中获得了广泛应用。然而，这些模型通常存在计算成本高、推理速度慢的问题。同时，现有高效SRS方法难以将高质量的语义和位置信息嵌入潜在表征。为解决这些挑战，本文提出GLINT-RU——一种轻量高效的SRS框架，通过单层密集选择性门控循环单元（GRU）模块加速推理。该模型通过引入密集选择性门机制，自适应地捕获时序依赖和细粒度位置信息，生成高质量的潜在表征。此外，并行混合块将细粒度位置特征注入用户-项目交互过程，同步提升了推荐质量和效率。在三个数据集上的大量实验表明，GLINT-RU在预测精度和推理速度上均优于基于RNN、Transformer、MLP和SSM的基线模型，证实了其作为高效SRS解决方案的强大性能。

（注：根据学术翻译规范，对原文进行了以下处理：
1. "sequential recommender systems"统一译为专业术语"序列推荐系统"并标注缩写SRS
2. "dense selective gate"译为"密集选择性门机制"以保持技术准确性
3. "parallel mixing block"译为"并行混合块"符合计算机领域命名惯例
4. 长难句进行了符合中文表达习惯的拆分重组
5. 专业模型名称（GRU/RNN/SSM等）保留英文缩写形式
6. 通过增译"框架"等词使中文表述更完整
7. 保持被动语态与主动语态的合理转换）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GLINT-RU:+Gated+Lightweight+Intelligent+Recurrent+Units+for+Sequential+Recommender+Systems)|0|
|[Scenario Shared Instance Modeling for Click-through Rate Prediction](https://doi.org/10.1145/3690624.3709390)|Dugang Liu, Chaohua Yang, Yuwen Fu, Xing Tang, Gongfu Li, Fuyuan Lyu, Xiuqiang He, Zhong Ming||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario+Shared+Instance+Modeling+for+Click-through+Rate+Prediction)|0|
|[Multi-Branch Collaborative Learning Network for Video Quality Assessment in Industrial Video Search](https://doi.org/10.1145/3690624.3709408)|Hengzhu Tang, Zefeng Zhang, Zhiping Li, Zhenyu Zhang, Xing Wu, Li Gao, Suqi Cheng, Dawei Yin||Video Quality Assessment (VQA) is vital for large-scale video retrieval systems, aimed at identifying quality issues to prioritize high-quality videos. In industrial systems, low-quality video characteristics fall into four categories: visual-related issues like mosaics and black boxes, textual issues from video titles and OCR content, and semantic issues like frame incoherence and frame-text mismatch from AI-generated videos. Despite their prevalence in industrial settings, these low-quality videos have been largely overlooked in academic research, posing a challenge for accurate identification. To address this, we introduce the Multi-Branch Collaborative Network (MBCN) tailored for industrial video retrieval systems. MBCN features four branches, each designed to tackle one of the aforementioned quality issues. After each branch independently scores videos, we aggregate these scores using a weighted approach and a squeeze-and-excitation mechanism to dynamically address quality issues across different scenarios. We implement point-wise and pair-wise optimization objectives to ensure score stability and reasonableness. Extensive offline and online experiments on a world-level video search engine demonstrate MBCN's effectiveness in identifying video quality issues, significantly enhancing the retrieval system's ranking performance. Detailed experimental analyses confirm the positive contribution of all four evaluation branches. Furthermore, MBCN significantly improves recognition accuracy for low-quality AI-generated videos compared to the baseline.|视频质量评估（VQA）对于大规模视频检索系统至关重要，其核心目标是识别质量问题以优先展示优质视频。在工业级系统中，低质视频特征可归纳为四大类：视觉相关问题（如马赛克、黑边）、源自视频标题与OCR内容的文本问题，以及AI生成视频特有的语义问题（如帧间不连贯、画面文本失配）。尽管此类低质视频在工业场景中普遍存在，但学术界对其关注严重不足，导致精准识别成为技术难点。为此，我们提出专为工业视频检索系统设计的多分支协同网络（MBCN）。该框架创新性地构建四个并行分支，分别针对上述四类质量问题进行处理。各分支独立完成视频评分后，通过加权聚合与通道注意力机制动态调整不同场景下的质量问题权重。我们采用点对优化与配对优化的双重训练目标，确保评分稳定性与合理性。在全球头部视频搜索引擎的离线和在线实验中，MBCN在质量问题识别方面表现卓越，显著提升了检索系统的排序性能。详尽的实验分析证实四个评估分支均作出积极贡献。特别值得注意的是，相较于基线模型，MBCN对低质AI生成视频的识别准确率实现显著提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Branch+Collaborative+Learning+Network+for+Video+Quality+Assessment+in+Industrial+Video+Search)|0|
|[Generative Retrieval for Book Search](https://doi.org/10.1145/3690624.3709435)|Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Shihao Liu, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng||In book search, relevant book information should be returned in response to a query. Books contain complex, multi-faceted information such as metadata, outlines, and main text, where the outline provides hierarchical information between chapters and sections. Generative retrieval (GR) is a new retrieval paradigm that consolidates corpus information into a single model to generate identifiers of documents that are relevant to a given query. How can GR be applied to book search? Directly applying GR to book search is a challenge due to the unique characteristics of book search: The model needs to retain the complex, multi-faceted information of the book, which increases the demand for labeled data. Splitting book information and treating it as a collection of separate segments for learning might result in a loss of hierarchical information. We propose an effective Generative retrieval framework for Book Search (GBS) that features two main components: data augmentation and outline-oriented book encoding. For data augmentation, GBS constructs multiple query-book pairs for training; it constructs multiple book identifiers based on the outline, various forms of book contents, and simulates real book retrieval scenarios with varied pseudo-queries. This includes coverage-promoting book identifier augmentation, allowing the model to learn to index effectively, and diversity-enhanced query augmentation, allowing the model to learn to retrieve effectively. Outline-oriented book encoding improves length extrapolation through bi-level positional encoding and retentive attention mechanisms to maintain context over long sequences. Experiments on a proprietary Baidu dataset demonstrate that GBS outperforms strong baselines, achieving a 9.8% improvement in terms of MRR@20, over the state-of-the-art RIPOR method...|在图书搜索领域，系统需要根据查询返回相关的书籍信息。图书包含复杂多维的信息，如元数据、目录和正文，其中目录提供了章节间的层级结构关系。生成式检索（GR）作为一种新兴检索范式，通过将语料库信息整合至单一模型来生成与查询相关的文档标识符。如何将GR应用于图书搜索？由于图书搜索的特殊性，直接应用GR存在挑战：模型需保留图书复杂的多维信息，这增加了对标注数据的需求；若分割图书信息并将其作为独立片段集合进行学习，可能导致层级信息丢失。为此，我们提出了一种高效的生成式图书搜索框架GBS，其核心包含两大组件：数据增强与目录导向的图书编码。在数据增强方面，GBS通过构建多组查询-图书对进行训练：基于目录结构、多种形式的图书内容构建多样化图书标识符，并模拟真实搜索场景生成伪查询。具体包括促进覆盖率的图书标识符增强（使模型有效学习索引能力）和增强多样性的查询扩充（使模型有效学习检索能力）。目录导向的图书编码通过双层级位置编码和保持式注意力机制提升长序列上下文保持能力，从而改善长度外推性能。在百度自有数据集上的实验表明，GBS显著优于现有基线方法，较前沿的RIPOR方法在MRR@20指标上提升9.8%...|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Retrieval+for+Book+Search)|0|
|[Online Item Cold-Start Recommendation with Popularity-Aware Meta-Learning](https://doi.org/10.1145/3690624.3709336)|Yunze Luo, Yuezihan Jiang, Yinjie Jiang, Gaode Chen, Jingchi Wang, Kaigui Bian, Peiyi Li, Qi Zhang||With the rise of e-commerce and short videos, online recommender systems that can capture users' interests and update new items in real-time play an increasingly important role. In both online and offline recommendation systems, the cold-start problem caused by interaction sparsity has been impacting the effectiveness of recommendations for cold-start items. Many cold-start scheme based on fine-tuning or knowledge transferring shows excellent performance on offline recommendation. Yet, these schemes are infeasible for online recommendation on streaming data pipelines due to different training method, computational overhead and time constraints. Inspired by the above questions, we propose a model-agnostic recommendation algorithm called Popularity-Aware Meta-learning (PAM), to address the item cold-start problem under streaming data settings. PAM divides the incoming data into different meta-learning tasks by predefined item popularity thresholds. The model can distinguish and reweight behavior-related and content-related features in each task based on their different roles in different popularity levels, thus adapting to recommendations for cold-start samples. These task-fixing design significantly reduces additional computation and storage costs compared to offline methods. Furthermore, PAM also introduced data augmentation and an additional self-supervised loss specifically designed for low-popularity tasks, leveraging insights from high-popularity samples. This approach effectively mitigates the issue of inadequate supervision due to the scarcity of cold-start samples. Experimental results across multiple public datasets demonstrate the superiority of our approach over other baseline methods in addressing cold-start challenges in online streaming data scenarios.|随着电子商务和短视频的兴起，能够实时捕捉用户兴趣并更新新内容的在线推荐系统正发挥着日益重要的作用。无论是线上还是线下推荐系统，由交互稀疏性导致的冷启动问题始终影响着冷启动商品的推荐效果。现有基于微调或知识迁移的冷启动方案在离线推荐场景中表现优异，但由于训练方式、计算开销和时间限制的差异，这些方案难以适用于流式数据管道上的在线推荐场景。受上述问题启发，我们提出了一种与模型无关的推荐算法——流行度感知元学习（PAM），用于解决流式数据场景下的商品冷启动问题。PAM通过预设的商品流行度阈值将输入数据划分为不同的元学习任务，使模型能够根据不同流行度层级中行为特征与内容特征的不同作用进行区分和重加权，从而适配冷启动样本的推荐需求。这种任务固化设计相比离线方案显著降低了额外计算与存储成本。此外，PAM还针对低流行度任务引入数据增强和额外的自监督损失项，通过借鉴高流行度样本的信息，有效缓解了冷启动样本稀缺导致的监督不足问题。在多个公开数据集上的实验结果表明，我们的方法在解决在线流式数据场景的冷启动挑战方面优于其他基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Item+Cold-Start+Recommendation+with+Popularity-Aware+Meta-Learning)|0|
|[Achieving Nearly-Optimal Regret and Sample Complexity in Dueling Bandits with Applications in Online Recommendations](https://doi.org/10.1145/3690624.3709279)|Lanjihong Ma, YaoXiang Ding, ZhenYu Zhang, ZhiHua Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Achieving+Nearly-Optimal+Regret+and+Sample+Complexity+in+Dueling+Bandits+with+Applications+in+Online+Recommendations)|0|
|[Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems](https://doi.org/10.1145/3690624.3709429)|Yuyan Wang, Cheenar Banerjee, Samer Chucri, Fabio Soldo, Sriraj Badam, Ed H. Chi, Minmin Chen|Stanford University Stanford; Google DeepMind Mountain View; Google, Inc. Mountain View|It has become increasingly clear that recommender systems that overly focus on short-term engagement prevents users from exploring diverse interests, ultimately hurting long-term user experience. To tackle this challenge, numerous diversification algorithms have been proposed. These algorithms typically rely on measures of item similarity, aiming to maximize the dissimilarity across items in the final set of recommendations. However, in this work, we demonstrate the benefits of going beyond item-level similarities by utilizing higher-level user understanding–specifically, user intents that persist across multiple interactions–in diversification. Our approach is motivated by the observation that user behaviors on online platforms are largely driven by their underlying intents. Therefore, recommendations should ensure that diverse user intents are accurately represented. While intent has primarily been studied in the context of search, it is less clear how to incorporate real-time dynamic intent predictions into recommender systems. To address this gap, we develop a probabilistic intent-based whole-page diversification framework for the final stage of a recommender system. Starting with a prior belief of user intents, the proposed framework sequentially selects items for each position based on these beliefs and subsequently updates posterior beliefs about the intents. This approach ensures that different user intents are represented on a page, towards optimizing long-term user experience. We experiment with the intent diversification framework on YouTube, the world's largest video recommendation platform, serving billions of users daily. Live experiments on a diverse set of intents show that the proposed framework increases Daily Active Users (DAU) and overall user enjoyment, validating its effectiveness in facilitating long-term planning.|日益明显的是，过度聚焦短期参与度的推荐系统会阻碍用户探索多元兴趣，最终损害长期用户体验。为应对这一挑战，学界已提出诸多多样化算法。这类算法通常依赖项目相似性度量，旨在最大化推荐列表中项目间的差异性。然而，本研究证明，通过利用更高层次的用户理解——特别是跨多次交互持续存在的用户意图——来实现多样化，能带来超越项目级相似性的优势。我们的方法源于一项关键发现：在线平台上的用户行为主要受其潜在意图驱动。因此，推荐系统应确保准确呈现多样化的用户意图。尽管意图研究主要集中于搜索场景，但如何将实时动态意图预测融入推荐系统仍不明确。为此，我们开发了基于概率化意图的整页多样化框架，应用于推荐系统最终阶段。该框架从用户意图的先验信念出发，基于这些信念为每个展示位序贯选择项目，并同步更新意图的后验信念。这种方法能保证页面呈现不同的用户意图，从而优化长期用户体验。我们在全球最大视频推荐平台YouTube（日均服务数十亿用户）上对该框架进行了实验。针对多样化意图集的线上实验表明，该框架能有效提升日活跃用户数（DAU）和整体用户满意度，验证了其在促进长期规划方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Item+Dissimilarities:+Diversifying+by+Intent+in+Recommender+Systems)|0|
|[TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear Travelling Salesman Model](https://doi.org/10.1145/3690624.3709234)|Weixian Waylon Li, Yftah Ziser, Yifei Xie, Shay B. Cohen, Tiejun Ma||Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods like RankNet and LambdaMART, often fall short by solely focusing on pairwise comparisons, leading to sub-optimal global rankings. Conversely, deep learning based listwise methods, while aiming to optimise entire lists, require complex tuning and yield only marginal improvements over robust pairwise models. To overcome these limitations, we introduce Travelling Salesman Problem Rank (TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the ranking problem as a Travelling Salesman Problem (TSP), a well-known combinatorial optimisation challenge that has been extensively studied for its numerous solution algorithms and applications. This approach enables the modelling of pairwise relationships and leverages combinatorial optimisation to determine the listwise ranking. This approach can be directly integrated as an additional component into embeddings generated by existing backbone models to enhance ranking performance. Our extensive experiments across three backbone models on diverse tasks, including stock ranking, information retrieval, and historical events ordering, demonstrate that TSPRank significantly outperforms both pure pairwise and listwise methods. Our qualitative analysis reveals that TSPRank's main advantage over existing methods is its ability to harness global information better while ranking. TSPRank's robustness and superior performance across different domains highlight its potential as a versatile and effective LETOR solution. The code and preprocessed data are available at https://github.com/waylonli/TSPRank-KDD2025.|传统的学习排序（LETOR）方法（包括RankNet和LambdaMART等成对排序方法）往往因仅关注两两比较而导致全局排序结果欠佳。相比之下，基于深度学习的列表排序方法虽然致力于优化整个排序列表，但需要复杂的参数调优，且相对于鲁棒的成对模型仅能带来有限提升。为突破这些局限，我们提出旅行商问题排序法（TSPRank）——一种融合成对与列表排序的混合方法。该方法将排序问题重新定义为旅行商问题（TSP），这个经过广泛研究的经典组合优化问题拥有丰富的求解算法和应用场景。TSPRank既能建模元素间的成对关系，又能通过组合优化确定列表级排序，其模块化设计可直接嵌入现有主干模型生成的表征向量以提升排序性能。我们在股票排名、信息检索和历史事件排序等多样化任务中，对三种主干模型进行大量实验，结果表明TSPRank显著优于纯成对排序和列表排序方法。定性分析表明，TSPRank的核心优势在于排序时能更有效地利用全局信息。该方法在不同领域的鲁棒性和卓越性能，展现了其作为通用高效LETOR解决方案的潜力。代码与预处理数据详见https://github.com/waylonli/TSPRank-KDD2025。

（注：根据学术论文翻译规范，对部分表述进行了优化：
1. "LETOR"保留专业缩写并首次出现标注全称
2. "TSP"采用"旅行商问题"标准译法，首次出现标注英文全称
3. "backbone models"译为"主干模型"符合计算机领域术语
4. "combinatorial optimisation"统一译为"组合优化"
5. 长难句按中文习惯拆分重构，如将"reframes...applications"处理为因果句式
6. 技术陈述保持客观严谨，如"marginally improvements"译为"有限提升"而非主观表述）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSPRank:+Bridging+Pairwise+and+Listwise+Methods+with+a+Bilinear+Travelling+Salesman+Model)|0|
|[Exploring Feature-based Knowledge Distillation for Recommender System: A Frequency Perspective](https://doi.org/10.1145/3690624.3709248)|Zhangchi Zhu, Wei Zhang||In this paper, we analyze the feature-based knowledge distillation for recommendation from the frequency perspective. By defining knowledge as different frequency components of the features, we theoretically demonstrate that regular feature-based knowledge distillation is equivalent to equally minimizing losses on all knowledge and further analyze how this equal loss weight allocation method leads to important knowledge being overlooked. In light of this, we propose to emphasize important knowledge by redistributing knowledge weights. Furthermore, we propose FreqD, a lightweight knowledge reweighting method, to avoid the computational cost of calculating losses on each knowledge. Extensive experiments demonstrate that FreqD consistently and significantly outperforms state-of-the-art knowledge distillation methods for recommender systems. Our code is available at https://github.com/woriazzc/KDs.|本文从频域角度对基于特征的知识蒸馏推荐方法进行分析。通过将知识定义为特征的不同频率分量，我们从理论上证明传统的基于特征的知识蒸馏等同于对所有知识进行等权重的损失最小化，并深入分析了这种均等损失权重分配方式如何导致重要知识被忽视。基于此，我们提出通过知识权重再分配来强调重要知识。此外，我们设计了一种轻量级知识重加权方法FreqD，以避免计算各知识分量损失带来的计算开销。大量实验表明，FreqD在不同场景下均显著优于当前最先进的推荐系统知识蒸馏方法。代码已开源在https://github.com/woriazzc/KDs。

（说明：本译文严格遵循学术论文摘要的规范表达，具有以下特点：
1. 专业术语准确统一："frequency perspective"译为"频域角度"，"knowledge distillation"译为"知识蒸馏"
2. 技术概念清晰："frequency components"译为"频率分量"，"loss weight allocation"译为"损失权重分配"
3. 被动语态转化：将英文被动结构转换为中文主动表达（如"is equivalent to"译为"等同于"）
4. 长句拆分重组：将原文复合长句分解为符合中文表达习惯的短句
5. 逻辑关系显化：通过"通过"、"基于此"等连接词明确技术路线逻辑
6. 重要概念前置："FreqD"在首次出现时即注明其方法属性
7. 学术用语规范："extensive experiments"译为"大量实验"，"state-of-the-art"译为"最先进的"）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Feature-based+Knowledge+Distillation+for+Recommender+System:+A+Frequency+Perspective)|0|
|[Personalized Language Model Learning on Text Data Without User Identifiers](https://doi.org/10.1145/3690624.3709211)|Yucheng Ding, Yangwenjian Tan, Xiangyu Liu, Chaoyue Niu, Fandong Meng, Jie Zhou, Ning Liu, Fan Wu, Guihai Chen||In many practical natural language applications, user data are highly sensitive, requiring anonymous uploads of text data from mobile devices to the cloud without user identifiers. However, the absence of user identifiers restricts the ability of cloud-based language models to provide personalized services, which are essential for catering to diverse user needs. The trivial method of replacing an explicit user identifier with a static user embedding as model input still compromises data anonymization. In this work, we propose to let each mobile device maintain a user-specific distribution to dynamically generate user embeddings, thereby breaking the one-to-one mapping between an embedding and a specific user. We further theoretically demonstrate that to prevent the cloud from tracking users via uploaded embeddings, the local distributions of different users should either be derived from a linearly dependent space to avoid identifiability or be close to each other to prevent accurate attribution. Evaluation on both public and industrial datasets using different language models reveals a remarkable improvement in accuracy from incorporating anonymous user embeddings, while preserving real-time inference requirement.|在许多实际的自然语言应用场景中，用户数据具有高度敏感性，需要将移动设备上的文本数据匿名化上传至云端且不附带用户标识符。然而，用户标识符的缺失限制了云端语言模型提供个性化服务的能力，而这种能力对于满足多样化用户需求至关重要。传统方法采用静态用户嵌入向量替代显式用户标识符作为模型输入，仍会破坏数据匿名性。本研究提出让每个移动设备维护一个用户专属分布来动态生成用户嵌入，从而打破嵌入向量与特定用户之间的一一映射关系。我们进一步从理论上证明：为防止云端通过上传的嵌入向量追踪用户，不同用户的本地分布应当满足以下条件之一——要么源自线性相关空间以避免可识别性，要么彼此接近以阻止准确归因。在公开数据集和工业数据集上使用不同语言模型的评估表明，引入匿名用户嵌入在保持实时推理要求的同时，能显著提升模型准确率。

（注：根据技术文本翻译规范，对以下术语进行了标准化处理：
1. "user-specific distribution"译为"用户专属分布"而非"用户特定分布"，更符合机器学习领域惯用表述
2. "linearly dependent space"采用数学标准译法"线性相关空间"
3. "real-time inference"统一译为"实时推理"，与行业术语保持一致
4. 将原文被动语态"should be derived"主动化为"应当满足"，符合中文表达习惯）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Language+Model+Learning+on+Text+Data+Without+User+Identifiers)|0|
|[Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification](https://doi.org/10.1145/3690624.3709241)|Haipeng Ding, Zhewei Wei, Yuhang Ye|PhD student, Gaoling School of Artificial Intelligence, Renmin University of China; Researcher, Huawei Poisson Lab, Huawei Technologies Ltd.; Full Professor, Renmin University of China|Graph Neural Networks (GNNs) play a pivotal role in graph-based tasks for their proficiency in representation learning. Among the various GNN methods, spectral GNNs employing polynomial filters have shown promising performance on both homophilous and heterophilous graph structures. The scalability of spectral GNNs is limited because forward propagation requires multiple graph propagation executions, corresponding to the degree of the polynomial. On the other hand, scalable spectral GNNs detach the graph propagation and linear layers, allowing the message-passing phase to be pre-computed and ensuring effective scalability on large graphs. However, this pre-computation can disrupt end-to-end training, possibly impacting performance, and becomes impractical when dealing with high-dimensional input features. In response to these challenges, we propose a novel graph spectral sparsification method to approximate the propagation pattern of spectral GNNs. We prove that our proposed methods generate Laplacian sparsifiers for the random-walk matrix polynomial, incorporating both static and learnable polynomial coefficients. By considering multi-hop neighbor interactions into one-hop operations, our approach facilitates the use of scalable techniques. To empirically validate the effectiveness of our methods, we conduct an extensive experimental analysis on datasets spanning various graph scales and properties. The results show that our method yields superior results in comparison with the corresponding approximated base models.|图神经网络（GNN）凭借其卓越的表征学习能力，在图结构任务中发挥着关键作用。在各类GNN方法中，采用多项式滤波器的谱图神经网络在同配性与异配性图结构上均表现出优异性能。然而，由于前向传播需要执行与多项式次数相对应的多次图传播操作，传统谱图神经网络的可扩展性受到限制。相比之下，可扩展谱图神经网络将图传播层与线性层解耦，使消息传递阶段能够预先计算，从而确保其在大规模图上的高效扩展性。但这种预计算方式会中断端到端训练流程，可能影响模型性能，且在处理高维输入特征时变得不可行。针对这些挑战，我们提出了一种新颖的图谱稀疏化方法，用以逼近谱图神经网络的传播模式。我们通过理论证明，所提方法能为随机游走矩阵多项式生成拉普拉斯稀疏器，同时兼容静态与可学习多项式系数。通过将多跳邻居交互整合至单跳操作，本方法实现了可扩展技术的有效应用。为验证方法的有效性，我们在涵盖不同图规模与特性的数据集上进行了全面的实验分析。结果表明，与相应的近似基线模型相比，本方法能获得更优异的性能表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-Scale+Spectral+Graph+Neural+Networks+via+Laplacian+Sparsification)|0|
|[Why Not Together? A Multiple-Round Recommender System for Queries and Items](https://doi.org/10.1145/3690624.3709261)|Jiarui Jin, Xianyu Chen, Weinan Zhang, Yong Yu, Jun Wang||A fundamental technique of recommender systems involves modeling user preferences, where queries and items are widely used as symbolic representations of user interests. Queries delineate user needs at an abstract level, providing a high-level description, whereas items operate on a more specific and concrete level, representing the granular facets of user preference. While practical, both query and item recommendations encounter the challenge of sparse user feedback. To this end, we propose a novel approach named Multiple-round Auto Guess-and-Update System (MAGUS) that capitalizes on the synergies between both types, allowing us to leverage both query and item information to form user interests. This integrated system introduces a recursive framework that could be applied to any recommendation method to exploit queries and items in historical interactions and to provide recommendations for both queries and items in each interaction round. Empirical results from testing 12 different recommendation methods demonstrate that integrating queries into item recommendations via MAGUS significantly enhances the efficiency, with which users can identify their preferred items during multiple-round interactions.|推荐系统的一项基础技术在于用户偏好建模，其中查询(query)和物品(item)被广泛用作用户兴趣的符号化表征。查询在抽象层面刻画用户需求，提供高层级描述；而物品则作用于更具体和细致的层面，表征用户偏好的粒度化特征。尽管实用性强，但查询推荐与物品推荐都面临着用户反馈稀疏的挑战。为此，我们提出名为多轮自动猜测-更新系统(MAGUS)的创新方法，通过协同利用两类表征的优势，整合查询与物品信息来构建用户兴趣画像。该集成系统引入了一个递归框架，可适配于任何推荐方法：既能挖掘历史交互中的查询与物品信息，又能在每轮交互中同时提供查询与物品推荐。对12种不同推荐方法的实证测试表明，通过MAGUS将查询信息融入物品推荐后，能显著提升用户在多轮交互中定位心仪物品的效率。

（注：根据学术论文翻译规范，关键技术术语首次出现时保留英文原词并附中文释义，后续重复出现时直接使用中文术语。专业术语如"recommender systems"译为行业通用译法"推荐系统"，"granular facets"译为"粒度化特征"以准确传达技术内涵。通过拆分英文长句为符合中文表达习惯的短句结构，如将"capitalizes on the synergies..."处理为"通过协同利用...的优势"，确保技术表述的清晰性与可读性。）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+Not+Together?+A+Multiple-Round+Recommender+System+for+Queries+and+Items)|0|
|[Attribute-Enhanced Similarity Ranking for Sparse Link Prediction](https://doi.org/10.1145/3690624.3709314)|João Mattos, Zexi Huang, Mert Kosan, Ambuj Singh, Arlei Silva|; Researcher, TikTok Inc.; Assistant Professor, Computer Science, Rice University; PhD student, Computer Science, Rice University; Researcher, VISA|Link prediction is a fundamental problem in graph data. In its most realistic setting, the problem consists of predicting missing or future links between random pairs of nodes from the set of disconnected pairs. Graph Neural Networks (GNNs) have become the predominant framework for link prediction. GNN-based methods treat link prediction as a binary classification problem and handle the extreme class imbalance---real graphs are very sparse---by sampling (uniformly at random) a balanced number of disconnected pairs not only for training but also for evaluation. However, we show that the reported performance of GNNs for link prediction in the balanced setting does not translate to the more realistic imbalanced setting and that simpler topology-based approaches are often better at handling sparsity. These findings motivate Gelato, a similarity-based link-prediction method that applies (1) graph learning based on node attributes to enhance a topological heuristic, (2) a ranking loss for addressing class imbalance, and (3) a negative sampling scheme that efficiently selects hard training pairs via graph partitioning. Experiments show that Gelato is more accurate and faster than GNN-based alternatives.|链接预测是图数据中的一个基础性问题。在其最现实的设定中，该问题需要从断开连接的节点对集合中预测随机节点对之间缺失或未来可能出现的链接。图神经网络（GNNs）已成为链接预测的主流框架。基于GNN的方法将链接预测视为二元分类问题，并通过（均匀随机）采样平衡数量的断开连接节点对来处理极端类别不平衡问题（实际图数据往往非常稀疏），这种做法不仅用于训练，也用于评估。然而，我们发现GNN在平衡设定下报告的链接预测性能并不能推广到更现实的非平衡设定，而更简单的基于拓扑结构的方法往往能更好地处理稀疏性问题。这些发现促使我们提出了Gelato——一种基于相似性的链接预测方法，该方法具有三个关键特征：（1）基于节点属性的图学习来增强拓扑启发式算法；（2）采用排序损失函数解决类别不平衡问题；（3）通过图分区高效筛选困难训练样本对的负采样策略。实验表明，Gelato相比基于GNN的替代方案具有更高的预测准确性和更快的运算速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attribute-Enhanced+Similarity+Ranking+for+Sparse+Link+Prediction)|0|
|[Combinatorial Optimization Perspective based Framework for Multi-behavior Recommendation](https://doi.org/10.1145/3690624.3709278)|Chenhao Zhai, Chang Meng, Yu Yang, Kexin Zhang, Xuhao Zhao, Xiu Li||In real-world recommendation scenarios, users engage with items through various types of behaviors. Leveraging diversified user behavior information for learning can enhance the recommendation of target behaviors (e.g., buy), as demonstrated by recent multi-behavior methods. The mainstream multi-behavior recommendation framework consists of two steps: fusion and prediction. Recent approaches utilize graph neural networks for multi-behavior fusion and employ multi-task learning paradigms for joint optimization in the prediction step, achieving significant success. However, these methods have limited perspectives on multi-behavior fusion, which leads to inaccurate capture of user behavior patterns in the fusion step. Moreover, when using multi-task learning for prediction, the relationship between the target task and auxiliary tasks is not sufficiently coordinated, resulting in negative information transfer. To address these problems, we propose a novel multi-behavior recommendation framework based on the combinatorial optimization perspective, named COPF. Specifically, we treat multi-behavior fusion as a combinatorial optimization problem, imposing different constraints at various stages of each behavior to restrict the solution space, thus significantly enhancing fusion efficiency (COGCN). In the prediction step, we improve both forward and backward propagation during the generation and aggregation of multiple experts to mitigate negative transfer caused by differences in both feature and label distributions (DFME). Comprehensive experiments on three real-world datasets indicate the superiority of COPF. Further analyses also validate the effectiveness of the COGCN and DFME modules. Our code is available at https://github.com/1918190/COPF.|在实际推荐场景中，用户会通过多种行为类型与物品进行交互。如近期多行为推荐方法所示，利用多样化的用户行为信息进行学习可以有效提升目标行为（如购买）的推荐效果。主流的多行为推荐框架包含融合与预测两个步骤：现有方法通常采用图神经网络进行多行为融合，并在预测步骤使用多任务学习范式进行联合优化，取得了显著成效。然而这些方法对多行为融合的视角存在局限，导致融合步骤难以准确捕捉用户行为模式；同时在使用多任务学习进行预测时，未能充分协调目标任务与辅助任务的关系，从而产生负向信息迁移。

为解决这些问题，我们提出基于组合优化视角的新型多行为推荐框架COPF。具体而言：1）将多行为融合建模为组合优化问题，通过在各类行为的不同阶段施加差异化约束来限定解空间，显著提升融合效率（COGCN模块）；2）在预测步骤改进多专家生成与聚合过程中的前向/反向传播机制，缓解由特征分布和标签分布差异共同导致的负迁移现象（DFME模块）。在三个真实数据集上的综合实验表明COPF的优越性，进一步分析也验证了COGCN与DFME模块的有效性。代码已开源：https://github.com/1918190/COPF。

（注：根据学术摘要翻译规范，技术术语如"multi-task learning"译为"多任务学习"、"graph neural networks"译为"图神经网络"等均采用领域标准译法；长句按中文表达习惯拆分为短句；被动语态转换为主动表述；算法名称COGCN/DFME保留英文缩写并在首次出现时标注中文解释）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combinatorial+Optimization+Perspective+based+Framework+for+Multi-behavior+Recommendation)|0|
|[HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation with Hourly Feedback](https://doi.org/10.1145/3690624.3709409)|Xufeng Cai, Ziwei Guan, Lei Yuan, Ali Selman Aydin, Tengyu Xu, Boying Liu, Wenbo Ren, Renkai Xiang, Songyi He, Haichuan Yang, Serena Li, Mingze Gao, Yue Weng, Ji Liu||Modern recommendation systems can be broadly divided into two key stages: the ranking stage, where the system predicts various user engagements (e.g., click-through rate, like rate, follow rate, watch time), and the value model stage, which aggregates these predictive scores through a function (e.g., a linear combination defined by a weight vector) to measure the value of each content by a single numerical score. Both stages play roughly equally important roles in real industrial systems; however, how to optimize the model weights for the second stage still lacks systematic study. This paper focuses on optimizing the second stage through auto-tuning technology. Although general auto-tuning systems and solutions - both from established production practices and open-source solutions - can address this problem, they typically require weeks or even months to identify a feasible solution. Such prolonged tuning processes are unacceptable in production environments for recommendation systems, as suboptimal value models can severely degrade user experience. An effective auto-tuning solution is required to identify a viable model within 2-3 days, rather than the extended timelines typically associated with existing approaches. In this paper, we introduce a practical auto-tuning system named HyperZero that addresses these time constraints while effectively solving the unique challenges inherent in modern recommendation systems. Moreover, this framework has the potential to be expanded to broader tuning tasks within recommendation systems.|现代推荐系统可大致分为两个关键阶段：排名阶段（系统预测用户各类互动行为，如点击率、点赞率、关注率、观看时长等）和价值模型阶段（通过函数聚合这些预测分数，例如使用权重向量定义的线性组合，以单一数值衡量内容价值）。在真实工业系统中，这两个阶段的重要性基本相当；然而如何优化第二阶段的模型权重仍缺乏系统性研究。本文聚焦通过自动调优技术优化第二阶段。虽然现有通用自动调优系统（包括成熟的生产实践和开源解决方案）可解决该问题，但它们通常需要数周甚至数月才能找到可行方案。这种冗长的调优过程对推荐系统生产环境而言不可接受，因为次优的价值模型会严重损害用户体验。我们需要一种能在2-3天内确定可行模型的自动调优方案，而非现有方法所需的漫长时间周期。本文提出名为HyperZero的实用自动调优系统，在满足严格时间约束的同时，有效解决了现代推荐系统特有的技术挑战。该框架还具有扩展至推荐系统其他调优任务的潜力。

（翻译说明：
1. 专业术语处理："user engagements"译为"用户互动行为"符合行业惯例，"click-through rate"等指标保留中文标准译法
2. 技术概念转换：将"function"具体化为"函数"而非笼统的"功能"，"linear combination"明确译为"线性组合"
3. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如价值模型阶段的描述
4. 时间表述优化："2-3 days"译为"2-3天"而非"两到三天"以保持技术文档严谨性
5. 产品命名保留：HyperZero保持原名不翻译，符合技术命名惯例
6. 被动语态转换："are required"等被动结构转为中文主动句式
7. 逻辑衔接强化：通过"然而""虽然""因为"等连接词保持论证逻辑清晰）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HyperZero:+A+Customized+End-to-End+Auto-Tuning+System+for+Recommendation+with+Hourly+Feedback)|0|
|[Multi-Task Combinatorial Bandits for Budget Allocation](https://doi.org/10.1145/3690624.3709434)|Lin Ge, Yang Xu, Jianing Chu, David Cramer, Fuhong Li, Kelly Paulson, Rui Song||Today's top advertisers typically manage hundreds of campaigns simultaneously and consistently launch new ones throughout the year. A crucial challenge for marketing managers is determining the optimal allocation of limited budgets across various ad lines in each campaign to maximize cumulative returns, especially given the huge uncertainty in return outcomes. In this paper, we propose to formulate budget allocation as a multi-task combinatorial bandit problem and introduce a novel online budget allocation system. The proposed system: i) integrates a Bayesian hierarchical model to intelligently utilize the metadata of campaigns and ad lines and budget size, ensuring efficient information sharing; ii) provides the flexibility to incorporate diverse modeling techniques such as Linear Regression, Gaussian Processes, and Neural Networks, catering to diverse environmental complexities; and iii) employs the Thompson sampling (TS) technique to strike a balance between exploration and exploitation. Through offline evaluation and online experiments, our system demonstrates robustness and adaptability, effectively maximizing the overall cumulative returns. A Python implementation of the proposed procedure is available at https://anonymous.4open.science/r/MCMAB.|当今顶级广告主通常需要同时管理数百个营销活动，并全年持续推出新活动。对营销管理者而言，核心挑战在于如何将有限预算最优分配到各活动的不同广告线，以最大化累计收益——尤其是在收益结果存在巨大不确定性的情况下。本文提出将预算分配问题建模为多任务组合老虎机问题，并创新性地设计了一个在线预算分配系统。该系统具有以下特征：1）采用贝叶斯层次模型智能利用活动元数据、广告线特征及预算规模，确保高效的信息共享；2）可灵活集成线性回归、高斯过程和神经网络等多种建模技术，适应不同环境复杂度；3）运用汤普森采样技术实现探索与利用的平衡。通过离线评估与在线实验验证，本系统展现出优异的鲁棒性和适应能力，能有效实现累计收益最大化。相关Python实现代码已开源（https://anonymous.4open.science/r/MCMAB）。

（注：根据学术论文摘要的翻译规范，我们进行了以下处理：
1. 专业术语如"multi-task combinatorial bandit"译为学界通用译法"多任务组合老虎机"
2. 技术名词"Thompson sampling"保留专业译名"汤普森采样"
3. 长难句按中文习惯拆分为短句，如将包含三个特征的并列句拆分为数字标号条目
4. 被动语态转换为主动表述（如"are managed"译为"需要管理"）
5. 保持技术细节的精准性，如"Bayesian hierarchical model"译为"贝叶斯层次模型"而非模糊处理|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Combinatorial+Bandits+for+Budget+Allocation)|0|
|[MerRec:  A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems](https://doi.org/10.1145/3690624.3709394)|Lichi Li, Zainul Abi Din, Zhen Tan, Sam London, Tianlong Chen, Ajay H. Daptardar|; University of North Carolina at Chapel Hill; Arizona State University; Mercari, Inc|In the evolving e-commerce field, recommendation systems crucially shape userexperience and engagement. The rise of Consumer-to-Consumer (C2C)recommendation systems, noted for their flexibility and ease of access forcustomer vendors, marks a significant trend. However, the academic focusremains largely on Business-to-Consumer (B2C) models, leaving a gap filled bythe limited C2C recommendation datasets that lack in item attributes, userdiversity, and scale. The intricacy of C2C recommendation systems is furtheraccentuated by the dual roles users assume as both sellers and buyers,introducing a spectrum of less uniform and varied inputs. Addressing this, weintroduce MerRec, the first large-scale dataset specifically for C2Crecommendations, sourced from the Mercari e-commerce platform, coveringmillions of users and products over 6 months in 2023. MerRec not only includesstandard features such as user_id, item_id, and session_id, but also uniqueelements like timestamped action types, product taxonomy, and textual productattributes, offering a comprehensive dataset for research. This dataset,extensively evaluated across six recommendation tasks, establishes a newbenchmark for the development of advanced recommendation algorithms inreal-world scenarios, bridging the gap between academia and industry andpropelling the study of C2C recommendations.|在不断发展的电子商务领域，推荐系统对用户体验和参与度起着关键作用。以灵活性和低门槛著称的消费者间（C2C）推荐系统的兴起已成为重要趋势。然而学术界研究仍主要集中于企业对消费者（B2C）模式，现有C2C推荐数据集普遍存在商品属性缺失、用户多样性不足和规模有限等问题。用户兼具卖家和买家的双重身份，导致输入数据呈现非标准化和高度异质性，进一步增加了C2C推荐系统的复杂性。为此，我们推出首个面向C2C推荐的大规模数据集MerRec，该数据集源自Mercari电商平台，涵盖2023年6个月内数百万用户和商品。除user_id、item_id和session_id等标准特征外，MerRec还包含带时间戳的行为类型、商品分类体系和文本型商品属性等独特元素，为研究提供全面数据支持。通过对六项推荐任务的广泛评估，该数据集为现实场景中高级推荐算法的开发树立了新基准，弥合了学术界与工业界的鸿沟，有力推动了C2C推荐研究的发展。

（翻译说明：
1. 专业术语处理："Consumer-to-Consumer"译为行业通用术语"消费者间（C2C）"，"taxonomy"译为"分类体系"符合计算机领域表述
2. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句，如用户双重身份的描述部分
3. 被动语态转换："are noted for"转化为主动式"以...著称"
4. 数据特征保留：完整保留"user_id"等技术字段原文形式
5. 概念显化："less uniform and varied inputs"意译为"非标准化和高度异质性"
6. 动态动词运用："propelling"译为"有力推动"增强文本感染力
7. 学术规范：保持"基准测试（benchmark）"等术语的准确性与一致性）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MerRec:++A+Large-scale+Multipurpose+Mercari+Dataset+for+Consumer-to-Consumer+Recommendation+Systems)|0|
|[Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System](https://doi.org/10.1145/3690624.3709387)|Chao Wang, Yue Zheng, Yujing Zhang, Yan Feng, Zhe Wang, Xiaowei Shi, An You, Yu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaker:+Removing+Shortcut+Cues+with+User+Clustering+for+Single-slot+Recommendation+System)|0|
|[Producer-Side Experiments Based on Counterfactual Interleaving Designs for Online Recommender Systems](https://doi.org/10.1145/3690624.3709428)|Yan Wang, Shan Ba|LinkedIn Corporation|Recommender systems have become an integral part of online platforms, providing personalized recommendations for purchases, content consumption, and interpersonal connections. These systems consist of two sides: the producer side comprises product sellers, content creators, or service providers, etc., and the consumer side includes buyers, viewers, or customers, etc. To optimize online recommender systems, A/B tests serve as the golden standard for comparing different ranking models and evaluating their impact on both the consumers and producers. While consumer-side experiments is relatively straightforward to design and commonly employed to assess the impact of ranking changes on the behavior of consumers (buyers, viewers, etc.), designing producer-side experiments for an online recommender/ranking system is notably more intricate because producer items in the treatment and control groups need to be ranked by different models and then merged into a unified ranking to be presented to each consumer. Current design solutions in the literature are ad hoc and lacking rigorous guiding principles. In this paper, we examine limitations of these existing methods and propose the principle of consistency and principle of monotonicity for designing producer-side experiments of online recommender systems. Building upon these principles, we also present a systematic solution based on counterfactual interleaving designs to accurately measure the impacts of ranking changes on the producers (sellers, creators, etc.).|推荐系统已成为在线平台的核心组成部分，为用户的购买决策、内容消费及社交连接提供个性化推荐。这类系统包含两个主体维度：生产者端（涵盖商品销售者、内容创作者或服务提供商等）与消费者端（包括购买者、观看者或顾客等）。为优化在线推荐系统，A/B测试作为黄金标准被广泛应用于比较不同排序模型，并评估其对消费者和生产者双方的影响。尽管消费者端实验设计相对直观（通常用于评估排序变化对消费者行为的影响），但在线推荐/排序系统的生产者端实验设计则显著复杂——因为实验组与对照组的生产者项目需经不同模型排序后，再合并为统一排序呈现给每位消费者。现有文献中的设计方案多为临时性方法，缺乏严谨的指导原则。本文系统分析了当前方法的局限性，提出在线推荐系统生产者端实验设计的一致性准则与单调性准则，并基于反事实交错排序设计构建系统性解决方案，以精准量化排序变化对生产者（销售方、创作者等）的影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Producer-Side+Experiments+Based+on+Counterfactual+Interleaving+Designs+for+Online+Recommender+Systems)|0|
|[Mutual Information-aware Knowledge Distillation for Short Video Recommendation](https://doi.org/10.1145/3690624.3709403)|Han Xu, Taoxing Pan, Zhiqiang Liu, Xiaoxiao Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Information-aware+Knowledge+Distillation+for+Short+Video+Recommendation)|0|
|[Large-scale Human Mobility Data Regeneration for Open Urban Research](https://doi.org/10.1145/3690624.3709380)|Ruixing Zhang, Yunqi Liu, Liangzhe Han, Leilei Sun, Chuanren Liu, Jibin Wang, Weifeng Lv||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-scale+Human+Mobility+Data+Regeneration+for+Open+Urban+Research)|0|
|[Chainlet Orbits: Topological Address Embedding for Blockchain](https://doi.org/10.1145/3690624.3709322)|Poupak Azad, Baris Coskunuzer, Murat Kantarcioglu, Cuneyt Gurcan Akcora||The rise of cryptocurrencies like Bitcoin, which enable transactions with a degree of pseudonymity, has led to a surge in various illicit activities, including ransomware payments and transactions on darknet markets. These illegal activities often utilize Bitcoin as the preferred payment method. However, current tools for detecting illicit behavior either rely on a few heuristics and laborious data collection processes or employ computationally inefficient graph neural network (GNN) models that are challenging to interpret. To overcome the computational and interpretability limitations of existing techniques, we introduce an effective solution called Chainlet Orbits. This approach embeds Bitcoin addresses by leveraging their topological characteristics in transactions. By employing our innovative address embedding, we investigate e-crime in Bitcoin networks by focusing on distinctive substructures that arise from illicit behavior. The results of our node classification experiments demonstrate superior performance compared to state-of-the-art methods, including both topological and GNN-based approaches. Moreover, our approach enables the use of interpretable and explainable machine learning models in as little as 15 minutes for most days on the Bitcoin transaction network.|以比特币为代表的加密货币因其具备一定程度的匿名交易特性，其兴起导致勒索软件支付、暗网市场交易等各类非法活动激增。这些非法行为往往将比特币作为首选支付手段。然而，当前检测非法行为的工具要么依赖少量启发式规则和繁琐的数据收集流程，要么采用计算效率低下且难以解释的图神经网络（GNN）模型。为克服现有技术在计算效率和可解释性方面的局限，我们提出了一种名为"链式轨道"（Chainlet Orbits）的有效解决方案。该方法通过挖掘比特币地址在交易中的拓扑特征来实现地址嵌入。借助我们创新的地址嵌入技术，我们通过追踪非法行为产生的独特子结构来研究比特币网络中的电子犯罪。节点分类实验结果表明，相较于最先进的拓扑方法和基于GNN的方法，我们的方案展现出更优越的性能。更重要的是，该方法使得在比特币交易网络上使用可解释的机器学习模型成为可能，且对多数交易日而言仅需不到15分钟即可完成分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chainlet+Orbits:+Topological+Address+Embedding+for+Blockchain)|0|
|[Advancing Confidence Calibration and Quantification in Medication Recommendation](https://doi.org/10.1145/3690624.3709232)|Qianyu Chen, Xin Li, Yujie Fang, Mingzhong Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Confidence+Calibration+and+Quantification+in+Medication+Recommendation)|0|
|[Scalable Link Recommendation for Influence Maximization](https://doi.org/10.1145/3690624.3709190)|Xiaolong Chen, Jing Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Link+Recommendation+for+Influence+Maximization)|0|
|[Seeing the Unseen in Micro-Video Popularity Prediction: Self-Correlation Retrieval for Missing Modality Generation](https://doi.org/10.1145/3690624.3709308)|Zhangtao Cheng, Jian Lang, Ting Zhong, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Seeing+the+Unseen+in+Micro-Video+Popularity+Prediction:+Self-Correlation+Retrieval+for+Missing+Modality+Generation)|0|
|[Controlling Diversity at Inference: Guiding Diffusion Recommender Models with Targeted Category Preferences](https://doi.org/10.1145/3690624.3709216)|Gwangseok Han, Wonbin Kweon, Minsoo Kim, Hwanjo Yu||Diversity control is an important task to alleviate bias amplification and filter bubble problems. The desired degree of diversity may fluctuate based on users' daily moods or business strategies. However, existing methods for controlling diversity often lack flexibility, as diversity is decided during training and cannot be easily modified during inference. We propose D3Rec (Disentangled Diffusion model for Diversified Recommendation), an end-to-end method that controls the accuracy-diversity trade-off at inference. D3Rec meets our three desiderata by (1) generating recommendations based on category preferences, (2) controlling category preferences during the inference phase, and (3) adapting to arbitrary targeted category preferences. In the forward process, D3Rec removes category preferences lurking in user interactions by adding noises. Then, in the reverse process, D3Rec generates recommendations through denoising steps while reflecting desired category preferences. Extensive experiments on real-world and synthetic datasets validate the effectiveness of D3Rec in controlling diversity at inference.|多样性控制是缓解偏见放大和信息茧房问题的重要任务。理想的多样性程度可能随用户当日情绪或商业策略动态变化。然而现有多样性控制方法往往缺乏灵活性，因其多样性程度在训练阶段就已确定且难以在推理阶段调整。我们提出D3Rec（解耦扩散多样性推荐模型），这是一种能在推理阶段控制准确性与多样性权衡的端到端方法。D3Rec通过以下特性满足三大核心需求：（1）基于类别偏好生成推荐；（2）在推理阶段动态调控类别偏好；（3）适配任意目标类别偏好。在正向过程中，D3Rec通过添加噪声消除用户交互行为中潜在的类别偏好；在逆向过程中，则通过去噪步骤生成推荐结果，同时融入预期的类别偏好。基于真实场景数据集与合成数据集的广泛实验验证了D3Rec在推理阶段控制多样性的有效性。

（注：根据技术文档翻译规范，对关键术语进行以下处理：
1. "filter bubble"译为"信息茧房"（学术界通用译法）
2. "disentangled diffusion model"译为"解耦扩散模型"（保持计算机视觉领域术语一致性）
3. "forward/reverse process"分别译为"正向/逆向过程"（遵循扩散模型领域术语惯例）
4. 保持"inference"统一译为"推理阶段"以区分训练阶段）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controlling+Diversity+at+Inference:+Guiding+Diffusion+Recommender+Models+with+Targeted+Category+Preferences)|0|
|[Multi-level Matching Network for Multimodal Entity Linking](https://doi.org/10.1145/3690624.3709306)|Zhiwei Hu, Víctor GutiérrezBasulto, Ru Li, Jeff Z. Pan||Multimodal entity linking (MEL) aims to link ambiguous mentions within multimodal contexts to corresponding entities in a multimodal knowledge base. Most existing approaches to MEL are based on representation learning or vision-and-language pre-training mechanisms for exploring the complementary effect among multiple modalities. However, these methods suffer from two limitations. On the one hand, they overlook the possibility of considering negative samples from the same modality. On the other hand, they lack mechanisms to capture bidirectional cross-modal interaction. To address these issues, we propose a Multi-level Matching network for Multimodal Entity Linking (M3EL). Specifically, M3EL is composed of three different modules: (i) a Multimodal Feature Extraction module, which extracts modality-specific representations with a multimodal encoder and introduces an intra-modal contrastive learning sub-module to obtain better discriminative embeddings based on uni-modal differences; (ii) an Intra-modal Matching Network module, which contains two levels of matching granularity: Coarse-grained Global-to-Global and Fine-grained Global-to-Local, to achieve local and global level intra-modal interaction; (iii) a Cross-modal Matching Network module, which applies bidirectional strategies, Textual-to-Visual and Visual-to-Textual matching, to implement bidirectional cross-modal interaction. Extensive experiments conducted on WikiMEL, RichpediaMEL, and WikiDiverse datasets demonstrate the outstanding performance of M3EL when compared to the state-of-the-art baselines.|多模态实体链接（MEL）旨在将多模态语境中的歧义指称项关联到多模态知识库中的对应实体。现有MEL方法大多基于表示学习或视觉-语言预训练机制来探索多模态间的互补效应，但这些方法存在两个局限：一方面忽略了同一模态内负样本的利用可能，另一方面缺乏双向跨模态交互的捕获机制。为此，我们提出多层次匹配的多模态实体链接网络（M3EL）。具体而言，M3EL包含三个核心模块：（1）多模态特征提取模块，通过多模态编码器获取模态专属表示，并引入模态内对比学习子模块，基于单模态差异获取更具判别性的嵌入；（2）模态内匹配网络模块，采用粗粒度全局-全局匹配和细粒度全局-局部匹配的双层匹配机制，实现局部与全局层次的模态内交互；（3）跨模态匹配网络模块，通过文本-视觉和视觉-文本的双向匹配策略实现跨模态双向交互。在WikiMEL、RichpediaMEL和WikiDiverse数据集上的大量实验表明，M3EL相较现有最优基线模型展现出卓越性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-level+Matching+Network+for+Multimodal+Entity+Linking)|0|
|[Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation](https://doi.org/10.1145/3690624.3709178)|Kairui Fu, Zheqi Lv, Shengyu Zhang, Fan Wu, Kun Kuang||In cloud-centric recommender system, regular data exchanges between user devices and cloud could potentially elevate bandwidth demands and privacy risks. On-device recommendation emerges as a viable solution by performing reranking locally to alleviate these concerns. Existing methods primarily focus on developing local adaptive parameters, while potentially neglecting the critical role of tailor-made model architecture. Insights from broader research domains suggest that varying data distributions might favor distinct architectures for better fitting. In addition, imposing a uniform model structure across heterogeneous devices may result in risking inefficacy on less capable devices or sub-optimal performance on those with sufficient capabilities. In response to these gaps, our paper introduces Forward-OFA, a novel approach for the dynamic construction of device-specific networks (both structure and parameters). Forward-OFA employs a structure controller to selectively determine whether each block needs to be assembled for a given device. However, during the training of the structure controller, these assembled heterogeneous structures are jointly optimized, where the co-adaption among blocks might encounter gradient conflicts. To mitigate this, Forward-OFA is designed to establish a structure-guided mapping of real-time behaviors to the parameters of assembled networks. Structure-related parameters and parallel components within the mapper prevent each part from receiving heterogeneous gradients from others, thus bypassing the gradient conflicts for coupled optimization. Besides, direct mapping enables Forward-OFA to achieve adaptation through only one forward pass, allowing for swift adaptation to changing interests and eliminating the requirement for on-device backpropagation. Experiments on real-world datasets demonstrate the effectiveness and efficiency of Forward-OFA.|在以云为中心的推荐系统中，用户设备与云端之间的常规数据交换可能导致带宽需求增加和隐私风险上升。设备端推荐通过本地重排序来缓解这些问题，成为一种可行的解决方案。现有方法主要聚焦于开发本地自适应参数，却可能忽视了定制化模型架构的关键作用。跨领域研究启示表明，不同的数据分布可能需要特定的架构以获得更优的拟合效果。此外，在异构设备上强制采用统一模型结构，可能导致性能不足设备上的低效运行或高配置设备上的次优表现。

针对这些不足，本文提出Forward-OFA——一种动态构建设备专属网络（包含结构和参数）的创新方法。该方法采用结构控制器来选择性判定每个模块是否需要为特定设备组装。然而在控制器训练过程中，这些组装的异构结构会进行联合优化，此时模块间的协同适应可能引发梯度冲突。为解决此问题，Forward-OFA设计了结构引导的实时行为到组装网络参数的映射机制。映射器中的结构相关参数和平行组件能防止各部分接收来自其他模块的异质梯度，从而规避耦合优化中的梯度冲突问题。

此外，直接映射机制使得Forward-OFA仅需单次前向传播即可实现适配，既能快速响应用户兴趣变化，又无需在设备端执行反向传播。真实场景数据集上的实验验证了该方法的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Forward+Once+for+All:+Structural+Parameterized+Adaptation+for+Efficient+Cloud-coordinated+On-device+Recommendation)|0|
|[Progressive Dependency Representation Learning for Stock Ranking in Uncertain Risk Contrasting](https://doi.org/10.1145/3690624.3709189)|Li Huang, Yanzhe Xie, Qiang Gao, Kunpeng Zhang, Guisong Liu, Xueqin Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Progressive+Dependency+Representation+Learning+for+Stock+Ranking+in+Uncertain+Risk+Contrasting)|0|
|[Path Complex Neural Networks for Sequential Process Activities Classification](https://doi.org/10.1145/3690624.3709193)|Liang Huang, Kelin Xia, ChuanShen Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path+Complex+Neural+Networks+for+Sequential+Process+Activities+Classification)|0|
|[PipeRAG: Fast Retrieval-Augmented Generation via Adaptive Pipeline Parallelism](https://doi.org/10.1145/3690624.3709194)|Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PipeRAG:+Fast+Retrieval-Augmented+Generation+via+Adaptive+Pipeline+Parallelism)|0|
|[MGS3: A Multi-Granularity Self-Supervised Code Search Framework](https://doi.org/10.1145/3690624.3709263)|Rui Li, Junfeng Kang, Qi Liu, Liyang He, Zheng Zhang, Yunhao Sha, Linbo Zhu, Zhenya Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGS3:+A+Multi-Granularity+Self-Supervised+Code+Search+Framework)|0|
|[APEX2: Adaptive and Extreme Summarization for Personalized Knowledge Graphs](https://doi.org/10.1145/3690624.3709213)|Zihao Li, Dongqi Fu, Mengting Ai, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=APEX2:+Adaptive+and+Extreme+Summarization+for+Personalized+Knowledge+Graphs)|0|
|[Spectral Subspace Clustering for Attributed Graphs](https://doi.org/10.1145/3690624.3709207)|Xiaoyang Lin, Renchi Yang, Haoran Zheng, Xiangyu Ke||Subspace clustering seeks to identify subspaces that segment a set of n data points into k (k<<n) groups, which has emerged as a powerful tool for analyzing data from various domains, especially images and videos. Recently, several studies have demonstrated the great potential of subspace clustering models for partitioning vertices in attributed graphs, referred to as SCAG. However, these works either demand significant computational overhead for constructing the nxn self-expressive matrix, or fail to incorporate graph topology and attribute data into the subspace clustering framework effectively, and thus, compromise result quality. Motivated by this, this paper presents two effective and efficient algorithms, S2CAG and M-S2CAG, for SCAG computation. Particularly, S2CAG obtains superb performance through three major contributions. First, we formulate a new objective function for SCAG with a refined representation model for vertices and two non-trivial constraints. On top of that, an efficient linear-time optimization solver is developed based on our theoretically grounded problem transformation and well-thought-out adaptive strategy. We then conduct an in-depth analysis to disclose the theoretical connection of S2CAG to conductance minimization, which further inspires the design of M-S2CAG that maximizes the modularity. Our extensive experiments, comparing S2CAG and M-S2CAG against 17 competitors over 8 benchmark datasets, exhibit that our solutions outperform all baselines in terms of clustering quality measured against the ground truth while delivering high efficiency|子空间聚类旨在识别能够将n个数据点划分为k（k<<n）个子集的低维子空间，这一技术已成为分析图像、视频等多领域数据的强有力工具。近年来，多项研究表明子空间聚类模型在属性图顶点划分（简称SCAG）方面展现出巨大潜力。然而现有方法要么需要构建n×n自表达矩阵而产生高昂计算开销，要么未能有效整合图拓扑结构与属性数据到聚类框架中，导致结果质量受损。为此，本文提出两种高效SCAG计算算法S2CAG与M-S2CAG。其中S2CAG通过三大创新实现卓越性能：首先，我们构建了包含顶点精炼表示模型与两项关键约束的新目标函数；在此基础上，通过理论论证的问题转化与精心设计的自适应策略，开发出线性时间复杂度的优化求解器；最后通过深入理论分析揭示S2CAG与电导最小化的内在关联，进而启发设计出最大化模块度的M-S2CAG。在8个基准数据集上与17种对比方法的实验表明，我们的方案在真实标签衡量的聚类质量上全面超越基线方法，同时保持高效计算。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spectral+Subspace+Clustering+for+Attributed+Graphs)|0|
|[Dynamic Deep Clustering of High-Dimensional Directional Data via Hyperspherical Embeddings with Bayesian Nonparametric Mixtures](https://doi.org/10.1145/3690624.3709230)|Zhiwen Luo, Wentao Fan, Manar Amayri, Nizar Bouguila||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Deep+Clustering+of+High-Dimensional+Directional+Data+via+Hyperspherical+Embeddings+with+Bayesian+Nonparametric+Mixtures)|0|
|[Collaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation](https://doi.org/10.1145/3690624.3709335)|Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu||Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices. In light of this, we designed the Device-Cloud LLM-SRM Collaborative Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec.|用于推荐的大语言模型（LLM4Rec）是一个极具前景的研究方向，其在该领域已展现出卓越性能。然而，其无法捕捉实时用户偏好的特性极大地限制了LLM4Rec的实际应用，原因在于：（1）大语言模型的训练和频繁推理成本高昂；（2）大语言模型难以获取实时数据（其庞大的参数量对设备端部署构成障碍）。值得庆幸的是，小型推荐模型（SRM）能有效弥补LLM4Rec的这些不足——它们仅需消耗极少资源即可实现频繁训练与推理，并能便捷地访问设备端实时数据。基于此，我们在端云协同环境下设计了设备-云端LLM-SRM协同推荐框架（LSC4Rec）。LSC4Rec旨在整合大语言模型与小型推荐模型的双重优势，同时结合云计算与边缘计算的特性，实现互补协同效应。我们通过设计三种策略来增强LSC4Rec的实用性：协同训练、协同推理和智能请求。在训练阶段，大语言模型生成候选列表以增强SRM在协同场景下的排序能力，并使SRM能够自适应更新以捕捉实时用户兴趣。在推理阶段，大语言模型和小型推荐模型分别部署于云端和设备端：大语言模型根据用户行为生成候选列表及初始排序结果，SRM基于候选列表进行重排序，最终结果融合二者的评分。设备端通过比较大语言模型与SRM排序结果的一致性来判断是否需要请求新的候选列表。我们全面而深入的实验分析验证了LSC4Rec中各项策略的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaboration+of+Large+Language+Models+and+Small+Recommendation+Models+for+Device-Cloud+Recommendation)|0|
|[AutoSTF: Decoupled Neural Architecture Search for Cost-Effective Automated Spatio-Temporal Forecasting](https://doi.org/10.1145/3690624.3709323)|Tengfei Lyu, Weijia Zhang, Jinliang Deng, Hao Liu||Spatio-temporal forecasting is a critical component of various smart city applications, such as transportation optimization, energy management, and socio-economic analysis. Recently, several automated spatio-temporal forecasting methods have been proposed to automatically search the optimal neural network architecture for capturing complex spatio-temporal dependencies. However, the existing automated approaches suffer from expensive neural architecture search overhead, which hinders their practical use and the further exploration of diverse spatio-temporal operators in a finer granularity. In this paper, we propose AutoSTF, a decoupled automatic neural architecture search framework for cost-effective automated spatio-temporal forecasting. From the efficiency perspective, we first decouple the mixed search space into temporal space and spatial space and respectively devise representation compression and parameter-sharing schemes to mitigate the parameter explosion. The decoupled spatio-temporal search not only expedites the model optimization process but also leaves new room for more effective spatio-temporal dependency modeling. From the effectiveness perspective, we propose a multi-patch transfer module to jointly capture multi-granularity temporal dependencies and extend the spatial search space to enable finer-grained layer-wise spatial dependency search. Extensive experiments on eight datasets demonstrate the superiority of AutoSTF in terms of both accuracy and efficiency. Specifically, our proposed method achieves up to 13.48x speed-up compared to state-of-the-art automatic spatio-temporal forecasting methods while maintaining the best forecasting accuracy.|时空预测是智能城市诸多应用（如交通优化、能源管理和社会经济分析）的核心组成部分。近年来，学界提出了若干自动化时空预测方法，旨在通过自动搜索最优神经网络架构来捕捉复杂的时空依赖关系。然而现有自动化方法受限于高昂的神经网络架构搜索开销，这不仅阻碍了其实际应用，也限制了在更细粒度上探索多样化时空运算子的可能性。本文提出AutoSTF框架——一种解耦式自动神经架构搜索方案，致力于实现高性价比的自动化时空预测。在效率层面，我们首先将混合搜索空间解耦为时间空间与空间空间，并分别设计表示压缩与参数共享机制来缓解参数爆炸问题。这种解耦的时空搜索策略不仅加速了模型优化过程，还为更有效的时空依赖建模开辟了新路径。在效能层面，我们提出多片段迁移模块以联合捕捉多粒度时间依赖，并扩展空间搜索空间以实现细粒度的逐层空间依赖搜索。在八个数据集上的大量实验表明，AutoSTF在精度与效率方面均具有显著优势。具体而言，相较于最先进的自动化时空预测方法，本方案在保持最佳预测精度的同时，最高可实现13.48倍的加速效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoSTF:+Decoupled+Neural+Architecture+Search+for+Cost-Effective+Automated+Spatio-Temporal+Forecasting)|0|
|[Enhancing Black-Box Adversarial Attacks on Discrete Sequential Data via Bilevel Bayesian Optimization in Hybrid Spaces](https://doi.org/10.1145/3690624.3709265)|Tianxing Man, Xingchen Li, Zhaogeng Liu, Haozhen Zhang, Bin Gu, Yi Chang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Black-Box+Adversarial+Attacks+on+Discrete+Sequential+Data+via+Bilevel+Bayesian+Optimization+in+Hybrid+Spaces)|0|
|[Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics](https://doi.org/10.1145/3690624.3709270)|Gyoung S. Na, Chanyoung Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Electron-Informed+Coarse-Graining+Molecular+Representation+Learning+for+Real-World+Molecular+Physics)|0|
|[Understanding the Effect of Loss Functions on the Generalization of Recommendations](https://doi.org/10.1145/3690624.3709169)|Yuanhao Pu, Defu Lian, Xiaolong Chen, Jin Chen, Ze Liu, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Effect+of+Loss+Functions+on+the+Generalization+of+Recommendations)|0|
|[Fast Causal Discovery by Approximate Kernel-based Generalized Score Functions with Linear Computational Complexity](https://doi.org/10.1145/3690624.3709338)|Yixin Ren, Haocheng Zhang, Yewei Xia, Hao Zhang, Jihong Guan, Shuigeng Zhou||Score-based causal discovery methods can effectively identify causal relationships by evaluating candidate graphs and selecting the one with the highest score. One popular class of scores is kernel-based generalized score functions, which can adapt to a wide range of scenarios and work well in practice because they circumvent assumptions about causal mechanisms and data distributions. Despite these advantages, kernel-based generalized score functions pose serious computational challenges in time and space, with a time complexity of 𝒪(n^3) and a memory complexity of 𝒪(n^2), where n is the sample size. In this paper, we propose an approximate kernel-based generalized score function with 𝒪(n) time and space complexities by using low-rank technique and designing a set of rules to handle the complex composite matrix operations required to calculate the score, as well as developing sampling algorithms for different data types to benefit the handling of diverse data types efficiently. Our extensive causal discovery experiments on both synthetic and real-world data demonstrate that compared to the state-of-the-art method, our method can not only significantly reduce computational costs, but also achieve comparable accuracy, especially for large datasets.|基于评分的因果发现方法通过评估候选图并选择得分最高的图，能够有效识别因果关系。其中一类广受欢迎的评分函数是基于核的广义评分函数，这类函数由于规避了对因果机制和数据分布的假设，能够适应多种场景并在实践中表现良好。尽管具有这些优势，基于核的广义评分函数在时间和空间上存在严重的计算挑战——其时间复杂度为𝒪(n^3)，内存复杂度为𝒪(n^2)，其中n为样本量。本文提出一种近似核广义评分函数，通过采用低秩技术和设计一套规则来处理计算评分所需的复杂复合矩阵运算，同时针对不同数据类型开发采样算法以高效处理多样化数据，实现了𝒪(n)级的时间和空间复杂度。我们在合成数据和真实数据上开展的广泛因果发现实验表明：相较于现有最优方法，本方法不仅能显著降低计算成本，还能保持相当的准确度，尤其在大规模数据集上表现突出。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Causal+Discovery+by+Approximate+Kernel-based+Generalized+Score+Functions+with+Linear+Computational+Complexity)|0|
|[R2MR: Review and Rewrite Modality for Recommendation](https://doi.org/10.1145/3690624.3709250)|Gu Tang, Jinghe Wang, Xiaoying Gan, Bin Lu, Ze Zhao, Luoyi Fu, Xinbing Wang, Chenghu Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R2MR:+Review+and+Rewrite+Modality+for+Recommendation)|0|
|[Scaling the Vocabulary of Non-autoregressive Models for Fast Generative Retrieval](https://doi.org/10.1145/3690624.3709330)|Ravisri Valluri, Akash Kumar Mohankumar, Kushal Dave, Amit Singh, Jian Jiao, Manik Varma, Gaurav Sinha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+the+Vocabulary+of+Non-autoregressive+Models+for+Fast+Generative+Retrieval)|0|
|[Mitigating Redundancy in Deep Recommender Systems: A Field Importance Distribution Perspective](https://doi.org/10.1145/3690624.3709275)|Xianquan Wang, Likang Wu, Zhi Li, Haitao Yuan, Shuanghong Shen, Huibo Xu, Yu Su, Chenyi Lei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Redundancy+in+Deep+Recommender+Systems:+A+Field+Importance+Distribution+Perspective)|0|
|[Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning](https://doi.org/10.1145/3690624.3709276)|Fuchao Yang, Jianhong Cheng, Hui Liu, Yongqiang Dong, Yuheng Jia, Junhui Hou||In partial label learning (PLL), every sample is associated with a candidate label set comprising the ground-truth label and several noisy labels. The conventional PLL assumes the noisy labels are randomly generated (instance-independent), while in practical scenarios, the noisy labels are always instance-dependent and are highly related to the sample features, leading to the instance-dependent partial label learning (IDPLL) problem. Instance-dependent noisy label is a double-edged sword. On one side, it may promote model training as the noisy labels can depict the sample to some extent. On the other side, it brings high label ambiguity as the noisy labels are quite undistinguishable from the ground-truth label. To leverage the nuances of IDPLL effectively, for the first time we create class-wise embeddings for each sample, which allow us to explore the relationship of instance-dependent noisy labels, i.e., the class-wise embeddings in the candidate label set should have high similarity, while the class-wise embeddings between the candidate label set and the non-candidate label set should have high dissimilarity. Moreover, to reduce the high label ambiguity, we introduce the concept of class prototypes containing global feature information to disambiguate the candidate label set. Extensive experimental comparisons with twelve methods on six benchmark data sets, including four fine-grained data sets, demonstrate the effectiveness of the proposed method. The code implementation is publicly available at https://github.com/Yangfc-ML/CEL.|在部分标签学习（PLL）中，每个样本都关联着一个包含真实标签和若干噪声标签的候选标签集。传统PLL假设噪声标签是随机生成的（与实例无关），而实际场景中噪声标签往往与实例相关且和样本特征高度关联，由此产生了实例依赖型部分标签学习（IDPLL）问题。实例依赖型噪声标签是一把双刃剑：一方面，由于噪声标签能在一定程度上描述样本特征，可能促进模型训练；另一方面，这些噪声标签与真实标签难以区分，会带来较高的标签模糊性。为有效利用IDPLL的细微特征，我们首次为每个样本创建类感知嵌入，通过这种嵌入能够探究实例依赖型噪声标签之间的关系——即候选标签集中的类感知嵌入应具有高度相似性，而候选标签集与非候选标签集之间的类感知嵌入应具有高度差异性。此外，为降低标签模糊性，我们引入包含全局特征信息的类原型概念来消除候选标签集的歧义。在六个基准数据集（包括四个细粒度数据集）上与十二种方法进行的广泛实验对比，验证了所提方法的有效性。代码实现已开源：https://github.com/Yangfc-ML/CEL。

（翻译说明：
1. 专业术语处理："ground-truth label"译为"真实标签"，"instance-dependent"译为"实例依赖型"，"class-wise embeddings"译为"类感知嵌入"以体现其区分不同类别的特性
2. 技术概念解释："double-edged sword"采用中文常用表达"双刃剑"，并添加冒号保持论述连贯性
3. 长句拆分：将原文复合长句拆分为多个短句，如将"the class-wise embeddings...dissimilarity"处理为两个并列分句
4. 被动语态转换："are highly related to"主动化为"和...高度关联"
5. 补充说明：在"六个基准数据集"后添加括号说明包含细粒度数据集，帮助读者理解实验设置
6. 学术规范：保留算法名称IDPLL和代码库链接的原始形式）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mixed+Blessing:+Class-Wise+Embedding+guided+Instance-Dependent+Partial+Label+Learning)|0|
|[GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning](https://doi.org/10.1145/3690624.3709186)|ZheRui Yang, Jindong Han, ChangDong Wang, Hao Liu||Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in handling a range of graph analytical tasks across various domains, such as e-commerce and social networks. Despite their versatility, GNNs face significant challenges in transferability, limiting their utility in real-world applications. Existing research in GNN transfer learning overlooks discrepancies in distribution among various graph datasets, facing challenges when transferring across different distributions. How to effectively adopt a well-trained GNN to new graphs with varying feature and structural distributions remains an under-explored problem. Taking inspiration from the success of Low-Rank Adaptation (LoRA) in adapting large language models to various domains, we propose GraphLoRA, an effective and parameter-efficient method for transferring well-trained GNNs to diverse graph domains. Specifically, we first propose a Structure-aware Maximum Mean Discrepancy (SMMD) to align divergent node feature distributions across source and target graphs. Moreover, we introduce low-rank adaptation by injecting a small trainable GNN alongside the pre-trained one, effectively bridging structural distribution gaps while mitigating the catastrophic forgetting. Additionally, a structure-aware regularization objective is proposed to enhance the adaptability of the pre-trained GNN to target graph with scarce supervision labels. Extensive experiments on six real-world datasets demonstrate the effectiveness of GraphLoRA against eleven baselines by tuning only 20 parameters, even across disparate graph domains. The code is available at https://anonymous.4open.science/r/GraphLoRA.|图神经网络（GNNs）在电子商务、社交网络等多个领域的图分析任务中展现出卓越的处理能力。尽管具有通用性，GNNs在可迁移性方面仍面临重大挑战，这限制了其在实际应用中的效用。现有GNN迁移学习研究忽略了不同图数据集间的分布差异，在跨分布迁移时遭遇困境。如何将训练良好的GNN有效适配到具有不同特征和结构分布的新图数据，仍是一个尚未充分探索的问题。受低秩适配（LoRA）成功将大语言模型适配到不同领域的启发，我们提出GraphLoRA——一种高效且参数节俭的方法，用于将训练良好的GNN迁移到多样化的图领域。具体而言，我们首先提出结构感知最大均值差异（SMMD）来对齐源图与目标图间不同的节点特征分布。此外，我们通过在预训练GNN旁注入小型可训练GNN实现低秩适配，有效弥合结构分布差距的同时缓解灾难性遗忘问题。还提出了结构感知正则化目标，以增强预训练GNN在监督标签稀缺的目标图上的适应能力。在六个真实数据集上的大量实验表明，GraphLoRA仅需调整20个参数即可显著优于11个基线方法，甚至能跨越迥异的图领域。代码已开源：https://anonymous.4open.science/r/GraphLoRA。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphLoRA:+Structure-Aware+Contrastive+Low-Rank+Adaptation+for+Cross-Graph+Transfer+Learning)|0|
|[Generalizable Recommender System During Temporal Popularity Distribution Shifts](https://doi.org/10.1145/3690624.3709299)|Hyunsik Yoo, Ruizhong Qiu, Charlie Xu, Fei Wang, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizable+Recommender+System+During+Temporal+Popularity+Distribution+Shifts)|0|
|[DimCL: Dimension-Aware Augmentation in Contrastive Learning for Recommendation](https://doi.org/10.1145/3690624.3709200)|Chi Zhang, Qilong Han, Qiaoyu Tan, Shengjie Wang, Xiangyu Zhao, Rui Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DimCL:+Dimension-Aware+Augmentation+in+Contrastive+Learning+for+Recommendation)|0|
|[Generalizing Personalized Federated Graph Augmentation via Min-max Adversarial Learning](https://doi.org/10.1145/3690624.3709311)|Liang Zhang, Tao Long, Yang Liu, Lei Zhang, Laizhong Cui, Qingjiang Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizing+Personalized+Federated+Graph+Augmentation+via+Min-max+Adversarial+Learning)|0|
|[PrivDPR: Synthetic Graph Publishing with Deep PageRank under Differential Privacy](https://doi.org/10.1145/3690624.3709334)|Sen Zhang, Haibo Hu, Qingqing Ye, Jianliang Xu||The objective of privacy-preserving synthetic graph publishing is to safeguard individuals' privacy while retaining the utility of original data. Most existing methods focus on graph neural networks under differential privacy (DP), and yet two fundamental problems in generating synthetic graphs remain open. First, the current research often encounters high sensitivity due to the intricate relationships between nodes in a graph. Second, DP is usually achieved through advanced composition mechanisms that tend to converge prematurely when working with a small privacy budget. In this paper, inspired by the simplicity, effectiveness, and ease of analysis of PageRank, we design PrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In particular, we achieve DP by adding noise to the gradient for a specific weight during learning. Utilizing weight normalization as a bridge, we theoretically reveal that increasing the number of layers in PrivDPR can effectively mitigate the high sensitivity and privacy budget splitting. Through formal privacy analysis, we prove that the synthetic graph generated by PrivDPR satisfies node-level DP. Experiments on real-world graph datasets show that PrivDPR preserves high data utility across multiple graph structural properties.|隐私保护合成图发布的目的是在保留原始数据效用的同时保护个体隐私。当前大多数方法聚焦于差分隐私（DP）框架下的图神经网络，但生成合成图仍面临两个核心难题：其一，由于图中节点间复杂的关联关系，现有研究常面临高敏感性问题；其二，差分隐私通常通过高级组合机制实现，但在小隐私预算下易出现早熟收敛现象。本文受PageRank算法简洁性、高效性和易分析性的启发，提出一种新型隐私保护深度PageRank模型PrivDPR用于图合成。具体而言，我们通过在特定权重梯度学习过程中添加噪声来实现差分隐私。借助权重归一化作为理论桥梁，我们严格证明增加PrivDPR网络层数能有效缓解高敏感性和隐私预算分割问题。通过形式化隐私分析，我们证实PrivDPR生成的合成图满足节点级差分隐私。真实图数据集的实验表明，该模型在多种图结构属性上均能保持优异的数据效用。

（注：根据学术论文摘要的翻译规范，本译文进行了以下专业处理：
1. 技术术语统一："differential privacy"统一译为"差分隐私"，"sensitivity"译为"敏感性"，"privacy budget"译为"隐私预算"
2. 算法名称保留："PageRank"作为经典算法名保持原名，"PrivDPR"作为新提出方法名保留不译
3. 被动语态转换：将英文被动式"DP is usually achieved"等处理为中文主动式"通过...实现"
4. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句
5. 概念显化：如"advanced composition mechanisms"译为"高级组合机制"而非字面直译）|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PrivDPR:+Synthetic+Graph+Publishing+with+Deep+PageRank+under+Differential+Privacy)|0|
|[IDentity with Locality: An Ideal Hash for Gene Sequence Search](https://doi.org/10.1145/3690624.3709233)|Tianyi Zhang, Gaurav Gupta, Aditya Desai, Anshumali Shrivastava||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDentity+with+Locality:+An+Ideal+Hash+for+Gene+Sequence+Search)|0|
|[Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation](https://doi.org/10.1145/3690624.3709214)|Bowen Zheng, Junjie Zhang, Hongyu Lu, Yu Chen, Ming Chen, Wayne Xin Zhao, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Graph+Contrastive+Learning+with+Reliable+and+Informative+Augmentation+for+Recommendation)|0|
|[ECGrecover: A Deep Learning Approach for Electrocardiogram Signal Completion](https://doi.org/10.1145/3690624.3709405)|Alex Lence, Federica Granese, Ahmad Fall, Blaise Hanczar, JoeElie Salem, JeanDaniel Zucker, Edi Prifti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ECGrecover:+A+Deep+Learning+Approach+for+Electrocardiogram+Signal+Completion)|0|
|[LinkSAGE: Optimizing Job Matching Using Graph Neural Networks](https://doi.org/10.1145/3690624.3709396)|Ping Liu, Haichao Wei, Xiaochen Hou, Jianqiang Shen, Shihai He, Qianqi Shen, Zhujun Chen, Fedor Borisyuk, Daniel Hewlett, Liang Wu, Srikant Veeraraghavan, Alex Tsun, Chengming Jiang, Wenjing Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinkSAGE:+Optimizing+Job+Matching+Using+Graph+Neural+Networks)|0|
|[Roadside Multi-LiDAR Data Fusion for Enhanced Traffic Safety](https://doi.org/10.1145/3690624.3709410)|Md. Parvez Mollah, Biplob Debnath, Murugan Sankaradas, Srimat Chakradhar, Abdullah Mueen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Roadside+Multi-LiDAR+Data+Fusion+for+Enhanced+Traffic+Safety)|0|
|[AntAkso: Claims Management System for Health Insurance in Alipay](https://doi.org/10.1145/3690624.3709398)|Qitao Shi, Jun Zhou, YaLin Zhang, Longfei Li, Chaoyi Ma, Yifan Wu, Xiaobo Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AntAkso:+Claims+Management+System+for+Health+Insurance+in+Alipay)|0|
|[HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou](https://doi.org/10.1145/3690624.3709416)|Xu Wang, Jiangxia Cao, Zhiyi Fu, Kun Gai, Guorui Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HoME:+Hierarchy+of+Multi-Gate+Experts+for+Multi-Task+Learning+at+Kuaishou)|0|
|[NoteLLM-2: Multimodal Large Representation Models for Recommendation](https://doi.org/10.1145/3690624.3709440)|Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NoteLLM-2:+Multimodal+Large+Representation+Models+for+Recommendation)|0|
|[Safe Online Bid Optimization with Return on Investment and Budget Constraints](https://doi.org/10.1145/3690624.3709288)|Matteo Castiglioni, Alessandro Nuara, Giulia Romano, Giorgio Spadaro, Francesco Trovò, Nicola Gatti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Safe+Online+Bid+Optimization+with+Return+on+Investment+and+Budget+Constraints)|0|
|[Mixing Time Matters: Accelerating Effective Resistance Estimation via Bidirectional Method](https://doi.org/10.1145/3690624.3709298)|Guanyu Cui, Hanzhi Wang, Zhewei Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mixing+Time+Matters:+Accelerating+Effective+Resistance+Estimation+via+Bidirectional+Method)|0|
|[D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams](https://doi.org/10.1145/3690624.3709192)|Shingo Higashiguchi, Yasuko Matsubara, Koki Kawabata, Taichi Murayama, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=D-Tracker:+Modeling+Interest+Diffusion+in+Social+Activity+Tensor+Data+Streams)|0|
|[On the Hyperparameter Loss Landscapes of Machine Learning Models: An Exploratory Study](https://doi.org/10.1145/3690624.3709229)|Mingyu Huang, Ke Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Hyperparameter+Loss+Landscapes+of+Machine+Learning+Models:+An+Exploratory+Study)|0|
|[Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing](https://doi.org/10.1145/3690624.3709293)|Zexu Sun, Qiyu Han, Minqin Zhu, Hao Gong, Dugang Liu, Chen Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Uplift+Modeling+with+Large-Scale+Contexts+for+Real-time+Marketing)|0|
|[Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://doi.org/10.1145/3690624.3709280)|Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Connecting+Domains+and+Contrasting+Samples:+A+Ladder+for+Domain+Generalization)|0|
|[An Adaptable Budget Planner for Enhancing Budget-Constrained Auto-Bidding in Online Advertising](https://doi.org/10.1145/3690624.3709414)|Zhijian Duan, Yusen Huo, Tianyu Wang, Zhilin Zhang, Yeshu Li, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Adaptable+Budget+Planner+for+Enhancing+Budget-Constrained+Auto-Bidding+in+Online+Advertising)|0|
|[Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://doi.org/10.1145/3690624.3709419)|Qing Feng, Samuel Daulton, Benjamin Letham, Maximilian Balandat, Eytan Bakshy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experimenting,+Fast+and+Slow:+Bayesian+Optimization+of+Long-term+Outcomes+with+Online+Experiments)|0|
|[SWaT: Statistical Modeling of Video Watch Time through User Behavior Analysis](https://doi.org/10.1145/3690624.3709415)|Shentao Yang, Haichuan Yang, Linna Du, Adithya Ganesh, Bo Peng, Boying Liu, Serena Li, Ji Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SWaT:+Statistical+Modeling+of+Video+Watch+Time+through+User+Behavior+Analysis)|0|
|[Modeling Time-evolving Causality over Data Streams](https://doi.org/10.1145/3690624.3709283)|Naoki Chihara, Yasuko Matsubara, Ren Fujiwara, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Time-evolving+Causality+over+Data+Streams)|0|
|[UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed Graphs](https://doi.org/10.1145/3690624.3709277)|Yufei He, Yuan Sui, Xiaoxin He, Bryan Hooi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniGraph:+Learning+a+Unified+Cross-Domain+Foundation+Model+for+Text-Attributed+Graphs)|0|
|[Adaptive Domain Inference Attack with Concept Hierarchy](https://doi.org/10.1145/3690624.3709332)|Yuechun Gu, Jiajie He, Keke Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Domain+Inference+Attack+with+Concept+Hierarchy)|0|
|[InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models](https://doi.org/10.1145/3690624.3709165)|Min Hou, Yueying Wu, Chang Xu, YuHao Huang, Chenxi Bai, Le Wu, Jiang Bian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InvDiff:+Invariant+Guidance+for+Bias+Mitigation+in+Diffusion+Models)|0|
|[DIPS: Optimal Dynamic Index for Poisson πps Sampling](https://doi.org/10.1145/3690624.3709162)|Jinchao Huang, Sibo Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIPS:+Optimal+Dynamic+Index+for+Poisson+πps+Sampling)|0|
|[Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning Problem](https://doi.org/10.1145/3690624.3709268)|Oleg Kachan, Andrey V. Savchenko, Gleb Gusev||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simplicial+SMOTE:+Oversampling+Solution+to+the+Imbalanced+Learning+Problem)|0|
|[Learnable Prompt as Pseudo-Imputation: Rethinking the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction](https://doi.org/10.1145/3690624.3709166)|Weibin Liao, Yinghao Zhu, Zhongji Zhang, Yuhang Wang, Zixiang Wang, Xu Chu, Yasha Wang, Liantao Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learnable+Prompt+as+Pseudo-Imputation:+Rethinking+the+Necessity+of+Traditional+EHR+Data+Imputation+in+Downstream+Clinical+Prediction)|0|
|[Language Representation Favored Zero-Shot Cross-Domain Cognitive Diagnosis](https://doi.org/10.1145/3690624.3709281)|Shuo Liu, Zihan Zhou, Yuanhao Liu, Jing Zhang, Hong Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Language+Representation+Favored+Zero-Shot+Cross-Domain+Cognitive+Diagnosis)|0|
|[Fine-tuning Multimodal Large Language Models for Product Bundling](https://doi.org/10.1145/3690624.3709255)|Xiaohao Liu, Jie Wu, Zhulin Tao, Yunshan Ma, Yinwei Wei, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-tuning+Multimodal+Large+Language+Models+for+Product+Bundling)|0|
|[Adapting to Generalized Online Label Shift by Invariant Representation Learning](https://doi.org/10.1145/3690624.3709182)|YuYang Qian, YiHan Wang, ZhenYu Zhang, Yuan Jiang, ZhiHua Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+to+Generalized+Online+Label+Shift+by+Invariant+Representation+Learning)|0|
|[Exploring Heterogeneity and Uncertainty for Graph-based Cognitive Diagnosis Models in Intelligent Education](https://doi.org/10.1145/3690624.3709264)|Pengyang Shao, Yonghui Yang, Chen Gao, Lei Chen, Kun Zhang, Chenyi Zhuang, Le Wu, Yong Li, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Heterogeneity+and+Uncertainty+for+Graph-based+Cognitive+Diagnosis+Models+in+Intelligent+Education)|0|
|[HeavyLocker: Lock Heavy Hitters in Distributed Data Streams](https://doi.org/10.1145/3690624.3709167)|Qilong Shi, Xirui Li, Hanyue Zheng, Tong Yang, Yangyang Wang, Mingwei Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HeavyLocker:+Lock+Heavy+Hitters+in+Distributed+Data+Streams)|0|
|[MLDGG: Meta-Learning for Domain Generalization on Graphs](https://doi.org/10.1145/3690624.3709188)|Qin Tian, Chen Zhao, Minglai Shao, Wenjun Wang, Yujie Lin, Dong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MLDGG:+Meta-Learning+for+Domain+Generalization+on+Graphs)|0|
|[Dynamic Causal Structure Discovery and Causal Effect Estimation](https://doi.org/10.1145/3690624.3709345)|Jianian Wang, Rui Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Causal+Structure+Discovery+and+Causal+Effect+Estimation)|0|
|[Progressive Generalization Risk Reduction for Data-Efficient Causal Effect Estimation](https://doi.org/10.1145/3690624.3709305)|Hechuan Wen, Tong Chen, Guanhua Ye, Li Kheng Chai, Shazia Sadiq, Hongzhi Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Progressive+Generalization+Risk+Reduction+for+Data-Efficient+Causal+Effect+Estimation)|0|
|[Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention](https://doi.org/10.1145/3690624.3709226)|Wen Xiong, Jinduo Liu, Junzhong Ji, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Brain+Effective+Connectivity+Estimation+via+Fourier+Spatiotemporal+Attention)|0|
|[CausalMob: Causal Human Mobility Prediction with LLMs-derived Human Intentions toward Public Events](https://doi.org/10.1145/3690624.3709231)|Xiaojie Yang, Hangli Ge, Jiawei Wang, Zipei Fan, Renhe Jiang, Ryosuke Shibasaki, Noboru Koshizuka||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CausalMob:+Causal+Human+Mobility+Prediction+with+LLMs-derived+Human+Intentions+toward+Public+Events)|0|
|[Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning](https://doi.org/10.1145/3690624.3709195)|Gongzhu Yin, Hongli Zhang, Yuchen Yang, Yi Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Link+Prediction+on+N-ary+Relational+Facts+via+Semantic+Hypergraph+Reasoning)|0|
|[Annotation-guided Protein Design with Multi-Level Domain Alignment](https://doi.org/10.1145/3690624.3709199)|Chaohao Yuan, Songyou Li, Geyan Ye, Yikun Zhang, LongKai Huang, Wenbing Huang, Wei Liu, Jianhua Yao, Yu Rong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Annotation-guided+Protein+Design+with+Multi-Level+Domain+Alignment)|0|
|[Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge Graph](https://doi.org/10.1145/3690624.3709187)|Yutong Zhang, Lixing Chen, Shenghong Li, Nan Cao, Yang Shi, Jiaxin Ding, Zhe Qu, Pan Zhou, Yang Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Way+to+Specialist:+Closing+Loop+Between+Specialized+LLM+and+Evolving+Domain+Knowledge+Graph)|0|
|[Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting](https://doi.org/10.1145/3690624.3709210)|Lifan Zhao, Yanyan Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Proactive+Model+Adaptation+Against+Concept+Drift+for+Online+Time+Series+Forecasting)|0|
|[Variational Graph Autoencoder for Heterogeneous Information Networks with Missing and Inaccurate Attributes](https://doi.org/10.1145/3690624.3709251)|Yige Zhao, Jianxiang Yu, Yao Cheng, Chengcheng Yu, Yiding Liu, Xiang Li, Shuaiqiang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Variational+Graph+Autoencoder+for+Heterogeneous+Information+Networks+with+Missing+and+Inaccurate+Attributes)|0|
|[A Two-Stage Pretraining-Finetuning Framework for Treatment Effect Estimation with Unmeasured Confounding](https://doi.org/10.1145/3690624.3709161)|Chuan Zhou, Yaxuan Li, Chunyuan Zheng, Haiteng Zhang, Min Zhang, Haoxuan Li, Mingming Gong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Two-Stage+Pretraining-Finetuning+Framework+for+Treatment+Effect+Estimation+with+Unmeasured+Confounding)|0|
|[HRSTORY: Historical News Review Based Online Story Discovery](https://doi.org/10.1145/3690624.3709198)|Renjie Zhou, Haoran Ye, Jian Wan, Yong Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HRSTORY:+Historical+News+Review+Based+Online+Story+Discovery)|0|
|[Contextual Generative Auction with Permutation-level Externalities for Online Advertising](https://doi.org/10.1145/3690624.3709313)|Ruitao Zhu, Yangsu Liu, Dagui Chen, Zhenjia Ma, Chufeng Shi, Zhenzhe Zheng, Jie Zhang, Jian Xu, Bo Zheng, Fan Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Generative+Auction+with+Permutation-level+Externalities+for+Online+Advertising)|0|
|[ForTune: Running Offline Scenarios to Estimate Impact on Business Metrics](https://doi.org/10.1145/3690624.3709431)|Georges Dupret, Konstantin Sozinov, Carmen Barcena Gonzalez, Ziggy Zacks, Amber Yuan, Ben Carterette, Manuel Mai, Andrey Gatash, Gwo Liang Lien, Shubham Bansal, Roberto SanchisOjeda, Mounia Lalmas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ForTune:+Running+Offline+Scenarios+to+Estimate+Impact+on+Business+Metrics)|0|
|[TEMPER: Capturing Consistent and Fluctuating TEMPoral User Behaviour for EtheReum Phishing Scam Detection](https://doi.org/10.1145/3690624.3709399)|Medhasree Ghosh, Chirag Dinesh Jain, Raju Halder, Joydeep Chandra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TEMPER:+Capturing+Consistent+and+Fluctuating+TEMPoral+User+Behaviour+for+EtheReum+Phishing+Scam+Detection)|0|
|[TGDataset: Collecting and Exploring the Largest Telegram Channels Dataset](https://doi.org/10.1145/3690624.3709397)|Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TGDataset:+Collecting+and+Exploring+the+Largest+Telegram+Channels+Dataset)|0|
|[Session-Level Dynamic Ad Load Optimization using Offline Robust Reinforcement Learning](https://doi.org/10.1145/3690624.3709437)|Tao Liu, Qi Xu, Wei Shi, Zhigang Hua, Shuang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Session-Level+Dynamic+Ad+Load+Optimization+using+Offline+Robust+Reinforcement+Learning)|0|
|[SSE: Multimodal Semantic Data Selection and Enrichment for Industrial-scale Data Assimilation](https://doi.org/10.1145/3690624.3709417)|Maying Shen, Nadine Chang, Sifei Liu, José M. Álvarez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSE:+Multimodal+Semantic+Data+Selection+and+Enrichment+for+Industrial-scale+Data+Assimilation)|0|
|[A Framework for Leveraging Partially-Labeled Data for Product Attribute-Value Identification](https://doi.org/10.1145/3690624.3709427)|D. Subhalingam, Keshav Kolluru, Mausam, Saurabh Singal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Framework+for+Leveraging+Partially-Labeled+Data+for+Product+Attribute-Value+Identification)|0|
|[Instruction Semantics Enhanced Dual-Flow Graph Model for GPU Error Resilience Prediction](https://doi.org/10.1145/3690624.3709424)|Pengfei Yu, Jingjing Gu, Dazhong Shen, Xin Dong, Yang Liu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instruction+Semantics+Enhanced+Dual-Flow+Graph+Model+for+GPU+Error+Resilience+Prediction)|0|
|[Prices Do Matter: Modeling Price Competitiveness for Online Hotel Industry](https://doi.org/10.1145/3690624.3709420)|Ruitao Zhu, Wendong Xiao, Yao Yu, Yangsu Liu, Zhenzhe Zheng, Shuqi Zhang, Dong Li, Fan Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prices+Do+Matter:+Modeling+Price+Competitiveness+for+Online+Hotel+Industry)|0|
|[Hypergraph Motif Representation Learning](https://doi.org/10.1145/3690624.3709274)|Alessia Antelmi, Gennaro Cordasco, Daniele De Vinco, Valerio Di Pasquale, Mirko Polato, Carmine Spagnuolo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypergraph+Motif+Representation+Learning)|0|
|[ResMoE: Space-efficient Compression of Mixture of Experts LLMs via Residual Restoration](https://doi.org/10.1145/3690624.3709196)|Mengting Ai, Tianxin Wei, Yifan Chen, Zhichen Zeng, Ritchie Zhao, Girish Varatkar, Bita Darvish Rouhani, Xianfeng Tang, Hanghang Tong, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ResMoE:+Space-efficient+Compression+of+Mixture+of+Experts+LLMs+via+Residual+Restoration)|0|
|[Fast and Effective GNN Training through Sequences of Random Path Graphs](https://doi.org/10.1145/3690624.3709301)|Francesco Bonchi, Claudio Gentile, Francesco Paolo Nerini, André Panisson, Fabio Vitale||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Effective+GNN+Training+through+Sequences+of+Random+Path+Graphs)|0|
|[Correlation-Aware Graph Convolutional Networks for Multi-Label Node Classification](https://doi.org/10.1145/3690624.3709197)|Yuanchen Bei, Weizhi Chen, Hao Chen, Sheng Zhou, Carl Ji Yang, Jiapei Fan, Longtao Huang, Jiajun Bu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Correlation-Aware+Graph+Convolutional+Networks+for+Multi-Label+Node+Classification)|0|
|[NodeImport: Imbalanced Node Classification with Node Importance Assessment](https://doi.org/10.1145/3690624.3709215)|Nan Chen, Zemin Liu, Bryan Hooi, Bingsheng He, Jun Hu, Jia Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NodeImport:+Imbalanced+Node+Classification+with+Node+Importance+Assessment)|0|
|[How to use Graph Data in the Wild to Help Graph Anomaly Detection?](https://doi.org/10.1145/3690624.3709320)|Yuxuan Cao, Jiarong Xu, Chen Zhao, Jiaan Wang, Carl Ji Yang, Chunping Wang, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+use+Graph+Data+in+the+Wild+to+Help+Graph+Anomaly+Detection?)|0|
|[Probabilistic Hypergraph Recurrent Neural Networks for Time-series Forecasting](https://doi.org/10.1145/3690624.3709202)|Hongjie Chen, Ryan A. Rossi, Sungchul Kim, Kanak Mahadik, Hoda Eldardiry||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Probabilistic+Hypergraph+Recurrent+Neural+Networks+for+Time-series+Forecasting)|0|
|[Locally Balancing Signed Graphs](https://doi.org/10.1145/3690624.3709342)|Weizhe Chen, Wentao Li, Min Gao, Dong Wen, Maolin Cai, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Locally+Balancing+Signed+Graphs)|0|
|[CSPI-MT: Calibrated Safe Policy Improvement with Multiple Testing for Threshold Policies](https://doi.org/10.1145/3690624.3709176)|Brian Cho, AnaRoxana Pop, Kyra Gan, Sam CorbettDavies, Israel Nir, Ariel Evnine, Nathan Kallus||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CSPI-MT:+Calibrated+Safe+Policy+Improvement+with+Multiple+Testing+for+Threshold+Policies)|0|
|[Revisiting Synthetic Human Trajectories: Imitative Generation and Benchmarks Beyond Datasaurus](https://doi.org/10.1145/3690624.3709180)|Bangchao Deng, Xin Jing, Tianyue Yang, Bingqing Qu, Dingqi Yang, Philippe CudréMauroux||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Synthetic+Human+Trajectories:+Imitative+Generation+and+Benchmarks+Beyond+Datasaurus)|0|
|[Stabilizing Modality Gap & Lowering Gradient Norms Improve Zero-Shot Adversarial Robustness of VLMs](https://doi.org/10.1145/3690624.3709296)|Junhao Dong, Piotr Koniusz, Xinghua Qu, YewSoon Ong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stabilizing+Modality+Gap+&+Lowering+Gradient+Norms+Improve+Zero-Shot+Adversarial+Robustness+of+VLMs)|0|
|[Conditional Generative Modeling for High-dimensional Marked Temporal Point Processes](https://doi.org/10.1145/3690624.3709258)|Zheng Dong, Zekai Fan, Shixiang Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conditional+Generative+Modeling+for+High-dimensional+Marked+Temporal+Point+Processes)|0|
|[The k-Trine Cohesive Subgraph and Its Efficient Algorithms](https://doi.org/10.1145/3690624.3709174)|Jinyu Duan, Haicheng Guo, Fan Zhang, Kai Wang, Zhengping Qian, Zhihong Tian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+k-Trine+Cohesive+Subgraph+and+Its+Efficient+Algorithms)|0|
|[Bi-Dynamic Graph ODE for Opinion Evolution](https://doi.org/10.1145/3690624.3709297)|Bowen Duan, Henggang Deng, Jinghua Piao, Huandong Wang, Yue Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-Dynamic+Graph+ODE+for+Opinion+Evolution)|0|
|[IN-Flow: Instance Normalization Flow for Non-stationary Time Series Forecasting](https://doi.org/10.1145/3690624.3709260)|Wei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Kun Yi, Qi Zhang, Jiang Bian, Yanjie Fu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IN-Flow:+Instance+Normalization+Flow+for+Non-stationary+Time+Series+Forecasting)|0|
|[Dynamic Localisation of Spatial-Temporal Graph Neural Network](https://doi.org/10.1145/3690624.3709331)|Wenying Duan, Shujun Guo, Zimu Zhou, Wei Huang, Hong Rao, Xiaoxi He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Localisation+of+Spatial-Temporal+Graph+Neural+Network)|0|
|[FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://doi.org/10.1145/3690624.3709253)|Kaiyuan Gao, Qizhi Pei, Gongbo Zhang, Jinhua Zhu, Kun He, Lijun Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FABind+:+Enhancing+Molecular+Docking+through+Improved+Pocket+Prediction+and+Pose+Generation)|0|
|[Wedjat: Detecting Sophisticated Evasion Attacks via Real-time Causal Analysis](https://doi.org/10.1145/3690624.3709218)|Li Gao, Chuanpu Fu, Xinhao Deng, Ke Xu, Qi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wedjat:+Detecting+Sophisticated+Evasion+Attacks+via+Real-time+Causal+Analysis)|0|
|[Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor](https://doi.org/10.1145/3690624.3709172)|Weibo Gao, Qi Liu, Rui Li, Yuze Zhao, Hao Wang, Linan Yue, Fangzhou Yao, Zheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoising+Programming+Knowledge+Tracing+with+a+Code+Graph-based+Tuning+Adaptor)|0|
|[Benchmarking Fraud Detectors on Private Graph Data](https://doi.org/10.1145/3690624.3709170)|Alexander Goldberg, Giulia Fanti, Nihar B. Shah, Steven Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+Fraud+Detectors+on+Private+Graph+Data)|0|
|[Detecting Interpretable Subgroup Drifts](https://doi.org/10.1145/3690624.3709259)|Flavio Giobergia, Eliana Pastor, Luca de Alfaro, Elena Baralis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Interpretable+Subgroup+Drifts)|0|
|[Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation](https://doi.org/10.1145/3690624.3709239)|Peiliang Gong, Mohamed Ragab, Min Wu, Zhenghua Chen, Yongyi Su, Xiaoli Li, Daoqiang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmented+Contrastive+Clustering+with+Uncertainty-Aware+Prototyping+for+Time+Series+Test+Time+Adaptation)|0|
|[Revisiting Cognition in Neural Cognitive Diagnosis](https://doi.org/10.1145/3690624.3709319)|Hengnian Gu, Guoqian Luo, Xiaoxiao Dong, Shulin Li, Dongdai Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Cognition+in+Neural+Cognitive+Diagnosis)|0|
|[TransPlace: Transferable Circuit Global Placement via Graph Neural Network](https://doi.org/10.1145/3690624.3709185)|Yunbo Hou, Haoran Ye, Shuwen Yang, Yingxue Zhang, Siyuan Xu, Guojie Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransPlace:+Transferable+Circuit+Global+Placement+via+Graph+Neural+Network)|0|
|[AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation](https://doi.org/10.1145/3690624.3709321)|Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, JianGuang Lou, Qingwei Lin, Ping Luo, Saravan Rajmohan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AgentGen:+Enhancing+Planning+Abilities+for+Large+Language+Model+based+Agent+via+Environment+and+Task+Generation)|0|
|[Fair Set Cover](https://doi.org/10.1145/3690624.3709184)|Mohsen Dehghankar, Rahul Raychaudhury, Stavros Sintos, Abolfazl Asudeh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Set+Cover)|0|
|[Partial Pre-Post Code Tree: A Memory-Efficient Tree Structure for Conjunctive Rule Mining](https://doi.org/10.1145/3690624.3709303)|Van Quoc Phuong Huynh, Florian Beck, Johannes Fürnkranz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Partial+Pre-Post+Code+Tree:+A+Memory-Efficient+Tree+Structure+for+Conjunctive+Rule+Mining)|0|
|[Seeing the Unseen: Learning Basis Confounder Representations for Robust Traffic Prediction](https://doi.org/10.1145/3690624.3709201)|Jiahao Ji, Wentao Zhang, Jingyuan Wang, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Seeing+the+Unseen:+Learning+Basis+Confounder+Representations+for+Robust+Traffic+Prediction)|0|
|[On Measuring Unnoticeability of Graph Adversarial Attacks: Observations, New Measure, and Applications](https://doi.org/10.1145/3690624.3709163)|Hyeonsoo Jo, Hyunjin Hwang, Fanchen Bu, Soo Yong Lee, Chanyoung Park, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Measuring+Unnoticeability+of+Graph+Adversarial+Attacks:+Observations,+New+Measure,+and+Applications)|0|
|[LH-Mix: Local Hierarchy Correlation Guided Mixup over Hierarchical Prompt Tuning](https://doi.org/10.1145/3690624.3709326)|Fanshuang Kong, Richong Zhang, Ziqiao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LH-Mix:+Local+Hierarchy+Correlation+Guided+Mixup+over+Hierarchical+Prompt+Tuning)|0|
|[CAPER: Enhancing Career Trajectory Prediction using Temporal Knowledge Graph and Ternary Relationship](https://doi.org/10.1145/3690624.3709329)|YeonChang Lee, JaeHyun Lee, Michiharu Yamashita, Dongwon Lee, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAPER:+Enhancing+Career+Trajectory+Prediction+using+Temporal+Knowledge+Graph+and+Ternary+Relationship)|0|
|[Reasoning-Enhanced Object-Centric Learning for Videos](https://doi.org/10.1145/3690624.3709168)|Jian Li, Pu Ren, Yang Liu, Hao Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reasoning-Enhanced+Object-Centric+Learning+for+Videos)|0|
|[TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection](https://doi.org/10.1145/3690624.3709266)|Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSINR:+Capturing+Temporal+Continuity+via+Implicit+Neural+Representations+for+Time+Series+Anomaly+Detection)|0|
|[Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning](https://doi.org/10.1145/3690624.3709181)|Qi Li, Zhiguang Cao, Yining Ma, Yaoxin Wu, YueJiao Gong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diversity+Optimization+for+Travelling+Salesman+Problem+via+Deep+Reinforcement+Learning)|0|
|[Harnessing Scale and Physics: A Multi-Graph Neural Operator Framework for PDEs on Arbitrary Geometries](https://doi.org/10.1145/3690624.3709173)|Zhihao Li, Haoze Song, Di Xiao, Zhilu Lai, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harnessing+Scale+and+Physics:+A+Multi-Graph+Neural+Operator+Framework+for+PDEs+on+Arbitrary+Geometries)|0|
|[DistPred: A Distribution-Free Probabilistic Inference Method for Regression and Forecasting](https://doi.org/10.1145/3690624.3709286)|Daojun Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DistPred:+A+Distribution-Free+Probabilistic+Inference+Method+for+Regression+and+Forecasting)|0|
|[Stealing Training Graphs from Graph Neural Networks](https://doi.org/10.1145/3690624.3709289)|Minhua Lin, Enyan Dai, Junjie Xu, Jinyuan Jia, Xiang Zhang, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stealing+Training+Graphs+from+Graph+Neural+Networks)|0|
|[SEPTQ: A Simple and Effective Post-Training Quantization Paradigm for Large Language Models](https://doi.org/10.1145/3690624.3709287)|Han Liu, Haotian Gao, Xiaotong Zhang, Changya Li, Feng Zhang, Wei Wang, Fenglong Ma, Hong Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEPTQ:+A+Simple+and+Effective+Post-Training+Quantization+Paradigm+for+Large+Language+Models)|0|
|[SCode: A Spherical Code Metric Learning Approach to Continuously Monitoring Predictive Events in Networked Data](https://doi.org/10.1145/3690624.3709246)|Qu Liu, Emil Zulawnik, Tingjian Ge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCode:+A+Spherical+Code+Metric+Learning+Approach+to+Continuously+Monitoring+Predictive+Events+in+Networked+Data)|0|
|[3DGraphX: Explaining 3D Molecular Graph Models via Incorporating Chemical Priors](https://doi.org/10.1145/3690624.3709302)|Xufeng Liu, Dongsheng Luo, Wenhan Gao, Yi Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3DGraphX:+Explaining+3D+Molecular+Graph+Models+via+Incorporating+Chemical+Priors)|0|
|[Enhancing Unsupervised Graph Few-shot Learning via Set Functions and Optimal Transport](https://doi.org/10.1145/3690624.3709208)|Yonghao Liu, Fausto Giunchiglia, Ximing Li, Lan Huang, Xiaoyue Feng, Renchu Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Unsupervised+Graph+Few-shot+Learning+via+Set+Functions+and+Optimal+Transport)|0|
|[MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions](https://doi.org/10.1145/3690624.3709171)|Yuxuan Liu, Hongda Sun, Wei Liu, Jian Luan, Bo Du, Rui Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MobileSteward:+Integrating+Multiple+App-Oriented+Agents+with+Self-Evolution+to+Automate+Cross-App+Instructions)|0|
|[A Universal Model for Human Mobility Prediction](https://doi.org/10.1145/3690624.3709236)|Qingyue Long, Yuan Yuan, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Universal+Model+for+Human+Mobility+Prediction)|0|
|[Future Matters for Present: Towards Effective Physical Simulation over Meshes](https://doi.org/10.1145/3690624.3709340)|Xiao Luo, Junyu Luo, Huiyu Jiang, Hang Zhou, Zhiping Xiao, Wei Ju, Carl Ji Yang, Ming Zhang, Yizhou Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Future+Matters+for+Present:+Towards+Effective+Physical+Simulation+over+Meshes)|0|
|[Fairness without Demographics through Learning Graph of Gradients](https://doi.org/10.1145/3690624.3709160)|Yingtao Luo, Zhixun Li, Qiang Liu, Jun Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+without+Demographics+through+Learning+Graph+of+Gradients)|0|
|[Towards Controllable Hybrid Fairness in Graph Neural Networks](https://doi.org/10.1145/3690624.3709224)|Zihan Luo, Hong Huang, Jianxun Lian, Xiran Song, Hai Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Controllable+Hybrid+Fairness+in+Graph+Neural+Networks)|0|
|[Task Diversity in Bayesian Federated Learning: Simultaneous Processing of Classification and Regression](https://doi.org/10.1145/3690624.3709341)|Junliang Lyu, Yixuan Zhang, Xiaoling Lu, Feng Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task+Diversity+in+Bayesian+Federated+Learning:+Simultaneous+Processing+of+Classification+and+Regression)|0|
|[On the Support Vector Effect in DNNs: Rethinking Data Selection and Attribution](https://doi.org/10.1145/3690624.3709295)|Syed Hasan Amin Mahmood, Ming Yin, Rajiv Khanna||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Support+Vector+Effect+in+DNNs:+Rethinking+Data+Selection+and+Attribution)|0|
|[Conservation-informed Graph Learning for Spatiotemporal Dynamics Prediction](https://doi.org/10.1145/3690624.3709244)|Yuan Mi, Pu Ren, Hongteng Xu, Hongsheng Liu, Zidong Wang, Yike Guo, JiRong Wen, Hao Sun, Yang Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conservation-informed+Graph+Learning+for+Spatiotemporal+Dynamics+Prediction)|0|
|[Data Glitches Discovery using Influence-based Model Explanations](https://doi.org/10.1145/3690624.3709285)|Nikolaos Myrtakis, Ioannis Tsamardinos, Vassilis Christophides||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Glitches+Discovery+using+Influence-based+Model+Explanations)|0|
|[Weight-Constrained Simple Path Enumeration in Weighted Graph](https://doi.org/10.1145/3690624.3709310)|Dian Ouyang, Dong Wen, Jianye Yang, Wentao Li, Xuemin Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weight-Constrained+Simple+Path+Enumeration+in+Weighted+Graph)|0|
|[Distributional Prototype Learning for Out-of-distribution Detection](https://doi.org/10.1145/3690624.3709294)|Bo Peng, Jie Lu, Yonggang Zhang, Guangquan Zhang, Zhen Fang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributional+Prototype+Learning+for+Out-of-distribution+Detection)|0|
|[On the Necessity of World Knowledge for Mitigating Missing Labels in Extreme Classification](https://doi.org/10.1145/3690624.3709290)|Jatin Prakash, Anirudh Buvanesh, Bishal Santra, Deepak Saini, Sachin Yadav, Jian Jiao, Yashoteja Prabhu, Amit Sharma, Manik Varma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Necessity+of+World+Knowledge+for+Mitigating+Missing+Labels+in+Extreme+Classification)|0|
|[Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural Networks](https://doi.org/10.1145/3690624.3709316)|QingGuo Qi, Hongyang Chen, Minhao Cheng, Han Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Input+Snapshots+Fusion+for+Scalable+Discrete-Time+Dynamic+Graph+Neural+Networks)|0|
|[Tackling the Length Barrier: Dynamic Context Browsing for Knowledge-Intensive Task](https://doi.org/10.1145/3690624.3709240)|Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, Zhicheng Dou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+the+Length+Barrier:+Dynamic+Context+Browsing+for+Knowledge-Intensive+Task)|0|
|[Quantum Time-index Models with Reservoir for Time Series Forecasting](https://doi.org/10.1145/3690624.3709228)|Wenbo Qiao, Jiaming Zhao, Peng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantum+Time-index+Models+with+Reservoir+for+Time+Series+Forecasting)|0|
|[DUET: Dual Clustering Enhanced Multivariate Time Series Forecasting](https://doi.org/10.1145/3690624.3709325)|Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, Bin Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DUET:+Dual+Clustering+Enhanced+Multivariate+Time+Series+Forecasting)|0|
|[ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://doi.org/10.1145/3690624.3709254)|Hyunwoo Seo, Chiehyeon Lim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ST-MTM:+Masked+Time+Series+Modeling+with+Seasonal-Trend+Decomposition+for+Time+Series+Forecasting)|0|
|[Abductive Learning for Neuro-Symbolic Grounded Imitation](https://doi.org/10.1145/3690624.3709344)|JieJing Shao, HaoRan Hao, XiaoWen Yang, YuFeng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Abductive+Learning+for+Neuro-Symbolic+Grounded+Imitation)|0|
|[Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://doi.org/10.1145/3690624.3709237)|Tatsuhiro Shimizu, Kazuki Kawamura, Takanori Muroi, Yusuke Narita, Kei Tateno, Takuma Udagawa, Yuta Saito||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+and+Learning+for+the+Future+under+Non-Stationarity)|0|
|[Covering Cracks in Content Moderation: Delexicalized Distant Supervision for Illicit Drug Jargon Detection](https://doi.org/10.1145/3690624.3709183)|Minkyoo Song, Eugene Jang, Jaehan Kim, Seungwon Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Covering+Cracks+in+Content+Moderation:+Delexicalized+Distant+Supervision+for+Illicit+Drug+Jargon+Detection)|0|
|[Counterfactual Explanations with Probabilistic Guarantees on their Robustness to Model Change](https://doi.org/10.1145/3690624.3709300)|Ignacy Stepka, Jerzy Stefanowski, Mateusz Lango||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Explanations+with+Probabilistic+Guarantees+on+their+Robustness+to+Model+Change)|0|
|[Handling Feature Heterogeneity with Learnable Graph Patches](https://doi.org/10.1145/3690624.3709242)|Yifei Sun, Yang Yang, Xiao Feng, Zijun Wang, Haoyang Zhong, Chunping Wang, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Handling+Feature+Heterogeneity+with+Learnable+Graph+Patches)|0|
|[A Unified Invariant Learning Framework for Graph Classification](https://doi.org/10.1145/3690624.3709203)|Yongduo Sui, Jie Sun, Shuyao Wang, Zemin Liu, Qing Cui, Longfei Li, Xiang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Invariant+Learning+Framework+for+Graph+Classification)|0|
|[CLEAR: Addressing Representation Contamination in Multimodal Healthcare Analytics](https://doi.org/10.1145/3690624.3709164)|Ge Su, Kaiping Zheng, Tiancheng Zhao, Jianwei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLEAR:+Addressing+Representation+Contamination+in+Multimodal+Healthcare+Analytics)|0|
|[Spatially Compact Dense Block Mining in Spatial Tensors](https://doi.org/10.1145/3690624.3709221)|Weike Tang, Dingming Wu, Tsz Nam Chan, Kezhong Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatially+Compact+Dense+Block+Mining+in+Spatial+Tensors)|0|
|[GROOT: Effective Design of Biological Sequences with Limited Experimental Data](https://doi.org/10.1145/3690624.3709291)|Thanh V. T. Tran, Nhat Khang Ngo, Viet Anh Nguyen, Truong Son Hy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GROOT:+Effective+Design+of+Biological+Sequences+with+Limited+Experimental+Data)|0|
|[How Well Calibrated are Extreme Multi-label Classifiers? An Empirical Analysis](https://doi.org/10.1145/3690624.3709333)|Nasib Ullah, Erik Schultheis, Jinbin Zhang, Rohit Babbar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Well+Calibrated+are+Extreme+Multi-label+Classifiers?+An+Empirical+Analysis)|0|
|[Interpretable Prediction and Feature Selection for Survival Analysis](https://doi.org/10.1145/3690624.3709245)|Mike Van Ness, Madeleine Udell||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Prediction+and+Feature+Selection+for+Survival+Analysis)|0|
|[Asymmetrical Reciprocity-based Federated Learning for Resolving Disparities in Medical Diagnosis](https://doi.org/10.1145/3690624.3709235)|Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asymmetrical+Reciprocity-based+Federated+Learning+for+Resolving+Disparities+in+Medical+Diagnosis)|0|
|[CoopRide: Cooperate All Grids in City-Scale Ride-Hailing Dispatching with Multi-Agent Reinforcement Learning](https://doi.org/10.1145/3690624.3709205)|Jingwei Wang, Qianyue Hao, Wenzhen Huang, Xiaochen Fan, Qin Zhang, Zhentao Tang, Bin Wang, Jianye Hao, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoopRide:+Cooperate+All+Grids+in+City-Scale+Ride-Hailing+Dispatching+with+Multi-Agent+Reinforcement+Learning)|0|
|[Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation](https://doi.org/10.1145/3690624.3709337)|Qi (Cheems) Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, Xiangyang Ji||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Fast+Adaptation+from+Adversarially+Explicit+Task+Distribution+Generation)|0|
|[An Efficient Diffusion-based Non-Autoregressive Solver for Traveling Salesman Problem](https://doi.org/10.1145/3690624.3709343)|Mingzhao Wang, You Zhou, Zhiguang Cao, Yubin Xiao, Xuan Wu, Wei Pang, Yuan Jiang, Hui Yang, Peng Zhao, Yuanshu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Diffusion-based+Non-Autoregressive+Solver+for+Traveling+Salesman+Problem)|0|
|[GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction](https://doi.org/10.1145/3690624.3709238)|Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, Ke Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphTool-Instruction:+Revolutionizing+Graph+Reasoning+in+LLMs+through+Decomposed+Subtask+Instruction)|0|
|[Graph Triple Attention Networks: A Decoupled Perspective](https://doi.org/10.1145/3690624.3709223)|Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, Chuntao Hong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Triple+Attention+Networks:+A+Decoupled+Perspective)|0|
|[Runtime-Aware Pipeline for Vertical Federated Learning with Bounded Model Staleness](https://doi.org/10.1145/3690624.3709243)|Xiong Wang, Yi Zhang, Yuxin Chen, Yuqing Li, Chuanhu Ma, Bo Li, Hai Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Runtime-Aware+Pipeline+for+Vertical+Federated+Learning+with+Bounded+Model+Staleness)|0|
|[Noise-Resilient Point-wise Anomaly Detection in Time Series Using Weak Segment Labels](https://doi.org/10.1145/3690624.3709257)|Yaxuan Wang, Hao Cheng, Jing Xiong, Qingsong Wen, Han Jia, Ruixuan Song, Liyuan Zhang, Zhaowei Zhu, Yang Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Noise-Resilient+Point-wise+Anomaly+Detection+in+Time+Series+Using+Weak+Segment+Labels)|0|
|[FLMarket: Enabling Privacy-preserved Pre-training Data Pricing for Federated Learning](https://doi.org/10.1145/3690624.3709346)|Zhenyu Wen, Wanglei Feng, Di Wu, Haozhen Hu, Chang Xu, Bin Qian, Zhen Hong, Cong Wang, Shouling Ji||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLMarket:+Enabling+Privacy-preserved+Pre-training+Data+Pricing+for+Federated+Learning)|0|
|[Feature Selection for Network Intrusion Detection](https://doi.org/10.1145/3690624.3709339)|Charles Westphal, Stephen Hailes, Mirco Musolesi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Feature+Selection+for+Network+Intrusion+Detection)|0|
|[Classifying Treatment Responders: Bounds and Algorithms](https://doi.org/10.1145/3690624.3709191)|Anpeng Wu, Haoxuan Li, Chunyuan Zheng, Kun Kuang, Kun Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Classifying+Treatment+Responders:+Bounds+and+Algorithms)|0|
|[Breaking the Memory Wall for Heterogeneous Federated Learning via Progressive Training](https://doi.org/10.1145/3690624.3709284)|Yebo Wu, Li Li, ChengZhong Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Memory+Wall+for+Heterogeneous+Federated+Learning+via+Progressive+Training)|0|
|[ProgDiffusion: Progressively Self-encoding Diffusion Models](https://doi.org/10.1145/3690624.3709222)|Zhangkai Wu, Xuhui Fan, Longbing Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProgDiffusion:+Progressively+Self-encoding+Diffusion+Models)|0|
|[ProST: Prompt Future Snapshot on Dynamic Graphs for Spatio-Temporal Prediction](https://doi.org/10.1145/3690624.3709273)|Kaiwen Xia, Li Lin, Shuai Wang, Qi Zhang, Shuai Wang, Tian He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProST:+Prompt+Future+Snapshot+on+Dynamic+Graphs+for+Spatio-Temporal+Prediction)|0|
|[ScalaGBM: Memory Efficient GBDT Training for High-Dimensional Data on GPU](https://doi.org/10.1145/3690624.3709271)|Borui Xu, Zeyi Wen, Yao Chen, Weiguo Liu, WengFai Wong, Bingsheng He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ScalaGBM:+Memory+Efficient+GBDT+Training+for+High-Dimensional+Data+on+GPU)|0|
|[Incremental Label Distribution Learning](https://doi.org/10.1145/3690624.3709318)|Chao Xu, Xijia Tang, Hong Tao, Chenping Hou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incremental+Label+Distribution+Learning)|0|
|[Neural Network Pruning for Invariance Learning](https://doi.org/10.1145/3690624.3709262)|Derek Xu, Yuanzhou Chen, Yizhou Sun, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Network+Pruning+for+Invariance+Learning)|0|
|[Succinct Interaction-Aware Explanations](https://doi.org/10.1145/3690624.3709175)|Sascha Xu, Joscha Cüppers, Jilles Vreeken||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Succinct+Interaction-Aware+Explanations)|0|
|[MM-Path: Multi-modal, Multi-granularity Path Representation Learning](https://doi.org/10.1145/3690624.3709209)|Ronghui Xu, Hanyin Cheng, Chenjuan Guo, Hongfan Gao, Jilin Hu, Bin Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MM-Path:+Multi-modal,+Multi-granularity+Path+Representation+Learning)|0|
|[Fast and Accurate Temporal Hypergraph Representation for Hyperedge Prediction](https://doi.org/10.1145/3690624.3709327)|Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Xiwei Xu, Xuemin Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Temporal+Hypergraph+Representation+for+Hyperedge+Prediction)|0|
|[Learning Universal Multi-level Market Irrationality Factors to Improve Stock Return Forecasting](https://doi.org/10.1145/3690624.3709328)|Chen Yang, Jingyuan Wang, Xiaohan Jiang, Junjie Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Universal+Multi-level+Market+Irrationality+Factors+to+Improve+Stock+Return+Forecasting)|0|
|[Causal Discovery from Shifted Multiple Environments](https://doi.org/10.1145/3690624.3709247)|Dezhi Yang, Guoxian Yu, Jun Wang, Jinglin Zhang, Carlotta Domeniconi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Discovery+from+Shifted+Multiple+Environments)|0|
|[PraFFL: A Preference-Aware Scheme in Fair Federated Learning](https://doi.org/10.1145/3690624.3709217)|Rongguang Ye, WeiBin Kou, Ming Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PraFFL:+A+Preference-Aware+Scheme+in+Fair+Federated+Learning)|0|
|[Boosting Explainability through Selective Rationalization in Pre-trained Language Models](https://doi.org/10.1145/3690624.3709212)|Libing Yuan, Shuaibo Hu, Kui Yu, Le Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boosting+Explainability+through+Selective+Rationalization+in+Pre-trained+Language+Models)|0|
|[A Structure-aware Invariant Learning Framework for Node-level Graph OOD Generalization](https://doi.org/10.1145/3690624.3709227)|Ruiwen Yuan, Yongqiang Tang, Wensheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Structure-aware+Invariant+Learning+Framework+for+Node-level+Graph+OOD+Generalization)|0|
|[Semi-supervised Multi-view Clustering with Active Constraints](https://doi.org/10.1145/3690624.3709204)|Chao Zhang, Deng Xu, Chunlin Chen, Huaxiong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-supervised+Multi-view+Clustering+with+Active+Constraints)|0|
|[LLM-Eraser: Optimizing Large Language Model Unlearning through Selective Pruning](https://doi.org/10.1145/3690624.3709312)|Shengming Zhang, Le Zhang, Jingbo Zhou, Zhi Zheng, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-Eraser:+Optimizing+Large+Language+Model+Unlearning+through+Selective+Pruning)|0|
|[Stable Representation Learning on Graphs from Multiple Environments with Structure Distribution Shift](https://doi.org/10.1145/3690624.3709269)|Tong Zhao, Daixin Wang, Zhiqiang Zhang, Yulin Kang, Jun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stable+Representation+Learning+on+Graphs+from+Multiple+Environments+with+Structure+Distribution+Shift)|0|
|[Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory](https://doi.org/10.1145/3690624.3709324)|Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo, Tianshu Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Oversmoothing+in+Diffusion-Based+GNNs+From+the+Perspective+of+Operator+Semigroup+Theory)|0|
|[Graph Learning with Distributional Edge Layouts](https://doi.org/10.1145/3690624.3709206)|Xinjian Zhao, Chaolong Ying, Yaoyao Xu, Tianshu Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Learning+with+Distributional+Edge+Layouts)|0|
|[Towards Context-Aware Traffic Classification via Time-Wavelet Fusion Network](https://doi.org/10.1145/3690624.3709315)|Ziming Zhao, Zhuoxue Song, Xiaofei Xie, Zhaoxuan Li, Jiongchi Yu, Fan Zhang, Tingting Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Context-Aware+Traffic+Classification+via+Time-Wavelet+Fusion+Network)|0|
|[Graph Contrastive Learning with Progressive Augmentations](https://doi.org/10.1145/3690624.3709307)|Yuhai Zhao, Yejiang Wang, Zhengkui Wang, Wen Shan, Miaomiao Huang, Xingwei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Progressive+Augmentations)|0|
|[Grid and Road Expressions Are Complementary for Trajectory Representation Learning](https://doi.org/10.1145/3690624.3709272)|Silin Zhou, Shuo Shang, Lisi Chen, Peng Han, Christian S. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grid+and+Road+Expressions+Are+Complementary+for+Trajectory+Representation+Learning)|0|
|[BTFL: A Bayesian-based Test-Time Generalization Method for Internal and External Data Distributions in Federated learning](https://doi.org/10.1145/3690624.3709309)|Yu Zhou, Bingyan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BTFL:+A+Bayesian-based+Test-Time+Generalization+Method+for+Internal+and+External+Data+Distributions+in+Federated+learning)|0|
|[RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning](https://doi.org/10.1145/3690624.3709252)|Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RELIEF:+Reinforcement+Learning+Empowered+Graph+Feature+Prompt+Tuning)|0|
|[Powerformer: A Section-adaptive Transformer for Power Flow Adjustment](https://doi.org/10.1145/3690624.3709433)|Kaixuan Chen, Wei Luo, Shunyu Liu, Yaoquan Wei, Yihe Zhou, Yunpeng Qing, Quan Zhang, Yong Wang, Jie Song, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Powerformer:+A+Section-adaptive+Transformer+for+Power+Flow+Adjustment)|0|
|[Efficient Multi-Expert Tabular Language Model for Banking](https://doi.org/10.1145/3690624.3709400)|Yue Guo, Wentao Zhang, Xiaojun Zhang, Vincent W. Zheng, Yi Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Multi-Expert+Tabular+Language+Model+for+Banking)|0|
|[Learning Adaptive Reserve Price in Display Advertising](https://doi.org/10.1145/3690624.3709439)|Kun Hu, Shumin Zhang, Lixia Wu, Yongjun Dai, Minfang Lu, Yuting Qiang, Minglong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Adaptive+Reserve+Price+in+Display+Advertising)|0|
|[Synthetic Survey Data Generation and Evaluation](https://doi.org/10.1145/3690624.3709421)|Yanru Jiang, Siyu Liang, Junwon Choi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthetic+Survey+Data+Generation+and+Evaluation)|0|
|[Large Vison-Language Foundation Model in Baidu AIGC Image Advertising](https://doi.org/10.1145/3690624.3709401)|Zhipeng Jin, Wen Tao, Yafei Li, Yi Yang, Cong Han, Shuanglong Li, Lin Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Vison-Language+Foundation+Model+in+Baidu+AIGC+Image+Advertising)|0|
|[YaART: Yet Another ART Rendering Technology](https://doi.org/10.1145/3690624.3709404)|Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Dmitry Baranchuk, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=YaART:+Yet+Another+ART+Rendering+Technology)|0|
|[A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data](https://doi.org/10.1145/3690624.3709418)|Seungyeon Lee, Ruoqi Liu, Feixiong Cheng, Ping Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Deep+Subgrouping+Framework+for+Precision+Drug+Repurposing+via+Emulating+Clinical+Trials+on+Real-world+Patient+Data)|0|
|[Contrastive Learning for Inventory Add Prediction at Fliggy](https://doi.org/10.1145/3690624.3709384)|Manwei Li, Detao Lv, Yao Yu, Zihao Jiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+Inventory+Add+Prediction+at+Fliggy)|0|
|[FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities](https://doi.org/10.1145/3690624.3709393)|Mingyuan Li, Jiahao Wang, Bo Du, Jun Shen, Qiang Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FuzzyLight:+A+Robust+Two-Stage+Fuzzy+Approach+for+Traffic+Signal+Control+Works+in+Real+Cities)|0|
|[Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective](https://doi.org/10.1145/3690624.3709392)|Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Synthetic+Image+Detection+Towards+Generalization:+An+Image+Transformation+Perspective)|0|
|[Automatic Radiotherapy Treatment Planning with Deep Functional Reinforcement Learning](https://doi.org/10.1145/3690624.3709430)|Bin Liu, Yu Liu, Zhiqian Li, Jianghong Xiao, Guosheng Yin, Huazhen Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Radiotherapy+Treatment+Planning+with+Deep+Functional+Reinforcement+Learning)|0|
|[Using Instruction-Tuned LMs for Scalable Use Case-Based Shopping - Where Customers Meet Their Needs](https://doi.org/10.1145/3690624.3709411)|Rajdeep Mukherjee, Sonali Singh, Sachin Farfade||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Instruction-Tuned+LMs+for+Scalable+Use+Case-Based+Shopping+-+Where+Customers+Meet+Their+Needs)|0|
|[Understanding Team Collapse via Probabilistic Graphical Models](https://doi.org/10.1145/3690624.3709386)|Iasonas Nikolaou, Konstantinos Pelechrinis, Evimaria Terzi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Team+Collapse+via+Probabilistic+Graphical+Models)|0|
|[Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence](https://doi.org/10.1145/3690624.3709436)|Zhan Qu, Shuzhou Yuan, Michael Färber, Marius Brennfleck, Niklas Wartha, Anton Stephan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+LiDAR+3D+Point+Cloud+Segmentation+and+Clustering+for+Detecting+Airplane-Generated+Wind+Turbulence)|0|
|[Unifying Adversarial Multi-Deconfounded Learning Paradigm for Fake News Detection](https://doi.org/10.1145/3690624.3709406)|Zixun Sun, Mingye Xu, Guanming Liang, Qi Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Adversarial+Multi-Deconfounded+Learning+Paradigm+for+Fake+News+Detection)|0|
|[Struct-X: Enhancing the Reasoning Capabilities of Large Language Models in Structured Data Scenarios](https://doi.org/10.1145/3690624.3709381)|Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Leijun Cheng, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Struct-X:+Enhancing+the+Reasoning+Capabilities+of+Large+Language+Models+in+Structured+Data+Scenarios)|0|
|[Cross-Species Insights: Transforming Drug Efficacy from Rats to Humans Using Tissue-Specific Generative Models](https://doi.org/10.1145/3690624.3709389)|Sally Turutov, Kira Radinsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Species+Insights:+Transforming+Drug+Efficacy+from+Rats+to+Humans+Using+Tissue-Specific+Generative+Models)|0|
|[DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting](https://doi.org/10.1145/3690624.3709391)|Hao Wu, Haomin Wen, Guibin Zhang, Yutong Xia, Yuxuan Liang, Yu Zheng, Qingsong Wen, Kun Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DynST:+Dynamic+Sparse+Training+for+Resource-Constrained+Spatio-Temporal+Forecasting)|0|
|[LDMapNet-U: An End-to-End System for City-Scale Lane-Level Map Updating](https://doi.org/10.1145/3690624.3709383)|Deguo Xia, Weiming Zhang, Xiyan Liu, Wei Zhang, Chenting Gong, Xiao Tan, Jizhou Huang, Mengmeng Yang, Diange Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LDMapNet-U:+An+End-to-End+System+for+City-Scale+Lane-Level+Map+Updating)|0|
|[Effective AOI-level Parcel Volume Prediction: When Lookahead Parcels Matter](https://doi.org/10.1145/3690624.3709441)|Yinfeng Xiang, Jiangyi Fang, Chao Li, Haitao Yuan, Yiwei Song, Jiming Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+AOI-level+Parcel+Volume+Prediction:+When+Lookahead+Parcels+Matter)|0|
|[Disclosing Actual Controller based on Equity Knowledge Graph Learning](https://doi.org/10.1145/3690624.3709432)|Qingying Xu, Liang Hong, Mingxuan Shen, Baokun Yi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disclosing+Actual+Controller+based+on+Equity+Knowledge+Graph+Learning)|0|
|[AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics Data](https://doi.org/10.1145/3690624.3709425)|Qinchen Yang, Zhiqing Hong, Dongjiang Cao, Haotian Wang, Zejun Xie, Tian He, Yunhuai Liu, Yu Yang, Desheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AddrLLM:+Address+Rewriting+via+Large+Language+Model+on+Nationwide+Logistics+Data)|0|
|[SepsisCalc: Integrating Clinical Calculators into Early Sepsis Prediction via Dynamic Temporal Graph Construction](https://doi.org/10.1145/3690624.3709402)|Changchang Yin, Shihan Fu, Bingsheng Yao, ThaiHoang Pham, Weidan Cao, Dakuo Wang, Jeffrey M. Caterino, Ping Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SepsisCalc:+Integrating+Clinical+Calculators+into+Early+Sepsis+Prediction+via+Dynamic+Temporal+Graph+Construction)|0|
|[BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation](https://doi.org/10.1145/3690624.3709385)|Haiyang Yu, Tian Xie, Jiaping Gui, Pengyang Wang, Pengzhou Cheng, Ping Yi, Yue Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BackdoorMBTI:+A+Backdoor+Learning+Multimodal+Benchmark+Tool+Kit+for+Backdoor+Defense+Evaluation)|0|
|[MentorPDM: Learning Data-Driven Curriculum for Multi-Modal Predictive Maintenance](https://doi.org/10.1145/3690624.3709388)|Shuaicheng Zhang, Tuo Wang, Stephen Adams, Sanmitra Bhattacharya, Sunil Reddy Tiyyagura, Edward Bowen, Balaji Veeramani, Dawei Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MentorPDM:+Learning+Data-Driven+Curriculum+for+Multi-Modal+Predictive+Maintenance)|0|
|[Multi-period Learning for Financial Time Series Forecasting](https://doi.org/10.1145/3690624.3709422)|Xu Zhang, Zhengang Huang, Yunzhi Wu, Xun Lu, Erpeng Qi, Yunkai Chen, Zhongya Xue, Qitong Wang, Peng Wang, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-period+Learning+for+Financial+Time+Series+Forecasting)|0|
|[Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination](https://doi.org/10.1145/3690624.3709423)|Daniel ZhangLi, Zheyuan Zhang, Jifan Yu, Joy Lim Jia Yin, Shangqing Tu, Linlu Gong, Haohua Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Awaking+the+Slides:+A+Tuning-free+and+Knowledge-regulated+AI+Tutoring+System+via+Language+Model+Coordination)|0|
